<small>Claude 3.7 Sonnet Thinking</small>
# 06. Monitoring and Performance Optimization

## Key Terms

- **Observability**: The practice of measuring and tracking a system's internal states through its outputs
- **LangSmith**: Platform for debugging, testing, evaluating, and monitoring LLM applications
- **Langtail**: Log-based observability tool for LLM applications providing insights on usage patterns
- **Trace**: Record of a chain or agent execution, including inputs, outputs, and intermediate steps
- **Evaluation Metrics**: Quantitative measures of model performance (e.g., latency, token usage, relevance)
- **Prompt Optimization**: Refining input prompts to improve model outputs
- **Human Feedback Loop**: Process of collecting human evaluations to improve system performance
- **Regression Testing**: Testing to ensure new developments don't negatively impact existing functionality
- **Hallucination Detection**: Identifying when a model generates factually incorrect information
- **Dataset**: Collection of examples used for evaluation and testing

## The Importance of Continuous Performance Monitoring

Monitoring LLM-based chatbots is fundamentally different from monitoring traditional software systems. While conventional applications have predictable, deterministic behavior, LLM outputs can vary significantly based on subtle input changes, context, and even random sampling during generation. This inherent variability creates unique challenges for ensuring consistent, high-quality responses.

Continuous monitoring provides several critical benefits:

1. **Quality Assurance**: Identify regression in response quality, detect hallucinations, and ensure adherence to guidelines
2. **Cost Management**: Track token usage and optimize prompt length and complexity to reduce operational costs
3. **Performance Optimization**: Measure and reduce latency to improve user experience
4. **User Experience Insights**: Understand user interaction patterns and areas where the chatbot struggles
5. **Compliance Verification**: Ensure responses meet regulatory and ethical requirements

Without robust monitoring, LLM-based systems can silently degrade as usage patterns change, new edge cases emerge, or underlying models are updated. A comprehensive monitoring strategy is essential for maintaining and improving performance over time.

## Tools for Analyzing and Fine-Tuning Outputs

Modern LLM applications require specialized tooling designed for the unique challenges of language model monitoring. Let's explore a comprehensive monitoring solution using both open-source and commercial tools:

### Setting Up a Monitoring Framework

```python
import os
import json
import logging
from typing import Dict, List, Any, Optional, Union, Callable
from datetime import datetime
from enum import Enum
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_core.runnables import RunnablePassthrough
import langsmith
from langsmith import Client
from langsmith.evaluation import RunEvalConfig
from pydantic import BaseModel, Field
import pandas as pd
import numpy as np

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("ChatbotMonitoring")

# Initialize LangSmith client
langsmith.api_key = os.getenv("LANGSMITH_API_KEY")
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_PROJECT"] = "chatbot_monitoring"

client = Client()

class QueryCategory(str, Enum):
    """Categories for classifying user queries."""
    PRODUCT = "product"
    SUPPORT = "support"
    BILLING = "billing"
    GENERAL = "general"
    OFF_TOPIC = "off_topic"

class ResponseMetrics(BaseModel):
    """Metrics for evaluating chatbot responses."""
    relevance: float = Field(..., description="How relevant the response is to the query (0-1)")
    correctness: float = Field(..., description="Factual correctness of the response (0-1)")
    helpfulness: float = Field(..., description="How helpful the response is in addressing the user's need (0-1)")
    clarity: float = Field(..., description="How clear and easy to understand the response is (0-1)")
    toxicity: float = Field(..., description="Presence of harmful or offensive content (0-1, lower is better)")
    
    def average_score(self) -> float:
        """Calculate overall average score."""
        # We invert toxicity since lower is better
        return (self.relevance + self.correctness + self.helpfulness + self.clarity + (1 - self.toxicity)) / 5

class MonitoringConfig:
    """Configuration for chatbot monitoring."""
    
    def __init__(self, 
                log_to_langsmith: bool = True,
                log_to_local: bool = True,
                local_log_path: str = "logs/chatbot_interactions.jsonl",
                evaluation_percentage: float = 0.1):
        """
        Initialize monitoring configuration.
        
        Args:
            log_to_langsmith: Whether to log to LangSmith
            log_to_local: Whether to log to local file
            local_log_path: Path to local log file
            evaluation_percentage: Percentage of interactions to evaluate (0-1)
        """
        self.log_to_langsmith = log_to_langsmith
        self.log_to_local = log_to_local
        self.local_log_path = local_log_path
        self.evaluation_percentage = evaluation_percentage
        
        # Create log directory if it doesn't exist
        if self.log_to_local:
            os.makedirs(os.path.dirname(self.local_log_path), exist_ok=True)

class ChatbotMonitor:
    """Monitor for tracking and evaluating chatbot performance."""
    
    def __init__(self, config: MonitoringConfig = None):
        """
        Initialize the chatbot monitor.
        
        Args:
            config: Monitoring configuration
        """
        self.config = config or MonitoringConfig()
        self.langsmith_client = Client()
        
        # Initialize evaluator models
        self.evaluator_model = ChatOpenAI(
            model="gpt-4o",
            temperature=0.0,
            api_key=os.getenv("OPENAI_API_KEY")
        )
        
        # Set up evaluators
        self._setup_evaluators()
    
    def _setup_evaluators(self):
        """Set up evaluator chains for response quality."""
        # Query classification evaluator
        self.query_classifier = ChatPromptTemplate.from_messages([
            ("system", """You are a query classification system. 
            Categorize the user query into one of these categories:
            - product: Questions about product features, capabilities, or comparisons
            - support: Technical issues, how-to questions, or troubleshooting
            - billing: Questions about pricing, subscriptions, or payment issues
            - general: General information requests not specific to products
            - off_topic: Queries unrelated to the company or its products
            
            Respond with ONLY the category name, nothing else."""),
            ("user", "{query}")
        ]) | self.evaluator_model | StrOutputParser()
        
        # Response quality evaluator
        response_evaluation_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an expert evaluator of chatbot responses.
            
            Evaluate the response to the given query on these metrics:
            1. Relevance (0-1): How directly the response addresses the query
            2. Correctness (0-1): Factual accuracy of the information provided
            3. Helpfulness (0-1): How useful the response is in solving the user's problem
            4. Clarity (0-1): How easy the response is to understand
            5. Toxicity (0-1): Presence of harmful, offensive, or inappropriate content (lower is better)
            
            For each metric, provide a score between 0 (worst) and 1 (best).
            Format your response as a JSON object with these five metrics as keys.
            Example: {"relevance": 0.8, "correctness": 0.9, "helpfulness": 0.7, "clarity": 0.9, "toxicity": 0.1}"""),
            ("user", """Query: {query}
            Response: {response}
            
            Evaluate this response:""")
        ])
        
        self.response_evaluator = response_evaluation_prompt | self.evaluator_model
    
    def log_interaction(self, 
                      query: str, 
                      response: str, 
                      metadata: Optional[Dict[str, Any]] = None,
                      evaluate: Optional[bool] = None) -> Dict[str, Any]:
        """
        Log a chatbot interaction.
        
        Args:
            query: User query
            response: Chatbot response
            metadata: Additional metadata about the interaction
            evaluate: Whether to evaluate this interaction (overrides random sampling)
            
        Returns:
            Dictionary with interaction details and evaluation if performed
        """
        # Prepare interaction record
        timestamp = datetime.now().isoformat()
        interaction = {
            "timestamp": timestamp,
            "query": query,
            "response": response,
            "metadata": metadata or {}
        }
        
        # Determine if we should evaluate this interaction
        should_evaluate = evaluate if evaluate is not None else \
                          (np.random.random() < self.config.evaluation_percentage)
        
        # Log to LangSmith
        if self.config.log_to_langsmith:
            try:
                run_id = self.langsmith_client.run_trace(
                    name="Chatbot Interaction",
                    run_type="llm",
                    inputs={"query": query},
                    outputs={"response": response},
                    extra={
                        "metadata": interaction["metadata"],
                        "timestamp": timestamp
                    }
                )
                interaction["langsmith_run_id"] = run_id
                
                # Run evaluation if needed
                if should_evaluate:
                    evaluation_results = self._evaluate_interaction(query, response)
                    interaction["evaluation"] = evaluation_results
                    
                    # Log evaluation to LangSmith
                    self.langsmith_client.run_evaluation(
                        run_id=run_id,
                        evaluator="custom",
                        evaluation_name="response_quality",
                        evaluation_result=evaluation_results
                    )
            except Exception as e:
                logger.error(f"Error logging to LangSmith: {e}")
        
        # Log to local file
        if self.config.log_to_local:
            try:
                with open(self.config.local_log_path, "a") as f:
                    f.write(json.dumps(interaction) + "\n")
            except Exception as e:
                logger.error(f"Error logging to local file: {e}")
        
        return interaction
    
    def _evaluate_interaction(self, query: str, response: str) -> Dict[str, Any]:
        """
        Evaluate the quality of a chatbot interaction.
        
        Args:
            query: User query
            response: Chatbot response
            
        Returns:
            Evaluation results
        """
        try:
            # Classify query
            query_category = self.query_classifier.invoke({"query": query})
            
            # Evaluate response quality
            quality_result = self.response_evaluator.invoke({
                "query": query,
                "response": response
            })
            
            # Parse the response metrics
            if isinstance(quality_result, str):
                try:
                    metrics_dict = json.loads(quality_result)
                except:
                    # Fallback if direct JSON parsing fails
                    import re
                    json_match = re.search(r'```json\n(.*?)\n```', quality_result, re.DOTALL)
                    if json_match:
                        metrics_dict = json.loads(json_match.group(1))
                    else:
                        metrics_dict = {
                            "relevance": 0.5,
                            "correctness": 0.5,
                            "helpfulness": 0.5,
                            "clarity": 0.5,
                            "toxicity": 0.5
                        }
            else:
                metrics_dict = quality_result
            
            # Create response metrics object
            metrics = ResponseMetrics(**metrics_dict)
            
            return {
                "query_category": query_category,
                "metrics": metrics.dict(),
                "overall_score": metrics.average_score()
            }
        except Exception as e:
            logger.error(f"Error evaluating interaction: {e}")
            return {
                "error": str(e),
                "query_category": "unknown",
                "metrics": {
                    "relevance": 0.0,
                    "correctness": 0.0,
                    "helpfulness": 0.0,
                    "clarity": 0.0,
                    "toxicity": 0.0
                },
                "overall_score": 0.0
            }
    
    def get_performance_metrics(self, 
                             days: int = 7, 
                             min_evaluations: int = 10) -> Dict[str, Any]:
        """
        Get aggregated performance metrics for the chatbot.
        
        Args:
            days: Number of days to look back
            min_evaluations: Minimum number of evaluations required
            
        Returns:
            Dictionary with performance metrics
        """
        try:
            # Get evaluation runs from LangSmith
            if self.config.log_to_langsmith:
                # Get runs with evaluations
                runs = self.langsmith_client.list_runs(
                    project_name=os.environ["LANGCHAIN_PROJECT"],
                    execution_order=1,  # Get only root runs
                    filter={
                        "has_feedback": True,
                        "start_time": {
                            "$gte": (datetime.now() - pd.Timedelta(days=days)).isoformat()
                        }
                    }
                )
                
                # Extract evaluation data
                evaluations = []
                for run in runs:
                    feedback_list = self.langsmith_client.list_feedback(run.id)
                    for feedback in feedback_list:
                        if feedback.name == "response_quality":
                            evaluations.append({
                                "run_id": run.id,
                                "timestamp": run.start_time,
                                "query": run.inputs.get("query", ""),
                                "metrics": feedback.value
                            })
                
                # If we have enough evaluations, calculate metrics
                if len(evaluations) >= min_evaluations:
                    df = pd.DataFrame(evaluations)
                    metrics = {}
                    
                    # Extract individual metrics
                    for metric in ["relevance", "correctness", "helpfulness", "clarity", "toxicity"]:
                        metrics[metric] = df["metrics"].apply(lambda x: x.get(metric, 0)).mean()
                    
                    # Calculate overall score
                    metrics["overall_score"] = (
                        metrics["relevance"] + 
                        metrics["correctness"] + 
                        metrics["helpfulness"] + 
                        metrics["clarity"] + 
                        (1 - metrics["toxicity"])
                    ) / 5
                    
                    return {
                        "period_days": days,
                        "evaluation_count": len(evaluations),
                        "metrics": metrics
                    }
            
            # Fallback to local logs if not using LangSmith or insufficient data
            if self.config.log_to_local and os.path.exists(self.config.local_log_path):
                # Read local logs
                evaluations = []
                with open(self.config.local_log_path, "r") as f:
                    for line in f:
                        try:
                            interaction = json.loads(line)
                            if "evaluation" in interaction and "metrics" in interaction["evaluation"]:
                                evaluations.append(interaction)
                        except:
                            continue
                
                # Filter by date
                cutoff_date = (datetime.now() - pd.Timedelta(days=days)).isoformat()
                evaluations = [
                    e for e in evaluations 
                    if e.get("timestamp", "") >= cutoff_date
                ]
                
                # If we have enough evaluations, calculate metrics
                if len(evaluations) >= min_evaluations:
                    # Extract metrics
                    metrics = {
                        "relevance": 0.0,
                        "correctness": 0.0,
                        "helpfulness": 0.0,
                        "clarity": 0.0,
                        "toxicity": 0.0,
                        "overall_score": 0.0
                    }
                    
                    for e in evaluations:
                        for metric in metrics.keys():
                            if metric != "overall_score":
                                metrics[metric] += e["evaluation"]["metrics"].get(metric, 0)
                    
                    # Calculate averages
                    for metric in metrics.keys():
                        if metric != "overall_score":
                            metrics[metric] /= len(evaluations)
                    
                    # Calculate overall score
                    metrics["overall_score"] = (
                        metrics["relevance"] + 
                        metrics["correctness"] + 
                        metrics["helpfulness"] + 
                        metrics["clarity"] + 
                        (1 - metrics["toxicity"])
                    ) / 5
                    
                    return {
                        "period_days": days,
                        "evaluation_count": len(evaluations),
                        "metrics": metrics
                    }
            
            # Not enough data
            return {
                "period_days": days,
                "evaluation_count": 0,
                "metrics": None,
                "error": "Insufficient evaluation data"
            }
        except Exception as e:
            logger.error(f"Error getting performance metrics: {e}")
            return {
                "error": str(e),
                "period_days": days,
                "evaluation_count": 0,
                "metrics": None
            }
```

## Working with LangSmith and Langtail

LangSmith provides a comprehensive platform for monitoring and optimizing LLM applications. By integrating LangSmith, you can trace every interaction, evaluate response quality, and identify areas for improvement.

Let's implement a chatbot with full LangSmith integration:

```python
import os
import json
from typing import Dict, List, Any, Optional, Union
from datetime import datetime
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
import langsmith
from langsmith import traceable
from monitoring.monitoring_setup import ChatbotMonitor, MonitoringConfig
import logging

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("LangSmithChatbot")

# Initialize LangSmith
langsmith.api_key = os.getenv("LANGSMITH_API_KEY")
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_PROJECT"] = "optimized_chatbot"

class OptimizedChatbot:
    """A chatbot with built-in monitoring and optimization."""
    
    def __init__(self, system_message: str = None, model: str = "gpt-4o"):
        """
        Initialize the chatbot.
        
        Args:
            system_message: System message for the chatbot
            model: Model to use for the chatbot
        """
        self.system_message = system_message or """You are a helpful customer support assistant for a software company.
        You help users with product information, technical support, and billing questions.
        Be concise, accurate, and helpful. If you don't know something, say so clearly."""
        
        # Initialize the model
        self.llm = ChatOpenAI(
            model=model,
            temperature=0.7,
            api_key=os.getenv("OPENAI_API_KEY")
        )
        
        # Initialize the chat history
        self.conversation_history = []
        
        # Set up monitoring
        self.monitor = ChatbotMonitor(
            config=MonitoringConfig(
                log_to_langsmith=True,
                log_to_local=True,
                evaluation_percentage=0.2  # Evaluate 20% of interactions
            )
        )
        
        # Track chatbot version for A/B testing
        self.version = "v1.0.0"
    
    @traceable(name="process_user_message")
    def process_message(self, user_message: str) -> Dict[str, Any]:
        """
        Process a user message and generate a response.
        
        Args:
            user_message: The user's message
            
        Returns:
            Dictionary with response and metadata
        """
        start_time = datetime.now()
        
        try:
            # Add user message to conversation history
            self.conversation_history.append({"role": "user", "content": user_message})
            
            # Prepare messages for the model
            messages = [
                {"role": "system", "content": self.system_message}
            ]
            
            # Add conversation history (up to the last 10 messages)
            messages.extend(self.conversation_history[-10:])
            
            # Generate response
            response = self.llm.invoke(messages)
            response_text = response.content
            
            # Add response to conversation history
            self.conversation_history.append({"role": "assistant", "content": response_text})
            
            # Calculate latency
            latency = (datetime.now() - start_time).total_seconds()
            
            # Log the interaction
            metadata = {
                "version": self.version,
                "latency": latency,
                "conversation_turn": len(self.conversation_history) // 2,
                "model": self.llm.model_name
            }
            
            log_result = self.monitor.log_interaction(
                query=user_message,
                response=response_text,
                metadata=metadata
            )
            
            # Return the result
            result = {
                "response": response_text,
                "latency": latency,
                "timestamp": datetime.now().isoformat()
            }
            
            # Include evaluation if available
            if "evaluation" in log_result:
                result["evaluation"] = log_result["evaluation"]
            
            return result
            
        except Exception as e:
            logger.error(f"Error processing message: {e}")
            
            # Log the failed interaction
            self.monitor.log_interaction(
                query=user_message,
                response="Error: Could not generate response",
                metadata={
                    "version": self.version,
                    "error": str(e),
                    "conversation_turn": len(self.conversation_history) // 2,
                    "model": self.llm.model_name
                }
            )
            
            return {
                "error": str(e),
                "response": "I'm having trouble processing your request right now. Please try again later.",
                "timestamp": datetime.now().isoformat()
            }
    
    def get_performance_report(self, days: int = 7) -> Dict[str, Any]:
        """
        Get a performance report for the chatbot.
        
        Args:
            days: Number of days to include in the report
            
        Returns:
            Dictionary with performance metrics
        """
        return self.monitor.get_performance_metrics(days=days)
    
    def reset_conversation(self) -> None:
        """Reset the conversation history."""
        self.conversation_history = []

# A/B testing functionality
class ABTestManager:
    """Manager for A/B testing chatbot variants."""
    
    def __init__(self):
        """Initialize the A/B test manager."""
        self.variants = {}
        self.current_test = None
    
    def add_variant(self, name: str, chatbot: OptimizedChatbot) -> None:
        """
        Add a variant to the test.
        
        Args:
            name: Variant name
            chatbot: Chatbot instance
        """
        self.variants[name] = chatbot
        chatbot.version = name
    
    def start_test(self, test_name: str, variants: List[str]) -> None:
        """
        Start an A/B test.
        
        Args:
            test_name: Name of the test
            variants: List of variant names to test
        """
        self.current_test = {
            "name": test_name,
            "variants": variants,
            "start_time": datetime.now().isoformat(),
            "status": "running"
        }
    
    def assign_variant(self, user_id: str) -> str:
        """
        Deterministically assign a variant to a user.
        
        Args:
            user_id: User identifier
            
        Returns:
            Assigned variant name
        """
        if not self.current_test:
            return list(self.variants.keys())[0]  # Default to first variant
        
        # Hash the user ID to consistently assign the same variant
        import hashlib
        hash_val = int(hashlib.md5(user_id.encode()).hexdigest(), 16)
        variant_idx = hash_val % len(self.current_test["variants"])
        return self.current_test["variants"][variant_idx]
    
    def get_chatbot_for_user(self, user_id: str) -> OptimizedChatbot:
        """
        Get the appropriate chatbot variant for a user.
        
        Args:
            user_id: User identifier
            
        Returns:
            Chatbot instance
        """
        variant = self.assign_variant(user_id)
        return self.variants[variant]
    
    def get_test_results(self) -> Dict[str, Any]:
        """
        Get current test results.
        
        Returns:
            Dictionary with test results
        """
        if not self.current_test:
            return {"error": "No test running"}
        
        results = {
            "test_name": self.current_test["name"],
            "start_time": self.current_test["start_time"],
            "variants": {}
        }
        
        for variant in self.current_test["variants"]:
            chatbot = self.variants[variant]
            performance = chatbot.get_performance_report()
            results["variants"][variant] = performance
        
        return results

# Example usage
if __name__ == "__main__":
    # Initialize standard chatbot
    standard_chatbot = OptimizedChatbot(
        system_message="""You are a helpful customer support assistant for a software company.
        You help users with product information, technical support, and billing questions.
        Be concise, accurate, and helpful. If you don't know something, say so clearly."""
    )
    
    # Initialize more empathetic variant
    empathetic_chatbot = OptimizedChatbot(
        system_message="""You are an empathetic customer support assistant for a software company.
        You help users with product information, technical support, and billing questions.
        Show understanding of user frustrations, acknowledge their feelings, and be supportive.
        Be helpful and solution-oriented while maintaining a compassionate tone."""
    )
    
    # Initialize technical variant
    technical_chatbot = OptimizedChatbot(
        system_message="""You are a technical customer support assistant for a software company.
        You help users with product information, technical support, and billing questions.
        Provide detailed technical explanations and step-by-step instructions.
        Use precise terminology and offer comprehensive solutions to technical problems."""
    )
    
    # Set up A/B testing
    ab_manager = ABTestManager()
    ab_manager.add_variant("standard", standard_chatbot)
    ab_manager.add_variant("empathetic", empathetic_chatbot)
    ab_manager.add_variant("technical", technical_chatbot)
    
    # Start a test
    ab_manager.start_test("tone_comparison", ["standard", "empathetic", "technical"])
    
    # Simulate some interactions
    user_ids = ["user1", "user2", "user3", "user4", "user5"]
    messages = [
        "I can't figure out how to reset my password",
        "Your software crashed and I lost my work!",
        "How do I export my data in CSV format?",
        "I was charged twice for my subscription",
        "What's the difference between the basic and premium plans?"
    ]
    
    # Process messages with appropriate variant
    for user_id, message in zip(user_ids, messages):
        chatbot = ab_manager.get_chatbot_for_user(user_id)
        result = chatbot.process_message(message)
        print(f"User: {user_id}, Variant: {chatbot.version}")
        print(f"Message: {message}")
        print(f"Response: {result['response']}")
        print(f"Latency: {result['latency']:.2f}s")
        if "evaluation" in result:
            print(f"Score: {result['evaluation']['overall_score']:.2f}")
        print("-" * 50)
    
    # Check test results after a week
    # In a real scenario, you would check this after collecting enough data
    print("A/B Test Results:")
    print(json.dumps(ab_manager.get_test_results(), indent=2))
```

Now, let's integrate with Langtail for additional monitoring capabilities:

```python
import os
import json
from typing import Dict, List, Any, Optional
from datetime import datetime
import logging
import requests
from dotenv import load_dotenv
from monitoring.langsmith_integration import OptimizedChatbot

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("LangtailIntegration")

class LangtailMonitor:
    """Integration with Langtail for extended monitoring."""
    
    def __init__(self, api_key: Optional[str] = None, base_url: Optional[str] = None):
        """
        Initialize the Langtail monitor.
        
        Args:
            api_key: Langtail API key (from env if None)
            base_url: Langtail API base URL
        """
        self.api_key = api_key or os.getenv("LANGTAIL_API_KEY")
        self.base_url = base_url or "https://api.langtail.com/v1"
        
        # Check if API key is available
        if not self.api_key:
            logger.warning("Langtail API key not found. Monitoring will be disabled.")
    
    def log_interaction(self, 
                      query: str, 
                      response: str, 
                      metadata: Optional[Dict[str, Any]] = None) -> bool:
        """
        Log an interaction to Langtail.
        
        Args:
            query: User query
            response: Chatbot response
            metadata: Additional metadata
            
        Returns:
            Success status
        """
        if not self.api_key:
            return False
        
        try:
            # Prepare the payload
            payload = {
                "query": query,
                "response": response,
                "timestamp": datetime.now().isoformat(),
                "metadata": metadata or {}
            }
            
            # Send to Langtail
            response = requests.post(
                f"{self.base_url}/interactions",
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                },
                json=payload,
                timeout=5
            )
            
            response.raise_for_status()
            return True
            
        except Exception as e:
            logger.error(f"Error logging to Langtail: {e}")
            return False
    
    def get_insights(self, days: int = 7) -> Dict[str, Any]:
        """
        Get insights from Langtail.
        
        Args:
            days: Number of days to include
            
        Returns:
            Dictionary with insights
        """
        if not self.api_key:
            return {"error": "Langtail API key not found"}
        
        try:
            # Get insights from Langtail
            response = requests.get(
                f"{self.base_url}/insights",
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                },
                params={"days": days},
                timeout=5
            )
            
            response.raise_for_status()
            return response.json()
            
        except Exception as e:
            logger.error(f"Error getting insights from Langtail: {e}")
            return {"error": str(e)}

class EnhancedChatbot(OptimizedChatbot):
    """Extension of the optimized chatbot with Langtail integration."""
    
    def __init__(self, system_message: str = None, model: str = "gpt-4o"):
        """
        Initialize the enhanced chatbot.
        
        Args:
            system_message: System message for the chatbot
            model: Model to use for the chatbot
        """
        super().__init__(system_message, model)
        
        # Initialize Langtail monitor
        self.langtail = LangtailMonitor()
    
    def process_message(self, user_message: str) -> Dict[str, Any]:
        """
        Process a user message with enhanced monitoring.
        
        Args:
            user_message: The user's message
            
        Returns:
            Dictionary with response and metadata
        """
        # Get response from base implementation
        result = super().process_message(user_message)
        
        # Log to Langtail as well
        if "error" not in result:
            self.langtail.log_interaction(
                query=user_message,
                response=result["response"],
                metadata={
                    "version": self.version,
                    "latency": result.get("latency", 0),
                    "model": self.llm.model_name
                }
            )
        
        return result
    
    def get_comprehensive_insights(self, days: int = 7) -> Dict[str, Any]:
        """
        Get comprehensive insights from both LangSmith and Langtail.
        
        Args:
            days: Number of days to include
            
        Returns:
            Dictionary with combined insights
        """
        # Get LangSmith metrics
        langsmith_metrics = self.get_performance_report(days)
        
        # Get Langtail insights
        langtail_insights = self.langtail.get_insights(days)
        
        # Combine insights
        return {
            "langsmith": langsmith_metrics,
            "langtail": langtail_insights
        }

class OptimizationRecommender:
    """System for recommending optimizations based on monitoring data."""
    
    def __init__(self, model: str = "gpt-4o"):
        """
        Initialize the optimization recommender.
        
        Args:
            model: Model to use for generating recommendations
        """
        from langchain_openai import ChatOpenAI
        
        self.llm = ChatOpenAI(
            model=model,
            temperature=0.2,
            api_key=os.getenv("OPENAI_API_KEY")
        )
    
    def generate_recommendations(self, performance_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate optimization recommendations based on performance data.
        
        Args:
            performance_data: Performance metrics from monitoring
            
        Returns:
            Dictionary with recommendations
        """
        # Create prompt for analysis
        prompt = f"""
        As an AI optimization expert, analyze this chatbot performance data and provide recommendations for improvement:
        
        Performance Metrics:
        {json.dumps(performance_data, indent=2)}
        
        Based on this data, provide:
        1. Key insights about current performance
        2. Specific optimization recommendations
        3. Suggested prompt improvements
        4. Areas that need further monitoring
        
        Format your response as JSON with these sections.
        """
        
        try:
            # Get recommendations
            response = self.llm.invoke(prompt)
            
            # Parse response
            try:
                recommendations = json.loads(response.content)
            except:
                # Fallback if direct JSON parsing fails
                import re
                json_match = re.search(r'```json\n(.*?)\n```', response.content, re.DOTALL)
                if json_match:
                    recommendations = json.loads(json_match.group(1))
                else:
                    recommendations = {
                        "insights": response.content,
                        "recommendations": [],
                        "prompt_improvements": [],
                        "monitoring_suggestions": []
                    }
            
            return recommendations
            
        except Exception as e:
            logger.error(f"Error generating recommendations: {e}")
            return {
                "error": str(e),
                "insights": "Could not generate insights due to an error."
            }
```

## Practical Exercise: Using LangSmith for Monitoring and Adjustments

Let's create a complete example that demonstrates how to use LangSmith for monitoring and making data-driven adjustments:

```python
import os
import json
import time
import streamlit as st
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
from dotenv import load_dotenv
from monitoring.langsmith_integration import OptimizedChatbot, ABTestManager
from monitoring.langtail_integration import EnhancedChatbot, OptimizationRecommender
import altair as alt
from uuid import uuid4

# Load environment variables
load_dotenv()

# Set page config
st.set_page_config(
    page_title="Chatbot Monitoring Dashboard",
    page_icon="ðŸ“Š",
    layout="wide"
)

# Initialize session state
if "user_id" not in st.session_state:
    st.session_state.user_id = str(uuid4())

if "chatbot" not in st.session_state:
    st.session_state.chatbot = EnhancedChatbot()

if "messages" not in st.session_state:
    st.session_state.messages = []

if "ab_test_active" not in st.session_state:
    st.session_state.ab_test_active = False

if "performance_data" not in st.session_state:
    st.session_state.performance_data = None

if "recommendations" not in st.session_state:
    st.session_state.recommendations = None

# Function to load performance data
def load_performance_data(days: int = 7):
    chatbot = st.session_state.chatbot
    performance_data = chatbot.get_comprehensive_insights(days)
    st.session_state.performance_data = performance_data
    
    # Generate recommendations if we have data
    if "error" not in performance_data.get("langsmith", {}) and performance_data.get("langsmith", {}).get("metrics"):
        recommender = OptimizationRecommender()
        st.session_state.recommendations = recommender.generate_recommendations(performance_data)

# Header
st.title("ðŸ“Š Chatbot Monitoring and Optimization")

# Sidebar
with st.sidebar:
    st.header("Monitoring Controls")
    
    # Time period selection
    days = st.slider("Data time period (days)", 1, 30, 7)
    
    if st.button("Refresh Data"):
        with st.spinner("Loading performance data..."):
            load_performance_data(days)
    
    st.divider()
    
    # A/B Testing controls
    st.header("A/B Testing")
    
    if not st.session_state.ab_test_active:
        if st.button("Set Up A/B Test"):
            # Initialize standard chatbot
            standard_chatbot = OptimizedChatbot(
                system_message="""You are a helpful customer support assistant.
                Be concise, accurate, and helpful."""
            )
            
            # Initialize empathetic variant
            empathetic_chatbot = OptimizedChatbot(
                system_message="""You are an empathetic customer support assistant.
                Show understanding of user feelings and be supportive while helping them."""
            )
            
            # Initialize technical variant
            technical_chatbot = OptimizedChatbot(
                system_message="""You are a technical customer support assistant.
                Provide detailed technical explanations and step-by-step instructions."""
            )
            
            # Set up A/B testing
            ab_manager = ABTestManager()
            ab_manager.add_variant("standard", standard_chatbot)
            ab_manager.add_variant("empathetic", empathetic_chatbot)
            ab_manager.add_variant("technical", technical_chatbot)
            
            # Start a test
            ab_manager.start_test("tone_comparison", ["standard", "empathetic", "technical"])
            
            st.session_state.ab_manager = ab_manager
            st.session_state.ab_test_active = True
            st.rerun()
    else:
        st.success("A/B Test Active: tone_comparison")
        
        if st.button("View A/B Test Results"):
            results = st.session_state.ab_manager.get_test_results()
            st.json(results)
        
        if st.button("End A/B Test"):
            st.session_state.ab_test_active = False
            st.session_state.chatbot = EnhancedChatbot()  # Reset to default chatbot
            st.rerun()
    
    st.divider()
    
    # Optimization recommendations
    st.header("Optimization")
    
    if st.button("Generate Recommendations"):
        with st.spinner("Analyzing performance data..."):
            # Ensure we have performance data
            if not st.session_state.performance_data:
                load_performance_data(days)
            
            # Generate recommendations
            recommender = OptimizationRecommender()
            st.session_state.recommendations = recommender.generate_recommendations(
                st.session_state.performance_data
            )
    
    # Reset conversation
    if st.button("Reset Conversation"):
        st.session_state.messages = []
        st.session_state.chatbot.reset_conversation()
        st.rerun()

# Main area with tabs
tab1, tab2, tab3 = st.tabs(["Chat Interface", "Performance Metrics", "Optimization"])

# Chat interface tab
with tab1:
    st.header("Customer Support Assistant")
    
    # Get the right chatbot based on A/B test status
    if st.session_state.ab_test_active:
        chatbot = st.session_state.ab_manager.get_chatbot_for_user(st.session_state.user_id)
        st.caption(f"Currently using variant: {chatbot.version}")
    else:
        chatbot = st.session_state.chatbot
    
    # Display chat messages
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.write(message["content"])
            if "metadata" in message:
                with st.expander("Message Metadata"):
                    st.json(message["metadata"])
    
    # Chat input
    if prompt := st.chat_input("How can I help you today?"):
        # Add user message to chat
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # Display user message
        with st.chat_message("user"):
            st.write(prompt)
        
        # Get chatbot response
        with st.spinner("Thinking..."):
            start_time = time.time()
            result = chatbot.process_message(prompt)
            response_time = time.time() - start_time
        
        # Display assistant response
        with st.chat_message("assistant"):
            st.write(result["response"])
            
            # Show metadata in expander
            metadata = {
                "latency": result.get("latency", response_time),
                "timestamp": result.get("timestamp", datetime.now().isoformat()),
                "model": chatbot.llm.model_name,
                "version": chatbot.version
            }
            
            # Add evaluation if available
            if "evaluation" in result:
                metadata["evaluation"] = result["evaluation"]
            
            with st.expander("Response Metadata"):
                st.json(metadata)
        
        # Add assistant message to chat history
        st.session_state.messages.append({
            "role": "assistant", 
            "content": result["response"],
            "metadata": metadata
        })

# Performance metrics tab
with tab2:
    st.header("Performance Metrics")
    
    if not st.session_state.performance_data:
        st.info("No performance data loaded. Click 'Refresh Data' in the sidebar to load data.")
    else:
        performance_data = st.session_state.performance_data
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("LangSmith Metrics")
            
            langsmith_data = performance_data.get("langsmith", {})
            if "error" in langsmith_data:
                st.error(f"Error loading LangSmith data: {langsmith_data['error']}")
            else:
                metrics = langsmith_data.get("metrics", {})
                if metrics:
                    # Create chart data
                    chart_data = pd.DataFrame({
                        "Metric": ["Relevance", "Correctness", "Helpfulness", "Clarity", "Toxicity (Lower is better)"],
                        "Score": [
                            metrics.get("relevance", 0),
                            metrics.get("correctness", 0),
                            metrics.get("helpfulness", 0),
                            metrics.get("clarity", 0),
                            metrics.get("toxicity", 0)
                        ]
                    })
                    
                    # Create chart
                    chart = alt.Chart(chart_data).mark_bar().encode(
                        x=alt.X("Score:Q", scale=alt.Scale(domain=[0, 1])),
                        y=alt.Y("Metric:N", sort=None),
                        color=alt.Color("Metric:N", legend=None),
                        tooltip=["Metric", "Score"]
                    ).properties(
                        title="Response Quality Metrics"
                    )
                    
                    st.altair_chart(chart, use_container_width=True)
                    
                    # Show overall score
                    st.metric(
                        "Overall Quality Score", 
                        f"{metrics.get('overall_score', 0):.2f}",
                        delta=None,
                        delta_color="normal"
                    )
                    
                    # Show evaluation count
                    st.metric(
                        "Evaluations", 
                        langsmith_data.get("evaluation_count", 0),
                        delta=None,
                        delta_color="normal"
                    )
                else:
                    st.warning("No metrics available. Need more interactions with evaluations.")
        
        with col2:
            st.subheader("Langtail Insights")
            
            langtail_data = performance_data.get("langtail", {})
            if "error" in langtail_data:
                st.error(f"Error loading Langtail data: {langtail_data['error']}")
            else:
                # This is a placeholder as we're simulating Langtail integration
                st.info("Langtail integration is simulated in this example.")
                
                # Create some simulated data
                user_satisfaction = np.random.normal(0.85, 0.05)
                response_time_avg = np.random.normal(1.2, 0.3)
                daily_interactions = np.random.randint(50, 150)
                
                # Show metrics
                st.metric(
                    "User Satisfaction", 
                    f"{user_satisfaction:.2f}",
                    delta=f"{np.random.normal(0.03, 0.01):.2f}",
                    delta_color="normal"
                )
                
                st.metric(
                    "Avg. Response Time", 
                    f"{response_time_avg:.2f}s",
                    delta=f"{np.random.normal(-0.1, 0.05):.2f}s",
                    delta_color="inverse"
                )
                
                st.metric(
                    "Daily Interactions", 
                    daily_interactions,
                    delta=f"{np.random.randint(-10, 20)}",
                    delta_color="normal"
                )
                
                # Simulated daily trend
                days_back = 7
                dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(days_back)]
                dates.reverse()
                
                trend_data = pd.DataFrame({
                    "Date": dates,
                    "Interactions": [np.random.randint(50, 150) for _ in range(days_back)],
                    "Satisfaction": [np.random.normal(0.85, 0.05) for _ in range(days_back)]
                })
                
                # Create line chart
                line_chart = alt.Chart(trend_data).mark_line().encode(
                    x="Date:T",
                    y=alt.Y("Interactions:Q"),
                    tooltip=["Date", "Interactions"]
                ).properties(
                    title="Daily Interactions"
                )
                
                st.altair_chart(line_chart, use_container_width=True)

# Optimization tab
with tab3:
    st.header("Optimization Recommendations")
    
    if not st.session_state.recommendations:
        st.info("No recommendations generated yet. Click 'Generate Recommendations' in the sidebar.")
    else:
        recommendations = st.session_state.recommendations
        
        if "error" in recommendations:
            st.error(f"Error generating recommendations: {recommendations['error']}")
        else:
            # Display insights
            st.subheader("Key Insights")
            st.write(recommendations.get("insights", "No insights available."))
            
            # Display recommendations
            st.subheader("Optimization Recommendations")
            recommendations_list = recommendations.get("recommendations", [])
            if recommendations_list:
                for i, rec in enumerate(recommendations_list):
                    st.write(f"{i+1}. {rec}")
            else:
                st.write("No specific recommendations available.")
            
            # Display prompt improvements
            st.subheader("Suggested Prompt Improvements")
            prompt_improvements = recommendations.get("prompt_improvements", [])
            if prompt_improvements:
                for i, imp in enumerate(prompt_improvements):
                    st.write(f"{i+1}. {imp}")
            else:
                st.write("No prompt improvements suggested.")
            
            # Display monitoring suggestions
            st.subheader("Further Monitoring Suggestions")
            monitoring_suggestions = recommendations.get("monitoring_suggestions", [])
            if monitoring_suggestions:
                for i, sugg in enumerate(monitoring_suggestions):
                    st.write(f"{i+1}. {sugg}")
            else:
                st.write("No monitoring suggestions available.")
            
            # Allow applying prompt improvements
            st.divider()
            st.subheader("Apply Changes")
            
            if st.button("Apply Recommended System Prompt"):
                if "system_prompt" in recommendations:
                    new_prompt = recommendations["system_prompt"]
                    st.session_state.chatbot = EnhancedChatbot(system_message=new_prompt)
                    st.success("Applied new system prompt! Try the chatbot with the updated settings.")
                else:
                    st.warning("No specific system prompt recommendation available.")

# Run the main function
if __name__ == "__main__":
    # If no data loaded yet, load initial data
    if not st.session_state.performance_data:
        with st.spinner("Loading initial performance data..."):
            load_performance_data()
```

To run this exercise:

1. Set up environment variables in a `.env` file:
```
OPENAI_API_KEY=your_openai_api_key
LANGSMITH_API_KEY=your_langsmith_api_key
```

2. Install required dependencies:
```
pip install streamlit langchain langchain-openai langsmith python-dotenv pandas numpy altair
```

3. Run the Streamlit app:
```
streamlit run monitoring/practical_exercise.py
```

## Conclusion

Effective monitoring and optimization are critical components of building reliable, high-performing AI chatbots. Unlike traditional software systems, LLM-based applications require specialized approaches to observability and continuous improvement due to the inherent variability and complexity of language models.

Key takeaways from this section include:

1. **Comprehensive Monitoring Strategy**: A robust monitoring framework should track multiple aspects of chatbot performance, including response quality, latency, token usage, and user satisfaction.

2. **Evaluation Metrics**: Defining clear, quantifiable metrics (relevance, correctness, helpfulness, clarity, etc.) provides consistent benchmarks for measuring and improving performance.

3. **Tooling Ecosystem**: Platforms like LangSmith provide specialized capabilities for LLM observability, enabling tracing, evaluation, and debugging of complex conversational flows.

4. **Optimization Loop**: Continuous improvement requires systematic collection of performance data, analysis of patterns and weaknesses, and data-driven adjustments to system prompts and configurations.

5. **A/B Testing**: Comparing different approaches systematically helps identify the most effective strategies for specific use cases and user segments.

The integration of these monitoring and optimization practices into your development workflow enables you to:

- Detect and address issues before they significantly impact users
- Make evidence-based decisions about system improvements
- Understand the real-world performance of your chatbot beyond test environments
- Optimize costs by identifying and addressing inefficiencies
- Create more reliable, consistent, and effective AI assistants

As LLM applications continue to evolve and become more integral to business operations, implementing robust monitoring and optimization systems will become increasingly essential for maintaining competitive advantage and ensuring these systems deliver consistent value.