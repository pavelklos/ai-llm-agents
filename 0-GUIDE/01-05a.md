<small>Claude 3.7 Sonnet Thinking</small>
# 05. Custom Agent Framework

## Key Terms

- **Agent**: An autonomous system that perceives its environment and takes actions to achieve goals.
- **Autonomous Agent**: Operates independently with minimal supervision, making decisions based on its own reasoning.
- **Workflow Agent**: Follows a predefined sequence of steps with decision points, typically executing within a structured process.
- **ReAct Pattern**: Reasoning and Acting framework where agents alternate between reasoning about observations and taking actions.
- **State Management**: Methods for tracking and updating agent context and progress.
- **Decision Making**: Process by which agents choose actions based on observations and goals.
- **Agent Loop**: The iterative cycle of observation, reasoning, and action that forms an agent's operation.
- **Planning**: The ability to formulate a sequence of actions to achieve a specific goal.
- **Reflection**: An agent's ability to evaluate its own reasoning and actions.

## Types of Agents: Autonomous vs. Workflow

Agents broadly fall into two categories based on their execution model and decision-making autonomy:

### Autonomous Agents

Autonomous agents operate independently, making decisions based on their internal reasoning without following rigid pathways. These agents:

- Maintain their own state and memory
- Dynamically decide next actions without predefined steps
- Can self-correct and adapt to unexpected situations
- Often leverage planning capabilities
- Handle complex, open-ended tasks better

### Workflow Agents

Workflow agents follow structured processes with predefined decision points. They:

- Execute along predefined paths with conditional branches
- Have limited autonomy within the constraints of the workflow
- Typically handle well-defined, repetitive tasks
- Are easier to audit, debug, and understand
- Often operate within business process automation systems

## Building a ReAct Agent from Scratch

The ReAct (Reasoning + Action) pattern is a powerful approach for creating more capable agents. It involves alternating between reasoning about observations and taking actions to gather more information.

Below is a comprehensive implementation of a custom ReAct agent framework:

```python
import os
from typing import Dict, List, Any, Optional, Callable, Tuple, TypeVar, Generic, Union
from enum import Enum
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
import json
import uuid
import asyncio
import logging
from datetime import datetime
import openai
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("ReActAgent")

class ActionStatus(Enum):
    """Status of an action execution"""
    SUCCESS = "success"
    ERROR = "error"
    PENDING = "pending"

@dataclass
class Observation:
    """Represents the result of an action"""
    content: Any
    status: ActionStatus = ActionStatus.SUCCESS
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_str(self) -> str:
        """Convert observation to string representation"""
        if self.status == ActionStatus.ERROR:
            return f"ERROR: {self.content}"
        
        if isinstance(self.content, dict):
            return json.dumps(self.content, indent=2)
        return str(self.content)

@dataclass
class Action:
    """Action requested by the agent"""
    name: str
    args: Dict[str, Any]
    action_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())

@dataclass
class AgentState:
    """State maintained by the agent across steps"""
    conversation_history: List[Dict[str, str]] = field(default_factory=list)
    action_history: List[Tuple[Action, Observation]] = field(default_factory=list)
    memory: Dict[str, Any] = field(default_factory=dict)
    step_count: int = 0
    max_steps: int = 10
    goal: Optional[str] = None
    
    def add_message(self, role: str, content: str) -> None:
        """Add a message to the conversation history"""
        self.conversation_history.append({"role": role, "content": content})
    
    def add_action_result(self, action: Action, observation: Observation) -> None:
        """Record an action and its result"""
        self.action_history.append((action, observation))
    
    def get_recent_actions(self, n: int = 3) -> List[Tuple[Action, Observation]]:
        """Get the most recent n actions and observations"""
        return self.action_history[-n:] if self.action_history else []
    
    @property
    def exceeded_max_steps(self) -> bool:
        """Check if the agent has exceeded its maximum allowed steps"""
        return self.step_count >= self.max_steps
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert state to dictionary for serialization"""
        return {
            "conversation_history": self.conversation_history,
            "action_history": [
                {
                    "action": {
                        "name": action.name,
                        "args": action.args,
                        "action_id": action.action_id,
                        "timestamp": action.timestamp
                    },
                    "observation": {
                        "content": observation.content,
                        "status": observation.status.value,
                        "metadata": observation.metadata
                    }
                }
                for action, observation in self.action_history
            ],
            "memory": self.memory,
            "step_count": self.step_count,
            "max_steps": self.max_steps,
            "goal": self.goal
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'AgentState':
        """Reconstruct state from dictionary"""
        state = cls(
            conversation_history=data.get("conversation_history", []),
            memory=data.get("memory", {}),
            step_count=data.get("step_count", 0),
            max_steps=data.get("max_steps", 10),
            goal=data.get("goal")
        )
        
        # Rebuild action history
        action_history = []
        for item in data.get("action_history", []):
            action = Action(
                name=item["action"]["name"],
                args=item["action"]["args"],
                action_id=item["action"].get("action_id", str(uuid.uuid4())),
                timestamp=item["action"].get("timestamp", datetime.now().isoformat())
            )
            
            observation = Observation(
                content=item["observation"]["content"],
                status=ActionStatus(item["observation"]["status"]),
                metadata=item["observation"].get("metadata", {})
            )
            
            action_history.append((action, observation))
        
        state.action_history = action_history
        return state

class Tool(ABC):
    """Abstract base class for tools that an agent can use"""
    
    @property
    @abstractmethod
    def name(self) -> str:
        """Name of the tool"""
        pass
    
    @property
    @abstractmethod
    def description(self) -> str:
        """Description of what the tool does"""
        pass
    
    @property
    @abstractmethod
    def parameters(self) -> Dict[str, Any]:
        """Parameters schema for the tool"""
        pass
    
    @abstractmethod
    async def execute(self, **kwargs) -> Observation:
        """Execute the tool with the given parameters"""
        pass
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert tool to dictionary format for LLM"""
        return {
            "type": "function",
            "function": {
                "name": self.name,
                "description": self.description,
                "parameters": {
                    "type": "object",
                    "properties": self.parameters,
                    "required": [
                        k for k, v in self.parameters.items()
                        if v.get("required", False)
                    ]
                }
            }
        }

class SearchTool(Tool):
    """Tool for searching web information"""
    
    @property
    def name(self) -> str:
        return "search"
    
    @property
    def description(self) -> str:
        return "Search the web for information on a specific topic"
    
    @property
    def parameters(self) -> Dict[str, Any]:
        return {
            "query": {
                "type": "string",
                "description": "The search query",
                "required": True
            }
        }
    
    async def execute(self, query: str, **kwargs) -> Observation:
        """Execute a web search (simulated)"""
        try:
            # In a real implementation, this would call a search API
            # For this example, we'll return mock results
            logger.info(f"Searching for: {query}")
            
            search_results = {
                "python": "Python is a high-level, interpreted programming language known for its readability.",
                "machine learning": "Machine learning is a subset of AI focused on building systems that learn from data.",
                "artificial intelligence": "AI involves creating systems capable of performing tasks that typically require human intelligence."
            }
            
            # Find the most relevant result based on the query
            result = None
            for key, value in search_results.items():
                if key.lower() in query.lower():
                    result = value
                    break
            
            if result:
                return Observation(content=result)
            return Observation(
                content=f"No specific information found for '{query}'. Try a more general query.",
                metadata={"found": False}
            )
            
        except Exception as e:
            logger.error(f"Error in search tool: {str(e)}")
            return Observation(
                content=f"Search failed: {str(e)}",
                status=ActionStatus.ERROR
            )

class CalculatorTool(Tool):
    """Tool for performing calculations"""
    
    @property
    def name(self) -> str:
        return "calculator"
    
    @property
    def description(self) -> str:
        return "Perform mathematical calculations"
    
    @property
    def parameters(self) -> Dict[str, Any]:
        return {
            "expression": {
                "type": "string",
                "description": "The mathematical expression to evaluate",
                "required": True
            }
        }
    
    async def execute(self, expression: str, **kwargs) -> Observation:
        """Evaluate a mathematical expression"""
        try:
            # CAUTION: Using eval can be dangerous with user input
            # In production, use a safer evaluation method or library
            # For demonstration purposes only
            logger.info(f"Calculating: {expression}")
            
            # Here we'd implement a safe calculation method
            # For example purposes, we'll handle a few simple cases
            if "+" in expression:
                parts = expression.split("+")
                if len(parts) == 2:
                    try:
                        result = float(parts[0].strip()) + float(parts[1].strip())
                        return Observation(content=result)
                    except ValueError:
                        pass
            
            # For more complex expressions, we could use a real calculator service or API
            return Observation(
                content="Cannot safely evaluate this expression. Please provide a simpler calculation.",
                status=ActionStatus.ERROR
            )
            
        except Exception as e:
            logger.error(f"Error in calculator tool: {str(e)}")
            return Observation(
                content=f"Calculation failed: {str(e)}",
                status=ActionStatus.ERROR
            )

class LLMProvider(ABC):
    """Abstract base class for LLM providers"""
    
    @abstractmethod
    async def generate_response(self, 
                             messages: List[Dict[str, str]], 
                             tools: Optional[List[Dict]] = None) -> Dict[str, Any]:
        """Generate a response from the LLM"""
        pass

class OpenAIProvider(LLMProvider):
    """OpenAI LLM provider implementation"""
    
    def __init__(self, model: str = "gpt-4-turbo"):
        """Initialize the OpenAI provider with specified model"""
        self.client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.model = model
    
    async def generate_response(self, 
                             messages: List[Dict[str, str]], 
                             tools: Optional[List[Dict]] = None) -> Dict[str, Any]:
        """Generate a response using OpenAI's API"""
        try:
            if tools:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    tools=tools,
                    temperature=0.7
                )
            else:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=0.7
                )
            
            # Convert to a plain dictionary for easier handling
            return {
                "id": response.id,
                "message": {
                    "role": response.choices[0].message.role,
                    "content": response.choices[0].message.content,
                    "tool_calls": [
                        {
                            "id": tc.id,
                            "type": tc.type,
                            "function": {
                                "name": tc.function.name,
                                "arguments": json.loads(tc.function.arguments)
                            }
                        }
                        for tc in response.choices[0].message.tool_calls
                    ] if response.choices[0].message.tool_calls else None
                }
            }
            
        except Exception as e:
            logger.error(f"Error generating response from OpenAI: {str(e)}")
            raise

class ReActAgent:
    """Implementation of a ReAct agent that iteratively reasons and acts"""
    
    def __init__(self, llm_provider: LLMProvider, tools: List[Tool] = None):
        """Initialize the agent with an LLM provider and available tools"""
        self.llm_provider = llm_provider
        self.tools = tools or []
        self.tool_map = {tool.name: tool for tool in self.tools}
        
    def _get_system_prompt(self) -> str:
        """Generate system prompt with instructions for ReAct reasoning"""
        tool_descriptions = "\n".join([
            f"- {tool.name}: {tool.description}" for tool in self.tools
        ])
        
        return f"""You are an advanced assistant that can perceive, reason, and act to help solve problems.

For each step, follow this process:
1. Observe the environment and previous actions
2. Think about what to do next
3. Choose either:
   a) Use a tool to gather more information
   b) Respond with a final answer

When using tools, format your response as:
```
Thought: <your reasoning about what information you need>
Action: <tool_name>
Action Input: <tool parameters as JSON>
```

When providing a final answer, format your response as:
```
Thought: <your reasoning>
Answer: <your final answer>
```

Available tools:
{tool_descriptions}

Remember to reason step by step and use tools when needed to gather information.
"""

    def _parse_llm_response(self, response: Dict[str, Any]) -> Tuple[str, Optional[Action]]:
        """Parse the LLM response to extract thought process and action"""
        message = response["message"]
        content = message.get("content", "")
        
        # Check if the LLM is using a tool
        if message.get("tool_calls"):
            tool_call = message["tool_calls"][0]
            action = Action(
                name=tool_call["function"]["name"],
                args=tool_call["function"]["arguments"]
            )
            return content, action
        
        # Otherwise, parse the content for thought and action
        thought = ""
        action = None
        
        # Try to extract structured reasoning and action
        if "Thought:" in content and "Action:" in content:
            thought_parts = content.split("Thought:")
            if len(thought_parts) > 1:
                thought_and_action = thought_parts[1].strip()
                thought_end = thought_and_action.find("Action:")
                if thought_end > 0:
                    thought = thought_and_action[:thought_end].strip()
                    action_part = thought_and_action[thought_end:]
                    
                    # Extract action name
                    action_name_start = action_part.find("Action:") + len("Action:")
                    action_name_end = action_part.find("Action Input:")
                    if action_name_end > 0:
                        action_name = action_part[action_name_start:action_name_end].strip()
                        
                        # Extract action input
                        action_input_start = action_part.find("Action Input:") + len("Action Input:")
                        action_input = action_part[action_input_start:].strip()
                        
                        try:
                            # Try to parse action input as JSON
                            action_args = json.loads(action_input)
                            action = Action(name=action_name, args=action_args)
                        except json.JSONDecodeError:
                            # Fallback for non-JSON action inputs
                            action = Action(name=action_name, args={"raw_input": action_input})
        
        # Check for final answer
        if "Answer:" in content:
            # This is a final answer, no action needed
            thought_parts = content.split("Thought:")
            if len(thought_parts) > 1:
                answer_parts = thought_parts[1].split("Answer:")
                if len(answer_parts) > 1:
                    thought = answer_parts[0].strip()
        
        return thought, action

    async def _execute_action(self, action: Action) -> Observation:
        """Execute the specified action using the appropriate tool"""
        tool = self.tool_map.get(action.name)
        if not tool:
            return Observation(
                content=f"Unknown tool: {action.name}",
                status=ActionStatus.ERROR
            )
        
        try:
            return await tool.execute(**action.args)
        except Exception as e:
            logger.error(f"Error executing {action.name}: {str(e)}")
            return Observation(
                content=f"Error executing {action.name}: {str(e)}",
                status=ActionStatus.ERROR
            )

    def _build_messages(self, state: AgentState, user_input: str) -> List[Dict[str, str]]:
        """Build the message list for the LLM"""
        messages = [{"role": "system", "content": self._get_system_prompt()}]
        
        # Add conversation history
        for message in state.conversation_history:
            messages.append(message)
        
        # Add recent action history with observations
        recent_actions = state.get_recent_actions()
        if recent_actions:
            action_history = ""
            for action, observation in recent_actions:
                action_str = f"Action: {action.name}\nAction Input: {json.dumps(action.args, indent=2)}"
                observation_str = f"Observation: {observation.to_str()}"
                action_history += f"{action_str}\n\n{observation_str}\n\n"
            
            messages.append({"role": "system", "content": f"Recent actions:\n\n{action_history}"})
        
        # Add user input if provided
        if user_input:
            messages.append({"role": "user", "content": user_input})
        
        return messages

    async def step(self, state: AgentState, user_input: Optional[str] = None) -> Tuple[str, bool]:
        """Perform one step of the ReAct process"""
        # Check if we've exceeded max steps
        if state.exceeded_max_steps:
            return "I've reached my maximum number of steps. Let me provide my best answer based on what I've learned so far.", True
        
        # Increment step counter
        state.step_count += 1
        
        # Add user input to conversation history if provided
        if user_input:
            state.add_message("user", user_input)
        
        # Build messages for LLM
        messages = self._build_messages(state, user_input if state.step_count == 1 else None)
        
        # Get tools in LLM-compatible format
        tool_dicts = [tool.to_dict() for tool in self.tools] if self.tools else None
        
        # Generate LLM response
        llm_response = await self.llm_provider.generate_response(messages, tool_dicts)
        
        # Parse response to get thought and action
        thought, action = self._parse_llm_response(llm_response)
        
        # Add assistant's thought to conversation history
        state.add_message("assistant", thought)
        
        # If no action, we have a final answer
        if not action:
            return thought, True
        
        # Execute the action
        observation = await self._execute_action(action)
        
        # Record the action and observation
        state.add_action_result(action, observation)
        
        # Return thought and indication that we should continue
        return thought, False

    async def run(self, 
                query: str, 
                max_steps: int = 10, 
                state: Optional[AgentState] = None) -> Tuple[str, AgentState]:
        """Run the agent on a query until completion or max steps"""
        # Initialize or use provided state
        if not state:
            state = AgentState(max_steps=max_steps, goal=query)
        else:
            state.goal = query
            state.max_steps = max_steps
        
        # Run the initial step with the user query
        thought, is_done = await self.step(state, query)
        result = thought
        
        # Continue steps until done or max steps reached
        while not is_done and not state.exceeded_max_steps:
            thought, is_done = await self.step(state)
            result = thought
        
        return result, state
```

## Working with State, Decision Making, and Cycles

Beyond the basic ReAct framework, advanced agents need sophisticated state management, decision-making capabilities, and handling of execution cycles. Let's explore these concepts with a practical implementation:

```python
from typing import Dict, List, Any, Optional, Callable, Union, TypeVar, Generic
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
import json
import uuid
from datetime import datetime
import asyncio
import logging
from enum import Enum

from .react_agent import AgentState, Tool, LLMProvider

# Configure logging
logger = logging.getLogger("AdvancedAgentComponents")

class StateManager(ABC):
    """Abstract base class for state management strategies"""
    
    @abstractmethod
    async def save_state(self, agent_id: str, state: AgentState) -> bool:
        """Save agent state"""
        pass
    
    @abstractmethod
    async def load_state(self, agent_id: str) -> Optional[AgentState]:
        """Load agent state"""
        pass
    
    @abstractmethod
    async def delete_state(self, agent_id: str) -> bool:
        """Delete agent state"""
        pass

class FileStateManager(StateManager):
    """File-based state manager implementation"""
    
    def __init__(self, directory: str = "./agent_states"):
        """Initialize with state storage directory"""
        self.directory = directory
        import os
        os.makedirs(self.directory, exist_ok=True)
    
    def _get_file_path(self, agent_id: str) -> str:
        """Get file path for an agent state"""
        return f"{self.directory}/{agent_id}.json"
    
    async def save_state(self, agent_id: str, state: AgentState) -> bool:
        """Save agent state to file"""
        try:
            file_path = self._get_file_path(agent_id)
            state_dict = state.to_dict()
            
            # Use executor to avoid blocking the event loop
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, self._write_state, file_path, state_dict)
            return True
        except Exception as e:
            logger.error(f"Error saving state: {str(e)}")
            return False
    
    def _write_state(self, file_path: str, state_dict: Dict):
        """Helper to write state to file"""
        with open(file_path, 'w') as f:
            json.dump(state_dict, f, indent=2)
    
    async def load_state(self, agent_id: str) -> Optional[AgentState]:
        """Load agent state from file"""
        file_path = self._get_file_path(agent_id)
        import os
        if not os.path.exists(file_path):
            return None
        
        try:
            # Use executor to avoid blocking the event loop
            loop = asyncio.get_event_loop()
            state_dict = await loop.run_in_executor(None, self._read_state, file_path)
            return AgentState.from_dict(state_dict)
        except Exception as e:
            logger.error(f"Error loading state: {str(e)}")
            return None
    
    def _read_state(self, file_path: str) -> Dict:
        """Helper to read state from file"""
        with open(file_path, 'r') as f:
            return json.load(f)
    
    async def delete_state(self, agent_id: str) -> bool:
        """Delete agent state file"""
        file_path = self._get_file_path(agent_id)
        import os
        if os.path.exists(file_path):
            try:
                # Use executor to avoid blocking
                loop = asyncio.get_event_loop()
                await loop.run_in_executor(None, os.remove, file_path)
                return True
            except Exception as e:
                logger.error(f"Error deleting state: {str(e)}")
                return False
        return True  # File doesn't exist, so already "deleted"

class DecisionStrategy(ABC):
    """Abstract base class for decision-making strategies"""
    
    @abstractmethod
    async def make_decision(self, 
                        state: AgentState, 
                        options: List[str],
                        context: Optional[str] = None) -> str:
        """Make a decision among available options"""
        pass

class LLMDecisionStrategy(DecisionStrategy):
    """Decision strategy that uses LLM for decision making"""
    
    def __init__(self, llm_provider: LLMProvider):
        """Initialize with an LLM provider"""
        self.llm_provider = llm_provider
    
    async def make_decision(self, 
                        state: AgentState, 
                        options: List[str],
                        context: Optional[str] = None) -> str:
        """Use LLM to make a decision based on state and options"""
        # Construct prompt for decision making
        context_msg = context if context else "Based on the current state and conversation"
        options_text = "\n".join([f"{i+1}. {option}" for i, option in enumerate(options)])
        
        prompt = f"""
{context_msg}, decide which of the following options is the best next step:

{options_text}

Analyze the situation carefully and choose the option number that would be most effective.
Respond with ONLY the option number, without any explanation.
"""
        
        # Build messages for the LLM
        messages = [
            {"role": "system", "content": "You are a decision-making assistant. Your job is to select the best option from a list."},
            {"role": "user", "content": prompt}
        ]
        
        # Get response from LLM
        response = await self.llm_provider.generate_response(messages)
        decision_text = response["message"]["content"].strip()
        
        # Try to extract a number from the response
        try:
            # Look for a number in the response
            import re
            numbers = re.findall(r'\d+', decision_text)
            if numbers:
                option_index = int(numbers[0]) - 1
                if 0 <= option_index < len(options):
                    return options[option_index]
            
            # If we can't extract a number, check if the response contains one of the options
            for option in options:
                if option.lower() in decision_text.lower():
                    return option
            
            # Default to the first option if we can't determine the decision
            return options[0]
            
        except Exception as e:
            logger.error(f"Error parsing decision: {str(e)}")
            return options[0]  # Default to first option on error

class ExecutionMode(Enum):
    """Agent execution modes"""
    SINGLE_STEP = "single_step"  # Execute one step at a time
    AUTO = "auto"                # Execute until completion
    INTERACTIVE = "interactive"  # Execute with user interaction

@dataclass
class ExecutionConfig:
    """Configuration for agent execution"""
    mode: ExecutionMode = ExecutionMode.AUTO
    max_steps: int = 10
    interactive_callback: Optional[Callable[[str, List[str]], str]] = None
    state_manager: Optional[StateManager] = None
    decision_strategy: Optional[DecisionStrategy] = None
    agent_id: str = field(default_factory=lambda: f"agent-{uuid.uuid4()}")

class WorkflowAgent:
    """Advanced workflow-based agent implementation"""
    
    def __init__(self, 
                llm_provider: LLMProvider,
                tools: List[Tool] = None,
                config: ExecutionConfig = None):
        """Initialize the workflow agent"""
        from .react_agent import ReActAgent
        
        self.react_agent = ReActAgent(llm_provider, tools)
        self.config = config or ExecutionConfig()
        
        # Use provided or default components
        self.state_manager = self.config.state_manager or FileStateManager()
        self.decision_strategy = (self.config.decision_strategy or 
                                 LLMDecisionStrategy(llm_provider))
    
    async def execute_workflow(self, 
                          query: str, 
                          workflow: List[Dict[str, Any]]) -> Tuple[str, AgentState]:
        """Execute a predefined workflow with decision points"""
        # Load or create state
        state = await self.state_manager.load_state(self.config.agent_id)
        if not state:
            state = AgentState(max_steps=self.config.max_steps, goal=query)
        
        # Add initial query if this is a new conversation
        if not state.conversation_history:
            state.add_message("user", query)
        
        # Execute workflow nodes
        result = ""
        current_node_id = workflow[0]["id"]  # Start with the first node
        
        while current_node_id and not state.exceeded_max_steps:
            # Find current node
            current_node = next((node for node in workflow if node["id"] == current_node_id), None)
            if not current_node:
                break
            
            # Execute node action
            node_type = current_node["type"]
            
            if node_type == "react_step":
                # Execute one step of the ReAct agent
                thought, is_done = await self.react_agent.step(state)
                result = thought
                
                # Save state after each step
                await self.state_manager.save_state(self.config.agent_id, state)
                
                # Determine next node
                if is_done:
                    current_node_id = current_node.get("next_on_done")
                else:
                    current_node_id = current_node.get("next")
            
            elif node_type == "decision":
                # Make a decision to determine the next node
                options = current_node["options"]
                option_keys = list(options.keys())
                
                if self.config.mode == ExecutionMode.INTERACTIVE and self.config.interactive_callback:
                    # Get decision from user interaction
                    decision = self.config.interactive_callback(
                        current_node.get("prompt", "Choose an option:"),
                        option_keys
                    )
                else:
                    # Use decision strategy
                    decision = await self.decision_strategy.make_decision(
                        state, 
                        option_keys,
                        current_node.get("context")
                    )
                
                # Move to the selected option's node
                current_node_id = options.get(decision, current_node.get("default"))
            
            elif node_type == "tool_execution":
                # Execute a specific tool
                tool_name = current_node["tool_name"]
                tool = next((t for t in self.react_agent.tools if t.name == tool_name), None)
                
                if tool:
                    # Get arguments from node config or state
                    args = current_node.get("arguments", {})
                    
                    # Execute tool
                    action = Action(name=tool_name, args=args)
                    observation = await self.react_agent._execute_action(action)
                    
                    # Record result
                    state.add_action_result(action, observation)
                    result = observation.to_str()
                    
                    # Save state
                    await self.state_manager.save_state(self.config.agent_id, state)
                    
                    # Determine next node
                    if observation.status == ActionStatus.SUCCESS:
                        current_node_id = current_node.get("next_on_success")
                    else:
                        current_node_id = current_node.get("next_on_error")
                else:
                    # Tool not found, move to error path
                    current_node_id = current_node.get("next_on_error")
            
            elif node_type == "final":
                # Final node, end workflow
                result = current_node.get("message", result)
                break
            
            # Increment step count
            state.step_count += 1
            
            # Handle single step mode
            if self.config.mode == ExecutionMode.SINGLE_STEP:
                break
        
        return result, state
    
    async def execute(self, query: str) -> Tuple[str, AgentState]:
        """Choose between workflow and autonomous execution"""
        # For simple interface, default to ReAct agent execution
        return await self.react_agent.run(
            query, 
            max_steps=self.config.max_steps
        )
```

## Implementing a Complete Agent System

Let's create an example of how to use our custom agent framework:

```python
import asyncio
import logging
from dotenv import load_dotenv
import os

from agent_framework.react_agent import (
    OpenAIProvider, ReActAgent, SearchTool, CalculatorTool, AgentState
)
from agent_framework.advanced_components import (
    ExecutionConfig, ExecutionMode, FileStateManager, WorkflowAgent
)

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("AgentExample")

async def simple_react_example():
    """Example of using the basic ReAct agent"""
    logger.info("Running simple ReAct agent example")
    
    # Initialize LLM provider
    llm = OpenAIProvider(model="gpt-4")
    
    # Create tools
    tools = [
        SearchTool(),
        CalculatorTool()
    ]
    
    # Create ReAct agent
    agent = ReActAgent(llm, tools)
    
    # Run agent with a query
    query = "Who developed Python programming language? Also, what is 123 + 456?"
    result, state = await agent.run(query, max_steps=5)
    
    # Display results
    logger.info(f"Query: {query}")
    logger.info(f"Final answer: {result}")
    logger.info(f"Steps taken: {state.step_count}")
    
    # Show action history
    logger.info("Action history:")
    for i, (action, observation) in enumerate(state.action_history):
        logger.info(f"Step {i+1}:")
        logger.info(f"  Action: {action.name}")
        logger.info(f"  Arguments: {action.args}")
        logger.info(f"  Observation: {observation.to_str()[:100]}...")
    
    return state

async def workflow_agent_example():
    """Example of using the workflow agent"""
    logger.info("Running workflow agent example")
    
    # Initialize LLM provider
    llm = OpenAIProvider(model="gpt-4")
    
    # Create tools
    tools = [
        SearchTool(),
        CalculatorTool()
    ]
    
    # Create state manager
    state_manager = FileStateManager("./agent_states")
    
    # Create execution config
    config = ExecutionConfig(
        mode=ExecutionMode.AUTO,
        max_steps=10,
        state_manager=state_manager,
        agent_id="demo-workflow-agent"
    )
    
    # Create workflow agent
    agent = WorkflowAgent(llm, tools, config)
    
    # Define a simple workflow
    workflow = [
        {
            "id": "start",
            "type": "decision",
            "prompt": "Should I perform a search or calculation first?",
            "options": {
                "search": "do_search",
                "calculation": "do_calculation"
            },
            "default": "do_search",
            "context": "The user wants information about Python and also a calculation."
        },
        {
            "id": "do_search",
            "type": "tool_execution",
            "tool_name": "search",
            "arguments": {"query": "Python programming language"},
            "next_on_success": "do_calculation",
            "next_on_error": "react_fallback"
        },
        {
            "id": "do_calculation", 
            "type": "tool_execution",
            "tool_name": "calculator",
            "arguments": {"expression": "123 + 456"},
            "next_on_success": "finalize",
            "next_on_error": "react_fallback" 
        },
        {
            "id": "react_fallback",
            "type": "react_step",
            "next": "react_fallback",
            "next_on_done": "finalize"
        },
        {
            "id": "finalize",
            "type": "final",
            "message": "I've completed the tasks you requested."
        }
    ]
    
    # Execute the workflow
    query = "Tell me about Python and calculate 123 + 456"
    result, state = await agent.execute_workflow(query, workflow)
    
    # Display results
    logger.info(f"Query: {query}")
    logger.info(f"Final result: {result}")
    logger.info(f"Steps taken: {state.step_count}")
    
    return state

async def interactive_agent_example():
    """Example of using the agent interactively"""
    logger.info("Running interactive agent example")
    
    # Initialize LLM provider
    llm = OpenAIProvider(model="gpt-4")
    
    # Create tools
    tools = [
        SearchTool(),
        CalculatorTool()
    ]
    
    # Create ReAct agent
    agent = ReActAgent(llm, tools)
    state = AgentState(max_steps=10)
    
    # Interactive loop
    print("Interactive Agent Demo (type 'exit' to quit)")
    
    while True:
        # Get user input
        user_input = input("\nYou: ")
        if user_input.lower() == "exit":
            break
        
        # Run a single step
        thought, is_done = await agent.step(state, user_input)
        print(f"\nAgent: {thought}")
        
        # Continue with ReAct loop until done
        while not is_done and not state.exceeded_max_steps:
            # Ask if user wants to continue
            continue_input = input("\nContinue agent reasoning? (y/n): ")
            if continue_input.lower() != "y":
                break
                
            # Run next step
            thought, is_done = await agent.step(state)
            print(f"\nAgent: {thought}")
            
            # Show any actions taken
            if state.action_history:
                last_action, last_obs = state.action_history[-1]
                print(f"\nAction taken: {last_action.name}")
                print(f"Result: {last_obs.to_str()[:100]}")
    
    return state

if __name__ == "__main__":
    # Run the examples
    loop = asyncio.get_event_loop()
    
    # Uncomment the example you want to run
    # state = loop.run_until_complete(simple_react_example())
    # state = loop.run_until_complete(workflow_agent_example())
    state = loop.run_until_complete(interactive_agent_example())
```

## Conclusion

Building a custom agent framework provides unparalleled flexibility and control over AI agents' behavior. Through this exploration, we've developed a comprehensive system that encompasses:

1. **Agent Architecture Types**
   - Autonomous agents that decide their own actions
   - Workflow agents that follow predefined processes with decision points

2. **ReAct Pattern Implementation**
   - Alternating between reasoning and actions
   - Maintaining context and history between steps
   - Tool integration for extending agent capabilities

3. **State Management**
   - Conversation history tracking
   - Action and observation recording
   - Persistent state storage and retrieval

4. **Decision Making**
   - LLM-based decision strategies
   - Structured option evaluation
   - Interactive and automatic modes

5. **Execution Flow Control**
   - Step-by-step execution
   - Workflow-based orchestration
   - Cycle detection and handling

Custom frameworks are particularly valuable when existing tools don't meet specific requirements, when deeper integration with proprietary systems is needed, or when greater control over agent behavior is required. The approach demonstrated here provides a solid foundation that can be extended to support more advanced features like planning, reflection, and multi-agent collaboration.

When building your own agent framework, focus on modularity and separation of concerns so components can be reused and recombined to support diverse use cases. With this foundation, you can create agents that are tailored exactly to your needs while maintaining the flexibility to evolve as requirements change.