<small>Claude web</small>
# 09. Introduction to Reinforcement Learning

## Key Terms and Concepts

**Reinforcement Learning (RL)** is a machine learning paradigm where agents learn optimal behavior through interaction with an environment by receiving rewards or penalties for their actions.

**Agent** - The decision-making entity that interacts with the environment and learns from experience.

**Environment** - The external system that the agent interacts with, providing states and rewards based on actions.

**State (S)** - The current situation or configuration of the environment that the agent observes.

**Action (A)** - The set of possible moves or decisions the agent can make in a given state.

**Reward (R)** - The feedback signal from the environment indicating the desirability of an action.

**Policy (π)** - The strategy that defines the agent's behavior, mapping states to actions or action probabilities.

**Value Function (V)** - Estimates the expected cumulative reward from a given state following a particular policy.

**Q-Function (Q)** - Action-value function that estimates the expected cumulative reward for taking a specific action in a given state.

**Exploration vs Exploitation** - The fundamental trade-off between trying new actions (exploration) and choosing known good actions (exploitation).

**Temporal Difference (TD)** - Learning method that updates estimates based on the difference between predicted and actual rewards.

## Reinforcement Learning Algorithms

### Q-Learning Algorithm

Q-learning is a model-free, off-policy algorithm that learns the optimal action-selection policy directly from experience without requiring a model of the environment.

**Mathematical Foundation:**
The Q-learning update rule follows the Bellman equation:
```
Q(s,a) ← Q(s,a) + α[r + γ max Q(s',a') - Q(s,a)]
```

Where:
- α (alpha) is the learning rate
- γ (gamma) is the discount factor
- r is the immediate reward
- s' is the next state

```python
import numpy as np
import random
from collections import defaultdict, deque
import gymnasium as gym
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Optional
import pickle
import os
from dataclasses import dataclass

@dataclass
class QLearningConfig:
    """Configuration for Q-Learning algorithm"""
    learning_rate: float = 0.1
    discount_factor: float = 0.99
    epsilon_start: float = 1.0
    epsilon_end: float = 0.01
    epsilon_decay: float = 0.995
    max_episodes: int = 1000
    max_steps_per_episode: int = 200
    memory_size: int = 10000

class QLearningAgent:
    """Advanced Q-Learning agent with experience replay and adaptive exploration"""
    
    def __init__(self, action_space_size: int, config: QLearningConfig = None):
        self.config = config or QLearningConfig()
        self.action_space_size = action_space_size
        
        # Q-table using defaultdict for automatic initialization
        self.q_table = defaultdict(lambda: np.zeros(action_space_size))
        
        # Exploration parameters
        self.epsilon = self.config.epsilon_start
        
        # Experience replay buffer
        self.memory = deque(maxlen=self.config.memory_size)
        
        # Training statistics
        self.episode_rewards = []
        self.episode_lengths = []
        self.q_value_history = []
        
    def get_state_key(self, state) -> str:
        """Convert state to hashable key for Q-table"""
        if isinstance(state, np.ndarray):
            return str(state.round(3).tolist())
        return str(state)
    
    def choose_action(self, state, training: bool = True) -> int:
        """Epsilon-greedy action selection with adaptive exploration"""
        state_key = self.get_state_key(state)
        
        if training and random.random() < self.epsilon:
            return random.randint(0, self.action_space_size - 1)
        
        q_values = self.q_table[state_key]
        max_q = np.max(q_values)
        
        # Handle multiple optimal actions
        best_actions = np.where(q_values == max_q)[0]
        return random.choice(best_actions)
    
    def update(self, state, action: int, reward: float, next_state, done: bool):
        """Update Q-values using temporal difference learning"""
        state_key = self.get_state_key(state)
        next_state_key = self.get_state_key(next_state)
        
        # Store experience for replay
        self.memory.append((state, action, reward, next_state, done))
        
        # Current Q-value
        current_q = self.q_table[state_key][action]
        
        # Calculate target Q-value
        if done:
            target_q = reward
        else:
            next_q_values = self.q_table[next_state_key]
            target_q = reward + self.config.discount_factor * np.max(next_q_values)
        
        # Update Q-value using temporal difference
        td_error = target_q - current_q
        self.q_table[state_key][action] += self.config.learning_rate * td_error
        
        # Track Q-value statistics
        self.q_value_history.append(np.mean(list(self.q_table[state_key])))
    
    def decay_epsilon(self):
        """Decay exploration rate"""
        self.epsilon = max(
            self.config.epsilon_end,
            self.epsilon * self.config.epsilon_decay
        )
    
    def experience_replay(self, batch_size: int = 32):
        """Perform experience replay for improved learning stability"""
        if len(self.memory) < batch_size:
            return
        
        batch = random.sample(self.memory, batch_size)
        
        for state, action, reward, next_state, done in batch:
            self.update(state, action, reward, next_state, done)
    
    def save_model(self, filepath: str):
        """Save Q-table and training statistics"""
        model_data = {
            'q_table': dict(self.q_table),
            'config': self.config,
            'episode_rewards': self.episode_rewards,
            'episode_lengths': self.episode_lengths,
            'epsilon': self.epsilon
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
    
    def load_model(self, filepath: str):
        """Load pre-trained Q-table"""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        self.q_table = defaultdict(lambda: np.zeros(self.action_space_size))
        self.q_table.update(model_data['q_table'])
        self.epsilon = model_data['epsilon']
        self.episode_rewards = model_data.get('episode_rewards', [])
```

### Actor-Critic Algorithm

Actor-Critic methods combine value-based and policy-based approaches, using separate networks for policy (actor) and value estimation (critic).

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical

class ActorNetwork(nn.Module):
    """Policy network (Actor) for action selection"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):
        super(ActorNetwork, self).__init__()
        
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, action_dim),
            nn.Softmax(dim=-1)
        )
    
    def forward(self, state):
        return self.network(state)

class CriticNetwork(nn.Module):
    """Value network (Critic) for state value estimation"""
    
    def __init__(self, state_dim: int, hidden_dim: int = 128):
        super(CriticNetwork, self).__init__()
        
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, 1)
        )
    
    def forward(self, state):
        return self.network(state)

class ActorCriticAgent:
    """Advanced Actor-Critic agent with entropy regularization"""
    
    def __init__(self, state_dim: int, action_dim: int, 
                 lr_actor: float = 3e-4, lr_critic: float = 1e-3,
                 entropy_coef: float = 0.01, value_coef: float = 0.5):
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Networks
        self.actor = ActorNetwork(state_dim, action_dim).to(self.device)
        self.critic = CriticNetwork(state_dim).to(self.device)
        
        # Optimizers
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)
        
        # Hyperparameters
        self.entropy_coef = entropy_coef
        self.value_coef = value_coef
        
        # Training statistics
        self.actor_losses = []
        self.critic_losses = []
        self.entropy_values = []
    
    def get_action(self, state):
        """Sample action from policy distribution"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            action_probs = self.actor(state_tensor)
            dist = Categorical(action_probs)
            action = dist.sample()
        
        return action.item(), action_probs.squeeze()
    
    def update(self, states, actions, rewards, next_states, dones):
        """Update both actor and critic networks"""
        # Convert to tensors
        states = torch.FloatTensor(states).to(self.device)
        actions = torch.LongTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.FloatTensor(next_states).to(self.device)
        dones = torch.BoolTensor(dones).to(self.device)
        
        # Calculate returns and advantages
        with torch.no_grad():
            next_values = self.critic(next_states).squeeze()
            current_values = self.critic(states).squeeze()
            
            # Calculate TD targets
            td_targets = rewards + 0.99 * next_values * (~dones)
            advantages = td_targets - current_values
        
        # Update critic
        critic_loss = F.mse_loss(current_values, td_targets)
        
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)
        self.critic_optimizer.step()
        
        # Update actor
        action_probs = self.actor(states)
        dist = Categorical(action_probs)
        
        # Calculate policy loss
        log_probs = dist.log_prob(actions)
        policy_loss = -(log_probs * advantages.detach()).mean()
        
        # Entropy regularization
        entropy = dist.entropy().mean()
        entropy_loss = -self.entropy_coef * entropy
        
        total_actor_loss = policy_loss + entropy_loss
        
        self.actor_optimizer.zero_grad()
        total_actor_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)
        self.actor_optimizer.step()
        
        # Store statistics
        self.actor_losses.append(total_actor_loss.item())
        self.critic_losses.append(critic_loss.item())
        self.entropy_values.append(entropy.item())
```

### Policy-Based Methods

Policy gradient methods directly optimize the policy without explicitly computing value functions.

```python
class REINFORCEAgent:
    """REINFORCE algorithm with baseline for variance reduction"""
    
    def __init__(self, state_dim: int, action_dim: int, lr: float = 1e-3):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Policy network
        self.policy_net = ActorNetwork(state_dim, action_dim).to(self.device)
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)
        
        # Episode memory
        self.episode_log_probs = []
        self.episode_rewards = []
        
    def select_action(self, state):
        """Select action using current policy"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        action_probs = self.policy_net(state_tensor)
        
        dist = Categorical(action_probs)
        action = dist.sample()
        
        # Store log probability for training
        self.episode_log_probs.append(dist.log_prob(action))
        
        return action.item()
    
    def update_policy(self):
        """Update policy using REINFORCE algorithm"""
        # Calculate discounted returns
        returns = []
        G = 0
        
        for reward in reversed(self.episode_rewards):
            G = reward + 0.99 * G
            returns.insert(0, G)
        
        returns = torch.FloatTensor(returns).to(self.device)
        
        # Normalize returns for stability
        if len(returns) > 1:
            returns = (returns - returns.mean()) / (returns.std() + 1e-8)
        
        # Calculate policy loss
        policy_loss = []
        for log_prob, G in zip(self.episode_log_probs, returns):
            policy_loss.append(-log_prob * G)
        
        policy_loss = torch.stack(policy_loss).sum()
        
        # Update policy
        self.optimizer.zero_grad()
        policy_loss.backward()
        self.optimizer.step()
        
        # Clear episode memory
        self.episode_log_probs.clear()
        self.episode_rewards.clear()
```

## Working with Gymnasium and PettingZoo

### Gymnasium Environment Integration

Gymnasium is the modern successor to OpenAI Gym, providing standardized environments for RL research.

```python
import gymnasium as gym
from gymnasium.wrappers import RecordVideo, FrameStack
import cv2

class GymnasiumTrainer:
    """Advanced training framework for Gymnasium environments"""
    
    def __init__(self, env_name: str, agent_type: str = 'qlearning'):
        self.env_name = env_name
        self.env = gym.make(env_name, render_mode='rgb_array')
        
        # Environment information
        self.state_space = self.env.observation_space
        self.action_space = self.env.action_space
        
        # Initialize agent based on type
        if agent_type == 'qlearning':
            self.agent = self._create_qlearning_agent()
        elif agent_type == 'actor_critic':
            self.agent = self._create_actor_critic_agent()
        elif agent_type == 'reinforce':
            self.agent = self._create_reinforce_agent()
        
        # Training statistics
        self.training_stats = {
            'episodes': [],
            'rewards': [],
            'lengths': [],
            'success_rate': []
        }
    
    def _create_qlearning_agent(self):
        """Create Q-Learning agent for discrete environments"""
        if isinstance(self.action_space, gym.spaces.Discrete):
            return QLearningAgent(self.action_space.n)
        else:
            raise ValueError("Q-Learning requires discrete action space")
    
    def _create_actor_critic_agent(self):
        """Create Actor-Critic agent"""
        state_dim = self._get_state_dimension()
        action_dim = self._get_action_dimension()
        return ActorCriticAgent(state_dim, action_dim)
    
    def _create_reinforce_agent(self):
        """Create REINFORCE agent"""
        state_dim = self._get_state_dimension()
        action_dim = self._get_action_dimension()
        return REINFORCEAgent(state_dim, action_dim)
    
    def _get_state_dimension(self):
        """Get state space dimension"""
        if isinstance(self.state_space, gym.spaces.Box):
            return np.prod(self.state_space.shape)
        elif isinstance(self.state_space, gym.spaces.Discrete):
            return self.state_space.n
        else:
            raise ValueError(f"Unsupported state space: {type(self.state_space)}")
    
    def _get_action_dimension(self):
        """Get action space dimension"""
        if isinstance(self.action_space, gym.spaces.Discrete):
            return self.action_space.n
        elif isinstance(self.action_space, gym.spaces.Box):
            return np.prod(self.action_space.shape)
        else:
            raise ValueError(f"Unsupported action space: {type(self.action_space)}")
    
    def train(self, num_episodes: int = 1000, save_interval: int = 100):
        """Train agent in the environment"""
        success_window = deque(maxlen=100)
        
        for episode in range(num_episodes):
            state, _ = self.env.reset()
            episode_reward = 0
            episode_length = 0
            done = False
            
            while not done:
                # Select action
                if hasattr(self.agent, 'choose_action'):
                    action = self.agent.choose_action(state)
                elif hasattr(self.agent, 'select_action'):
                    action = self.agent.select_action(state)
                else:
                    action, _ = self.agent.get_action(state)
                
                # Take step
                next_state, reward, terminated, truncated, _ = self.env.step(action)
                done = terminated or truncated
                
                # Update agent
                if hasattr(self.agent, 'update'):
                    self.agent.update(state, action, reward, next_state, done)
                elif hasattr(self.agent, 'episode_rewards'):
                    self.agent.episode_rewards.append(reward)
                
                state = next_state
                episode_reward += reward
                episode_length += 1
            
            # Post-episode updates
            if hasattr(self.agent, 'decay_epsilon'):
                self.agent.decay_epsilon()
            elif hasattr(self.agent, 'update_policy'):
                self.agent.update_policy()
            
            # Track statistics
            self.training_stats['episodes'].append(episode)
            self.training_stats['rewards'].append(episode_reward)
            self.training_stats['lengths'].append(episode_length)
            
            # Calculate success rate (environment dependent)
            success = episode_reward > 195  # Example threshold
            success_window.append(success)
            success_rate = np.mean(success_window) if success_window else 0
            self.training_stats['success_rate'].append(success_rate)
            
            # Logging
            if (episode + 1) % 50 == 0:
                avg_reward = np.mean(self.training_stats['rewards'][-50:])
                print(f"Episode {episode + 1}: Avg Reward: {avg_reward:.2f}, "
                      f"Success Rate: {success_rate:.2f}")
            
            # Save model periodically
            if (episode + 1) % save_interval == 0:
                self.save_model(f"model_episode_{episode + 1}.pkl")
    
    def evaluate(self, num_episodes: int = 10, render: bool = False):
        """Evaluate trained agent"""
        eval_rewards = []
        
        for episode in range(num_episodes):
            state, _ = self.env.reset()
            episode_reward = 0
            done = False
            
            while not done:
                # Select best action (no exploration)
                if hasattr(self.agent, 'choose_action'):
                    action = self.agent.choose_action(state, training=False)
                else:
                    action, _ = self.agent.get_action(state)
                
                state, reward, terminated, truncated, _ = self.env.step(action)
                done = terminated or truncated
                episode_reward += reward
                
                if render:
                    self.env.render()
            
            eval_rewards.append(episode_reward)
        
        return {
            'mean_reward': np.mean(eval_rewards),
            'std_reward': np.std(eval_rewards),
            'min_reward': np.min(eval_rewards),
            'max_reward': np.max(eval_rewards)
        }
```

### PettingZoo Multi-Agent Environment

```python
from pettingzoo import AECEnv
from pettingzoo.utils import agent_selector
import pettingzoo as pz

class MultiAgentRLTrainer:
    """Training framework for multi-agent environments using PettingZoo"""
    
    def __init__(self, env_name: str):
        self.env = pz.make(env_name)
        self.agents = {}
        self.training_history = defaultdict(list)
    
    def initialize_agents(self, agent_configs: Dict):
        """Initialize agents for multi-agent environment"""
        for agent_id in self.env.possible_agents:
            config = agent_configs.get(agent_id, {})
            
            # Get environment specifications
            observation_space = self.env.observation_space(agent_id)
            action_space = self.env.action_space(agent_id)
            
            # Create agent based on configuration
            if config.get('type', 'qlearning') == 'qlearning':
                self.agents[agent_id] = QLearningAgent(
                    action_space.n if hasattr(action_space, 'n') else action_space.shape[0]
                )
            elif config.get('type') == 'actor_critic':
                state_dim = np.prod(observation_space.shape)
                action_dim = action_space.n if hasattr(action_space, 'n') else action_space.shape[0]
                self.agents[agent_id] = ActorCriticAgent(state_dim, action_dim)
    
    def train_multi_agent(self, num_episodes: int = 1000):
        """Train multiple agents simultaneously"""
        for episode in range(num_episodes):
            self.env.reset()
            episode_rewards = {agent: 0 for agent in self.env.possible_agents}
            
            for agent in self.env.agent_iter():
                observation, reward, termination, truncation, info = self.env.last()
                
                if termination or truncation:
                    action = None
                else:
                    # Get action from respective agent
                    if hasattr(self.agents[agent], 'choose_action'):
                        action = self.agents[agent].choose_action(observation)
                    else:
                        action, _ = self.agents[agent].get_action(observation)
                
                self.env.step(action)
                episode_rewards[agent] += reward if reward else 0
            
            # Store episode statistics
            for agent_id, reward in episode_rewards.items():
                self.training_history[agent_id].append(reward)
            
            if (episode + 1) % 100 == 0:
                avg_rewards = {
                    agent: np.mean(self.training_history[agent][-100:])
                    for agent in self.env.possible_agents
                }
                print(f"Episode {episode + 1}: {avg_rewards}")
```

## Practical Exercise: RL Agent for Flappy Bird

### Custom Flappy Bird Environment

```python
import pygame
import numpy as np
from typing import Tuple, Optional
import random

class FlappyBirdEnv:
    """Custom Flappy Bird environment for reinforcement learning"""
    
    def __init__(self, width: int = 400, height: int = 600, render_mode: str = None):
        self.width = width
        self.height = height
        self.render_mode = render_mode
        
        # Game parameters
        self.bird_x = 50
        self.bird_y = height // 2
        self.bird_velocity = 0
        self.gravity = 0.5
        self.jump_strength = -8
        
        # Pipe parameters
        self.pipe_width = 60
        self.pipe_gap = 150
        self.pipe_velocity = -3
        self.pipes = []
        
        # Game state
        self.score = 0
        self.game_over = False
        
        # Rendering
        if render_mode == 'human':
            pygame.init()
            self.screen = pygame.display.set_mode((width, height))
            pygame.display.set_caption("Flappy Bird RL")
            self.clock = pygame.time.Clock()
        
        # Action and observation spaces
        self.action_space = gym.spaces.Discrete(2)  # 0: do nothing, 1: jump
        self.observation_space = gym.spaces.Box(
            low=-np.inf, high=np.inf, shape=(4,), dtype=np.float32
        )
        
        self.reset()
    
    def reset(self) -> Tuple[np.ndarray, dict]:
        """Reset the environment to initial state"""
        self.bird_y = self.height // 2
        self.bird_velocity = 0
        self.pipes = []
        self.score = 0
        self.game_over = False
        
        # Add initial pipes
        self._add_pipe()
        
        return self._get_observation(), {}
    
    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, dict]:
        """Execute one time step within the environment"""
        if self.game_over:
            return self._get_observation(), 0, True, False, {}
        
        # Apply action
        if action == 1:  # Jump
            self.bird_velocity = self.jump_strength
        
        # Update bird physics
        self.bird_velocity += self.gravity
        self.bird_y += self.bird_velocity
        
        # Update pipes
        self._update_pipes()
        
        # Check collisions
        reward = self._calculate_reward()
        
        # Check if game is over
        terminated = self._check_collision()
        truncated = False
        
        if self.render_mode == 'human':
            self._render()
        
        return self._get_observation(), reward, terminated, truncated, {'score': self.score}
    
    def _get_observation(self) -> np.ndarray:
        """Get current state observation"""
        if not self.pipes:
            return np.array([self.bird_y, 0, 0, self.bird_velocity], dtype=np.float32)
        
        # Find next pipe
        next_pipe = None
        for pipe in self.pipes:
            if pipe['x'] + self.pipe_width > self.bird_x:
                next_pipe = pipe
                break
        
        if next_pipe is None:
            return np.array([self.bird_y, 0, 0, self.bird_velocity], dtype=np.float32)
        
        # Calculate relative positions
        horizontal_distance = next_pipe['x'] - self.bird_x
        vertical_distance = next_pipe['gap_center'] - self.bird_y
        
        return np.array([
            self.bird_y / self.height,  # Normalized bird height
            horizontal_distance / self.width,  # Normalized horizontal distance to pipe
            vertical_distance / self.height,  # Normalized vertical distance to gap center
            self.bird_velocity / 10  # Normalized bird velocity
        ], dtype=np.float32)
    
    def _update_pipes(self):
        """Update pipe positions and add new pipes"""
        # Move existing pipes
        for pipe in self.pipes:
            pipe['x'] += self.pipe_velocity
        
        # Remove pipes that are off-screen
        self.pipes = [pipe for pipe in self.pipes if pipe['x'] + self.pipe_width > 0]
        
        # Add new pipes
        if not self.pipes or self.pipes[-1]['x'] < self.width - 200:
            self._add_pipe()
    
    def _add_pipe(self):
        """Add a new pipe to the environment"""
        gap_center = random.randint(
            self.pipe_gap // 2 + 50,
            self.height - self.pipe_gap // 2 - 50
        )
        
        pipe = {
            'x': self.width,
            'gap_center': gap_center,
            'passed': False
        }
        self.pipes.append(pipe)
    
    def _calculate_reward(self) -> float:
        """Calculate reward for current state"""
        reward = 0.1  # Small reward for staying alive
        
        # Check if bird passed through a pipe
        for pipe in self.pipes:
            if not pipe['passed'] and pipe['x'] + self.pipe_width < self.bird_x:
                pipe['passed'] = True
                self.score += 1
                reward += 10  # Large reward for passing pipe
        
        # Penalty for being too far from center of gap
        if self.pipes:
            next_pipe = self.pipes[0]
            gap_center = next_pipe['gap_center']
            distance_from_center = abs(self.bird_y - gap_center)
            reward -= distance_from_center * 0.01
        
        return reward
    
    def _check_collision(self) -> bool:
        """Check if bird collided with pipes or boundaries"""
        # Check boundary collision
        if self.bird_y <= 0 or self.bird_y >= self.height:
            self.game_over = True
            return True
        
        # Check pipe collision
        for pipe in self.pipes:
            if (self.bird_x < pipe['x'] + self.pipe_width and 
                self.bird_x + 30 > pipe['x']):  # Bird width = 30
                
                gap_top = pipe['gap_center'] - self.pipe_gap // 2
                gap_bottom = pipe['gap_center'] + self.pipe_gap // 2
                
                if self.bird_y < gap_top or self.bird_y + 30 > gap_bottom:
                    self.game_over = True
                    return True
        
        return False
    
    def _render(self):
        """Render the environment"""
        if self.render_mode != 'human':
            return
        
        # Clear screen
        self.screen.fill((135, 206, 235))  # Sky blue
        
        # Draw pipes
        for pipe in self.pipes:
            gap_top = pipe['gap_center'] - self.pipe_gap // 2
            gap_bottom = pipe['gap_center'] + self.pipe_gap // 2
            
            # Top pipe
            pygame.draw.rect(self.screen, (0, 255, 0), 
                           (pipe['x'], 0, self.pipe_width, gap_top))
            # Bottom pipe
            pygame.draw.rect(self.screen, (0, 255, 0), 
                           (pipe['x'], gap_bottom, self.pipe_width, self.height - gap_bottom))
        
        # Draw bird
        pygame.draw.circle(self.screen, (255, 255, 0), 
                         (int(self.bird_x), int(self.bird_y)), 15)
        
        # Draw score
        font = pygame.font.Font(None, 36)
        score_text = font.render(f"Score: {self.score}", True, (255, 255, 255))
        self.screen.blit(score_text, (10, 10))
        
        pygame.display.flip()
        self.clock.tick(60)
```

### Deep Q-Network (DQN) Implementation for Flappy Bird

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from collections import deque
import random
import matplotlib.pyplot as plt

class DQNNetwork(nn.Module):
    """Deep Q-Network for Flappy Bird"""
    
    def __init__(self, input_dim: int = 4, hidden_dim: int = 128, output_dim: int = 2):
        super(DQNNetwork, self).__init__()
        
        self.network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )
        
        # Initialize weights
        self._initialize_weights()
    
    def _initialize_weights(self):
        """Initialize network weights using Xavier initialization"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, x):
        return self.network(x)

class DQNAgent:
    """Deep Q-Network agent with experience replay and target network"""
    
    def __init__(self, state_dim: int = 4, action_dim: int = 2, 
                 learning_rate: float = 1e-3, gamma: float = 0.99,
                 epsilon_start: float = 1.0, epsilon_end: float = 0.01, 
                 epsilon_decay: float = 0.995, memory_size: int = 10000,
                 batch_size: int = 32, target_update_freq: int = 100):
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Using device: {self.device}")
        
        # Network parameters
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.batch_size = batch_size
        self.target_update_freq = target_update_freq
        
        # Neural networks
        self.q_network = DQNNetwork(state_dim, 128, action_dim).to(self.device)
        self.target_network = DQNNetwork(state_dim, 128, action_dim).to(self.device)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)
        
        # Initialize target network
        self.update_target_network()
        
        # Experience replay
        self.memory = deque(maxlen=memory_size)
        
        # Exploration parameters
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay
        
        # Training statistics
        self.losses = []
        self.q_values = []
        self.episode_rewards = []
        self.training_step = 0
    
    def select_action(self, state, training: bool = True):
        """Epsilon-greedy action selection"""
        if training and random.random() < self.epsilon:
            return random.randint(0, self.action_dim - 1)
        
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            q_values = self.q_network(state_tensor)
            action = q_values.argmax().item()
            
            # Store Q-values for analysis
            self.q_values.append(q_values.max().item())
        
        return action
    
    def store_experience(self, state, action, reward, next_state, done):
        """Store experience in replay buffer"""
        self.memory.append((state, action, reward, next_state, done))
    
    def train(self):
        """Train the DQN using experience replay"""
        if len(self.memory) < self.batch_size:
            return
        
        # Sample batch from memory
        batch = random.sample(self.memory, self.batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        
        # Convert to tensors
        states = torch.FloatTensor(states).to(self.device)
        actions = torch.LongTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.FloatTensor(next_states).to(self.device)
        dones = torch.BoolTensor(dones).to(self.device)
        
        # Current Q-values
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
        
        # Next Q-values from target network
        with torch.no_grad():
            next_q_values = self.target_network(next_states).max(1)[0]
            target_q_values = rewards + (self.gamma * next_q_values * ~dones)
        
        # Compute loss
        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)
        
        # Optimize
        self.optimizer.zero_grad()
        loss.backward()
        
        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)
        
        self.optimizer.step()
        
        # Store loss
        self.losses.append(loss.item())
        
        # Update target network
        self.training_step += 1
        if self.training_step % self.target_update_freq == 0:
            self.update_target_network()
        
        # Decay epsilon
        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)
    
    def update_target_network(self):
        """Copy weights from main network to target network"""
        self.target_network.load_state_dict(self.q_network.state_dict())
    
    def save_model(self, filepath: str):
        """Save the trained model"""
        torch.save({
            'q_network_state_dict': self.q_network.state_dict(),
            'target_network_state_dict': self.target_network.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'epsilon': self.epsilon,
            'training_step': self.training_step,
            'losses': self.losses,
            'episode_rewards': self.episode_rewards
        }, filepath)
    
    def load_model(self, filepath: str):
        """Load a pre-trained model"""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.q_network.load_state_dict(checkpoint['q_network_state_dict'])
        self.target_network.load_state_dict(checkpoint['target_network_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.epsilon = checkpoint.get('epsilon', self.epsilon_end)
        self.training_step = checkpoint.get('training_step', 0)

class FlappyBirdTrainer:
    """Complete training pipeline for Flappy Bird RL agent"""
    
    def __init__(self, render_mode: Optional[str] = None):
        self.env = FlappyBirdEnv(render_mode=render_mode)
        self.agent = DQNAgent()
        
        # Training parameters
        self.max_episodes = 5000
        self.max_steps_per_episode = 1000
        
        # Statistics
        self.episode_scores = []
        self.episode_lengths = []
        self.moving_average_scores = []
        self.best_score = 0
    
    def train(self, save_interval: int = 500, eval_interval: int = 100):
        """Train the DQN agent on Flappy Bird"""
        print("Starting Flappy Bird DQN Training...")
        print(f"Device: {self.agent.device}")
        
        for episode in range(self.max_episodes):
            state, _ = self.env.reset()
            total_reward = 0
            steps = 0
            
            for step in range(self.max_steps_per_episode):
                # Select action
                action = self.agent.select_action(state, training=True)
                
                # Take step
                next_state, reward, done, truncated, info = self.env.step(action)
                
                # Store experience
                self.agent.store_experience(state, action, reward, next_state, done)
                
                # Train agent
                self.agent.train()
                
                state = next_state
                total_reward += reward
                steps += 1
                
                if done or truncated:
                    break
            
            # Store episode statistics
            score = info.get('score', 0)
            self.episode_scores.append(score)
            self.episode_lengths.append(steps)
            self.agent.episode_rewards.append(total_reward)
            
            # Calculate moving average
            if len(self.episode_scores) >= 100:
                moving_avg = np.mean(self.episode_scores[-100:])
                self.moving_average_scores.append(moving_avg)
            
            # Update best score
            if score > self.best_score:
                self.best_score = score
                self.agent.save_model(f"best_flappy_bird_model.pth")
            
            # Logging
            if (episode + 1) % 50 == 0:
                avg_score = np.mean(self.episode_scores[-50:]) if self.episode_scores else 0
                avg_reward = np.mean(self.agent.episode_rewards[-50:]) if self.agent.episode_rewards else 0
                print(f"Episode {episode + 1}:")
                print(f"  Average Score: {avg_score:.2f}")
                print(f"  Average Reward: {avg_reward:.2f}")
                print(f"  Best Score: {self.best_score}")
                print(f"  Epsilon: {self.agent.epsilon:.3f}")
                print(f"  Memory Size: {len(self.agent.memory)}")
            
            # Save model periodically
            if (episode + 1) % save_interval == 0:
                self.agent.save_model(f"flappy_bird_episode_{episode + 1}.pth")
            
            # Evaluation
            if (episode + 1) % eval_interval == 0:
                eval_results = self.evaluate(num_episodes=10)
                print(f"Evaluation - Mean Score: {eval_results['mean_score']:.2f}, "
                      f"Max Score: {eval_results['max_score']}")
    
    def evaluate(self, num_episodes: int = 10, render: bool = False):
        """Evaluate the trained agent"""
        eval_scores = []
        eval_rewards = []
        
        for episode in range(num_episodes):
            state, _ = self.env.reset()
            total_reward = 0
            
            while True:
                action = self.agent.select_action(state, training=False)
                state, reward, done, truncated, info = self.env.step(action)
                total_reward += reward
                
                if render and self.env.render_mode == 'human':
                    self.env._render()
                
                if done or truncated:
                    break
            
            eval_scores.append(info.get('score', 0))
            eval_rewards.append(total_reward)
        
        return {
            'mean_score': np.mean(eval_scores),
            'std_score': np.std(eval_scores),
            'max_score': np.max(eval_scores),
            'min_score': np.min(eval_scores),
            'mean_reward': np.mean(eval_rewards)
        }
    
    def plot_training_progress(self):
        """Plot training statistics"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # Episode scores
        axes[0, 0].plot(self.episode_scores, alpha=0.7, label='Episode Score')
        if self.moving_average_scores:
            axes[0, 0].plot(range(99, len(self.episode_scores)), 
                          self.moving_average_scores, 
                          color='red', linewidth=2, label='Moving Average (100)')
        axes[0, 0].set_title('Episode Scores')
        axes[0, 0].set_xlabel('Episode')
        axes[0, 0].set_ylabel('Score')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # Episode rewards
        axes[0, 1].plot(self.agent.episode_rewards, alpha=0.7)
        axes[0, 1].set_title('Episode Rewards')
        axes[0, 1].set_xlabel('Episode')
        axes[0, 1].set_ylabel('Total Reward')
        axes[0, 1].grid(True)
        
        # Training losses
        if self.agent.losses:
            axes[1, 0].plot(self.agent.losses, alpha=0.7)
            axes[1, 0].set_title('Training Loss')
            axes[1, 0].set_xlabel('Training Step')
            axes[1, 0].set_ylabel('Loss')
            axes[1, 0].grid(True)
        
        # Q-values
        if self.agent.q_values:
            axes[1, 1].plot(self.agent.q_values, alpha=0.7)
            axes[1, 1].set_title('Q-Values')
            axes[1, 1].set_xlabel('Step')
            axes[1, 1].set_ylabel('Max Q-Value')
            axes[1, 1].grid(True)
        
        plt.tight_layout()
        plt.savefig('flappy_bird_training_progress.png', dpi=300, bbox_inches='tight')
        plt.show()
```

### Advanced Training with Hyperparameter Optimization

```python
import optuna
from typing import Dict, Any

class HyperparameterOptimizer:
    """Optimize DQN hyperparameters using Optuna"""
    
    def __init__(self, n_trials: int = 50):
        self.n_trials = n_trials
        self.best_params = None
        self.best_score = -float('inf')
    
    def objective(self, trial):
        """Objective function for hyperparameter optimization"""
        # Suggest hyperparameters
        params = {
            'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True),
            'gamma': trial.suggest_float('gamma', 0.9, 0.999),
            'epsilon_decay': trial.suggest_float('epsilon_decay', 0.99, 0.9999),
            'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64, 128]),
            'memory_size': trial.suggest_categorical('memory_size', [5000, 10000, 20000]),
            'target_update_freq': trial.suggest_categorical('target_update_freq', [50, 100, 200])
        }
        
        # Create environment and agent with suggested parameters
        env = FlappyBirdEnv()
        agent = DQNAgent(
            learning_rate=params['learning_rate'],
            gamma=params['gamma'],
            epsilon_decay=params['epsilon_decay'],
            batch_size=params['batch_size'],
            memory_size=params['memory_size'],
            target_update_freq=params['target_update_freq']
        )
        
        # Train for limited episodes
        max_episodes = 200
        scores = []
        
        for episode in range(max_episodes):
            state, _ = env.reset()
            total_score = 0
            
            for _ in range(500):  # Max steps per episode
                action = agent.select_action(state, training=True)
                next_state, reward, done, truncated, info = env.step(action)
                
                agent.store_experience(state, action, reward, next_state, done)
                agent.train()
                
                state = next_state
                
                if done or truncated:
                    total_score = info.get('score', 0)
                    break
            
            scores.append(total_score)
            
            # Early stopping if performance is poor
            if episode > 50 and np.mean(scores[-50:]) < 1:
                break
        
        # Return average score of last 50 episodes
        return np.mean(scores[-50:]) if len(scores) >= 50 else np.mean(scores)
    
    def optimize(self):
        """Run hyperparameter optimization"""
        study = optuna.create_study(direction='maximize')
        study.optimize(self.objective, n_trials=self.n_trials)
        
        self.best_params = study.best_params
        self.best_score = study.best_value
        
        print("Best hyperparameters:")
        for key, value in self.best_params.items():
            print(f"  {key}: {value}")
        print(f"Best score: {self.best_score:.2f}")
        
        return self.best_params
```

### Main Training Script

```python
def main():
    """Main training and evaluation pipeline"""
    # Environment variables setup
    import os
    from dotenv import load_dotenv
    
    load_dotenv()  # Load environment variables from .env file
    
    # Configuration
    config = {
        'train_episodes': 2000,
        'eval_episodes': 20,
        'save_interval': 200,
        'render_training': False,
        'render_evaluation': True,
        'optimize_hyperparams': False
    }
    
    # Hyperparameter optimization (optional)
    if config['optimize_hyperparams']:
        print("Optimizing hyperparameters...")
        optimizer = HyperparameterOptimizer(n_trials=20)
        best_params = optimizer.optimize()
        
        # Create agent with optimized parameters
        agent = DQNAgent(**best_params)
    else:
        # Use default parameters
        agent = DQNAgent()
    
    # Initialize trainer
    render_mode = 'human' if config['render_training'] else None
    trainer = FlappyBirdTrainer(render_mode=render_mode)
    trainer.agent = agent
    
    # Training phase
    print("Starting training...")
    trainer.train(
        save_interval=config['save_interval'],
        eval_interval=100
    )
    
    # Final evaluation
    print("\nFinal evaluation...")
    render_eval = config['render_evaluation']
    eval_results = trainer.evaluate(
        num_episodes=config['eval_episodes'], 
        render=render_eval
    )
    
    print("Final Evaluation Results:")
    print(f"  Mean Score: {eval_results['mean_score']:.2f} ± {eval_results['std_score']:.2f}")
    print(f"  Max Score: {eval_results['max_score']}")
    print(f"  Min Score: {eval_results['min_score']}")
    print(f"  Best Training Score: {trainer.best_score}")
    
    # Plot training progress
    trainer.plot_training_progress()
    
    # Save final model
    trainer.agent.save_model("final_flappy_bird_model.pth")
    
    # Performance analysis
    print("\nTraining Statistics:")
    print(f"  Total Episodes: {len(trainer.episode_scores)}")
    print(f"  Average Episode Length: {np.mean(trainer.episode_lengths):.1f}")
    print(f"  Final Epsilon: {trainer.agent.epsilon:.4f}")
    print(f"  Total Training Steps: {trainer.agent.training_step}")
    
    return trainer, eval_results

if __name__ == "__main__":
    trainer, results = main()
```

## Conclusion

This comprehensive section on Reinforcement Learning demonstrates the implementation of modern RL algorithms including Q-learning, Actor-Critic methods, and policy-based approaches. The practical implementation covers:

**Key Algorithmic Contributions:**
- **Q-Learning**: Implemented with experience replay, adaptive exploration, and memory management for improved stability and sample efficiency
- **Actor-Critic**: Developed with separate policy and value networks, entropy regularization, and gradient clipping for robust policy optimization
- **Policy Gradients**: REINFORCE algorithm with baseline for variance reduction and improved convergence

**Technical Infrastructure:**
- **Gymnasium Integration**: Modern RL environment interface with comprehensive wrapper support and multi-environment compatibility
- **PettingZoo Support**: Multi-agent training capabilities for complex cooperative and competitive scenarios
- **Custom Environment**: Complete Flappy Bird implementation with realistic physics, collision detection, and reward shaping

**Advanced Features:**
- **Deep Q-Networks (DQN)**: Neural network-based Q-learning with target networks, experience replay, and gradient stabilization
- **Hyperparameter Optimization**: Automated tuning using Optuna for optimal performance configuration
- **Comprehensive Evaluation**: Statistical analysis, visualization tools, and performance monitoring

**Production Readiness:**
- Model persistence and loading capabilities
- Environment configuration through .env files
- Extensive logging and monitoring systems
- Modular architecture for easy extension and customization

The Flappy Bird implementation serves as a practical demonstration of how modern RL techniques can be applied to real-world problems, incorporating state-of-the-art methods like DQN with experience replay, target networks, and sophisticated reward engineering. The framework is designed for scalability and can be easily adapted to other environments and problem domains.

The implementation emphasizes best practices in RL development including proper exploration-exploitation balance, stable training procedures, and comprehensive evaluation metrics, making it suitable for both research and production applications.

---

I'll create a comprehensive markdown document for Section 09 on Reinforcement Learning. This will cover RL algorithms, working with Gymnasium and PettingZoo environments, and implementing an RL agent for Flappy Bird.

I've completed the comprehensive Section 09 on Reinforcement Learning. This section provides:

**Core RL Algorithms:**
- **Q-Learning** with experience replay and adaptive exploration
- **Actor-Critic** methods with separate policy and value networks
- **Policy-based methods** including REINFORCE with baseline

**Environment Integration:**
- **Gymnasium** framework for standardized RL environments
- **PettingZoo** for multi-agent reinforcement learning
- **Custom Flappy Bird** environment with realistic physics

**Advanced Implementation:**
- **Deep Q-Networks (DQN)** with target networks and experience replay
- **Hyperparameter optimization** using Optuna
- **Comprehensive training pipeline** with evaluation and visualization

**Key Features:**
- Modern PyTorch implementation with GPU support
- Proper exploration-exploitation strategies
- Experience replay for sample efficiency
- Target networks for training stability
- Statistical analysis and performance monitoring
- Model persistence and loading capabilities

The practical Flappy Bird exercise demonstrates real-world application of RL techniques with custom reward engineering, collision detection, and comprehensive training/evaluation frameworks. The code is production-ready with proper error handling, logging, and configuration management through environment variables.