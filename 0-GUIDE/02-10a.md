<small>Claude 3.7 Sonnet Thinking</small>
# 10. Autogen (Advanced AI Agent Framework)

## Key Terms

- **Autogen**: A framework developed by Microsoft for building LLM-powered multi-agent systems that can collaborate to solve complex tasks.
- **Agent**: An autonomous entity defined by a persona that can interact with other agents and tools to accomplish goals.
- **GroupChat**: A conversation environment where multiple agents can interact with each other.
- **Agent Workflow**: The orchestrated sequence of interactions between agents to solve a task.
- **AutogenStudio**: A visual interface for creating, testing, and deploying Autogen agent workflows.
- **Tool Use**: The capability of agents to invoke external functions to perform actions outside their language model capabilities.
- **Function Calling**: A mechanism enabling LLMs to invoke predefined functions with structured parameters.
- **Conversable Agent**: The base agent class in Autogen that handles message management and response generation.
- **Assistant Agent**: An agent designed to respond to user queries and perform tasks using an LLM.
- **User Proxy Agent**: An agent that can act on behalf of a human user, including running code and using tools.

## Orchestrating AI Agents in Autogen

Autogen enables sophisticated multi-agent orchestration, where agents with different roles collaborate to solve complex problems:

```python
import os
import json
from typing import Dict, List, Any, Optional, Union, Callable
import autogen
from autogen import Agent, AssistantAgent, UserProxyAgent, GroupChat, GroupChatManager
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

class AutogenOrchestrator:
    """
    Orchestrator for building and managing complex multi-agent systems with Autogen.
    """
    
    def __init__(
        self,
        config_path: Optional[str] = None,
        default_llm_config: Optional[Dict[str, Any]] = None,
        verbose: bool = False
    ):
        """
        Initialize the Autogen orchestrator.
        
        Args:
            config_path: Path to a JSON configuration file for agents
            default_llm_config: Default configuration for LLM-based agents
            verbose: Whether to print detailed logs
        """
        self.verbose = verbose
        
        # Set up default LLM config if not provided
        if default_llm_config is None:
            self.default_llm_config = {
                "config_list": [
                    {
                        "model": "gpt-4o",
                        "api_key": os.getenv("OPENAI_API_KEY")
                    }
                ],
                "temperature": 0.1,
                "request_timeout": 120,
                "seed": 42
            }
        else:
            self.default_llm_config = default_llm_config
        
        # Load config from file if provided
        if config_path and os.path.exists(config_path):
            with open(config_path, 'r') as f:
                self.config = json.load(f)
            if self.verbose:
                print(f"Loaded configuration from {config_path}")
        else:
            self.config = {}
        
        # Initialize agent registry
        self.agents = {}
        self.group_chats = {}
    
    def create_assistant_agent(
        self,
        name: str,
        system_message: str,
        llm_config: Optional[Dict[str, Any]] = None,
        human_input_mode: str = "NEVER",
        max_consecutive_auto_reply: int = 10
    ) -> AssistantAgent:
        """
        Create an assistant agent with specified configuration.
        
        Args:
            name: Name of the agent
            system_message: System message defining agent behavior
            llm_config: Configuration for the LLM
            human_input_mode: Mode for human input (NEVER, TERMINATE, or ALWAYS)
            max_consecutive_auto_reply: Maximum consecutive auto replies
            
        Returns:
            The created assistant agent
        """
        if not llm_config:
            llm_config = self.default_llm_config
        
        agent = AssistantAgent(
            name=name,
            system_message=system_message,
            llm_config=llm_config,
            human_input_mode=human_input_mode,
            max_consecutive_auto_reply=max_consecutive_auto_reply
        )
        
        # Register the agent
        self.agents[name] = agent
        
        if self.verbose:
            print(f"Created assistant agent: {name}")
        
        return agent
    
    def create_user_proxy_agent(
        self,
        name: str,
        human_input_mode: str = "TERMINATE",
        system_message: Optional[str] = None,
        code_execution_config: Optional[Dict[str, Any]] = None,
        default_auto_reply: Optional[Union[str, Callable]] = None
    ) -> UserProxyAgent:
        """
        Create a user proxy agent.
        
        Args:
            name: Name of the agent
            human_input_mode: Mode for human input
            system_message: Optional system message
            code_execution_config: Configuration for code execution
            default_auto_reply: Default response when no human input is needed
            
        Returns:
            The created user proxy agent
        """
        # Set up default code execution config if not provided
        if code_execution_config is None:
            code_execution_config = {
                "work_dir": "workspace",
                "use_docker": False,  # Set to True to use docker for code execution
                "last_n_messages": 3,
                "timeout": 60
            }
        
        # Initialize the agent
        agent = UserProxyAgent(
            name=name,
            human_input_mode=human_input_mode,
            system_message=system_message,
            code_execution_config=code_execution_config,
            default_auto_reply=default_auto_reply,
            llm_config=False  # User proxy doesn't use LLM by default
        )
        
        # Register the agent
        self.agents[name] = agent
        
        if self.verbose:
            print(f"Created user proxy agent: {name}")
        
        return agent
    
    def create_group_chat(
        self,
        name: str,
        agents: List[Agent],
        manager_llm_config: Optional[Dict[str, Any]] = None,
        max_round: int = 10,
        speaker_selection_method: str = "auto"
    ) -> GroupChat:
        """
        Create a group chat for multiple agents to collaborate.
        
        Args:
            name: Name of the group chat
            agents: List of agents to include in the chat
            manager_llm_config: LLM config for the group chat manager
            max_round: Maximum rounds of conversation
            speaker_selection_method: Method to select the next speaker
            
        Returns:
            The created group chat
        """
        if not manager_llm_config:
            manager_llm_config = self.default_llm_config
        
        # Create the group chat
        group_chat = GroupChat(
            agents=agents,
            messages=[],
            max_round=max_round,
            speaker_selection_method=speaker_selection_method
        )
        
        # Create the group chat manager
        manager = GroupChatManager(
            groupchat=group_chat,
            llm_config=manager_llm_config
        )
        
        # Store both the group chat and its manager
        self.group_chats[name] = {
            "chat": group_chat,
            "manager": manager,
            "agents": agents
        }
        
        if self.verbose:
            print(f"Created group chat '{name}' with {len(agents)} agents")
        
        return group_chat
    
    def run_group_chat(
        self,
        group_chat_name: str,
        message: str,
        sender: Optional[Agent] = None
    ) -> List[Dict[str, Any]]:
        """
        Run a conversation in a group chat.
        
        Args:
            group_chat_name: Name of the group chat to use
            message: Initial message to start the conversation
            sender: The agent sending the initial message (defaults to first agent)
            
        Returns:
            The conversation history
        """
        if group_chat_name not in self.group_chats:
            raise ValueError(f"Group chat '{group_chat_name}' not found")
        
        chat_info = self.group_chats[group_chat_name]
        group_chat = chat_info["chat"]
        manager = chat_info["manager"]
        
        # Use the first agent as sender if not specified
        if not sender:
            sender = chat_info["agents"][0]
        
        # Reset the chat and start a new conversation
        group_chat.reset()
        
        if self.verbose:
            print(f"Starting group chat '{group_chat_name}' with message: {message}")
        
        # Initiate the chat
        manager.initiate_chat(
            sender=sender,
            message=message
        )
        
        # Return the chat history
        return group_chat.messages
    
    def create_specialized_team(self) -> Dict[str, Any]:
        """
        Create a specialized team of agents with predefined roles.
        
        Returns:
            Dictionary containing the created agents and group chat
        """
        # Create a planner agent
        planner = self.create_assistant_agent(
            name="Planner",
            system_message="""You are a strategic planner. Your role is to:
            1. Analyze complex problems and break them down into manageable steps
            2. Delegate tasks to appropriate team members based on their expertise
            3. Track progress and ensure all aspects of the problem are addressed
            4. Synthesize information from different team members
            Always start by understanding the problem completely and creating a clear plan."""
        )
        
        # Create a researcher agent
        researcher = self.create_assistant_agent(
            name="Researcher",
            system_message="""You are an expert researcher. Your role is to:
            1. Find and analyze relevant information for the current task
            2. Provide comprehensive and factual information
            3. Cite sources when possible
            4. Consider multiple perspectives on complex topics
            Focus on accuracy, thoroughness, and relevance in your research."""
        )
        
        # Create a developer agent
        developer = self.create_assistant_agent(
            name="Developer",
            system_message="""You are an expert software developer. Your role is to:
            1. Write clean, efficient, and well-documented code
            2. Design software architectures and solutions
            3. Debug issues and optimize performance
            4. Explain technical concepts clearly
            Prioritize code quality, maintainability, and best practices."""
        )
        
        # Create a critic agent
        critic = self.create_assistant_agent(
            name="Critic",
            system_message="""You are a thoughtful critic. Your role is to:
            1. Review work produced by the team and identify potential issues
            2. Suggest specific improvements with clear reasoning
            3. Consider edge cases and potential failures
            4. Ensure outputs meet quality standards
            Be constructive, specific, and thorough in your feedback."""
        )
        
        # Create a user proxy agent that can execute code
        user_proxy = self.create_user_proxy_agent(
            name="UserProxy",
            human_input_mode="TERMINATE"
        )
        
        # Create a group chat with all agents
        group_chat = self.create_group_chat(
            name="SpecializedTeam",
            agents=[user_proxy, planner, researcher, developer, critic],
            max_round=20
        )
        
        return {
            "planner": planner,
            "researcher": researcher,
            "developer": developer,
            "critic": critic,
            "user_proxy": user_proxy,
            "group_chat": group_chat,
            "group_chat_name": "SpecializedTeam"
        }
```

## Workflow Management in AutogenStudio

AutogenStudio provides a visual interface for configuring and managing agent workflows. Below, we integrate with AutogenStudio programmatically:

```python
import os
import sys
import subprocess
from pathlib import Path
import requests
import time
import json
from typing import Dict, Any, Optional, List, Union
import logging

class AutogenStudioManager:
    """
    Manager for interacting with AutogenStudio programmatically.
    """
    
    def __init__(
        self,
        workspace_dir: str = "./autogenstudio_workspace",
        port: int = 8081,
        install_if_needed: bool = True,
        verbose: bool = False
    ):
        """
        Initialize the AutogenStudio manager.
        
        Args:
            workspace_dir: Directory for AutogenStudio workspace
            port: Port to run AutogenStudio on
            install_if_needed: Whether to install AutogenStudio if not available
            verbose: Whether to print detailed logs
        """
        self.workspace_dir = Path(workspace_dir)
        self.port = port
        self.verbose = verbose
        self.process = None
        self.base_url = f"http://localhost:{port}/api"
        
        # Set up logging
        logging.basicConfig(
            level=logging.INFO if verbose else logging.WARNING,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger("AutogenStudioManager")
        
        # Check if AutogenStudio is installed, install if needed
        if install_if_needed and self._check_installation():
            self.logger.info("AutogenStudio is ready to use")
    
    def _check_installation(self) -> bool:
        """
        Check if AutogenStudio is installed, install if needed.
        
        Returns:
            True if installation is successful or already installed
        """
        try:
            # Try importing autogenstudio
            import autogenstudio
            self.logger.info("AutogenStudio is already installed")
            return True
        except ImportError:
            if self.verbose:
                self.logger.info("AutogenStudio not found, attempting to install...")
            
            try:
                # Install AutogenStudio using pip
                subprocess.check_call([
                    sys.executable, "-m", "pip", "install", "autogenstudio"
                ])
                self.logger.info("Successfully installed AutogenStudio")
                return True
            except subprocess.CalledProcessError as e:
                self.logger.error(f"Failed to install AutogenStudio: {e}")
                return False
    
    def start_studio(self, wait_for_server: bool = True) -> bool:
        """
        Start the AutogenStudio server.
        
        Args:
            wait_for_server: Whether to wait for the server to be ready
            
        Returns:
            True if the server was started successfully
        """
        try:
            # Create workspace directory if it doesn't exist
            self.workspace_dir.mkdir(parents=True, exist_ok=True)
            
            # Build the command to start AutogenStudio
            command = [
                sys.executable, "-m", "autogenstudio", "ui",
                "--host", "localhost",
                "--port", str(self.port),
                "--working-dir", str(self.workspace_dir)
            ]
            
            # Start the server
            self.logger.info(f"Starting AutogenStudio on port {self.port}...")
            
            # Start the server as a subprocess
            self.process = subprocess.Popen(
                command,
                stdout=subprocess.PIPE if not self.verbose else None,
                stderr=subprocess.PIPE if not self.verbose else None,
                text=True
            )
            
            # Wait for the server to be ready
            if wait_for_server:
                max_attempts = 20
                for attempt in range(max_attempts):
                    try:
                        # Check if the server is ready by making a request
                        response = requests.get(f"{self.base_url}/status")
                        if response.status_code == 200:
                            self.logger.info("AutogenStudio server is ready")
                            return True
                    except requests.RequestException:
                        pass
                    
                    time.sleep(1)
                    self.logger.info(f"Waiting for server to start (attempt {attempt+1}/{max_attempts})...")
                
                self.logger.warning("Timed out waiting for AutogenStudio server to start")
                return False
            
            return True
            
        except Exception as e:
            self.logger.error(f"Error starting AutogenStudio: {e}")
            return False
    
    def stop_studio(self) -> bool:
        """
        Stop the AutogenStudio server.
        
        Returns:
            True if the server was stopped successfully
        """
        if self.process:
            self.logger.info("Stopping AutogenStudio server...")
            self.process.terminate()
            try:
                self.process.wait(timeout=10)
                self.logger.info("AutogenStudio server stopped")
                return True
            except subprocess.TimeoutExpired:
                self.logger.warning("Timeout waiting for server to stop, forcing...")
                self.process.kill()
                return True
        
        return False
    
    def create_workflow(
        self,
        name: str,
        description: str,
        agent_configs: List[Dict[str, Any]],
        flow_config: Dict[str, Any]
    ) -> Optional[str]:
        """
        Create a new workflow in AutogenStudio.
        
        Args:
            name: Name of the workflow
            description: Description of the workflow
            agent_configs: List of agent configurations
            flow_config: Configuration for the workflow
            
        Returns:
            Workflow ID if successful, None otherwise
        """
        try:
            # Prepare the request data
            data = {
                "name": name,
                "description": description,
                "agents": agent_configs,
                "flow": flow_config
            }
            
            # Send the request to create a workflow
            response = requests.post(
                f"{self.base_url}/workflows/create",
                json=data
            )
            
            if response.status_code == 200:
                workflow_id = response.json().get("workflow_id")
                self.logger.info(f"Created workflow '{name}' with ID: {workflow_id}")
                return workflow_id
            else:
                self.logger.error(f"Failed to create workflow: {response.text}")
                return None
                
        except Exception as e:
            self.logger.error(f"Error creating workflow: {e}")
            return None
    
    def run_workflow(
        self,
        workflow_id: str,
        input_message: str
    ) -> Optional[Dict[str, Any]]:
        """
        Run a workflow with the given input.
        
        Args:
            workflow_id: ID of the workflow to run
            input_message: Input message to start the workflow
            
        Returns:
            Workflow execution results if successful, None otherwise
        """
        try:
            # Prepare the request data
            data = {
                "workflow_id": workflow_id,
                "input": input_message
            }
            
            # Send the request to run the workflow
            response = requests.post(
                f"{self.base_url}/workflows/run",
                json=data
            )
            
            if response.status_code == 200:
                result = response.json()
                self.logger.info(f"Successfully ran workflow {workflow_id}")
                return result
            else:
                self.logger.error(f"Failed to run workflow: {response.text}")
                return None
                
        except Exception as e:
            self.logger.error(f"Error running workflow: {e}")
            return None
    
    def list_workflows(self) -> Optional[List[Dict[str, Any]]]:
        """
        List all workflows in AutogenStudio.
        
        Returns:
            List of workflow information if successful, None otherwise
        """
        try:
            # Send the request to list workflows
            response = requests.get(f"{self.base_url}/workflows/list")
            
            if response.status_code == 200:
                workflows = response.json().get("workflows", [])
                self.logger.info(f"Retrieved {len(workflows)} workflows")
                return workflows
            else:
                self.logger.error(f"Failed to list workflows: {response.text}")
                return None
                
        except Exception as e:
            self.logger.error(f"Error listing workflows: {e}")
            return None
    
    def export_workflow(self, workflow_id: str, export_path: Optional[str] = None) -> Optional[Dict[str, Any]]:
        """
        Export a workflow to a file.
        
        Args:
            workflow_id: ID of the workflow to export
            export_path: Path to save the exported workflow
            
        Returns:
            Workflow configuration if successful, None otherwise
        """
        try:
            # Send the request to export the workflow
            response = requests.get(f"{self.base_url}/workflows/{workflow_id}")
            
            if response.status_code == 200:
                workflow_data = response.json()
                
                # Save to file if path is provided
                if export_path:
                    with open(export_path, 'w') as f:
                        json.dump(workflow_data, f, indent=2)
                    self.logger.info(f"Exported workflow to {export_path}")
                
                return workflow_data
            else:
                self.logger.error(f"Failed to export workflow: {response.text}")
                return None
                
        except Exception as e:
            self.logger.error(f"Error exporting workflow: {e}")
            return None
```

## Integrating AI Agents with External Tools

Autogen's power lies in its ability to integrate with external tools, enabling agents to perform actions beyond language generation:

```python
import requests
import os
import json
import datetime
from typing import Dict, Any, List, Optional, Callable, Tuple
from urllib.parse import quote_plus
from dotenv import load_dotenv
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import sqlite3

class ToolRegistry:
    """
    Registry for tools that can be used by Autogen agents.
    """
    
    def __init__(self):
        """Initialize the tool registry."""
        self.tools = {}
        self.categories = {}
    
    def register_tool(
        self,
        name: str,
        description: str,
        function: Callable,
        category: str = "general",
        parameters: Optional[Dict[str, Dict[str, Any]]] = None
    ) -> None:
        """
        Register a tool with the registry.
        
        Args:
            name: Name of the tool
            description: Description of what the tool does
            function: The function to call when the tool is invoked
            category: Category for organizing tools
            parameters: JSON Schema for function parameters
        """
        # Create default parameters if none provided
        if parameters is None:
            parameters = {}
        
        tool_info = {
            "name": name,
            "description": description,
            "function": function,
            "parameters": parameters
        }
        
        # Register the tool
        self.tools[name] = tool_info
        
        # Add to category
        if category not in self.categories:
            self.categories[category] = []
        
        self.categories[category].append(name)
    
    def get_tool(self, name: str) -> Optional[Dict[str, Any]]:
        """
        Get a tool by name.
        
        Args:
            name: Name of the tool to retrieve
            
        Returns:
            Tool information or None if not found
        """
        return self.tools.get(name)
    
    def list_tools(self, category: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        List all registered tools, optionally filtered by category.
        
        Args:
            category: Optional category to filter by
            
        Returns:
            List of tool information
        """
        if category and category in self.categories:
            tool_names = self.categories[category]
            return [
                {
                    "name": name,
                    "description": self.tools[name]["description"],
                    "parameters": self.tools[name]["parameters"]
                }
                for name in tool_names
            ]
        
        return [
            {
                "name": name,
                "description": info["description"],
                "parameters": info["parameters"]
            }
            for name, info in self.tools.items()
        ]
    
    def execute_tool(self, name: str, **kwargs) -> Any:
        """
        Execute a tool by name with provided arguments.
        
        Args:
            name: Name of the tool to execute
            **kwargs: Arguments to pass to the tool function
            
        Returns:
            Result of the tool execution
        """
        tool = self.get_tool(name)
        if not tool:
            raise ValueError(f"Tool '{name}' not found")
        
        # Execute the tool function with provided arguments
        return tool["function"](**kwargs)


class ToolIntegratedAgent:
    """
    Autogen agent with integrated tools and enhanced capabilities.
    """
    
    def __init__(
        self,
        orchestrator: "AutogenOrchestrator",
        tool_registry: ToolRegistry,
        name: str = "ToolAgent",
        system_message: Optional[str] = None,
        verbose: bool = False
    ):
        """
        Initialize a tool-integrated agent.
        
        Args:
            orchestrator: Autogen orchestrator instance
            tool_registry: Tool registry containing available tools
            name: Name of the agent
            system_message: System message for the agent
            verbose: Whether to print detailed logs
        """
        self.orchestrator = orchestrator
        self.tool_registry = tool_registry
        self.name = name
        self.verbose = verbose
        
        # Generate default system message if not provided
        if system_message is None:
            system_message = self._generate_system_message()
        
        # Create the underlying agent
        self.agent = orchestrator.create_assistant_agent(
            name=name,
            system_message=system_message
        )
        
        # Create a user proxy agent for execution
        self.executor = orchestrator.create_user_proxy_agent(
            name=f"{name}Executor",
            human_input_mode="NEVER"
        )
        
        # Set up function registry for the executor
        self._setup_function_registry()
    
    def _generate_system_message(self) -> str:
        """
        Generate a system message based on available tools.
        
        Returns:
            Generated system message
        """
        tools_by_category = {}
        for category, tool_names in self.tool_registry.categories.items():
            tools_by_category[category] = [
                self.tool_registry.get_tool(name) for name in tool_names
            ]
        
        # Create tool descriptions by category
        tool_descriptions = []
        for category, tools in tools_by_category.items():
            tool_descriptions.append(f"\n## {category.title()} Tools:")
            for tool in tools:
                params_str = ", ".join(tool["parameters"].keys()) if tool["parameters"] else "None"
                tool_descriptions.append(f"- {tool['name']}: {tool['description']} (Parameters: {params_str})")
        
        # Build the complete system message
        system_message = f"""You are an AI assistant with access to a variety of tools to help users.
        When you need to use a tool, specify the tool name and parameters clearly.
        
        # Available Tools:
        {"".join(tool_descriptions)}
        
        # How to Use Tools:
        1. When you need to use a tool, indicate the tool name and parameters in your response.
        2. Format tool calls as: 
           TOOL: tool_name(param1="value1", param2="value2")
        3. Wait for the tool execution result before proceeding.
        4. Based on the tool result, provide further information or take additional actions.
        
        Always try to help the user by using the most appropriate tools when needed.
        """
        
        return system_message
    
    def _setup_function_registry(self) -> None:
        """Set up function registry for tool execution."""
        # Create function mappings for all tools
        for name, tool_info in self.tool_registry.tools.items():
            # Define a wrapper function to execute the tool
            def tool_wrapper(name=name, **kwargs):
                result = self.tool_registry.execute_tool(name, **kwargs)
                return str(result)
            
            # Register the function with the executor
            self.executor.register_function(
                function_map={name: tool_wrapper}
            )
    
    def process_message(self, message: str) -> str:
        """
        Process a message and execute any tools called.
        
        Args:
            message: User message to process
            
        Returns:
            Agent response after processing
        """
        # Send the message to the agent
        self.agent.send(message, self.executor)
        
        # Get the final response
        response = ""
        for msg in self.executor.chat_history[-1:]:
            if msg["role"] == "assistant":
                response = msg["content"]
                break
        
        return response


# Define some example tools for the registry
def create_example_tools() -> ToolRegistry:
    """Create example tools for demonstration."""
    registry = ToolRegistry()
    
    # Web search tool
    def web_search(query: str, num_results: int = 5) -> str:
        """Search the web for information."""
        # In a real implementation, this would use a search API
        # For demo purposes, we'll return a mock response
        return f"Search results for '{query}':\n1. Result 1\n2. Result 2\n3. Result 3"
    
    registry.register_tool(
        name="web_search",
        description="Search the web for information",
        function=web_search,
        category="web",
        parameters={
            "query": {"type": "string", "description": "The search query"},
            "num_results": {"type": "integer", "description": "Number of results to return"}
        }
    )
    
    # Weather tool
    def get_weather(location: str) -> str:
        """Get current weather information for a location."""
        # In a real implementation, this would use a weather API
        return f"Weather in {location}: 72°F, Partly Cloudy"
    
    registry.register_tool(
        name="get_weather",
        description="Get current weather for a location",
        function=get_weather,
        category="weather",
        parameters={
            "location": {"type": "string", "description": "Location (city, address, etc.)"}
        }
    )
    
    # Data analysis tool
    def analyze_data(data_str: str, analysis_type: str = "summary") -> str:
        """Analyze data provided as CSV string."""
        try:
            # Convert string to DataFrame
            import io
            df = pd.read_csv(io.StringIO(data_str))
            
            if analysis_type == "summary":
                return df.describe().to_string()
            elif analysis_type == "correlation":
                return df.corr().to_string()
            else:
                return f"Unknown analysis type: {analysis_type}"
        except Exception as e:
            return f"Error analyzing data: {str(e)}"
    
    registry.register_tool(
        name="analyze_data",
        description="Analyze data provided in CSV format",
        function=analyze_data,
        category="data",
        parameters={
            "data_str": {"type": "string", "description": "CSV data as a string"},
            "analysis_type": {"type": "string", "description": "Type of analysis (summary, correlation)"}
        }
    )
    
    # Database query tool
    def query_database(query: str, db_path: str = ":memory:") -> str:
        """Execute SQL query on a database."""
        try:
            conn = sqlite3.connect(db_path)
            df = pd.read_sql_query(query, conn)
            conn.close()
            return df.to_string()
        except Exception as e:
            return f"Database query error: {str(e)}"
    
    registry.register_tool(
        name="query_database",
        description="Execute SQL query on a database",
        function=query_database,
        category="database",
        parameters={
            "query": {"type": "string", "description": "SQL query to execute"},
            "db_path": {"type": "string", "description": "Path to SQLite database file"}
        }
    )
    
    return registry
```

## Implementing an Alternative to OpenAI Operator

The OpenAI Operator enables seamless integration between tools and LLMs. Here's an implementation of a similar capability that works with multiple LLM providers:

```python
import json
import re
from typing import Dict, List, Any, Optional, Union, Callable, Tuple
import inspect
import logging
from dotenv import load_dotenv
import asyncio
from concurrent.futures import ThreadPoolExecutor
import time

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("llm_operator")

class LLMOperator:
    """
    A flexible operator that connects multiple LLM providers with tools.
    Alternative to OpenAI's function calling capability.
    """
    
    def __init__(
        self,
        llm_providers: Dict[str, Callable],
        default_provider: str = "openai",
        max_attempts: int = 3,
        verbose: bool = False
    ):
        """
        Initialize the LLM Operator.
        
        Args:
            llm_providers: Dictionary mapping provider names to LLM callables
            default_provider: Default LLM provider to use
            max_attempts: Maximum attempts for tool extraction
            verbose: Whether to print detailed logs
        """
        self.llm_providers = llm_providers
        self.default_provider = default_provider
        self.max_attempts = max_attempts
        self.verbose = verbose
        self.tools = {}
        self.logger = logger
        
        if verbose:
            self.logger.setLevel(logging.DEBUG)
    
    def register_tool(
        self,
        func: Callable,
        name: Optional[str] = None,
        description: Optional[str] = None
    ) -> None:
        """
        Register a tool with the operator.
        
        Args:
            func: Function to register as a tool
            name: Optional custom name for the tool
            description: Optional description of the tool
        """
        # Use function name if no custom name provided
        if name is None:
            name = func.__name__
        
        # Use docstring if no description provided
        if description is None:
            description = func.__doc__ or f"Tool: {name}"
        
        # Inspect function signature
        sig = inspect.signature(func)
        params = {}
        
        for param_name, param in sig.parameters.items():
            param_info = {
                "type": "string",  # Default type
                "required": param.default == inspect.Parameter.empty
            }
            
            # Extract type hints if available
            if param.annotation != inspect.Parameter.empty:
                if param.annotation == str:
                    param_info["type"] = "string"
                elif param.annotation == int:
                    param_info["type"] = "integer"
                elif param.annotation == float:
                    param_info["type"] = "number"
                elif param.annotation == bool:
                    param_info["type"] = "boolean"
                elif param.annotation == list or param.annotation == List:
                    param_info["type"] = "array"
                elif param.annotation == dict or param.annotation == Dict:
                    param_info["type"] = "object"
            
            # Add default value if available
            if param.default != inspect.Parameter.empty:
                param_info["default"] = param.default
            
            params[param_name] = param_info
        
        # Register the tool
        self.tools[name] = {
            "function": func,
            "description": description,
            "parameters": params
        }
        
        if self.verbose:
            self.logger.debug(f"Registered tool: {name}")
    
    def _extract_tool_calls(self, text: str) -> List[Dict[str, Any]]:
        """
        Extract tool calls from text using regex pattern matching.
        
        Args:
            text: Text containing tool calls
            
        Returns:
            List of extracted tool calls
        """
        tool_calls = []
        
        # Pattern for tool calls: TOOL: name(param1="value1", param2="value2")
        pattern = r'TOOL:\s+(\w+)\((.*?)\)'
        matches = re.findall(pattern, text)
        
        for tool_name, params_str in matches:
            if tool_name not in self.tools:
                continue
            
            # Extract parameters
            params = {}
            
            # Pattern for parameters: param="value" or param=value
            param_pattern = r'(\w+)\s*=\s*(?:"([^"]*?)"|\'([^\']*?)\'|(\S+))'
            param_matches = re.findall(param_pattern, params_str)
            
            for param_match in param_matches:
                param_name = param_match[0]
                # Get the first non-empty group as the value
                param_value = next((v for v in param_match[1:] if v), "")
                params[param_name] = param_value
            
            tool_calls.append({
                "name": tool_name,
                "parameters": params
            })
        
        return tool_calls
    
    def _format_tools_for_prompt(self) -> str:
        """
        Format tools as text for inclusion in prompts.
        
        Returns:
            Formatted tools as a string
        """
        if not self.tools:
            return "No tools available."
        
        tools_text = ["Available tools:"]
        
        for name, tool_info in self.tools.items():
            params_text = []
            for param_name, param_info in tool_info["parameters"].items():
                required = " (required)" if param_info.get("required", False) else ""
                default = f" (default: {param_info.get('default')})" if "default" in param_info else ""
                params_text.append(f"{param_name}: {param_info['type']}{required}{default}")
            
            tool_text = f"- {name}({', '.join(params_text)}): {tool_info['description']}"
            tools_text.append(tool_text)
        
        return "\n".join(tools_text)
    
    def create_prompt_with_tools(
        self,
        base_prompt: str,
        include_format_instructions: bool = True
    ) -> str:
        """
        Create a prompt that includes tool descriptions.
        
        Args:
            base_prompt: Base prompt text
            include_format_instructions: Whether to include tool calling format instructions
            
        Returns:
            Complete prompt with tool information
        """
        tools_text = self._format_tools_for_prompt()
        
        format_instructions = """
        To use a tool, write:
        TOOL: tool_name(param1="value1", param2="value2")
        
        After calling a tool, I'll show you the result.
        """
        
        if include_format_instructions:
            return f"{base_prompt}\n\n{tools_text}\n{format_instructions}"
        else:
            return f"{base_prompt}\n\n{tools_text}"
    
    async def run_conversation(
        self,
        messages: List[Dict[str, str]],
        provider: Optional[str] = None,
        max_turns: int = 5,
        system_prompt: Optional[str] = None
    ) -> List[Dict[str, str]]:
        """
        Run a conversation with tools until completion.
        
        Args:
            messages: Initial conversation messages
            provider: LLM provider to use
            max_turns: Maximum conversation turns
            system_prompt: Optional system prompt to include
            
        Returns:
            Complete conversation history
        """
        if provider is None:
            provider = self.default_provider
        
        if provider not in self.llm_providers:
            raise ValueError(f"Provider '{provider}' not found")
        
        llm_func = self.llm_providers[provider]
        conversation = messages.copy()
        
        # Add system prompt with tools if provided
        if system_prompt:
            enhanced_system_prompt = self.create_prompt_with_tools(system_prompt)
            # Insert system prompt at the beginning if not already present
            if not conversation or conversation[0].get("role") != "system":
                conversation.insert(0, {"role": "system", "content": enhanced_system_prompt})
        
        turn = 0
        while turn < max_turns:
            turn += 1
            
            # Get response from LLM
            try:
                llm_response = await self._call_llm_async(llm_func, conversation)
                
                if isinstance(llm_response, dict):
                    response_content = llm_response.get("content", "")
                else:
                    response_content = str(llm_response)
                
                # Add response to conversation
                conversation.append({"role": "assistant", "content": response_content})
                
                # Extract tool calls
                tool_calls = self._extract_tool_calls(response_content)
                
                if not tool_calls:
                    # No tool calls, end conversation
                    break
                
                # Execute each tool call and add results
                for tool_call in tool_calls:
                    tool_name = tool_call["name"]
                    parameters = tool_call["parameters"]
                    
                    if tool_name not in self.tools:
                        result = f"Error: Tool '{tool_name}' not found."
                    else:
                        try:
                            # Execute the tool
                            tool_func = self.tools[tool_name]["function"]
                            result = await self._execute_tool_async(tool_func, parameters)
                        except Exception as e:
                            result = f"Error executing tool '{tool_name}': {str(e)}"
                    
                    # Add tool result to conversation
                    conversation.append({
                        "role": "function",
                        "name": tool_name,
                        "content": str(result)
                    })
            
            except Exception as e:
                error_msg = f"Error in conversation turn {turn}: {str(e)}"
                self.logger.error(error_msg)
                conversation.append({"role": "system", "content": error_msg})
                break
        
        return conversation
    
    async def _call_llm_async(self, llm_func: Callable, messages: List[Dict[str, str]]) -> Any:
        """Call LLM function asynchronously."""
        # Run in thread pool if it's a blocking function
        if not asyncio.iscoroutinefunction(llm_func):
            with ThreadPoolExecutor() as executor:
                return await asyncio.get_event_loop().run_in_executor(
                    executor, llm_func, messages
                )
        else:
            return await llm_func(messages)
    
    async def _execute_tool_async(self, tool_func: Callable, parameters: Dict[str, Any]) -> Any:
        """Execute tool function asynchronously."""
        # Run in thread pool if it's a blocking function
        if not asyncio.iscoroutinefunction(tool_func):
            with ThreadPoolExecutor() as executor:
                return await asyncio.get_event_loop().run_in_executor(
                    executor, lambda: tool_func(**parameters)
                )
        else:
            return await tool_func(**parameters)
    
    def _format_param_value(self, value: Any, param_type: str) -> Any:
        """
        Format parameter value based on its type.
        
        Args:
            value: Parameter value to format
            param_type: Expected parameter type
            
        Returns:
            Formatted parameter value
        """
        if param_type == "integer":
            return int(value)
        elif param_type == "number":
            return float(value)
        elif param_type == "boolean":
            if isinstance(value, str):
                return value.lower() in ["true", "yes", "1", "t", "y"]
            return bool(value)
        elif param_type == "array":
            if isinstance(value, str):
                try:
                    return json.loads(value)
                except:
                    return [item.strip() for item in value.split(",")]
            return value
        elif param_type == "object":
            if isinstance(value, str):
                try:
                    return json.loads(value)
                except:
                    return value
            return value
        else:
            # Default to string
            return str(value)


# Example implementation of a multi-provider setup
async def demonstrate_llm_operator():
    """Demonstrate the LLM Operator with various providers."""
    load_dotenv()
    
    # Mock LLM functions for different providers
    def openai_llm(messages: List[Dict[str, str]]) -> Dict[str, str]:
        """Mock OpenAI LLM function."""
        # In real implementation, this would call OpenAI API
        last_message = messages[-1]["content"] if messages else ""
        
        if "weather" in last_message.lower():
            return {
                "role": "assistant", 
                "content": "I'll check the weather for you.\n\nTOOL: get_weather(location=\"New York\")"
            }
        elif "search" in last_message.lower():
            return {
                "role": "assistant",
                "content": "Let me search that for you.\n\nTOOL: web_search(query=\"Autogen framework\", num_results=3)"
            }
        else:
            return {
                "role": "assistant",
                "content": "I'm here to help. What would you like to know?"
            }
    
    def anthropic_llm(messages: List[Dict[str, str]]) -> Dict[str, str]:
        """Mock Anthropic LLM function."""
        # In real implementation, this would call Anthropic API
        last_message = messages[-1]["content"] if messages else ""
        
        if "weather" in last_message.lower():
            return {
                "role": "assistant", 
                "content": "I can help you check the weather.\n\nTOOL: get_weather(location=\"Los Angeles\")"
            }
        elif "search" in last_message.lower():
            return {
                "role": "assistant",
                "content": "I'll search for information on that.\n\nTOOL: web_search(query=\"Latest AI developments\", num_results=5)"
            }
        else:
            return {
                "role": "assistant",
                "content": "Hello! How can I assist you today?"
            }
    
    # Set up LLM Operator
    operator = LLMOperator(
        llm_providers={
            "openai": openai_llm,
            "anthropic": anthropic_llm
        },
        default_provider="openai",
        verbose=True
    )
    
    # Register example tools
    def get_weather(location: str) -> str:
        """Get weather for a specific location."""
        # This would normally call a weather API
        return f"Weather in {location}: 75°F, Sunny"
    
    def web_search(query: str, num_results: int = 3) -> str:
        """Search the web for information."""
        # This would normally call a search API
        results = [
            f"Result {i+1} for '{query}'" for i in range(min(num_results, 10))
        ]
        return "\n".join(results)
    
    operator.register_tool(get_weather)
    operator.register_tool(web_search)
    
    # Run example conversations
    print("Example 1: Weather query with OpenAI")
    conversation1 = await operator.run_conversation(
        messages=[{"role": "user", "content": "What's the weather like today?"}],
        provider="openai",
        system_prompt="You are a helpful assistant that can answer questions and use tools."
    )
    
    print("\nConversation 1:")
    for message in conversation1:
        print(f"{message['role']}: {message['content']}")
    
    print("\nExample 2: Search query with Anthropic")
    conversation2 = await operator.run_conversation(
        messages=[{"role": "user", "content": "Search for information about AI frameworks."}],
        provider="anthropic",
        system_prompt="You are a research assistant that can search for information."
    )
    
    print("\nConversation 2:")
    for message in conversation2:
        print(f"{message['role']}: {message['content']}")
    
    return operator

if __name__ == "__main__":
    load_dotenv()
    asyncio.run(demonstrate_llm_operator())
```

## Conclusion

Microsoft Autogen represents a significant advancement in AI agent frameworks by providing a flexible and powerful architecture for building multi-agent systems. Through its sophisticated orchestration capabilities, agents can work together to solve complex problems by leveraging specialized roles and collaborative interaction patterns.

The framework's focus on multiple agent collaboration creates new possibilities for complex reasoning, problem-solving, and task automation. By defining agents with different personas and capabilities, developers can build systems that combine the strengths of different specialized components while mitigating individual weaknesses.

AutogenStudio extends these capabilities with a visual interface that simplifies the creation and management of agent workflows, making advanced AI agent technology more accessible to a broader range of users. The ability to create, test, and deploy agent systems through a user-friendly interface accelerates development and enables rapid iteration.

One of Autogen's most powerful features is its robust tool integration, allowing agents to interact with external systems, retrieve information, and perform actions in the real world. This capability transforms LLMs from pure text generators into capable autonomous agents that can accomplish meaningful tasks.

The modular architecture of Autogen makes it easily extensible, allowing developers to create custom agents, integrate with various LLM providers, and build sophisticated multi-agent systems for specific domains. By providing alternatives to proprietary systems like OpenAI's function calling, Autogen enables more flexible and adaptable agent implementations across different technologies.

As AI agent technology continues to evolve, frameworks like Autogen will play a crucial role in enabling the development of more capable, reliable, and useful AI systems that can assist humans across a wide range of tasks and domains. The combination of multiple specialized agents working together through orchestrated workflows represents a promising direction for the future of AI applications.