<small>Claude Sonnet 4</small>
# 04. OpenAI Models and Fine-tuning

## Key Terms

**OpenAI API**: A comprehensive cloud-based application programming interface that provides access to OpenAI's large language models including GPT-4, GPT-3.5-turbo, and specialized models for text generation, embeddings, and content moderation through standardized REST endpoints.

**Chat Completion**: An API endpoint that enables conversational AI interactions by processing messages in a chat format, supporting system messages, user inputs, and assistant responses with advanced parameters for controlling generation behavior, temperature, and token limits.

**Embeddings**: High-dimensional vector representations of text that capture semantic meaning and relationships, enabling similarity search, clustering, classification, and recommendation systems through numerical representation of textual content in vector space.

**Content Moderation**: An automated content filtering system that analyzes text for potentially harmful, inappropriate, or policy-violating content across multiple categories including hate speech, harassment, violence, and adult content using specialized classification models.

**System Messages**: Special instructional prompts that define the AI assistant's behavior, personality, capabilities, and constraints, serving as persistent context that influences all subsequent interactions within a conversation session.

**Fine-tuning**: The process of customizing pre-trained OpenAI models by training them on domain-specific datasets to improve performance for particular tasks, use cases, or organizational requirements while maintaining the base model's general capabilities.

**Model Configuration**: The systematic setup and parameterization of model behavior through various settings including temperature, max tokens, top-p sampling, frequency penalties, and presence penalties to optimize output quality and characteristics.

**Token Management**: The practice of efficiently managing input and output token consumption to optimize costs and performance, including strategies for prompt optimization, response length control, and batch processing.

## Comprehensive OpenAI API Integration and Fine-tuning Framework

OpenAI's API ecosystem represents the gold standard for production-ready large language model deployment, offering sophisticated tools for chat completion, embeddings generation, and content moderation. This framework demonstrates advanced integration patterns and fine-tuning methodologies for enterprise applications.

### Advanced OpenAI API Implementation

````python
import asyncio
import aiohttp
import json
import time
import logging
import os
from typing import Dict, List, Any, Optional, Union, Tuple, AsyncGenerator
from dataclasses import dataclass, field
from datetime import datetime, timezone
from pathlib import Path
import hashlib
import pickle
from concurrent.futures import ThreadPoolExecutor
import threading
from contextlib import asynccontextmanager
import backoff

# OpenAI and data processing
import openai
from openai import AsyncOpenAI
import tiktoken
import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

# Fine-tuning and evaluation
import datasets
from datasets import Dataset
import torch
import transformers
from transformers import AutoTokenizer
import wandb

# Async and utilities
import httpx
from tenacity import retry, stop_after_attempt, wait_exponential
import jsonlines

from dotenv import load_dotenv

load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class OpenAIConfig:
    """Configuration for OpenAI API interactions"""
    api_key: str
    organization: Optional[str] = None
    base_url: str = "https://api.openai.com/v1"
    default_model: str = "gpt-4"
    max_retries: int = 3
    timeout: int = 30
    rate_limit_rpm: int = 3500  # requests per minute
    rate_limit_tpm: int = 90000  # tokens per minute

@dataclass
class ChatMessage:
    """Structured chat message"""
    role: str  # 'system', 'user', 'assistant'
    content: str
    name: Optional[str] = None
    function_call: Optional[Dict[str, Any]] = None

@dataclass
class CompletionRequest:
    """Chat completion request configuration"""
    messages: List[ChatMessage]
    model: str = "gpt-4"
    temperature: float = 0.7
    max_tokens: Optional[int] = None
    top_p: float = 1.0
    frequency_penalty: float = 0.0
    presence_penalty: float = 0.0
    stop: Optional[Union[str, List[str]]] = None
    stream: bool = False
    functions: Optional[List[Dict[str, Any]]] = None
    function_call: Optional[Union[str, Dict[str, str]]] = None

@dataclass
class EmbeddingRequest:
    """Embedding generation request"""
    input_text: Union[str, List[str]]
    model: str = "text-embedding-ada-002"
    user: Optional[str] = None

@dataclass
class ModerationRequest:
    """Content moderation request"""
    input_text: Union[str, List[str]]
    model: str = "text-moderation-latest"

@dataclass
class FineTuningJob:
    """Fine-tuning job configuration"""
    training_file: str
    validation_file: Optional[str] = None
    model: str = "gpt-3.5-turbo"
    n_epochs: int = 3
    batch_size: Optional[int] = None
    learning_rate_multiplier: Optional[float] = None
    prompt_loss_weight: float = 0.01
    suffix: Optional[str] = None
    hyperparameters: Dict[str, Any] = field(default_factory=dict)

class RateLimiter:
    """Advanced rate limiter for OpenAI API"""
    
    def __init__(self, rpm: int = 3500, tpm: int = 90000):
        self.rpm = rpm
        self.tpm = tpm
        
        # Request tracking
        self.request_times = []
        self.token_usage = []
        
        # Locks for thread safety
        self.request_lock = threading.Lock()
        self.token_lock = threading.Lock()
    
    async def acquire_request_slot(self):
        """Acquire a request slot respecting RPM limits"""
        with self.request_lock:
            current_time = time.time()
            
            # Remove requests older than 1 minute
            self.request_times = [t for t in self.request_times if current_time - t < 60]
            
            # Check if we can make a request
            if len(self.request_times) >= self.rpm:
                sleep_time = 60 - (current_time - self.request_times[0])
                if sleep_time > 0:
                    await asyncio.sleep(sleep_time)
            
            self.request_times.append(current_time)
    
    async def acquire_token_budget(self, estimated_tokens: int):
        """Acquire token budget respecting TPM limits"""
        with self.token_lock:
            current_time = time.time()
            
            # Remove token usage older than 1 minute
            self.token_usage = [(t, tokens) for t, tokens in self.token_usage 
                              if current_time - t < 60]
            
            # Calculate current token usage
            current_tokens = sum(tokens for _, tokens in self.token_usage)
            
            # Check if we can use tokens
            if current_tokens + estimated_tokens > self.tpm:
                # Calculate sleep time based on oldest token usage
                if self.token_usage:
                    sleep_time = 60 - (current_time - self.token_usage[0][0])
                    if sleep_time > 0:
                        await asyncio.sleep(sleep_time)
            
            self.token_usage.append((current_time, estimated_tokens))

class OpenAIClient:
    """Advanced OpenAI API client with comprehensive features"""
    
    def __init__(self, config: OpenAIConfig):
        self.config = config
        self.client = AsyncOpenAI(
            api_key=config.api_key,
            organization=config.organization,
            base_url=config.base_url,
            timeout=config.timeout,
            max_retries=config.max_retries
        )
        
        self.rate_limiter = RateLimiter(config.rate_limit_rpm, config.rate_limit_tpm)
        self.tokenizer = tiktoken.encoding_for_model("gpt-4")
        
        # Metrics tracking
        self.request_count = 0
        self.token_usage_total = 0
        self.error_count = 0
        self.cost_tracking = {
            "gpt-4": {"input": 0.03, "output": 0.06},
            "gpt-4-turbo": {"input": 0.01, "output": 0.03},
            "gpt-3.5-turbo": {"input": 0.0015, "output": 0.002},
            "text-embedding-ada-002": {"input": 0.0001, "output": 0}
        }
        self.total_cost = 0.0
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    async def chat_completion(self, request: CompletionRequest) -> Dict[str, Any]:
        """Execute chat completion with advanced error handling and rate limiting"""
        
        # Estimate tokens for rate limiting
        estimated_tokens = self._estimate_tokens(request.messages)
        
        # Apply rate limiting
        await self.rate_limiter.acquire_request_slot()
        await self.rate_limiter.acquire_token_budget(estimated_tokens)
        
        try:
            # Prepare messages
            messages = [
                {
                    "role": msg.role,
                    "content": msg.content,
                    **({"name": msg.name} if msg.name else {}),
                    **({"function_call": msg.function_call} if msg.function_call else {})
                }
                for msg in request.messages
            ]
            
            # Make API call
            start_time = time.time()
            
            if request.stream:
                return await self._handle_streaming_completion(request, messages)
            else:
                response = await self.client.chat.completions.create(
                    model=request.model,
                    messages=messages,
                    temperature=request.temperature,
                    max_tokens=request.max_tokens,
                    top_p=request.top_p,
                    frequency_penalty=request.frequency_penalty,
                    presence_penalty=request.presence_penalty,
                    stop=request.stop,
                    functions=request.functions,
                    function_call=request.function_call
                )
            
            # Track metrics
            execution_time = time.time() - start_time
            self.request_count += 1
            
            # Calculate cost
            input_tokens = response.usage.prompt_tokens
            output_tokens = response.usage.completion_tokens
            cost = self._calculate_cost(request.model, input_tokens, output_tokens)
            self.total_cost += cost
            self.token_usage_total += response.usage.total_tokens
            
            # Prepare response
            result = {
                "id": response.id,
                "model": response.model,
                "content": response.choices[0].message.content,
                "finish_reason": response.choices[0].finish_reason,
                "usage": {
                    "prompt_tokens": input_tokens,
                    "completion_tokens": output_tokens,
                    "total_tokens": response.usage.total_tokens
                },
                "execution_time": execution_time,
                "cost": cost,
                "timestamp": datetime.now(timezone.utc)
            }
            
            # Handle function calls
            if response.choices[0].message.function_call:
                result["function_call"] = response.choices[0].message.function_call
            
            return result
            
        except Exception as e:
            self.error_count += 1
            logger.error(f"Chat completion error: {e}")
            raise
    
    async def _handle_streaming_completion(self, request: CompletionRequest, 
                                         messages: List[Dict[str, Any]]) -> AsyncGenerator[Dict[str, Any], None]:
        """Handle streaming chat completion"""
        try:
            stream = await self.client.chat.completions.create(
                model=request.model,
                messages=messages,
                temperature=request.temperature,
                max_tokens=request.max_tokens,
                top_p=request.top_p,
                frequency_penalty=request.frequency_penalty,
                presence_penalty=request.presence_penalty,
                stop=request.stop,
                stream=True
            )
            
            async for chunk in stream:
                if chunk.choices[0].delta.content is not None:
                    yield {
                        "id": chunk.id,
                        "content": chunk.choices[0].delta.content,
                        "finish_reason": chunk.choices[0].finish_reason,
                        "model": chunk.model
                    }
                    
        except Exception as e:
            logger.error(f"Streaming completion error: {e}")
            raise
    
    async def generate_embeddings(self, request: EmbeddingRequest) -> Dict[str, Any]:
        """Generate embeddings with batching and optimization"""
        
        # Handle single string input
        if isinstance(request.input_text, str):
            texts = [request.input_text]
        else:
            texts = request.input_text
        
        # Batch processing for large inputs
        batch_size = 100  # OpenAI's recommended batch size
        all_embeddings = []
        total_tokens = 0
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            
            # Estimate tokens and apply rate limiting
            estimated_tokens = sum(len(self.tokenizer.encode(text)) for text in batch_texts)
            await self.rate_limiter.acquire_request_slot()
            await self.rate_limiter.acquire_token_budget(estimated_tokens)
            
            try:
                response = await self.client.embeddings.create(
                    input=batch_texts,
                    model=request.model,
                    user=request.user
                )
                
                # Extract embeddings
                batch_embeddings = [item.embedding for item in response.data]
                all_embeddings.extend(batch_embeddings)
                total_tokens += response.usage.total_tokens
                
                # Calculate cost
                cost = self._calculate_cost(request.model, response.usage.total_tokens, 0)
                self.total_cost += cost
                
            except Exception as e:
                logger.error(f"Embedding generation error for batch {i}: {e}")
                raise
        
        return {
            "embeddings": all_embeddings,
            "model": request.model,
            "usage": {"total_tokens": total_tokens},
            "cost": self._calculate_cost(request.model, total_tokens, 0),
            "dimensions": len(all_embeddings[0]) if all_embeddings else 0
        }
    
    async def moderate_content(self, request: ModerationRequest) -> Dict[str, Any]:
        """Perform content moderation"""
        
        # Handle single string input
        if isinstance(request.input_text, str):
            texts = [request.input_text]
        else:
            texts = request.input_text
        
        try:
            await self.rate_limiter.acquire_request_slot()
            
            response = await self.client.moderations.create(
                input=texts,
                model=request.model
            )
            
            # Process results
            results = []
            for result in response.results:
                results.append({
                    "flagged": result.flagged,
                    "categories": result.categories,
                    "category_scores": result.category_scores
                })
            
            return {
                "results": results,
                "model": request.model,
                "id": response.id
            }
            
        except Exception as e:
            logger.error(f"Content moderation error: {e}")
            raise
    
    def _estimate_tokens(self, messages: List[ChatMessage]) -> int:
        """Estimate token count for messages"""
        total_tokens = 0
        for message in messages:
            # Rough estimation: 4 tokens per message overhead + content tokens
            total_tokens += 4
            total_tokens += len(self.tokenizer.encode(message.content))
        return total_tokens
    
    def _calculate_cost(self, model: str, input_tokens: int, output_tokens: int) -> float:
        """Calculate API call cost"""
        pricing = self.cost_tracking.get(model, self.cost_tracking["gpt-4"])
        
        input_cost = (input_tokens / 1000) * pricing["input"]
        output_cost = (output_tokens / 1000) * pricing["output"]
        
        return input_cost + output_cost
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get comprehensive usage metrics"""
        return {
            "total_requests": self.request_count,
            "total_tokens": self.token_usage_total,
            "total_cost": self.total_cost,
            "error_count": self.error_count,
            "error_rate": self.error_count / max(self.request_count, 1),
            "average_tokens_per_request": self.token_usage_total / max(self.request_count, 1),
            "average_cost_per_request": self.total_cost / max(self.request_count, 1)
        }

class SystemMessageManager:
    """Advanced system message configuration and management"""
    
    def __init__(self):
        self.templates = {
            "assistant": {
                "professional": "You are a professional AI assistant. Provide accurate, helpful, and well-structured responses. Always maintain a professional tone and cite sources when applicable.",
                
                "creative": "You are a creative AI assistant. Think outside the box, provide innovative solutions, and use engaging language. Feel free to use analogies and creative examples.",
                
                "analytical": "You are an analytical AI assistant. Focus on data-driven insights, logical reasoning, and systematic problem-solving. Break down complex problems into manageable components.",
                
                "technical": "You are a technical AI assistant with expertise in software engineering, data science, and technology. Provide precise technical information, code examples, and best practices."
            },
            
            "specialist": {
                "data_scientist": "You are an expert data scientist with extensive experience in machine learning, statistics, and data analysis. Provide detailed technical explanations, mathematical reasoning, and practical implementation advice.",
                
                "software_engineer": "You are a senior software engineer with expertise in modern development practices, system design, and software architecture. Focus on clean code, scalability, and maintainability.",
                
                "business_analyst": "You are an experienced business analyst. Focus on business value, stakeholder needs, requirements analysis, and strategic thinking. Translate technical concepts into business language.",
                
                "researcher": "You are an academic researcher with expertise in conducting thorough investigations, analyzing literature, and presenting findings. Always support claims with evidence and maintain scientific rigor."
            },
            
            "domain_specific": {
                "healthcare": "You are a healthcare AI assistant. Provide evidence-based information while always emphasizing that users should consult healthcare professionals for medical advice. Maintain patient privacy and ethical standards.",
                
                "finance": "You are a financial AI assistant. Provide accurate financial information while emphasizing that this is not personalized financial advice. Always mention risk factors and encourage consulting financial professionals.",
                
                "legal": "You are a legal AI assistant. Provide general legal information while clearly stating this is not legal advice. Always recommend consulting qualified legal professionals for specific legal matters.",
                
                "education": "You are an educational AI assistant. Focus on clear explanations, learning objectives, and pedagogical best practices. Adapt explanations to the learner's level and provide multiple learning approaches."
            }
        }
        
        self.constraints = {
            "safety": [
                "Do not provide information that could be used to harm others",
                "Refuse requests for illegal activities or content",
                "Maintain user privacy and confidentiality",
                "Do not generate content that promotes discrimination or hate"
            ],
            
            "accuracy": [
                "Acknowledge uncertainty when you don't know something",
                "Provide sources and references when possible",
                "Distinguish between facts and opinions",
                "Update information based on context and corrections"
            ],
            
            "helpfulness": [
                "Ask clarifying questions when requests are ambiguous",
                "Provide actionable advice and next steps",
                "Offer alternative approaches when appropriate",
                "Explain complex concepts in accessible language"
            ]
        }
    
    def create_system_message(self, 
                            personality_type: str,
                            domain: Optional[str] = None,
                            custom_instructions: Optional[List[str]] = None,
                            constraints: Optional[List[str]] = None,
                            examples: Optional[List[Dict[str, str]]] = None) -> str:
        """Create comprehensive system message"""
        
        message_parts = []
        
        # Base personality
        if domain and domain in self.templates:
            if personality_type in self.templates[domain]:
                message_parts.append(self.templates[domain][personality_type])
            else:
                # Fallback to assistant type
                message_parts.append(self.templates["assistant"].get(personality_type, 
                                   self.templates["assistant"]["professional"]))
        else:
            message_parts.append(self.templates["assistant"].get(personality_type,
                               self.templates["assistant"]["professional"]))
        
        # Add constraints
        if constraints:
            constraints_text = "Important constraints:\n" + "\n".join([f"- {c}" for c in constraints])
            message_parts.append(constraints_text)
        
        # Add custom instructions
        if custom_instructions:
            instructions_text = "Additional instructions:\n" + "\n".join([f"- {i}" for i in custom_instructions])
            message_parts.append(instructions_text)
        
        # Add examples if provided
        if examples:
            examples_text = "Examples of desired interactions:\n"
            for i, example in enumerate(examples):
                examples_text += f"\nExample {i+1}:\n"
                examples_text += f"User: {example['user']}\n"
                examples_text += f"Assistant: {example['assistant']}\n"
            message_parts.append(examples_text)
        
        return "\n\n".join(message_parts)
    
    def get_template_categories(self) -> Dict[str, List[str]]:
        """Get available template categories and types"""
        return {category: list(templates.keys()) 
                for category, templates in self.templates.items()}

class ConversationManager:
    """Manage complex conversation flows and context"""
    
    def __init__(self, openai_client: OpenAIClient, max_context_length: int = 8000):
        self.client = openai_client
        self.max_context_length = max_context_length
        self.conversations: Dict[str, List[ChatMessage]] = {}
        self.conversation_metadata: Dict[str, Dict[str, Any]] = {}
    
    def start_conversation(self, conversation_id: str, 
                          system_message: str,
                          metadata: Optional[Dict[str, Any]] = None) -> None:
        """Start a new conversation with system message"""
        self.conversations[conversation_id] = [
            ChatMessage(role="system", content=system_message)
        ]
        self.conversation_metadata[conversation_id] = metadata or {}
        self.conversation_metadata[conversation_id]["created_at"] = datetime.now(timezone.utc)
    
    def add_message(self, conversation_id: str, role: str, content: str, 
                   name: Optional[str] = None) -> None:
        """Add message to conversation"""
        if conversation_id not in self.conversations:
            raise ValueError(f"Conversation {conversation_id} not found")
        
        message = ChatMessage(role=role, content=content, name=name)
        self.conversations[conversation_id].append(message)
        
        # Trim context if too long
        self._trim_context(conversation_id)
    
    async def continue_conversation(self, conversation_id: str, 
                                  user_message: str,
                                  **completion_kwargs) -> Dict[str, Any]:
        """Continue conversation with user message"""
        if conversation_id not in self.conversations:
            raise ValueError(f"Conversation {conversation_id} not found")
        
        # Add user message
        self.add_message(conversation_id, "user", user_message)
        
        # Prepare completion request
        request = CompletionRequest(
            messages=self.conversations[conversation_id],
            **completion_kwargs
        )
        
        # Get response
        response = await self.client.chat_completion(request)
        
        # Add assistant response to conversation
        self.add_message(conversation_id, "assistant", response["content"])
        
        # Update metadata
        self.conversation_metadata[conversation_id]["last_updated"] = datetime.now(timezone.utc)
        self.conversation_metadata[conversation_id]["message_count"] = len(self.conversations[conversation_id])
        
        return response
    
    def _trim_context(self, conversation_id: str) -> None:
        """Trim conversation context to stay within limits"""
        messages = self.conversations[conversation_id]
        
        # Estimate total tokens
        total_tokens = sum(len(self.client.tokenizer.encode(msg.content)) for msg in messages)
        
        # Keep system message and trim from oldest user/assistant pairs
        if total_tokens > self.max_context_length:
            system_message = messages[0]  # Assume first message is system
            other_messages = messages[1:]
            
            # Remove oldest messages until under limit
            while other_messages and total_tokens > self.max_context_length:
                removed_message = other_messages.pop(0)
                total_tokens -= len(self.client.tokenizer.encode(removed_message.content))
            
            self.conversations[conversation_id] = [system_message] + other_messages
    
    def get_conversation_summary(self, conversation_id: str) -> Dict[str, Any]:
        """Get conversation summary and statistics"""
        if conversation_id not in self.conversations:
            return {}
        
        messages = self.conversations[conversation_id]
        metadata = self.conversation_metadata[conversation_id]
        
        # Calculate statistics
        user_messages = [m for m in messages if m.role == "user"]
        assistant_messages = [m for m in messages if m.role == "assistant"]
        
        total_tokens = sum(len(self.client.tokenizer.encode(msg.content)) for msg in messages)
        
        return {
            "conversation_id": conversation_id,
            "total_messages": len(messages),
            "user_messages": len(user_messages),
            "assistant_messages": len(assistant_messages),
            "total_tokens": total_tokens,
            "created_at": metadata.get("created_at"),
            "last_updated": metadata.get("last_updated"),
            "metadata": metadata
        }

class FineTuningManager:
    """Comprehensive fine-tuning management for OpenAI models"""
    
    def __init__(self, openai_client: OpenAIClient):
        self.client = openai_client
        self.jobs: Dict[str, Dict[str, Any]] = {}
    
    def prepare_training_data(self, conversations: List[List[Dict[str, str]]], 
                            output_file: str = "training_data.jsonl") -> str:
        """Prepare training data in OpenAI fine-tuning format"""
        
        training_examples = []
        
        for conversation in conversations:
            # Convert conversation to OpenAI format
            messages = []
            for turn in conversation:
                messages.append({
                    "role": turn["role"],
                    "content": turn["content"]
                })
            
            training_examples.append({"messages": messages})
        
        # Save as JSONL
        with open(output_file, 'w') as f:
            for example in training_examples:
                f.write(json.dumps(example) + '\n')
        
        logger.info(f"Prepared {len(training_examples)} training examples in {output_file}")
        return output_file
    
    async def upload_training_file(self, file_path: str) -> str:
        """Upload training file to OpenAI"""
        try:
            with open(file_path, 'rb') as f:
                response = await self.client.client.files.create(
                    file=f,
                    purpose='fine-tune'
                )
            
            file_id = response.id
            logger.info(f"Training file uploaded: {file_id}")
            return file_id
            
        except Exception as e:
            logger.error(f"Error uploading training file: {e}")
            raise
    
    async def create_fine_tuning_job(self, job_config: FineTuningJob) -> str:
        """Create fine-tuning job"""
        try:
            # Prepare hyperparameters
            hyperparameters = job_config.hyperparameters.copy()
            if job_config.n_epochs:
                hyperparameters["n_epochs"] = job_config.n_epochs
            if job_config.batch_size:
                hyperparameters["batch_size"] = job_config.batch_size
            if job_config.learning_rate_multiplier:
                hyperparameters["learning_rate_multiplier"] = job_config.learning_rate_multiplier
            
            # Create job
            response = await self.client.client.fine_tuning.jobs.create(
                training_file=job_config.training_file,
                validation_file=job_config.validation_file,
                model=job_config.model,
                hyperparameters=hyperparameters,
                suffix=job_config.suffix
            )
            
            job_id = response.id
            
            # Track job
            self.jobs[job_id] = {
                "id": job_id,
                "config": job_config,
                "status": response.status,
                "created_at": datetime.now(timezone.utc),
                "model": response.model,
                "training_file": response.training_file
            }
            
            logger.info(f"Fine-tuning job created: {job_id}")
            return job_id
            
        except Exception as e:
            logger.error(f"Error creating fine-tuning job: {e}")
            raise
    
    async def monitor_job(self, job_id: str) -> Dict[str, Any]:
        """Monitor fine-tuning job progress"""
        try:
            response = await self.client.client.fine_tuning.jobs.retrieve(job_id)
            
            # Update job status
            if job_id in self.jobs:
                self.jobs[job_id]["status"] = response.status
                self.jobs[job_id]["fine_tuned_model"] = response.fine_tuned_model
                self.jobs[job_id]["updated_at"] = datetime.now(timezone.utc)
            
            job_info = {
                "id": job_id,
                "status": response.status,
                "created_at": response.created_at,
                "finished_at": response.finished_at,
                "fine_tuned_model": response.fine_tuned_model,
                "hyperparameters": response.hyperparameters,
                "result_files": response.result_files,
                "trained_tokens": response.trained_tokens
            }
            
            return job_info
            
        except Exception as e:
            logger.error(f"Error monitoring job {job_id}: {e}")
            raise
    
    async def get_job_events(self, job_id: str) -> List[Dict[str, Any]]:
        """Get fine-tuning job events"""
        try:
            events = await self.client.client.fine_tuning.jobs.list_events(
                fine_tuning_job_id=job_id
            )
            
            return [
                {
                    "level": event.level,
                    "message": event.message,
                    "created_at": event.created_at,
                    "type": event.type
                }
                for event in events.data
            ]
            
        except Exception as e:
            logger.error(f"Error getting job events for {job_id}: {e}")
            raise
    
    async def cancel_job(self, job_id: str) -> bool:
        """Cancel fine-tuning job"""
        try:
            response = await self.client.client.fine_tuning.jobs.cancel(job_id)
            
            if job_id in self.jobs:
                self.jobs[job_id]["status"] = "cancelled"
                self.jobs[job_id]["cancelled_at"] = datetime.now(timezone.utc)
            
            logger.info(f"Fine-tuning job cancelled: {job_id}")
            return response.status == "cancelled"
            
        except Exception as e:
            logger.error(f"Error cancelling job {job_id}: {e}")
            raise
    
    def get_all_jobs(self) -> Dict[str, Dict[str, Any]]:
        """Get all tracked fine-tuning jobs"""
        return self.jobs.copy()

class EmbeddingAnalyzer:
    """Advanced embedding analysis and visualization"""
    
    def __init__(self, openai_client: OpenAIClient):
        self.client = openai_client
        self.embeddings_cache: Dict[str, np.ndarray] = {}
    
    async def analyze_text_similarity(self, texts: List[str], 
                                    cache_key: Optional[str] = None) -> Dict[str, Any]:
        """Analyze similarity between texts using embeddings"""
        
        # Check cache
        if cache_key and cache_key in self.embeddings_cache:
            embeddings = self.embeddings_cache[cache_key]
        else:
            # Generate embeddings
            request = EmbeddingRequest(input_text=texts)
            response = await self.client.generate_embeddings(request)
            embeddings = np.array(response["embeddings"])
            
            # Cache if key provided
            if cache_key:
                self.embeddings_cache[cache_key] = embeddings
        
        # Calculate similarity matrix
        similarity_matrix = cosine_similarity(embeddings)
        
        # Find most similar pairs
        similar_pairs = []
        for i in range(len(texts)):
            for j in range(i + 1, len(texts)):
                similarity = similarity_matrix[i][j]
                similar_pairs.append({
                    "text1_index": i,
                    "text2_index": j,
                    "text1": texts[i][:100] + "..." if len(texts[i]) > 100 else texts[i],
                    "text2": texts[j][:100] + "..." if len(texts[j]) > 100 else texts[j],
                    "similarity": float(similarity)
                })
        
        # Sort by similarity
        similar_pairs.sort(key=lambda x: x["similarity"], reverse=True)
        
        return {
            "similarity_matrix": similarity_matrix.tolist(),
            "most_similar_pairs": similar_pairs[:10],
            "average_similarity": float(np.mean(similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)])),
            "embeddings_shape": embeddings.shape,
            "analysis_timestamp": datetime.now(timezone.utc)
        }
    
    async def cluster_texts(self, texts: List[str], n_clusters: int = 5,
                          cache_key: Optional[str] = None) -> Dict[str, Any]:
        """Cluster texts using embeddings"""
        
        # Get embeddings
        if cache_key and cache_key in self.embeddings_cache:
            embeddings = self.embeddings_cache[cache_key]
        else:
            request = EmbeddingRequest(input_text=texts)
            response = await self.client.generate_embeddings(request)
            embeddings = np.array(response["embeddings"])
            
            if cache_key:
                self.embeddings_cache[cache_key] = embeddings
        
        # Perform clustering
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        cluster_labels = kmeans.fit_predict(embeddings)
        
        # Organize results by cluster
        clusters = {}
        for i, label in enumerate(cluster_labels):
            if label not in clusters:
                clusters[label] = []
            clusters[label].append({
                "index": i,
                "text": texts[i],
                "distance_to_centroid": float(np.linalg.norm(embeddings[i] - kmeans.cluster_centers_[label]))
            })
        
        # Sort texts within each cluster by distance to centroid
        for cluster_id in clusters:
            clusters[cluster_id].sort(key=lambda x: x["distance_to_centroid"])
        
        return {
            "clusters": clusters,
            "n_clusters": n_clusters,
            "cluster_labels": cluster_labels.tolist(),
            "cluster_centers": kmeans.cluster_centers_.tolist(),
            "inertia": float(kmeans.inertia_)
        }
    
    def visualize_embeddings(self, embeddings: np.ndarray, labels: Optional[List[str]] = None,
                           output_file: str = "embeddings_visualization.png") -> str:
        """Create 2D visualization of embeddings using PCA"""
        
        # Reduce dimensionality to 2D
        pca = PCA(n_components=2)
        embeddings_2d = pca.fit_transform(embeddings)
        
        # Create plot
        plt.figure(figsize=(12, 8))
        
        if labels:
            # Color by labels
            unique_labels = list(set(labels))
            colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))
            
            for i, label in enumerate(unique_labels):
                mask = np.array(labels) == label
                plt.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1], 
                          c=[colors[i]], label=label, alpha=0.7)
            
            plt.legend()
        else:
            plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.7)
        
        plt.title('Embedding Visualization (PCA)')
        plt.xlabel(f'First Principal Component (explains {pca.explained_variance_ratio_[0]:.1%} of variance)')
        plt.ylabel(f'Second Principal Component (explains {pca.explained_variance_ratio_[1]:.1%} of variance)')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"Embedding visualization saved to {output_file}")
        return output_file

# Practical demonstration
async def demonstrate_openai_api():
    """Comprehensive demonstration of OpenAI API capabilities"""
    
    logger.info("=== OpenAI API Comprehensive Demonstration ===")
    
    # Initialize configuration
    config = OpenAIConfig(
        api_key=os.getenv('OPENAI_API_KEY'),
        default_model="gpt-4",
        rate_limit_rpm=3000,
        rate_limit_tpm=80000
    )
    
    if not config.api_key:
        logger.error("OpenAI API key not found. Please set OPENAI_API_KEY environment variable.")
        return
    
    # Initialize client
    client = OpenAIClient(config)
    
    # 1. System Message Management
    logger.info("\n1. System Message Management")
    
    system_manager = SystemMessageManager()
    
    # Create different system messages
    professional_system = system_manager.create_system_message(
        personality_type="professional",
        constraints=["Always provide sources", "Be concise but thorough"],
        custom_instructions=["Focus on actionable advice", "Use bullet points for clarity"]
    )
    
    technical_system = system_manager.create_system_message(
        personality_type="technical",
        domain="specialist",
        constraints=["Provide code examples", "Explain complex concepts clearly"]
    )
    
    logger.info(f"Created professional system message: {professional_system[:100]}...")
    logger.info(f"Created technical system message: {technical_system[:100]}...")
    
    # 2. Chat Completion Demonstrations
    logger.info("\n2. Chat Completion Demonstrations")
    
    # Basic completion
    messages = [
        ChatMessage(role="system", content=professional_system),
        ChatMessage(role="user", content="Explain the benefits of using OpenAI's API for business applications.")
    ]
    
    request = CompletionRequest(
        messages=messages,
        model="gpt-4",
        temperature=0.7,
        max_tokens=500
    )
    
    try:
        response = await client.chat_completion(request)
        logger.info(f"Completion response: {response['content'][:200]}...")
        logger.info(f"Tokens used: {response['usage']['total_tokens']}, Cost: ${response['cost']:.4f}")
    except Exception as e:
        logger.error(f"Chat completion error: {e}")
    
    # 3. Conversation Management
    logger.info("\n3. Conversation Management")
    
    conversation_manager = ConversationManager(client, max_context_length=4000)
    
    # Start conversation
    conversation_id = "demo_conversation"
    conversation_manager.start_conversation(
        conversation_id, 
        technical_system,
        metadata={"topic": "OpenAI API", "user_id": "demo_user"}
    )
    
    # Continue conversation
    try:
        conv_response = await conversation_manager.continue_conversation(
            conversation_id,
            "Can you help me understand how to implement rate limiting for OpenAI API calls?",
            model="gpt-4",
            temperature=0.3
        )
        
        logger.info(f"Conversation response: {conv_response['content'][:200]}...")
        
        # Get conversation summary
        summary = conversation_manager.get_conversation_summary(conversation_id)
        logger.info(f"Conversation summary: {summary}")
        
    except Exception as e:
        logger.error(f"Conversation error: {e}")
    
    # 4. Embedding Analysis
    logger.info("\n4. Embedding Analysis")
    
    embedding_analyzer = EmbeddingAnalyzer(client)
    
    # Sample texts for analysis
    sample_texts = [
        "OpenAI provides powerful language models through their API.",
        "The API offers chat completion, embeddings, and moderation endpoints.",
        "Machine learning models can be fine-tuned for specific tasks.",
        "Natural language processing enables computers to understand human language.",
        "Artificial intelligence is transforming various industries.",
        "Deep learning uses neural networks with multiple layers."
    ]
    
    try:
        # Analyze similarity
        similarity_analysis = await embedding_analyzer.analyze_text_similarity(
            sample_texts, 
            cache_key="demo_similarity"
        )
        
        logger.info(f"Average similarity: {similarity_analysis['average_similarity']:.3f}")
        logger.info(f"Most similar pair: {similarity_analysis['most_similar_pairs'][0]['similarity']:.3f}")
        
        # Cluster texts
        clustering_results = await embedding_analyzer.cluster_texts(
            sample_texts, 
            n_clusters=3,
            cache_key="demo_similarity"
        )
        
        logger.info(f"Clustering completed with {len(clustering_results['clusters'])} clusters")
        for cluster_id, texts in clustering_results['clusters'].items():
            logger.info(f"Cluster {cluster_id}: {len(texts)} texts")
            
    except Exception as e:
        logger.error(f"Embedding analysis error: {e}")
    
    # 5. Content Moderation
    logger.info("\n5. Content Moderation")
    
    moderation_texts = [
        "This is a normal, safe message about technology.",
        "I love using AI tools for productivity.",
        "The weather is nice today."
    ]
    
    try:
        moderation_request = ModerationRequest(input_text=moderation_texts)
        moderation_results = await client.moderate_content(moderation_request)
        
        for i, result in enumerate(moderation_results['results']):
            logger.info(f"Text {i+1} flagged: {result['flagged']}")
            if result['flagged']:
                flagged_categories = [cat for cat, flagged in result['categories'].items() if flagged]
                logger.info(f"  Flagged categories: {flagged_categories}")
                
    except Exception as e:
        logger.error(f"Content moderation error: {e}")
    
    # 6. Fine-tuning Demonstration (preparation only)
    logger.info("\n6. Fine-tuning Data Preparation")
    
    fine_tuning_manager = FineTuningManager(client)
    
    # Sample training conversations
    training_conversations = [
        [
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": "What is Python?"},
            {"role": "assistant", "content": "Python is a high-level programming language known for its simplicity and readability."}
        ],
        [
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": "How do I install packages in Python?"},
            {"role": "assistant", "content": "You can install Python packages using pip, the package installer for Python. Use 'pip install package_name' in your terminal."}
        ]
    ]
    
    # Prepare training data
    training_file = fine_tuning_manager.prepare_training_data(
        training_conversations, 
        "demo_training_data.jsonl"
    )
    
    logger.info(f"Training data prepared: {training_file}")
    
    # Note: Actual fine-tuning would require uploading data and creating job
    # This is commented out to avoid unnecessary API calls and costs
    """
    try:
        file_id = await fine_tuning_manager.upload_training_file(training_file)
        
        job_config = FineTuningJob(
            training_file=file_id,
            model="gpt-3.5-turbo",
            n_epochs=3,
            suffix="demo"
        )
        
        job_id = await fine_tuning_manager.create_fine_tuning_job(job_config)
        logger.info(f"Fine-tuning job created: {job_id}")
        
    except Exception as e:
        logger.error(f"Fine-tuning setup error: {e}")
    """
    
    # 7. Performance Metrics
    logger.info("\n7. Performance Metrics and Cost Analysis")
    
    metrics = client.get_metrics()
    logger.info(f"Total requests: {metrics['total_requests']}")
    logger.info(f"Total tokens: {metrics['total_tokens']}")
    logger.info(f"Total cost: ${metrics['total_cost']:.4f}")
    logger.info(f"Average tokens per request: {metrics['average_tokens_per_request']:.1f}")
    logger.info(f"Error rate: {metrics['error_rate']:.2%}")
    
    # Create cost breakdown visualization
    plt.figure(figsize=(10, 6))
    
    # Mock cost breakdown by model (in real scenario, track per model)
    models = ['gpt-4', 'gpt-3.5-turbo', 'text-embedding-ada-002']
    costs = [metrics['total_cost'] * 0.7, metrics['total_cost'] * 0.2, metrics['total_cost'] * 0.1]
    
    plt.subplot(1, 2, 1)
    plt.pie(costs, labels=models, autopct='%1.1f%%')
    plt.title('Cost Breakdown by Model')
    
    plt.subplot(1, 2, 2)
    usage_types = ['Input Tokens', 'Output Tokens']
    token_costs = [metrics['total_cost'] * 0.4, metrics['total_cost'] * 0.6]
    plt.pie(token_costs, labels=usage_types, autopct='%1.1f%%')
    plt.title('Cost Breakdown by Token Type')
    
    plt.tight_layout()
    plt.savefig('openai_usage_analysis.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    logger.info("Usage analysis visualization saved to 'openai_usage_analysis.png'")
    
    # 8. Generate comprehensive report
    report = {
        "demonstration_summary": {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "total_api_calls": metrics['total_requests'],
            "total_cost": metrics['total_cost'],
            "total_tokens": metrics['total_tokens']
        },
        "features_demonstrated": [
            "System message management",
            "Chat completions with advanced parameters",
            "Conversation management with context trimming",
            "Embedding generation and analysis",
            "Text similarity and clustering",
            "Content moderation",
            "Fine-tuning data preparation",
            "Cost tracking and analysis"
        ],
        "performance_metrics": metrics,
        "recommendations": [
            "Implement proper rate limiting for production use",
            "Monitor token usage and costs continuously",
            "Use appropriate models for different tasks",
            "Implement caching for embeddings when possible",
            "Consider fine-tuning for domain-specific applications"
        ]
    }
    
    with open("openai_demonstration_report.json", "w") as f:
        json.dump(report, f, indent=2)
    
    logger.info("Demonstration completed! Check 'openai_demonstration_report.json' for detailed results.")

# Main execution
async def main():
    """Main execution for OpenAI API demonstration"""
    try:
        await demonstrate_openai_api()
    except Exception as e:
        logger.error(f"Demonstration failed: {e}")
        import traceback
        logger.error(traceback.format_exc())

if __name__ == "__main__":
    asyncio.run(main())
````

## Conclusion

The comprehensive OpenAI API framework demonstrates the sophisticated capabilities available for integrating state-of-the-art language models into production applications. This implementation provides enterprise-ready solutions for chat completion, embeddings, content moderation, and fine-tuning workflows.

**Advanced API Integration** through sophisticated client management, rate limiting, and error handling ensures reliable, scalable interactions with OpenAI's services. The framework handles production concerns including cost tracking, token management, and performance monitoring.

**System Message Engineering** enables precise control over AI behavior through structured personality definition, constraint specification, and domain-specific customization. This systematic approach ensures consistent, appropriate responses across different use cases and applications.

**Conversation Management** provides sophisticated context handling, automatic trimming, and stateful interactions that maintain coherence across extended dialogues. The framework supports complex conversational flows while managing token limits efficiently.

**Embedding Analytics** offers powerful capabilities for semantic analysis, similarity detection, and clustering applications. The integration with visualization tools enables deeper understanding of text relationships and content organization.

**Fine-tuning Infrastructure** establishes comprehensive workflows for model customization, including data preparation, job management, monitoring, and evaluation. This capability enables organizations to adapt models to specific domains and requirements.

**Cost Optimization** through intelligent rate limiting, token estimation, and usage tracking provides essential financial controls for production deployments. The framework enables informed decisions about model selection and usage patterns.

**Content Safety** integration ensures responsible AI deployment through automated moderation capabilities that detect and flag potentially harmful content across multiple categories.

**Production Readiness** considerations including async processing, error recovery, metrics collection, and monitoring establish the foundation for reliable, scalable AI applications in enterprise environments.

This comprehensive framework provides developers with the tools and methodologies necessary to leverage OpenAI's powerful capabilities while maintaining control, efficiency, and safety in production applications. The modular design enables adaptation to specific requirements while following best practices for modern AI development.