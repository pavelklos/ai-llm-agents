<small>Claude 3.7 Sonnet Thinking</small>
# 08. LangGraph (AI Agents)

## Key Terms

- **LangGraph**: An extension of LangChain that provides graph-based orchestration for building stateful, multi-agent systems modeled as workflows.
- **AI Agent**: An autonomous system that perceives its environment, makes decisions, and takes actions to accomplish tasks.
- **Agent Orchestration**: The coordination and management of multiple AI agents working together to solve complex problems.
- **Supervision**: Monitoring and guiding agent behaviors, including human-in-the-loop oversight and automated guardrails.
- **State Management**: Tracking and persisting information across agent interactions and workflow steps.
- **Graph-Based Workflow**: A directed graph where nodes represent states or actions and edges represent transitions.
- **Thread**: An execution context that maintains state throughout a multi-step agent interaction.
- **Human Feedback**: Integration of human input at critical decision points in an agent workflow.

## Orchestrating Multiple AI Agents

LangGraph enables sophisticated orchestration of AI agents through graph-based workflows. Here's how to build a multi-agent system with supervision:

```python
import os
from typing import Dict, List, Tuple, Any, Optional, TypedDict, Annotated, Literal
from enum import Enum
import json
import uuid
from dotenv import load_dotenv
from pydantic import BaseModel, Field

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, FunctionMessage
from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatAnthropic
from langchain_core.runnables import RunnablePassthrough

from langgraph.graph import StateGraph, END
from langgraph.checkpoint import MemorySaver
from langgraph.prebuilt import ToolNode
from langgraph.prebuilt.tool_executor import ToolExecutor

# Load environment variables
load_dotenv()

class AgentRole(str, Enum):
    """Enumeration of possible agent roles in the system."""
    COORDINATOR = "coordinator"
    RESEARCHER = "researcher"
    WRITER = "writer"
    CRITIC = "critic"

class Message(BaseModel):
    """Message exchanged between agents."""
    role: str = Field(description="Role of the message sender")
    content: str = Field(description="Content of the message")

class AgentState(BaseModel):
    """State maintained throughout the agent workflow."""
    messages: List[Message] = Field(default_factory=list, description="Message history")
    task: str = Field(default="", description="Current task being worked on")
    context: Dict[str, Any] = Field(default_factory=dict, description="Contextual information")
    current_agent: AgentRole = Field(default=AgentRole.COORDINATOR, description="Currently active agent")
    supervisor_feedback: Optional[str] = Field(default=None, description="Feedback from the supervisor")
    status: str = Field(default="in_progress", description="Current status of the workflow")

class OrchestratedAgentSystem:
    """System for orchestrating multiple AI agents with supervision."""
    
    def __init__(
        self, 
        openai_model: str = "gpt-4o",
        anthropic_model: str = "claude-3-sonnet-20240229",
        verbose: bool = False
    ):
        """Initialize the multi-agent system.
        
        Args:
            openai_model: OpenAI model to use
            anthropic_model: Anthropic model to use
            verbose: Whether to enable verbose output
        """
        self.verbose = verbose
        
        # Initialize models - using different models for different roles
        self.openai_llm = ChatOpenAI(
            model=openai_model,
            temperature=0.1,
            api_key=os.getenv("OPENAI_API_KEY")
        )
        
        self.anthropic_llm = ChatAnthropic(
            model=anthropic_model,
            temperature=0.2,
            api_key=os.getenv("ANTHROPIC_API_KEY")
        )
        
        # Initialize agent prompts
        self._setup_agent_prompts()
        
        # Create the workflow graph
        self.graph = self._build_agent_graph()
        
        # Create the compiled agent workflow
        self.agent_executor = self.graph.compile(
            checkpointer=MemorySaver()
        )
    
    def _setup_agent_prompts(self):
        """Setup the prompts for each agent role."""
        
        # Coordinator agent prompt
        self.coordinator_prompt = ChatPromptTemplate.from_messages([
            SystemMessage(content="""You are the Coordinator agent responsible for managing the workflow.
                Your job is to understand the task, break it down, and delegate to specialized agents.
                You also make the final decision on when the task is complete or needs revision.
                
                Available agents:
                - Researcher: Finds and analyzes information
                - Writer: Creates content based on the research
                - Critic: Reviews work and provides constructive feedback
                
                Never pretend to be another agent. Your role is to coordinate."""),
            MessagesPlaceholder(variable_name="messages"),
            HumanMessage(content="""Current task: {task}
                Current status: {status}
                
                Decide what to do next:
                1. Direct a question or task to a specific agent
                2. Review the work and provide feedback
                3. Declare the task complete if all requirements are met
                
                Respond with your decision and reasoning.""")
        ])
        
        # Researcher agent prompt
        self.researcher_prompt = ChatPromptTemplate.from_messages([
            SystemMessage(content="""You are the Researcher agent. Your job is to gather and analyze information
                related to the current task. Provide thorough, accurate details that will help the team.
                Always cite your sources if applicable."""),
            MessagesPlaceholder(variable_name="messages"),
            HumanMessage(content="""Current task: {task}
                
                Conduct research on this topic and report your findings. Focus on providing:
                - Key facts and data
                - Different perspectives or approaches
                - Recent developments if relevant
                - Analysis of the information
                
                Be thorough but concise.""")
        ])
        
        # Writer agent prompt
        self.writer_prompt = ChatPromptTemplate.from_messages([
            SystemMessage(content="""You are the Writer agent. Your job is to create high-quality content
                based on the research and specifications provided. Focus on clarity, coherence, and 
                meeting the requirements of the task."""),
            MessagesPlaceholder(variable_name="messages"),
            HumanMessage(content="""Current task: {task}
                
                Create content based on the information provided. Your writing should be:
                - Well-structured and coherent
                - Accurate and based on the research
                - Tailored to the specific requirements
                - Professional in tone
                
                Produce your best work for this task.""")
        ])
        
        # Critic agent prompt
        self.critic_prompt = ChatPromptTemplate.from_messages([
            SystemMessage(content="""You are the Critic agent. Your job is to review work produced by the team
                and provide constructive feedback. Focus on identifying issues, gaps, and opportunities
                for improvement while being fair and specific."""),
            MessagesPlaceholder(variable_name="messages"),
            HumanMessage(content="""Current task: {task}
                
                Review the latest work produced and provide feedback on:
                - Accuracy and completeness
                - Structure and clarity
                - Adherence to requirements
                - Areas for improvement
                
                Be specific in your feedback with examples.""")
        ])
        
        # Supervisor prompt
        self.supervisor_prompt = ChatPromptTemplate.from_messages([
            SystemMessage(content="""You are a Supervisor overseeing a team of AI agents working on a task.
                Your job is to review the progress, provide guidance, and ensure quality work.
                You can intervene when needed to redirect the team."""),
            MessagesPlaceholder(variable_name="messages"),
            HumanMessage(content="""Current task: {task}
                Current status: {status}
                
                Review the conversation and work progress. Provide feedback or guidance to the team.
                You can:
                1. Provide specific advice to redirect the work
                2. Add information or context that is missing
                3. Approve the current direction
                4. Request revisions or improvements
                
                Give your feedback below:""")
        ])
    
    def _build_agent_graph(self) -> StateGraph:
        """Build the agent orchestration graph.
        
        Returns:
            StateGraph: The complete agent workflow graph
        """
        # Create the graph with the agent state
        graph = StateGraph(AgentState)
        
        # Define the coordinator node
        def coordinator_node(state: AgentState) -> AgentState:
            # Prepare the input context
            messages_for_prompt = [
                Message(role=msg.role, content=msg.content)
                for msg in state.messages
            ]
            
            # Get the coordinator response
            response = self.coordinator_prompt.invoke({
                "messages": messages_for_prompt,
                "task": state.task,
                "status": state.status
            })
            
            # Add the response to the message history
            state.messages.append(Message(
                role=AgentRole.COORDINATOR.value,
                content=response.content
            ))
            
            # Determine the next agent based on the coordinator's decision
            if "researcher" in response.content.lower():
                state.current_agent = AgentRole.RESEARCHER
            elif "writer" in response.content.lower():
                state.current_agent = AgentRole.WRITER
            elif "critic" in response.content.lower():
                state.current_agent = AgentRole.CRITIC
            elif "complete" in response.content.lower() or "finished" in response.content.lower():
                state.status = "completed"
            
            if self.verbose:
                print(f"Coordinator decided next agent: {state.current_agent}")
            
            return state
        
        # Define the researcher node
        def researcher_node(state: AgentState) -> AgentState:
            # Prepare the input context
            messages_for_prompt = [
                Message(role=msg.role, content=msg.content)
                for msg in state.messages
            ]
            
            # Get the researcher response
            response = self.researcher_prompt.invoke({
                "messages": messages_for_prompt,
                "task": state.task
            })
            
            # Add the response to the message history
            state.messages.append(Message(
                role=AgentRole.RESEARCHER.value,
                content=response.content
            ))
            
            # Return to coordinator
            state.current_agent = AgentRole.COORDINATOR
            
            return state
        
        # Define the writer node
        def writer_node(state: AgentState) -> AgentState:
            # Prepare the input context
            messages_for_prompt = [
                Message(role=msg.role, content=msg.content)
                for msg in state.messages
            ]
            
            # Get the writer response
            response = self.writer_prompt.invoke({
                "messages": messages_for_prompt,
                "task": state.task
            })
            
            # Add the response to the message history
            state.messages.append(Message(
                role=AgentRole.WRITER.value,
                content=response.content
            ))
            
            # Return to coordinator
            state.current_agent = AgentRole.COORDINATOR
            
            return state
        
        # Define the critic node
        def critic_node(state: AgentState) -> AgentState:
            # Prepare the input context
            messages_for_prompt = [
                Message(role=msg.role, content=msg.content)
                for msg in state.messages
            ]
            
            # Get the critic response
            response = self.critic_prompt.invoke({
                "messages": messages_for_prompt,
                "task": state.task
            })
            
            # Add the response to the message history
            state.messages.append(Message(
                role=AgentRole.CRITIC.value,
                content=response.content
            ))
            
            # Return to coordinator
            state.current_agent = AgentRole.COORDINATOR
            
            return state
        
        # Define the supervisor node
        def supervisor_node(state: AgentState) -> AgentState:
            # Prepare the input context
            messages_for_prompt = [
                Message(role=msg.role, content=msg.content)
                for msg in state.messages
            ]
            
            # Get the supervisor response
            response = self.supervisor_prompt.invoke({
                "messages": messages_for_prompt,
                "task": state.task,
                "status": state.status
            })
            
            # Add the supervisor feedback to the state
            state.supervisor_feedback = response.content
            
            # Add the response to the message history
            state.messages.append(Message(
                role="supervisor",
                content=response.content
            ))
            
            # Return to coordinator for next steps
            state.current_agent = AgentRole.COORDINATOR
            
            return state
        
        # Define conditional routing based on the current agent
        def router(state: AgentState) -> str:
            if state.status == "completed":
                return END
            
            # Check if supervisor should intervene
            # This could be based on a condition, random probability, or after certain steps
            if (len(state.messages) > 0 and len(state.messages) % 5 == 0 and 
                state.messages[-1].role != "supervisor"):
                return "supervisor"
            
            return state.current_agent.value
        
        # Add nodes to the graph
        graph.add_node(AgentRole.COORDINATOR.value, coordinator_node)
        graph.add_node(AgentRole.RESEARCHER.value, researcher_node)
        graph.add_node(AgentRole.WRITER.value, writer_node)
        graph.add_node(AgentRole.CRITIC.value, critic_node)
        graph.add_node("supervisor", supervisor_node)
        
        # Set the entry point
        graph.set_entry_point(AgentRole.COORDINATOR.value)
        
        # Add edges based on the router function
        graph.add_conditional_edges(
            router,
            {
                AgentRole.COORDINATOR.value: AgentRole.COORDINATOR.value,
                AgentRole.RESEARCHER.value: AgentRole.RESEARCHER.value,
                AgentRole.WRITER.value: AgentRole.WRITER.value,
                AgentRole.CRITIC.value: AgentRole.CRITIC.value,
                "supervisor": "supervisor",
                END: END
            }
        )
        
        return graph
    
    def run(self, task: str, max_iterations: int = 15) -> Dict[str, Any]:
        """Run the orchestrated agent system on a task.
        
        Args:
            task: The task description
            max_iterations: Maximum number of iterations to run
            
        Returns:
            Dict containing the final state and results
        """
        # Initialize the state
        initial_state = AgentState(
            messages=[],
            task=task,
            current_agent=AgentRole.COORDINATOR,
            status="in_progress"
        )
        
        # Track the number of iterations
        iteration = 0
        
        # Get the thread
        thread = {"configurable": {"thread_id": str(uuid.uuid4())}}
        
        # Run the agent workflow
        while iteration < max_iterations:
            # Run one step of the workflow
            result = self.agent_executor.invoke(
                {"state": initial_state, **thread}
            )
            
            # Update the state
            initial_state = result["state"]
            
            # Check if the workflow is complete
            if initial_state.status == "completed":
                break
                
            iteration += 1
            
            if self.verbose:
                print(f"Completed iteration {iteration}")
                
                # Print the last message
                if initial_state.messages:
                    last_msg = initial_state.messages[-1]
                    print(f"Last message from {last_msg.role}: {last_msg.content[:100]}...")
        
        # Prepare the final result
        result = {
            "status": initial_state.status,
            "iterations": iteration,
            "messages": [{"role": msg.role, "content": msg.content} for msg in initial_state.messages],
            "final_output": initial_state.messages[-1].content if initial_state.messages else None
        }
        
        return result
```

## Integrating RAG into AI Agents

Enhancing agents with Retrieval-Augmented Generation enables them to access external knowledge when making decisions:

```python
import os
from typing import Dict, List, Any, Optional
import uuid
from dotenv import load_dotenv

from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.tools import tool, Tool
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser

from langgraph.graph import StateGraph, END
from langgraph.checkpoint import MemorySaver
from langgraph.prebuilt import ToolNode
from langgraph.prebuilt.tool_executor import ToolExecutor

# Load environment variables
load_dotenv()

class RAGEnabledAgentSystem:
    """Agent system enhanced with RAG capabilities."""
    
    def __init__(
        self,
        model_name: str = "gpt-4o",
        embedding_model: str = "text-embedding-3-small",
        db_directory: str = "./rag_agent_db",
        verbose: bool = False
    ):
        """Initialize the RAG-enabled agent system.
        
        Args:
            model_name: Name of the LLM to use
            embedding_model: Name of the embedding model to use
            db_directory: Directory to store the vector database
            verbose: Whether to enable verbose output
        """
        self.verbose = verbose
        self.db_directory = db_directory
        
        # Initialize the LLM
        self.llm = ChatOpenAI(
            model=model_name,
            temperature=0.1,
            api_key=os.getenv("OPENAI_API_KEY")
        )
        
        # Initialize the embeddings
        self.embeddings = OpenAIEmbeddings(
            model=embedding_model,
            api_key=os.getenv("OPENAI_API_KEY")
        )
        
        # Setup the RAG components
        self._setup_rag_components()
        
        # Define tools
        self._define_tools()
        
        # Build the graph
        self.graph = self._build_graph()
        
        # Create the compiled agent workflow
        self.agent_executor = self.graph.compile(
            checkpointer=MemorySaver()
        )
    
    def _setup_rag_components(self):
        """Set up the RAG components including vector store."""
        
        # Create text splitter
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len
        )
        
        # Initialize vector store if it exists
        if os.path.exists(self.db_directory):
            if self.verbose:
                print(f"Loading existing vector store from {self.db_directory}")
            
            self.vectorstore = Chroma(
                persist_directory=self.db_directory,
                embedding_function=self.embeddings
            )
        else:
            # Create an empty vector store
            if self.verbose:
                print(f"Creating new vector store at {self.db_directory}")
            
            self.vectorstore = Chroma(
                persist_directory=self.db_directory,
                embedding_function=self.embeddings
            )
            os.makedirs(self.db_directory, exist_ok=True)
            self.vectorstore.persist()
    
    def _define_tools(self):
        """Define the tools available to the agent."""
        
        @tool
        def search_knowledge_base(query: str) -> str:
            """Search the knowledge base for information related to the query."""
            results = self.vectorstore.similarity_search(query, k=3)
            
            if not results:
                return "No relevant information found in the knowledge base."
            
            # Format the results
            formatted_results = []
            for i, doc in enumerate(results):
                content = doc.page_content
                source = doc.metadata.get("source", "Unknown source")
                
                formatted_results.append(f"Result {i+1} (from {source}):\n{content}\n")
            
            return "\n".join(formatted_results)
        
        @tool
        def save_to_knowledge_base(content: str, metadata: Dict[str, Any] = None) -> str:
            """Save new information to the knowledge base."""
            if not metadata:
                metadata = {"source": "agent_input", "timestamp": str(uuid.uuid4())}
            
            # Split content into chunks
            texts = self.text_splitter.split_text(content)
            
            # Add metadata to each chunk
            metadatas = [metadata] * len(texts)
            
            # Add to vector store
            self.vectorstore.add_texts(texts, metadatas)
            self.vectorstore.persist()
            
            return f"Successfully added {len(texts)} chunks to the knowledge base."
        
        # Create the tool list
        self.tools = [
            search_knowledge_base,
            save_to_knowledge_base
        ]
        
        # Create the tool executor
        self.tool_executor = ToolExecutor(self.tools)
    
    def _build_graph(self) -> StateGraph:
        """Build the agent workflow graph with RAG integration.
        
        Returns:
            The complete agent workflow graph
        """
        # Define the agent state
        class AgentState(TypedDict):
            messages: List[Dict[str, str]]
            tools_output: Optional[Dict[str, Any]]
            
        # Create the graph
        graph = StateGraph(AgentState)
        
        # Define the agent prompt
        agent_prompt = ChatPromptTemplate.from_messages([
            SystemMessage(content="""You are a knowledgeable assistant with access to a knowledge base.
                You can search for information and also store new information.
                
                Available tools:
                - search_knowledge_base: Search for information related to a query
                - save_to_knowledge_base: Save new information to the knowledge base
                
                When you don't know something, use the search tool to find information.
                If you learn something new and valuable, use the save tool to store it."""),
            MessagesPlaceholder(variable_name="messages"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ])
        
        # Create the agent node
        agent = agent_prompt | self.llm.bind_tools(self.tools)
        
        # Define the agent node function
        def agent_node(state):
            messages = state["messages"]
            
            # Format agent scratchpad with tool outputs
            if state.get("tools_output"):
                observation = state["tools_output"].get("output", "")
                tool_name = state["tools_output"].get("tool_name", "")
                
                if tool_name and observation:
                    agent_scratchpad = [
                        AIMessage(content=f"I'll use the {tool_name} tool."),
                        FunctionMessage(name=tool_name, content=observation)
                    ]
                else:
                    agent_scratchpad = []
            else:
                agent_scratchpad = []
            
            # Get the agent's response
            response = agent.invoke({
                "messages": messages,
                "agent_scratchpad": agent_scratchpad
            })
            
            # Add to the message history
            state["messages"].append(response)
            
            # Check if a tool was called
            tool_call = response.tool_calls
            
            if not tool_call:
                # No tool called, we're done
                return {"state": state, "next": END}
            
            # Extract the tool call details
            tool_name = tool_call[0].name
            tool_args = json.loads(tool_call[0].args)
            
            # Return the next step (call the tool)
            return {
                "state": state,
                "next": "tools",
                "tool_name": tool_name,
                "tool_args": tool_args
            }
        
        # Add the tool node
        tools = ToolNode(self.tools)
        
        # Add nodes to the graph
        graph.add_node("agent", agent_node)
        graph.add_node("tools", tools)
        
        # Configure edges
        graph.set_entry_point("agent")
        
        # Connect agent to tools and tools back to agent
        graph.add_edge("agent", "tools")
        graph.add_edge("tools", "agent")
        
        return graph
    
    def ingest_documents(self, document_path: str) -> int:
        """Ingest documents into the knowledge base.
        
        Args:
            document_path: Path to a file or directory to ingest
            
        Returns:
            Number of documents ingested
        """
        try:
            # Check if path is a directory or file
            if os.path.isdir(document_path):
                # Load all PDFs from directory
                loader = DirectoryLoader(
                    document_path,
                    glob="**/*.pdf",
                    loader_cls=PyPDFLoader
                )
            elif document_path.endswith(".pdf"):
                # Load single PDF
                loader = PyPDFLoader(document_path)
            else:
                raise ValueError(f"Unsupported document type: {document_path}")
            
            # Load the documents
            documents = loader.load()
            
            if self.verbose:
                print(f"Loaded {len(documents)} documents")
            
            # Split the documents
            texts = self.text_splitter.split_documents(documents)
            
            if self.verbose:
                print(f"Split into {len(texts)} chunks")
            
            # Add to vector store
            self.vectorstore.add_documents(texts)
            self.vectorstore.persist()
            
            return len(texts)
            
        except Exception as e:
            print(f"Error ingesting documents: {str(e)}")
            return 0
    
    def run_agent(self, query: str) -> List[Dict[str, str]]:
        """Run the RAG-enabled agent on a query.
        
        Args:
            query: The query to process
            
        Returns:
            List of messages in the conversation
        """
        # Initialize the state
        initial_state = {
            "messages": [HumanMessage(content=query)],
            "tools_output": None
        }
        
        # Get the thread
        thread = {"configurable": {"thread_id": str(uuid.uuid4())}}
        
        # Run the agent
        result = self.agent_executor.invoke(
            {"state": initial_state, **thread}
        )
        
        # Return the messages
        return [{
            "role": "human" if isinstance(msg, HumanMessage) else "ai",
            "content": msg.content
        } for msg in result["state"]["messages"]]
```

## AI Pipeline Design and Management

Creating sophisticated AI pipelines requires careful orchestration of components with proper state management:

```python
import os
from typing import Dict, List, Any, Optional, TypedDict, Annotated, Union
from enum import Enum
import json
import uuid
from dotenv import load_dotenv
from pydantic import BaseModel, Field

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.runnables import RunnableLambda

from langgraph.graph import StateGraph, END
from langgraph.checkpoint import MemorySaver
from langgraph.prebuilt import ToolNode

# Load environment variables
load_dotenv()

class PipelineStatus(str, Enum):
    """Status of the pipeline execution."""
    PLANNING = "planning"
    RESEARCHING = "researching"
    DRAFTING = "drafting"
    REFINING = "refining"
    REVIEWING = "reviewing"
    COMPLETED = "completed"
    ERROR = "error"

class PipelineState(BaseModel):
    """State maintained throughout the AI pipeline."""
    status: PipelineStatus = Field(default=PipelineStatus.PLANNING)
    task: str = Field(default="")
    plan: Optional[List[Dict[str, Any]]] = Field(default=None)
    current_step: int = Field(default=0)
    research_results: Optional[List[Dict[str, Any]]] = Field(default=None)
    draft: Optional[str] = Field(default=None)
    refinement_iterations: int = Field(default=0)
    review_feedback: Optional[List[str]] = Field(default=None)
    final_output: Optional[str] = Field(default=None)
    error: Optional[str] = Field(default=None)
    metadata: Dict[str, Any] = Field(default_factory=dict)

class AIPipelineDesigner:
    """Designer for sophisticated AI pipelines using LangGraph."""
    
    def __init__(
        self,
        model_name: str = "gpt-4o",
        embedding_model: str = "text-embedding-3-small",
        verbose: bool = False
    ):
        """Initialize the AI pipeline designer.
        
        Args:
            model_name: Name of the LLM to use
            embedding_model: Name of the embedding model
            verbose: Whether to enable verbose output
        """
        self.verbose = verbose
        
        # Initialize models
        self.llm = ChatOpenAI(
            model=model_name,
            temperature=0.2,
            api_key=os.getenv("OPENAI_API_KEY")
        )
        
        self.embeddings = OpenAIEmbeddings(
            model=embedding_model,
            api_key=os.getenv("OPENAI_API_KEY")
        )
        
        # Initialize the RAG components
        self.vectorstore = Chroma(
            collection_name="pipeline_knowledge",
            embedding_function=self.embeddings
        )
        
        # Create the pipeline graph
        self.pipeline_graph = self._build_pipeline_graph()
        
        # Compile the pipeline
        self.pipeline_executor = self.pipeline_graph.compile(
            checkpointer=MemorySaver()
        )
    
    def _build_pipeline_graph(self) -> StateGraph:
        """Build the AI pipeline graph.
        
        Returns:
            The complete pipeline graph
        """
        # Create the graph
        graph = StateGraph(PipelineState)
        
        # Define all the nodes
        
        # 1. Planner node - Creates a detailed plan for the task
        def planner_node(state: PipelineState) -> PipelineState:
            planner_prompt = ChatPromptTemplate.from_template(
                """You are an expert planner for AI content creation.
                Given the following task, create a detailed plan with steps.
                Each step should have a clear objective and approach.
                
                Task: {task}
                
                Create a JSON plan with the following structure:
                [
                    {{"step": 1, "name": "step name", "objective": "...", "approach": "..."}},
                    ...
                ]
                """
            )
            
            # Parse the output as JSON
            planner_chain = planner_prompt | self.llm | JsonOutputParser()
            
            try:
                # Generate the plan
                plan = planner_chain.invoke({"task": state.task})
                
                # Update the state
                state.plan = plan
                state.status = PipelineStatus.RESEARCHING
                
                if self.verbose:
                    print(f"Created plan with {len(plan)} steps")
                
                return state
                
            except Exception as e:
                state.error = f"Error in planning: {str(e)}"
                state.status = PipelineStatus.ERROR
                return state
        
        # 2. Researcher node - Gathers information
        def researcher_node(state: PipelineState) -> PipelineState:
            if not state.plan:
                state.error = "No plan available for research"
                state.status = PipelineStatus.ERROR
                return state
            
            research_prompt = ChatPromptTemplate.from_template(
                """You are an expert researcher.
                Based on the task and plan below, conduct thorough research 
                and provide comprehensive findings.
                
                Task: {task}
                
                Plan:
                {plan}
                
                Focus on finding detailed and accurate information.
                Organize your findings by topic in a structured JSON format:
                [
                    {{"topic": "topic name", "findings": "detailed findings", "sources": "if applicable"}}
                ]
                """
            )
            
            # Parse output as JSON
            research_chain = research_prompt | self.llm | JsonOutputParser()
            
            try:
                # Supplement with RAG if available
                rag_results = []
                if state.task:
                    # Search for relevant information
                    docs = self.vectorstore.similarity_search(state.task, k=3)
                    for doc in docs:
                        rag_results.append({
                            "content": doc.page_content,
                            "metadata": doc.metadata
                        })
                
                # Include RAG results in the prompt context
                plan_str = json.dumps(state.plan, indent=2)
                
                # Generate research
                research_input = {
                    "task": state.task,
                    "plan": plan_str
                }
                
                if rag_results:
                    research_input["rag_context"] = json.dumps(rag_results, indent=2)
                
                research_results = research_chain.invoke(research_input)
                
                # Update the state
                state.research_results = research_results
                state.status = PipelineStatus.DRAFTING
                
                if self.verbose:
                    print(f"Completed research with {len(research_results)} topics")
                
                return state
                
            except Exception as e:
                state.error = f"Error in research: {str(e)}"
                state.status = PipelineStatus.ERROR
                return state
        
        # 3. Drafter node - Creates initial content
        def drafter_node(state: PipelineState) -> PipelineState:
            if not state.research_results:
                state.error = "No research results available for drafting"
                state.status = PipelineStatus.ERROR
                return state
            
            drafter_prompt = ChatPromptTemplate.from_template(
                """You are an expert content creator.
                Based on the task, plan, and research below, create a comprehensive first draft.
                
                Task: {task}
                
                Plan:
                {plan}
                
                Research findings:
                {research}
                
                Create a well-structured, detailed draft that addresses all aspects of the task.
                Focus on clarity, accuracy, and completeness.
                """
            )
            
            drafter_chain = drafter_prompt | self.llm | RunnableLambda(lambda x: x.content)
            
            try:
                # Generate draft
                draft = drafter_chain.invoke({
                    "task": state.task,
                    "plan": json.dumps(state.plan, indent=2),
                    "research": json.dumps(state.research_results, indent=2)
                })
                
                # Update the state
                state.draft = draft
                state.status = PipelineStatus.REFINING
                
                if self.verbose:
                    print("Created initial draft")
                
                return state
                
            except Exception as e:
                state.error = f"Error in drafting: {str(e)}"
                state.status = PipelineStatus.ERROR
                return state
        
        # 4. Refiner node - Improves the draft
        def refiner_node(state: PipelineState) -> PipelineState:
            if not state.draft:
                state.error = "No draft available for refinement"
                state.status = PipelineStatus.ERROR
                return state
            
            refiner_prompt = ChatPromptTemplate.from_template(
                """You are an expert editor and refiner.
                Review and improve the draft below to enhance its quality.
                
                Task: {task}
                
                Original draft:
                {draft}
                
                Focus on:
                - Improving clarity and flow
                - Enhancing accuracy and detail
                - Strengthening structure and coherence
                - Fixing any grammatical or style issues
                
                Provide a substantially improved version of the content.
                """
            )
            
            refiner_chain = refiner_prompt | self.llm | RunnableLambda(lambda x: x.content)
            
            try:
                # Refine the draft
                refined_draft = refiner_chain.invoke({
                    "task": state.task,
                    "draft": state.draft
                })
                
                # Update the state
                state.draft = refined_draft
                state.refinement_iterations += 1
                
                # Check if we need more refinement
                if state.refinement_iterations >= 2:
                    state.status = PipelineStatus.REVIEWING
                
                if self.verbose:
                    print(f"Completed refinement iteration {state.refinement_iterations}")
                
                return state
                
            except Exception as e:
                state.error = f"Error in refinement: {str(e)}"
                state.status = PipelineStatus.ERROR
                return state
        
        # 5. Reviewer node - Evaluates the content
        def reviewer_node(state: PipelineState) -> PipelineState:
            if not state.draft:
                state.error = "No draft available for review"
                state.status = PipelineStatus.ERROR
                return state
            
            reviewer_prompt = ChatPromptTemplate.from_template(
                """You are an expert content reviewer.
                Critically evaluate the content below against the original task.
                
                Task: {task}
                
                Content to review:
                {draft}
                
                Provide a thorough assessment including:
                1. Overall quality (1-10 scale)
                2. Strengths of the content
                3. Areas for improvement
                4. Specific suggestions
                
                Format your response as JSON:
                {{
                    "rating": 0-10,
                    "strengths": ["strength1", "strength2", ...],
                    "weaknesses": ["weakness1", "weakness2", ...],
                    "suggestions": ["suggestion1", "suggestion2", ...],
                    "requires_revision": true/false
                }}
                """
            )
            
            reviewer_chain = reviewer_prompt | self.llm | JsonOutputParser()
            
            try:
                # Review the content
                review = reviewer_chain.invoke({
                    "task": state.task,
                    "draft": state.draft
                })
                
                # Update the state
                if not state.review_feedback:
                    state.review_feedback = []
                
                state.review_feedback.append(review)
                
                # Determine next step based on review
                if review.get("requires_revision", False) and state.refinement_iterations < 3:
                    # Needs more refinement
                    state.status = PipelineStatus.REFINING
                else:
                    # Finalize the content
                    state.final_output = state.draft
                    state.status = PipelineStatus.COMPLETED
                
                if self.verbose:
                    rating = review.get("rating", "N/A")
                    print(f"Completed review with rating: {rating}/10")
                
                return state
                
            except Exception as e:
                state.error = f"Error in review: {str(e)}"
                state.status = PipelineStatus.ERROR
                return state
        
        # Add all nodes to the graph
        graph.add_node("planner", planner_node)
        graph.add_node("researcher", researcher_node)
        graph.add_node("drafter", drafter_node)
        graph.add_node("refiner", refiner_node)
        graph.add_node("reviewer", reviewer_node)
        
        # Define the conditional routing
        def router(state: PipelineState) -> str:
            """Route to the next node based on the current status."""
            if state.status == PipelineStatus.ERROR:
                return END
            elif state.status == PipelineStatus.PLANNING:
                return "planner"
            elif state.status == PipelineStatus.RESEARCHING:
                return "researcher"
            elif state.status == PipelineStatus.DRAFTING:
                return "drafter"
            elif state.status == PipelineStatus.REFINING:
                return "refiner"
            elif state.status == PipelineStatus.REVIEWING:
                return "reviewer"
            elif state.status == PipelineStatus.COMPLETED:
                return END
            else:
                # Default to end if unknown status
                return END
        
        # Set the entry point
        graph.set_entry_point("planner")
        
        # Add conditional edges
        graph.add_conditional_edges(
            router,
            {
                "planner": "planner",
                "researcher": "researcher",
                "drafter": "drafter",
                "refiner": "refiner",
                "reviewer": "reviewer",
                END: END
            }
        )
        
        return graph
    
    def run_pipeline(self, task: str) -> Dict[str, Any]:
        """Run the AI pipeline on a task.
        
        Args:
            task: Description of the task to perform
            
        Returns:
            Dictionary with the pipeline results
        """
        # Initialize the state
        initial_state = PipelineState(
            status=PipelineStatus.PLANNING,
            task=task
        )
        
        # Get the thread
        thread = {"configurable": {"thread_id": str(uuid.uuid4())}}
        
        # Run the pipeline
        try:
            result = self.pipeline_executor.invoke(
                {"state": initial_state, **thread}
            )
            
            # Extract final state
            final_state = result["state"]
            
            # Prepare the result summary
            summary = {
                "status": final_state.status,
                "task": final_state.task,
                "plan_steps": len(final_state.plan) if final_state.plan else 0,
                "refinement_iterations": final_state.refinement_iterations,
                "final_output": final_state.final_output,
                "error": final_state.error
            }
            
            if self.verbose:
                print(f"Pipeline completed with status: {final_state.status}")
            
            return summary
            
        except Exception as e:
            error_msg = f"Pipeline execution error: {str(e)}"
            
            if self.verbose:
                print(error_msg)
                
            return {
                "status": "error",
                "error": error_msg,
                "task": task
            }
    
    def add_knowledge(self, content: str, metadata: Dict[str, Any] = None) -> bool:
        """Add knowledge to the pipeline's knowledge base.
        
        Args:
            content: Content to add to the knowledge base
            metadata: Optional metadata for the content
            
        Returns:
            Success status
        """
        if not metadata:
            metadata = {"source": "manual_input", "timestamp": str(uuid.uuid4())}
            
        try:
            # Add the content to the vector store
            self.vectorstore.add_texts(
                texts=[content],
                metadatas=[metadata]
            )
            
            if self.verbose:
                print(f"Added content to knowledge base with metadata: {metadata}")
                
            return True
            
        except Exception as e:
            if self.verbose:
                print(f"Error adding to knowledge base: {str(e)}")
                
            return False
```

## Running a Complete Multi-Agent System

Here's an example of how to put everything together into a comprehensive workflow:

```python
def demonstrate_agent_orchestration():
    """Demonstrate a complete agent orchestration workflow."""
    
    # 1. Set up the orchestrated agent system
    print("\n1. Setting up orchestrated agent system...")
    agent_system = OrchestratedAgentSystem(verbose=True)
    
    # 2. Run a complex task through the system
    print("\n2. Running a complex task through the agent system...")
    task = """Create a comprehensive guide to implementing AI agents
    in a business context. Include best practices, potential challenges,
    and a step-by-step implementation approach."""
    
    result = agent_system.run(task, max_iterations=10)
    
    print(f"\nTask completed with status: {result['status']}")
    print(f"Iterations: {result['iterations']}")
    
    # Print the final output
    if result['final_output']:
        print("\nFinal output preview:")
        print(result['final_output'][:500] + "...")
    
    # 3. Set up a RAG-enabled agent system
    print("\n3. Setting up RAG-enabled agent system...")
    rag_agent = RAGEnabledAgentSystem(verbose=True)
    
    # 4. Ingest some documents (assuming you have a PDF file)
    print("\n4. Ingesting documents into the RAG system...")
    documents_path = "./sample_docs"  # Replace with actual path
    if os.path.exists(documents_path):
        num_documents = rag_agent.ingest_documents(documents_path)
        print(f"Ingested {num_documents} document chunks")
    else:
        # Create some example knowledge
        print("Sample document path not found, adding example knowledge...")
        rag_agent.save_to_knowledge_base("""
        AI agents are software entities that can perceive their environment,
        make decisions, and take actions to achieve specific goals. They can be
        designed to operate autonomously or semi-autonomously and can be integrated
        into various business processes to automate tasks and augment human capabilities.
        
        Key components of AI agents include:
        1. Perception: The ability to process inputs from the environment
        2. Reasoning: The ability to make decisions based on inputs and goals
        3. Action: The ability to execute tasks based on decisions
        4. Learning: The ability to improve performance over time
        
        Implementing AI agents in a business requires careful planning, including
        defining clear goals, selecting appropriate technologies, and establishing
        governance frameworks to ensure responsible use.
        """, {"source": "example_knowledge", "topic": "ai_agents"})
    
    # 5. Query the RAG-enabled agent
    print("\n5. Querying the RAG-enabled agent...")
    query = "What are the key considerations when implementing AI agents in a business context?"
    
    rag_response = rag_agent.run_agent(query)
    
    print("\nRAG Agent Response:")
    for message in rag_response:
        role = message["role"]
        content = message["content"]
        print(f"\n{role.upper()}: {content[:300]}..." if len(content) > 300 else f"\n{role.upper()}: {content}")
    
    # 6. Set up an AI pipeline
    print("\n6. Setting up an AI pipeline...")
    pipeline = AIPipelineDesigner(verbose=True)
    
    # 7. Run a task through the pipeline
    print("\n7. Running a task through the AI pipeline...")
    pipeline_task = "Create a white paper on the ethical considerations of deploying AI agents in healthcare"
    
    pipeline_result = pipeline.run_pipeline(pipeline_task)
    
    print(f"\nPipeline completed with status: {pipeline_result['status']}")
    
    if pipeline_result.get('final_output'):
        print("\nPipeline output preview:")
        output = pipeline_result['final_output']
        print(output[:500] + "..." if len(output) > 500 else output)
    
    print("\nDemo completed successfully!")
    
    return {
        "agent_system_result": result,
        "rag_agent_result": rag_response,
        "pipeline_result": pipeline_result
    }

if __name__ == "__main__":
    load_dotenv()
    demonstrate_agent_orchestration()
```

## Conclusion

LangGraph represents a significant advancement in AI agent orchestration by providing a robust framework for designing, implementing, and managing complex agent-based systems. The graph-based approach to workflow management enables developers to create sophisticated multi-agent systems that can tackle complex tasks through collaboration and specialization.

The integration of Retrieval-Augmented Generation (RAG) into agent systems addresses one of the key limitations of traditional LLM-based agents: their limited knowledge context. By enabling agents to retrieve information from external knowledge bases, RAG integration creates more capable and accurate AI systems that can leverage domain-specific information beyond their training data.

The ability to design and manage complex AI pipelines with proper state management allows for the creation of robust production systems that can handle multi-step reasoning, parallel processing, and graceful error recovery. This is particularly important for mission-critical applications where reliability and predictability are essential.

Key benefits of the LangGraph approach include:

1. **Explicit State Management**: By tracking state throughout the workflow, agents can maintain context across multiple interactions and make decisions based on accumulated information.

2. **Flexible Orchestration**: The graph-based structure allows for complex routing logic, conditional execution, and dynamic workflow adaptation based on intermediate results.

3. **Human-in-the-Loop Integration**: LangGraph makes it straightforward to incorporate human supervision and feedback at critical decision points in the agent workflow.

4. **Scalability**: The modular nature of LangGraph enables the creation of complex agent systems that can scale to handle sophisticated tasks through compositional design.

As AI agent technology continues to evolve, frameworks like LangGraph will play an increasingly important role in helping developers build, deploy, and manage complex AI systems that can reliably perform valuable tasks in business, healthcare, research, and other domains.