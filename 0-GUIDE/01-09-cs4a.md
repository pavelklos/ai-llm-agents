<small>Claude Sonnet 4</small>
# 09. Introduction to Reinforcement Learning

## Key Terms

**Reinforcement Learning (RL)**: A machine learning paradigm where an agent learns to make decisions by taking actions in an environment to maximize cumulative reward through trial and error interactions.

**Q-Learning**: A model-free reinforcement learning algorithm that learns the quality of actions, telling an agent what action to take under what circumstances by learning a Q-function that estimates the expected future reward for each state-action pair.

**Actor-Critic**: A hybrid reinforcement learning architecture that combines value-based and policy-based methods, where the actor learns the policy (what actions to take) and the critic learns the value function (how good the current state is).

**Policy-Based Methods**: RL algorithms that directly learn the policy function that maps states to actions, optimizing the policy parameters to maximize expected cumulative reward without explicitly learning value functions.

**Gymnasium**: The successor to OpenAI Gym, a toolkit for developing and comparing reinforcement learning algorithms that provides a standardized API for various environments and benchmarks.

**PettingZoo**: A Python library for conducting research in multi-agent reinforcement learning, providing a collection of diverse environments with a unified API for multi-agent scenarios.

**Markov Decision Process (MDP)**: The mathematical framework underlying reinforcement learning, consisting of states, actions, transition probabilities, and rewards that satisfy the Markov property.

**Exploration vs Exploitation**: The fundamental trade-off in reinforcement learning between exploring new actions to gather information and exploiting known good actions to maximize reward.

## Reinforcement Learning Algorithms and Implementation

Reinforcement learning represents a paradigm shift from supervised learning, where agents learn optimal behavior through environmental interaction rather than labeled examples. The field encompasses various algorithmic approaches, each with distinct advantages for different problem domains.

### Advanced Q-Learning Implementation

````python
import numpy as np
import gymnasium as gym
import matplotlib.pyplot as plt
import pickle
import json
from typing import Dict, List, Tuple, Any, Optional, Union
from collections import defaultdict, deque
import random
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from dotenv import load_dotenv
import os

load_dotenv()

@dataclass
class QLearningConfig:
    """Configuration for Q-Learning algorithm"""
    learning_rate: float = 0.1
    discount_factor: float = 0.99
    epsilon: float = 1.0
    epsilon_min: float = 0.01
    epsilon_decay: float = 0.995
    memory_size: int = 10000
    batch_size: int = 32
    target_update_frequency: int = 100
    training_episodes: int = 1000

class QNetwork(nn.Module):
    """Deep Q-Network for continuous state spaces"""
    
    def __init__(self, state_size: int, action_size: int, hidden_sizes: List[int] = [128, 128]):
        super(QNetwork, self).__init__()
        
        layers = []
        input_size = state_size
        
        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(input_size, hidden_size),
                nn.ReLU(),
                nn.Dropout(0.2)
            ])
            input_size = hidden_size
        
        layers.append(nn.Linear(input_size, action_size))
        
        self.network = nn.Sequential(*layers)
        
        # Initialize weights
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.xavier_uniform_(module.weight)
            module.bias.data.fill_(0.01)
    
    def forward(self, state):
        return self.network(state)

class ExperienceReplay:
    """Experience replay buffer for DQN"""
    
    def __init__(self, capacity: int):
        self.capacity = capacity
        self.buffer = deque(maxlen=capacity)
        self.position = 0
    
    def push(self, state, action, reward, next_state, done):
        """Save experience in replay buffer"""
        experience = (state, action, reward, next_state, done)
        self.buffer.append(experience)
    
    def sample(self, batch_size: int) -> List[Tuple]:
        """Sample random batch from buffer"""
        return random.sample(self.buffer, batch_size)
    
    def __len__(self):
        return len(self.buffer)

class AdvancedQLearningAgent:
    """Advanced Q-Learning agent with deep Q-networks and experience replay"""
    
    def __init__(self, state_size: int, action_size: int, config: QLearningConfig):
        self.state_size = state_size
        self.action_size = action_size
        self.config = config
        
        # Neural networks
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.q_network_local = QNetwork(state_size, action_size).to(self.device)
        self.q_network_target = QNetwork(state_size, action_size).to(self.device)
        self.optimizer = optim.Adam(self.q_network_local.parameters(), lr=config.learning_rate)
        
        # Experience replay
        self.memory = ExperienceReplay(config.memory_size)
        
        # Training metrics
        self.training_history = {
            'episode_rewards': [],
            'episode_lengths': [],
            'losses': [],
            'epsilon_values': [],
            'q_values': []
        }
        
        # Current state
        self.epsilon = config.epsilon
        self.steps_done = 0
        
    def act(self, state: np.ndarray, training: bool = True) -> int:
        """Choose action using epsilon-greedy policy"""
        
        if training and random.random() <= self.epsilon:
            return random.choice(range(self.action_size))
        
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            q_values = self.q_network_local(state_tensor)
            action = q_values.max(1)[1].item()
        
        return action
    
    def step(self, state, action, reward, next_state, done):
        """Save experience and learn from batch of experiences"""
        
        self.memory.push(state, action, reward, next_state, done)
        
        self.steps_done += 1
        
        # Learn from experiences
        if len(self.memory) > self.config.batch_size:
            experiences = self.memory.sample(self.config.batch_size)
            self._learn(experiences)
        
        # Update target network
        if self.steps_done % self.config.target_update_frequency == 0:
            self._soft_update(self.q_network_local, self.q_network_target, 0.01)
    
    def _learn(self, experiences: List[Tuple]):
        """Update value parameters using batch of experience tuples"""
        
        states, actions, rewards, next_states, dones = zip(*experiences)
        
        states = torch.FloatTensor(np.array(states)).to(self.device)
        actions = torch.LongTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)
        dones = torch.BoolTensor(dones).to(self.device)
        
        # Current Q values
        current_q_values = self.q_network_local(states).gather(1, actions.unsqueeze(1))
        
        # Next Q values from target network
        next_q_values = self.q_network_target(next_states).detach().max(1)[0]
        target_q_values = rewards + (self.config.discount_factor * next_q_values * ~dones)
        
        # Compute loss
        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)
        
        # Optimize
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.q_network_local.parameters(), 1.0)
        self.optimizer.step()
        
        # Store metrics
        self.training_history['losses'].append(loss.item())
        self.training_history['q_values'].append(current_q_values.mean().item())
        
        # Decay epsilon
        if self.epsilon > self.config.epsilon_min:
            self.epsilon *= self.config.epsilon_decay
    
    def _soft_update(self, local_model, target_model, tau):
        """Soft update model parameters"""
        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):
            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)
    
    def train(self, env, episodes: int = None) -> Dict[str, Any]:
        """Train the agent in the given environment"""
        
        episodes = episodes or self.config.training_episodes
        scores = deque(maxlen=100)
        
        for episode in range(episodes):
            state, _ = env.reset()
            total_reward = 0
            steps = 0
            
            while True:
                action = self.act(state, training=True)
                next_state, reward, terminated, truncated, _ = env.step(action)
                done = terminated or truncated
                
                self.step(state, action, reward, next_state, done)
                
                state = next_state
                total_reward += reward
                steps += 1
                
                if done:
                    break
            
            scores.append(total_reward)
            self.training_history['episode_rewards'].append(total_reward)
            self.training_history['episode_lengths'].append(steps)
            self.training_history['epsilon_values'].append(self.epsilon)
            
            # Print progress
            if episode % 100 == 0:
                avg_score = np.mean(scores)
                print(f"Episode {episode}, Average Score: {avg_score:.2f}, Epsilon: {self.epsilon:.3f}")
        
        return self.training_history
    
    def save_model(self, filepath: str):
        """Save trained model"""
        torch.save({
            'q_network_local': self.q_network_local.state_dict(),
            'q_network_target': self.q_network_target.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'config': self.config,
            'training_history': self.training_history
        }, filepath)
    
    def load_model(self, filepath: str):
        """Load trained model"""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.q_network_local.load_state_dict(checkpoint['q_network_local'])
        self.q_network_target.load_state_dict(checkpoint['q_network_target'])
        self.optimizer.load_state_dict(checkpoint['optimizer'])
        self.training_history = checkpoint['training_history']

class ActorCriticAgent:
    """Actor-Critic agent implementation"""
    
    def __init__(self, state_size: int, action_size: int, lr_actor: float = 0.0003, lr_critic: float = 0.001):
        self.state_size = state_size
        self.action_size = action_size
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Actor network
        self.actor = nn.Sequential(
            nn.Linear(state_size, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, action_size),
            nn.Softmax(dim=-1)
        ).to(self.device)
        
        # Critic network
        self.critic = nn.Sequential(
            nn.Linear(state_size, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        ).to(self.device)
        
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)
        
        # Training history
        self.training_history = {
            'episode_rewards': [],
            'actor_losses': [],
            'critic_losses': []
        }
    
    def act(self, state: np.ndarray) -> Tuple[int, torch.Tensor]:
        """Choose action using actor policy"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            action_probs = self.actor(state_tensor)
        
        action_dist = torch.distributions.Categorical(action_probs)
        action = action_dist.sample()
        
        return action.item(), action_dist.log_prob(action)
    
    def learn(self, state, action_log_prob, reward, next_state, done, gamma=0.99):
        """Update actor and critic networks"""
        
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0).to(self.device)
        
        # Critic update
        current_value = self.critic(state_tensor)
        next_value = self.critic(next_state_tensor) if not done else torch.tensor([0.0]).to(self.device)
        target_value = reward + gamma * next_value
        
        critic_loss = F.mse_loss(current_value, target_value.detach())
        
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()
        
        # Actor update
        advantage = (target_value - current_value).detach()
        actor_loss = -action_log_prob * advantage
        
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
        
        # Store losses
        self.training_history['actor_losses'].append(actor_loss.item())
        self.training_history['critic_losses'].append(critic_loss.item())
    
    def train(self, env, episodes: int = 1000) -> Dict[str, Any]:
        """Train actor-critic agent"""
        
        for episode in range(episodes):
            state, _ = env.reset()
            total_reward = 0
            
            while True:
                action, action_log_prob = self.act(state)
                next_state, reward, terminated, truncated, _ = env.step(action)
                done = terminated or truncated
                
                self.learn(state, action_log_prob, reward, next_state, done)
                
                state = next_state
                total_reward += reward
                
                if done:
                    break
            
            self.training_history['episode_rewards'].append(total_reward)
            
            if episode % 100 == 0:
                avg_reward = np.mean(self.training_history['episode_rewards'][-100:])
                print(f"Episode {episode}, Average Reward: {avg_reward:.2f}")
        
        return self.training_history

class PolicyGradientAgent:
    """Policy gradient agent with REINFORCE algorithm"""
    
    def __init__(self, state_size: int, action_size: int, lr: float = 0.001):
        self.state_size = state_size
        self.action_size = action_size
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Policy network
        self.policy_network = nn.Sequential(
            nn.Linear(state_size, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, action_size),
            nn.Softmax(dim=-1)
        ).to(self.device)
        
        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=lr)
        
        # Episode storage
        self.episode_log_probs = []
        self.episode_rewards = []
        
        # Training history
        self.training_history = {
            'episode_rewards': [],
            'policy_losses': []
        }
    
    def act(self, state: np.ndarray) -> int:
        """Choose action using policy network"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        action_probs = self.policy_network(state_tensor)
        action_dist = torch.distributions.Categorical(action_probs)
        action = action_dist.sample()
        
        # Store log probability for learning
        self.episode_log_probs.append(action_dist.log_prob(action))
        
        return action.item()
    
    def finish_episode(self, gamma: float = 0.99):
        """Update policy at end of episode using REINFORCE"""
        
        # Calculate discounted rewards
        discounted_rewards = []
        G = 0
        
        for reward in reversed(self.episode_rewards):
            G = reward + gamma * G
            discounted_rewards.insert(0, G)
        
        # Normalize rewards
        discounted_rewards = torch.FloatTensor(discounted_rewards).to(self.device)
        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9)
        
        # Calculate policy loss
        policy_loss = []
        for log_prob, reward in zip(self.episode_log_probs, discounted_rewards):
            policy_loss.append(-log_prob * reward)
        
        policy_loss = torch.stack(policy_loss).sum()
        
        # Update policy
        self.optimizer.zero_grad()
        policy_loss.backward()
        self.optimizer.step()
        
        # Store metrics
        self.training_history['policy_losses'].append(policy_loss.item())
        self.training_history['episode_rewards'].append(sum(self.episode_rewards))
        
        # Clear episode data
        self.episode_log_probs = []
        self.episode_rewards = []
        
        return policy_loss.item()
    
    def train(self, env, episodes: int = 1000) -> Dict[str, Any]:
        """Train policy gradient agent"""
        
        for episode in range(episodes):
            state, _ = env.reset()
            
            while True:
                action = self.act(state)
                next_state, reward, terminated, truncated, _ = env.step(action)
                done = terminated or truncated
                
                self.episode_rewards.append(reward)
                state = next_state
                
                if done:
                    break
            
            # Update policy at end of episode
            self.finish_episode()
            
            if episode % 100 == 0:
                avg_reward = np.mean(self.training_history['episode_rewards'][-100:])
                print(f"Episode {episode}, Average Reward: {avg_reward:.2f}")
        
        return self.training_history

class RLEnvironmentManager:
    """Manager for RL environments and experiments"""
    
    def __init__(self):
        self.environments = {}
        self.experiment_results = {}
    
    def register_environment(self, name: str, env_id: str, **kwargs):
        """Register a Gymnasium environment"""
        try:
            env = gym.make(env_id, **kwargs)
            self.environments[name] = {
                'env': env,
                'env_id': env_id,
                'state_size': env.observation_space.shape[0] if hasattr(env.observation_space, 'shape') else env.observation_space.n,
                'action_size': env.action_space.n if hasattr(env.action_space, 'n') else env.action_space.shape[0],
                'kwargs': kwargs
            }
            print(f"Registered environment: {name} ({env_id})")
        except Exception as e:
            print(f"Failed to register environment {name}: {e}")
    
    def run_experiment(self, env_name: str, agent_type: str, agent_config: Dict[str, Any] = None, 
                      episodes: int = 1000) -> Dict[str, Any]:
        """Run RL experiment"""
        
        if env_name not in self.environments:
            raise ValueError(f"Environment {env_name} not registered")
        
        env_info = self.environments[env_name]
        env = env_info['env']
        
        agent_config = agent_config or {}
        
        # Create agent based on type
        if agent_type == "dqn":
            config = QLearningConfig(**agent_config)
            agent = AdvancedQLearningAgent(env_info['state_size'], env_info['action_size'], config)
        elif agent_type == "actor_critic":
            agent = ActorCriticAgent(env_info['state_size'], env_info['action_size'], **agent_config)
        elif agent_type == "policy_gradient":
            agent = PolicyGradientAgent(env_info['state_size'], env_info['action_size'], **agent_config)
        else:
            raise ValueError(f"Unknown agent type: {agent_type}")
        
        # Train agent
        print(f"Training {agent_type} agent on {env_name} for {episodes} episodes...")
        training_history = agent.train(env, episodes)
        
        # Evaluate agent
        evaluation_results = self._evaluate_agent(agent, env, episodes=10)
        
        experiment_key = f"{env_name}_{agent_type}_{episodes}"
        self.experiment_results[experiment_key] = {
            'env_name': env_name,
            'agent_type': agent_type,
            'episodes': episodes,
            'training_history': training_history,
            'evaluation': evaluation_results,
            'agent': agent
        }
        
        return self.experiment_results[experiment_key]
    
    def _evaluate_agent(self, agent, env, episodes: int = 10) -> Dict[str, Any]:
        """Evaluate trained agent"""
        
        episode_rewards = []
        episode_lengths = []
        
        # Temporarily disable exploration for evaluation
        original_epsilon = getattr(agent, 'epsilon', 0)
        if hasattr(agent, 'epsilon'):
            agent.epsilon = 0
        
        for episode in range(episodes):
            state, _ = env.reset()
            total_reward = 0
            steps = 0
            
            while True:
                if hasattr(agent, 'act'):
                    action = agent.act(state, training=False)
                else:
                    action = agent.act(state)
                
                next_state, reward, terminated, truncated, _ = env.step(action)
                done = terminated or truncated
                
                state = next_state
                total_reward += reward
                steps += 1
                
                if done:
                    break
            
            episode_rewards.append(total_reward)
            episode_lengths.append(steps)
        
        # Restore original epsilon
        if hasattr(agent, 'epsilon'):
            agent.epsilon = original_epsilon
        
        return {
            'mean_reward': np.mean(episode_rewards),
            'std_reward': np.std(episode_rewards),
            'min_reward': np.min(episode_rewards),
            'max_reward': np.max(episode_rewards),
            'mean_length': np.mean(episode_lengths),
            'episode_rewards': episode_rewards
        }
    
    def compare_algorithms(self, env_name: str, algorithms: List[Dict[str, Any]], episodes: int = 1000):
        """Compare multiple RL algorithms on the same environment"""
        
        comparison_results = {}
        
        for algo_config in algorithms:
            agent_type = algo_config['type']
            agent_config = algo_config.get('config', {})
            
            print(f"\nRunning {agent_type} experiment...")
            result = self.run_experiment(env_name, agent_type, agent_config, episodes)
            
            comparison_results[agent_type] = {
                'final_performance': result['evaluation']['mean_reward'],
                'training_stability': np.std(result['training_history']['episode_rewards'][-100:]),
                'convergence_speed': self._calculate_convergence_speed(result['training_history']['episode_rewards'])
            }
        
        return comparison_results
    
    def _calculate_convergence_speed(self, rewards: List[float], threshold: float = 0.8) -> int:
        """Calculate episodes needed to reach threshold of final performance"""
        if len(rewards) < 100:
            return len(rewards)
        
        final_performance = np.mean(rewards[-100:])
        target = threshold * final_performance
        
        for i, reward in enumerate(rewards):
            if reward >= target:
                return i
        
        return len(rewards)
````

## Working with Gymnasium and PettingZoo

Gymnasium and PettingZoo provide standardized environments for single-agent and multi-agent reinforcement learning respectively, enabling reproducible research and algorithm comparison.

### Advanced Environment Integration

````python
import gymnasium as gym
import pettingzoo
from pettingzoo.classic import tictactoe_v3, chess_v6
from pettingzoo.mpe import simple_spread_v3
import numpy as np
import pygame
import torch
from typing import Dict, List, Any, Optional, Tuple
import matplotlib.pyplot as plt
from collections import deque
import json

class CustomFlappyBirdEnv(gym.Env):
    """Custom Flappy Bird environment for RL training"""
    
    metadata = {'render_modes': ['human', 'rgb_array'], 'render_fps': 60}
    
    def __init__(self, render_mode: Optional[str] = None):
        super().__init__()
        
        self.render_mode = render_mode
        
        # Game parameters
        self.screen_width = 400
        self.screen_height = 600
        self.bird_x = 50
        self.bird_y = 300
        self.bird_velocity = 0
        self.gravity = 0.5
        self.jump_strength = -8
        self.pipe_width = 80
        self.pipe_gap = 150
        self.pipe_velocity = -3
        
        # Environment state
        self.pipes = []
        self.score = 0
        self.steps = 0
        self.max_steps = 10000
        
        # Action space: 0 = do nothing, 1 = jump
        self.action_space = gym.spaces.Discrete(2)
        
        # Observation space: [bird_y, bird_velocity, next_pipe_x, next_pipe_top, next_pipe_bottom]
        self.observation_space = gym.spaces.Box(
            low=np.array([-1, -20, -1, -1, -1], dtype=np.float32),
            high=np.array([1, 20, 1, 1, 1], dtype=np.float32),
            dtype=np.float32
        )
        
        # Rendering
        self.screen = None
        self.clock = None
        
        if self.render_mode == "human":
            pygame.init()
            pygame.display.init()
            self.screen = pygame.display.set_mode((self.screen_width, self.screen_height))
            pygame.display.set_caption("Flappy Bird RL")
            self.clock = pygame.time.Clock()
    
    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None):
        super().reset(seed=seed)
        
        # Reset game state
        self.bird_y = self.screen_height // 2
        self.bird_velocity = 0
        self.pipes = []
        self.score = 0
        self.steps = 0
        
        # Create initial pipes
        self._create_pipe(self.screen_width)
        self._create_pipe(self.screen_width + 200)
        
        observation = self._get_observation()
        info = {"score": self.score}
        
        return observation, info
    
    def step(self, action):
        self.steps += 1
        
        # Handle action
        if action == 1:  # Jump
            self.bird_velocity = self.jump_strength
        
        # Update bird physics
        self.bird_velocity += self.gravity
        self.bird_y += self.bird_velocity
        
        # Update pipes
        for pipe in self.pipes[:]:
            pipe['x'] += self.pipe_velocity
            
            # Remove pipes that have passed
            if pipe['x'] + self.pipe_width < 0:
                self.pipes.remove(pipe)
                self.score += 1
        
        # Add new pipes
        if len(self.pipes) < 3:
            last_pipe_x = max(pipe['x'] for pipe in self.pipes) if self.pipes else self.screen_width
            if last_pipe_x < self.screen_width - 200:
                self._create_pipe(self.screen_width + 100)
        
        # Check collisions
        terminated = self._check_collision()
        
        # Check if max steps reached
        truncated = self.steps >= self.max_steps
        
        # Calculate reward
        reward = self._calculate_reward(terminated)
        
        observation = self._get_observation()
        info = {"score": self.score, "steps": self.steps}
        
        if self.render_mode == "human":
            self.render()
        
        return observation, reward, terminated, truncated, info
    
    def _create_pipe(self, x: float):
        """Create a new pipe at given x position"""
        gap_center = np.random.randint(100, self.screen_height - 100)
        pipe = {
            'x': x,
            'top': gap_center - self.pipe_gap // 2,
            'bottom': gap_center + self.pipe_gap // 2
        }
        self.pipes.append(pipe)
    
    def _check_collision(self) -> bool:
        """Check if bird collided with pipes or boundaries"""
        
        # Boundary collision
        if self.bird_y < 0 or self.bird_y > self.screen_height:
            return True
        
        # Pipe collision
        bird_rect = pygame.Rect(self.bird_x - 15, self.bird_y - 15, 30, 30)
        
        for pipe in self.pipes:
            pipe_rect_top = pygame.Rect(pipe['x'], 0, self.pipe_width, pipe['top'])
            pipe_rect_bottom = pygame.Rect(pipe['x'], pipe['bottom'], self.pipe_width, 
                                         self.screen_height - pipe['bottom'])
            
            if bird_rect.colliderect(pipe_rect_top) or bird_rect.colliderect(pipe_rect_bottom):
                return True
        
        return False
    
    def _calculate_reward(self, terminated: bool) -> float:
        """Calculate reward for current step"""
        
        if terminated:
            return -100  # Heavy penalty for collision
        
        reward = 1  # Small reward for staying alive
        
        # Bonus for passing through pipes
        for pipe in self.pipes:
            if (pipe['x'] < self.bird_x < pipe['x'] + self.pipe_width and 
                pipe['top'] < self.bird_y < pipe['bottom']):
                reward += 10
        
        # Penalty for being too far from center of gap
        if self.pipes:
            next_pipe = min(self.pipes, key=lambda p: abs(p['x'] - self.bird_x) if p['x'] > self.bird_x else float('inf'))
            if next_pipe['x'] > self.bird_x:
                gap_center = (next_pipe['top'] + next_pipe['bottom']) / 2
                distance_from_center = abs(self.bird_y - gap_center) / (self.pipe_gap / 2)
                reward -= distance_from_center * 0.1
        
        return reward
    
    def _get_observation(self) -> np.ndarray:
        """Get current observation state"""
        
        # Normalize bird position and velocity
        bird_y_norm = (self.bird_y - self.screen_height / 2) / (self.screen_height / 2)
        bird_velocity_norm = self.bird_velocity / 20
        
        # Find next pipe
        next_pipe = None
        for pipe in self.pipes:
            if pipe['x'] + self.pipe_width > self.bird_x:
                next_pipe = pipe
                break
        
        if next_pipe is None:
            # No pipes ahead
            return np.array([bird_y_norm, bird_velocity_norm, 1, 0, 1], dtype=np.float32)
        
        # Normalize pipe information
        pipe_x_norm = (next_pipe['x'] - self.bird_x) / self.screen_width
        pipe_top_norm = (next_pipe['top'] - self.screen_height / 2) / (self.screen_height / 2)
        pipe_bottom_norm = (next_pipe['bottom'] - self.screen_height / 2) / (self.screen_height / 2)
        
        return np.array([
            bird_y_norm,
            bird_velocity_norm,
            pipe_x_norm,
            pipe_top_norm,
            pipe_bottom_norm
        ], dtype=np.float32)
    
    def render(self):
        if self.render_mode == "rgb_array":
            return self._render_frame()
        elif self.render_mode == "human":
            self._render_frame()
            pygame.display.flip()
            self.clock.tick(self.metadata["render_fps"])
    
    def _render_frame(self):
        if self.screen is None:
            pygame.init()
            pygame.display.init()
            self.screen = pygame.display.set_mode((self.screen_width, self.screen_height))
        
        # Clear screen
        self.screen.fill((135, 206, 235))  # Sky blue
        
        # Draw pipes
        for pipe in self.pipes:
            # Top pipe
            pygame.draw.rect(self.screen, (0, 255, 0), 
                           (pipe['x'], 0, self.pipe_width, pipe['top']))
            # Bottom pipe
            pygame.draw.rect(self.screen, (0, 255, 0), 
                           (pipe['x'], pipe['bottom'], self.pipe_width, 
                            self.screen_height - pipe['bottom']))
        
        # Draw bird
        pygame.draw.circle(self.screen, (255, 255, 0), 
                         (int(self.bird_x), int(self.bird_y)), 15)
        
        # Draw score
        font = pygame.font.Font(None, 36)
        score_text = font.render(f"Score: {self.score}", True, (255, 255, 255))
        self.screen.blit(score_text, (10, 10))
        
        if self.render_mode == "rgb_array":
            return np.transpose(
                np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)
            )
    
    def close(self):
        if self.screen is not None:
            pygame.display.quit()
            pygame.quit()

class FlappyBirdRLTrainer:
    """Specialized trainer for Flappy Bird RL agent"""
    
    def __init__(self, agent_type: str = "dqn"):
        self.agent_type = agent_type
        self.env = CustomFlappyBirdEnv(render_mode=None)
        self.agent = None
        self.training_metrics = {
            'episodes': [],
            'scores': [],
            'max_scores': [],
            'average_scores': [],
            'survival_times': []
        }
        
    def create_agent(self, config: Dict[str, Any] = None):
        """Create RL agent for Flappy Bird"""
        
        state_size = self.env.observation_space.shape[0]
        action_size = self.env.action_space.n
        
        if self.agent_type == "dqn":
            agent_config = QLearningConfig(
                learning_rate=0.001,
                discount_factor=0.99,
                epsilon=1.0,
                epsilon_min=0.01,
                epsilon_decay=0.995,
                memory_size=50000,
                batch_size=32,
                target_update_frequency=100,
                training_episodes=2000
            )
            if config:
                for key, value in config.items():
                    setattr(agent_config, key, value)
            
            self.agent = AdvancedQLearningAgent(state_size, action_size, agent_config)
            
        elif self.agent_type == "actor_critic":
            lr_actor = config.get('lr_actor', 0.001) if config else 0.001
            lr_critic = config.get('lr_critic', 0.002) if config else 0.002
            self.agent = ActorCriticAgent(state_size, action_size, lr_actor, lr_critic)
            
        elif self.agent_type == "policy_gradient":
            lr = config.get('lr', 0.001) if config else 0.001
            self.agent = PolicyGradientAgent(state_size, action_size, lr)
        
        else:
            raise ValueError(f"Unknown agent type: {self.agent_type}")
    
    def train(self, episodes: int = 2000, save_model: bool = True) -> Dict[str, Any]:
        """Train agent on Flappy Bird"""
        
        if self.agent is None:
            self.create_agent()
        
        print(f"Training {self.agent_type} agent on Flappy Bird for {episodes} episodes...")
        
        scores = deque(maxlen=100)
        max_score = 0
        
        for episode in range(episodes):
            state, _ = self.env.reset()
            total_reward = 0
            score = 0
            steps = 0
            
            while True:
                if self.agent_type == "dqn":
                    action = self.agent.act(state, training=True)
                elif self.agent_type == "actor_critic":
                    action, action_log_prob = self.agent.act(state)
                else:  # policy_gradient
                    action = self.agent.act(state)
                
                next_state, reward, terminated, truncated, info = self.env.step(action)
                done = terminated or truncated
                
                # Learn from experience
                if self.agent_type == "dqn":
                    self.agent.step(state, action, reward, next_state, done)
                elif self.agent_type == "actor_critic":
                    self.agent.learn(state, action_log_prob, reward, next_state, done)
                else:  # policy_gradient
                    self.agent.episode_rewards.append(reward)
                
                state = next_state
                total_reward += reward
                score = info.get('score', 0)
                steps += 1
                
                if done:
                    if self.agent_type == "policy_gradient":
                        self.agent.finish_episode()
                    break
            
            scores.append(score)
            max_score = max(max_score, score)
            avg_score = np.mean(scores)
            
            # Store metrics
            self.training_metrics['episodes'].append(episode)
            self.training_metrics['scores'].append(score)
            self.training_metrics['max_scores'].append(max_score)
            self.training_metrics['average_scores'].append(avg_score)
            self.training_metrics['survival_times'].append(steps)
            
            # Print progress
            if episode % 100 == 0:
                epsilon = getattr(self.agent, 'epsilon', 0)
                print(f"Episode {episode}: Score={score}, Avg={avg_score:.2f}, Max={max_score}, Epsilon={epsilon:.3f}")
        
        # Save trained model
        if save_model:
            model_path = f"flappy_bird_{self.agent_type}_model.pth"
            if hasattr(self.agent, 'save_model'):
                self.agent.save_model(model_path)
                print(f"Model saved to {model_path}")
        
        return self.training_metrics
    
    def evaluate(self, episodes: int = 10, render: bool = False) -> Dict[str, Any]:
        """Evaluate trained agent"""
        
        if self.agent is None:
            raise ValueError("No agent to evaluate. Train first or load a model.")
        
        if render:
            eval_env = CustomFlappyBirdEnv(render_mode="human")
        else:
            eval_env = self.env
        
        scores = []
        survival_times = []
        
        # Disable exploration for evaluation
        original_epsilon = getattr(self.agent, 'epsilon', 0)
        if hasattr(self.agent, 'epsilon'):
            self.agent.epsilon = 0
        
        for episode in range(episodes):
            state, _ = eval_env.reset()
            score = 0
            steps = 0
            
            while True:
                if self.agent_type == "dqn":
                    action = self.agent.act(state, training=False)
                elif self.agent_type == "actor_critic":
                    action, _ = self.agent.act(state)
                else:  # policy_gradient
                    action = self.agent.act(state)
                
                next_state, reward, terminated, truncated, info = eval_env.step(action)
                done = terminated or truncated
                
                state = next_state
                score = info.get('score', 0)
                steps += 1
                
                if done:
                    break
            
            scores.append(score)
            survival_times.append(steps)
            print(f"Evaluation Episode {episode + 1}: Score={score}, Survival Time={steps}")
        
        # Restore original epsilon
        if hasattr(self.agent, 'epsilon'):
            self.agent.epsilon = original_epsilon
        
        if render:
            eval_env.close()
        
        return {
            'mean_score': np.mean(scores),
            'std_score': np.std(scores),
            'max_score': np.max(scores),
            'min_score': np.min(scores),
            'mean_survival_time': np.mean(survival_times),
            'scores': scores,
            'survival_times': survival_times
        }
    
    def plot_training_progress(self):
        """Plot training progress"""
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        # Scores over time
        ax1.plot(self.training_metrics['episodes'], self.training_metrics['scores'], alpha=0.6, label='Episode Score')
        ax1.plot(self.training_metrics['episodes'], self.training_metrics['average_scores'], label='Average Score (100 episodes)')
        ax1.set_xlabel('Episode')
        ax1.set_ylabel('Score')
        ax1.set_title('Training Scores')
        ax1.legend()
        ax1.grid(True)
        
        # Max score progression
        ax2.plot(self.training_metrics['episodes'], self.training_metrics['max_scores'])
        ax2.set_xlabel('Episode')
        ax2.set_ylabel('Max Score')
        ax2.set_title('Maximum Score Achieved')
        ax2.grid(True)
        
        # Survival time
        ax3.plot(self.training_metrics['episodes'], self.training_metrics['survival_times'], alpha=0.6)
        ax3.set_xlabel('Episode')
        ax3.set_ylabel('Survival Time (steps)')
        ax3.set_title('Agent Survival Time')
        ax3.grid(True)
        
        # Score distribution (last 500 episodes)
        recent_scores = self.training_metrics['scores'][-500:] if len(self.training_metrics['scores']) > 500 else self.training_metrics['scores']
        ax4.hist(recent_scores, bins=20, alpha=0.7)
        ax4.set_xlabel('Score')
        ax4.set_ylabel('Frequency')
        ax4.set_title('Score Distribution (Recent Episodes)')
        ax4.grid(True)
        
        plt.tight_layout()
        plt.savefig(f'flappy_bird_{self.agent_type}_training_progress.png', dpi=300, bbox_inches='tight')
        plt.show()

# Multi-agent environment example
class MultiAgentRLManager:
    """Manager for multi-agent RL experiments using PettingZoo"""
    
    def __init__(self):
        self.environments = {}
        self.agents = {}
    
    def setup_environment(self, env_name: str, env_type: str):
        """Setup multi-agent environment"""
        
        if env_type == "tictactoe":
            env = tictactoe_v3.env()
        elif env_type == "simple_spread":
            env = simple_spread_v3.env(N=3, local_ratio=0.5, max_cycles=25)
        else:
            raise ValueError(f"Unknown environment type: {env_type}")
        
        self.environments[env_name] = env
        
        # Initialize agents for each player
        env.reset()
        for agent_id in env.agents:
            if hasattr(env.observation_space(agent_id), 'shape'):
                obs_size = env.observation_space(agent_id).shape[0]
            else:
                obs_size = env.observation_space(agent_id).n
            
            if hasattr(env.action_space(agent_id), 'n'):
                action_size = env.action_space(agent_id).n
            else:
                action_size = env.action_space(agent_id).shape[0]
            
            # Create simple policy gradient agent for each player
            self.agents[f"{env_name}_{agent_id}"] = PolicyGradientAgent(obs_size, action_size)
    
    def train_multi_agent(self, env_name: str, episodes: int = 1000):
        """Train multiple agents in competitive/cooperative environment"""
        
        if env_name not in self.environments:
            raise ValueError(f"Environment {env_name} not setup")
        
        env = self.environments[env_name]
        
        for episode in range(episodes):
            env.reset()
            
            # Store episode data for each agent
            agent_episodes = {agent: {'states': [], 'actions': [], 'rewards': []} 
                            for agent in env.agents}
            
            for agent in env.agent_iter():
                observation, reward, termination, truncation, info = env.last()
                
                if termination or truncation:
                    action = None
                else:
                    agent_key = f"{env_name}_{agent}"
                    if agent_key in self.agents:
                        action = self.agents[agent_key].act(observation)
                        agent_episodes[agent]['states'].append(observation)
                        agent_episodes[agent]['actions'].append(action)
                    else:
                        action = env.action_space(agent).sample()  # Random action
                
                env.step(action)
                
                # Store reward
                if agent in agent_episodes:
                    agent_episodes[agent]['rewards'].append(reward)
            
            # Update each agent's policy
            for agent in env.agents:
                agent_key = f"{env_name}_{agent}"
                if agent_key in self.agents and agent_episodes[agent]['rewards']:
                    # Set episode rewards for policy gradient update
                    self.agents[agent_key].episode_rewards = agent_episodes[agent]['rewards']
                    self.agents[agent_key].finish_episode()
            
            if episode % 100 == 0:
                print(f"Multi-agent training episode {episode}")

# Usage example
async def main():
    """Example usage of RL implementations"""
    
    # Setup environment manager
    env_manager = RLEnvironmentManager()
    
    # Register classic control environments
    env_manager.register_environment("cartpole", "CartPole-v1")
    env_manager.register_environment("lunarlander", "LunarLander-v2")
    env_manager.register_environment("mountaincar", "MountainCar-v0")
    
    # Compare algorithms on CartPole
    algorithms = [
        {"type": "dqn", "config": {"learning_rate": 0.001, "training_episodes": 500}},
        {"type": "actor_critic", "config": {"lr_actor": 0.001, "lr_critic": 0.002}},
        {"type": "policy_gradient", "config": {"lr": 0.001}}
    ]
    
    print("Comparing RL algorithms on CartPole...")
    comparison = env_manager.compare_algorithms("cartpole", algorithms, episodes=500)
    print("\nComparison Results:")
    for algo, metrics in comparison.items():
        print(f"{algo}: Performance={metrics['final_performance']:.2f}, "
              f"Stability={metrics['training_stability']:.2f}, "
              f"Convergence={metrics['convergence_speed']} episodes")
    
    # Train Flappy Bird agent
    print("\n" + "="*50)
    print("Training Flappy Bird RL Agent")
    print("="*50)
    
    trainer = FlappyBirdRLTrainer(agent_type="dqn")
    trainer.create_agent({
        "learning_rate": 0.0005,
        "epsilon_decay": 0.9995,
        "training_episodes": 1000
    })
    
    # Train agent
    training_results = trainer.train(episodes=1000)
    
    # Evaluate agent
    print("\nEvaluating trained agent...")
    evaluation_results = trainer.evaluate(episodes=5, render=False)
    print(f"Evaluation Results: Mean Score={evaluation_results['mean_score']:.2f}, "
          f"Max Score={evaluation_results['max_score']}")
    
    # Plot training progress
    trainer.plot_training_progress()

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
````

## Conclusion

Reinforcement Learning represents a paradigmatic shift in artificial intelligence, enabling agents to learn optimal behavior through environmental interaction rather than supervised instruction. The implementation of advanced RL algorithms demonstrates the field's evolution from simple tabular methods to sophisticated deep learning approaches capable of handling complex, high-dimensional problems.

**Q-Learning and Deep Q-Networks** showcase the progression from value-based methods to neural network approximations, enabling RL applications in environments with continuous state spaces. The integration of experience replay, target networks, and exploration strategies creates robust learning systems capable of stable convergence in challenging domains.

**Actor-Critic architectures** represent the synthesis of value-based and policy-based approaches, providing the advantages of both paradigms. The separation of policy learning (actor) and value estimation (critic) enables more stable and efficient learning, particularly in environments requiring both action selection and state evaluation.

**Policy Gradient methods** demonstrate direct optimization of action selection policies, offering advantages in environments with complex action spaces or when interpretable policies are required. The REINFORCE algorithm's simplicity belies its theoretical foundations and practical effectiveness in diverse applications.

**Gymnasium and PettingZoo integration** provides standardized environments that enable reproducible research and fair algorithm comparison. The transition from OpenAI Gym to Gymnasium reflects the field's maturation and commitment to sustainable development practices.

**Multi-agent reinforcement learning** through PettingZoo introduces the complexity of strategic interaction, where agents must learn not only optimal responses to the environment but also to other learning agents. This creates emergent behaviors and strategic dynamics that mirror real-world scenarios.

**Custom environment development**, exemplified by the Flappy Bird implementation, demonstrates the flexibility of RL frameworks in addressing domain-specific challenges. The careful design of state representations, action spaces, and reward functions significantly impacts learning efficiency and final performance.

**Practical considerations** include the critical importance of hyperparameter tuning, exploration strategy selection, and convergence monitoring. The trade-offs between sample efficiency, computational requirements, and final performance necessitate careful algorithm selection based on specific application constraints.

The field's future direction points toward more sample-efficient algorithms, better handling of partial observability, and integration with other machine learning paradigms. Reinforcement learning's ability to learn from interaction makes it particularly suited for robotics, game playing, recommendation systems, and autonomous systems where direct supervision is impractical or impossible.