<small>Claude 3.7 Sonnet Thinking</small>
# 01. Introduction to AI Assistants and Creating Your First GPT Assistant

## Key Terms

- **Transformer Architecture**: Neural network architecture using self-attention mechanisms to process sequential data
- **Prompt Engineering**: Strategic crafting of inputs to elicit desired outputs from LLMs
- **RAG (Retrieval-Augmented Generation)**: Technique combining retrieval from external sources with generative capabilities
- **Embeddings**: Dense vector representations of text used for semantic similarity
- **Few-shot Learning**: Technique where models learn from minimal examples provided in the prompt
- **Chain-of-Thought**: Prompting strategy that encourages step-by-step reasoning
- **Inference Optimization**: Techniques to improve response speed and reduce token usage

## Advanced Understanding of AI Assistants

AI assistants powered by transformer-based LLMs operate on a foundation of self-attention mechanisms that allow models to weigh the importance of different parts of input text. This architecture enables the contextual understanding that makes modern assistants effective.

The core technical components include:

1. **Token Embeddings**: Conversion of text into high-dimensional vector space
2. **Attention Mechanisms**: Computing relevance between all token pairs in the input
3. **Parallel Processing Layers**: Multiple transformer blocks processing different aspects of text
4. **Decoder Stack**: Generating coherent, contextually appropriate text token by token
5. **Optimization Algorithms**: Fine-tuning parameters for specific response patterns

Modern GPT assistants implement sophisticated context management through techniques like:

- Sliding window attention for handling longer contexts
- Recursive token compression to maintain conversation history
- Memory prioritization heuristics to preserve key information
- Semantic chunking for efficient information retrieval
- Dynamic temperature adjustment based on query characteristics

## Advanced Application Architectures

Beyond simple chatbots, advanced AI assistant architectures include:

### Multi-agent Systems
- Specialized agents with different expertise domains
- Orchestration layers to route queries to appropriate agents
- Consensus mechanisms to resolve conflicting information
- Meta-cognitive monitoring for self-correction

### Augmented Knowledge Systems
- Real-time integration with knowledge graphs
- Semantic search across enterprise data stores
- Document processing pipelines with entity extraction
- Citation and provenance tracking for factual statements

### Autonomous Decision Systems
- Conditional execution frameworks for complex workflows
- Action validation with safety constraints
- Multi-step planning capabilities with backtracking
- Event-driven conversation state management

### Hybrid Human-AI Systems
- Intelligent routing between AI and human experts
- Confidence scoring for escalation decisions
- Contextual handover protocols with complete state transfer
- Learning loops from human expert interventions

## Building an Advanced GPT Assistant

Let's implement a more sophisticated assistant with enhanced capabilities:

```python
import os
import json
import time
import logging
import tiktoken
import numpy as np
from typing import List, Dict, Any, Optional, Tuple, Callable
from datetime import datetime
from enum import Enum
from dataclasses import dataclass, field
import openai
from openai.types.chat import ChatCompletionMessage
from dotenv import load_dotenv

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("GPTAssistant")

# Load environment variables
load_dotenv()

# Configure OpenAI API
openai.api_key = os.getenv("OPENAI_API_KEY")

class MessageRole(str, Enum):
    """Enum for message roles."""
    SYSTEM = "system"
    USER = "user"
    ASSISTANT = "assistant"
    FUNCTION = "function"
    TOOL = "tool"


@dataclass
class Message:
    """Enhanced message class with metadata and analysis capabilities."""
    
    role: MessageRole
    content: str
    timestamp: datetime = field(default_factory=datetime.now)
    metadata: Dict[str, Any] = field(default_factory=dict)
    token_count: Optional[int] = None
    
    def to_dict(self) -> Dict[str, str]:
        """Convert message to dictionary format for API."""
        return {"role": self.role, "content": self.content}
    
    def count_tokens(self, encoding_name: str = "cl100k_base") -> int:
        """Count the number of tokens in the message."""
        if self.token_count is None:
            encoding = tiktoken.get_encoding(encoding_name)
            self.token_count = len(encoding.encode(self.content))
        return self.token_count
    
    def __str__(self) -> str:
        """String representation of message."""
        return f"[{self.timestamp.strftime('%Y-%m-%d %H:%M:%S')}] {self.role.upper()}: {self.content[:50]}{'...' if len(self.content) > 50 else ''}"


class ConversationManager:
    """Advanced conversation manager with analytics and optimization."""
    
    def __init__(
        self, 
        system_message: str = "You are a helpful assistant.",
        max_context_length: int = 4000,
        encoding_name: str = "cl100k_base"
    ):
        """
        Initialize a conversation manager.
        
        Args:
            system_message: The system message that defines assistant behavior
            max_context_length: Maximum number of tokens to keep in context
            encoding_name: Tokenizer encoding to use for token counting
        """
        self.messages: List[Message] = [Message(MessageRole.SYSTEM, system_message)]
        self.max_context_length = max_context_length
        self.encoding_name = encoding_name
        self.encoding = tiktoken.get_encoding(encoding_name)
        self.total_tokens_used = 0
        self.conversation_summary: Optional[str] = None
    
    def add_message(self, role: MessageRole, content: str, metadata: Dict[str, Any] = None) -> Message:
        """
        Add a message to the conversation.
        
        Args:
            role: The role of the message sender
            content: The content of the message
            metadata: Additional information about the message
            
        Returns:
            The created message object
        """
        message = Message(role, content, metadata=metadata or {})
        self.messages.append(message)
        
        # Update token usage statistics
        token_count = message.count_tokens(self.encoding_name)
        self.total_tokens_used += token_count
        
        # Check if context needs to be compressed
        if self._get_current_context_length() > self.max_context_length:
            self._compress_context()
            
        return message
    
    def _get_current_context_length(self) -> int:
        """Calculate the total number of tokens in the current conversation."""
        return sum(msg.count_tokens(self.encoding_name) for msg in self.messages)
    
    def _compress_context(self) -> None:
        """Compress the conversation context when it gets too long."""
        if len(self.messages) <= 1:
            # Only system message, nothing to compress
            return
            
        if not self.conversation_summary:
            # Generate a summary of the conversation so far
            self._generate_conversation_summary()
        
        # Keep system message and recent messages, replace older messages with summary
        system_message = self.messages[0]
        recent_messages = self.messages[-4:]  # Keep the most recent messages
        
        # Create a new summary message
        summary_message = Message(
            MessageRole.SYSTEM,
            f"Previous conversation summary: {self.conversation_summary}",
            metadata={"is_summary": True}
        )
        
        # Replace conversation with system message, summary, and recent messages
        self.messages = [system_message, summary_message] + recent_messages
        logger.info(f"Compressed conversation context. New length: {self._get_current_context_length()} tokens")
    
    def _generate_conversation_summary(self) -> None:
        """Generate a summary of the conversation using the OpenAI API."""
        # Skip if only system message is present
        if len(self.messages) <= 1:
            return
            
        try:
            # Prepare conversation history for summarization
            summary_prompt = [
                {"role": "system", "content": "Summarize the following conversation in a concise paragraph. Include key points discussed, questions asked, and information provided."},
                {"role": "user", "content": "\n\n".join([f"{msg.role}: {msg.content}" for msg in self.messages[1:]])}
            ]
            
            response = openai.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=summary_prompt,
                temperature=0.3,
                max_tokens=150
            )
            
            self.conversation_summary = response.choices[0].message.content
            logger.info("Generated new conversation summary")
        except Exception as e:
            logger.error(f"Failed to generate conversation summary: {str(e)}")
            # Fallback: use a simple concatenation of first few exchanges
            self.conversation_summary = "Previous conversation included: " + "; ".join([
                f"{msg.content[:50]}..." for msg in self.messages[1:5]
            ])
    
    def get_messages_for_api(self) -> List[Dict[str, str]]:
        """Get messages in format required by OpenAI API."""
        return [msg.to_dict() for msg in self.messages]
    
    def clear_history(self, keep_system_message: bool = True) -> None:
        """
        Clear conversation history.
        
        Args:
            keep_system_message: Whether to keep the system message
        """
        if keep_system_message and self.messages and self.messages[0].role == MessageRole.SYSTEM:
            system_message = self.messages[0]
            self.messages = [system_message]
        else:
            self.messages = []
        
        self.conversation_summary = None
        logger.info("Conversation history cleared")
    
    def get_conversation_statistics(self) -> Dict[str, Any]:
        """
        Get statistics about the conversation.
        
        Returns:
            Dictionary with conversation statistics
        """
        if not self.messages:
            return {"total_messages": 0, "total_tokens": 0}
            
        message_counts = {}
        for msg in self.messages:
            message_counts[msg.role] = message_counts.get(msg.role, 0) + 1
            
        return {
            "total_messages": len(self.messages),
            "message_counts": message_counts,
            "total_tokens": self.total_tokens_used,
            "current_context_length": self._get_current_context_length(),
            "has_summary": self.conversation_summary is not None
        }
    
    def save_to_file(self, filename: str) -> None:
        """
        Save conversation to a file.
        
        Args:
            filename: Path to the file
        """
        with open(filename, 'w', encoding='utf-8') as f:
            json_data = []
            for msg in self.messages:
                msg_dict = msg.to_dict()
                msg_dict['timestamp'] = msg.timestamp.isoformat()
                msg_dict['metadata'] = msg.metadata
                msg_dict['token_count'] = msg.token_count
                json_data.append(msg_dict)
                
            conversation_data = {
                "messages": json_data,
                "summary": self.conversation_summary,
                "statistics": self.get_conversation_statistics()
            }
            
            json.dump(conversation_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Conversation saved to {filename}")
    
    def load_from_file(self, filename: str) -> None:
        """
        Load conversation from a file.
        
        Args:
            filename: Path to the file
        """
        with open(filename, 'r', encoding='utf-8') as f:
            conversation_data = json.load(f)
            
            self.messages = []
            for msg_dict in conversation_data.get("messages", []):
                timestamp = datetime.fromisoformat(msg_dict['timestamp']) if 'timestamp' in msg_dict else None
                metadata = msg_dict.get('metadata', {})
                token_count = msg_dict.get('token_count')
                
                message = Message(
                    role=msg_dict['role'],
                    content=msg_dict['content'],
                    timestamp=timestamp,
                    metadata=metadata,
                    token_count=token_count
                )
                self.messages.append(message)
                
            self.conversation_summary = conversation_data.get("summary")
            self.total_tokens_used = conversation_data.get("statistics", {}).get("total_tokens", 0)
            
        logger.info(f"Conversation loaded from {filename}")


class ResponseEvaluator:
    """Evaluates the quality of assistant responses."""
    
    def __init__(self):
        """Initialize the response evaluator."""
        self.evaluation_criteria = {
            "relevance": "Does the response directly address the user's query?",
            "accuracy": "Is the information provided factually correct?",
            "completeness": "Does the response fully answer all aspects of the query?",
            "clarity": "Is the response clear and easy to understand?",
            "conciseness": "Is the response appropriately concise without omitting important details?"
        }
    
    def evaluate_response(self, 
                         query: str, 
                         response: str, 
                         criteria: List[str] = None) -> Dict[str, float]:
        """
        Evaluate a response against specified criteria.
        
        Args:
            query: The user's query
            response: The assistant's response
            criteria: List of criteria to evaluate (defaults to all)
            
        Returns:
            Dictionary with scores for each criterion
        """
        criteria = criteria or list(self.evaluation_criteria.keys())
        
        # Construct prompt for evaluation
        evaluation_prompt = [
            {"role": "system", "content": "You are an objective evaluator of AI assistant responses. Evaluate the assistant's response to the user's query on a scale of 1-10 for each criterion, where 1 is very poor and 10 is excellent. Provide only the numerical scores without explanation."},
            {"role": "user", "content": f"""
User Query: {query}

Assistant Response: {response}

Evaluate on the following criteria:
{', '.join([f"{c}" for c in criteria])}

Provide the score for each criterion as a number between 1 and 10.
"""
            }
        ]
        
        try:
            # Get evaluation from OpenAI
            evaluation_result = openai.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=evaluation_prompt,
                temperature=0.1
            )
            
            evaluation_text = evaluation_result.choices[0].message.content
            
            # Parse evaluation scores
            scores = {}
            for criterion in criteria:
                # Look for criterion name followed by number
                import re
                match = re.search(f"{criterion}[:\s]*([0-9]|10)[\s.,]", evaluation_text, re.IGNORECASE)
                if match:
                    scores[criterion] = float(match.group(1))
                else:
                    scores[criterion] = None
                    
            return scores
            
        except Exception as e:
            logger.error(f"Failed to evaluate response: {str(e)}")
            return {criterion: None for criterion in criteria}


class AdvancedGPTAssistant:
    """Advanced GPT assistant with enhanced capabilities."""
    
    def __init__(
        self,
        system_message: str = "You are a helpful assistant.",
        model: str = "gpt-4o",
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        max_context_length: int = 4000,
        log_conversations: bool = True,
        log_directory: str = "./conversation_logs"
    ):
        """
        Initialize an advanced GPT assistant.
        
        Args:
            system_message: The system message that defines assistant behavior
            model: The OpenAI model to use
            temperature: Controls randomness (0-1, lower is more deterministic)
            max_tokens: Maximum number of tokens in the response
            max_context_length: Maximum context length to maintain
            log_conversations: Whether to log conversations
            log_directory: Directory to store conversation logs
        """
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.conversation_manager = ConversationManager(
            system_message=system_message,
            max_context_length=max_context_length
        )
        self.evaluator = ResponseEvaluator()
        self.log_conversations = log_conversations
        self.log_directory = log_directory
        
        if log_conversations and not os.path.exists(log_directory):
            os.makedirs(log_directory)
            
        self.response_quality_history = []
        self.query_start_time = None
        
        logger.info(f"Assistant initialized with model {model}")
    
    def ask(
        self, 
        query: str,
        stream: bool = False,
        callback: Optional[Callable[[str], None]] = None,
        evaluate_response: bool = False
    ) -> Dict[str, Any]:
        """
        Ask the assistant a question and get a response with metadata.
        
        Args:
            query: The user's question or prompt
            stream: Whether to stream the response
            callback: Function to call with each chunk when streaming
            evaluate_response: Whether to evaluate the response quality
            
        Returns:
            Dictionary with response and metadata
        """
        # Start timing
        self.query_start_time = time.time()
        
        # Add user query to conversation
        self.conversation_manager.add_message(MessageRole.USER, query)
        
        # Prepare API call parameters
        params = {
            "model": self.model,
            "messages": self.conversation_manager.get_messages_for_api(),
            "temperature": self.temperature,
            "stream": stream
        }
        
        if self.max_tokens:
            params["max_tokens"] = self.max_tokens
        
        # Call OpenAI API
        try:
            if stream:
                return self._process_streaming_response(params, callback)
            else:
                return self._process_normal_response(params, evaluate_response)
                
        except Exception as e:
            error_msg = f"Error communicating with OpenAI API: {str(e)}"
            logger.error(error_msg)
            
            self.conversation_manager.add_message(
                MessageRole.SYSTEM, 
                error_msg,
                metadata={"error": True}
            )
            
            return {
                "response": error_msg,
                "success": False,
                "error": str(e),
                "processing_time": time.time() - self.query_start_time
            }
    
    def _process_normal_response(self, params: Dict[str, Any], evaluate_response: bool) -> Dict[str, Any]:
        """Process a normal (non-streaming) response."""
        response = openai.chat.completions.create(**params)
        assistant_response = response.choices[0].message.content
        
        # Calculate processing time
        processing_time = time.time() - self.query_start_time
        
        # Add assistant response to conversation
        self.conversation_manager.add_message(
            MessageRole.ASSISTANT, 
            assistant_response,
            metadata={
                "model": self.model,
                "temperature": self.temperature,
                "processing_time": processing_time,
                "finish_reason": response.choices[0].finish_reason
            }
        )
        
        result = {
            "response": assistant_response,
            "success": True,
            "processing_time": processing_time,
            "model": self.model,
            "finish_reason": response.choices[0].finish_reason,
            "usage": {
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens
            }
        }
        
        # Evaluate response quality if requested
        if evaluate_response:
            evaluation = self.evaluator.evaluate_response(query, assistant_response)
            result["evaluation"] = evaluation
            self.response_quality_history.append(evaluation)
        
        # Log conversation if enabled
        if self.log_conversations:
            self._log_conversation()
            
        return result
    
    def _process_streaming_response(self, params: Dict[str, Any], callback: Optional[Callable]) -> Dict[str, Any]:
        """Process a streaming response."""
        response_stream = openai.chat.completions.create(**params)
        assistant_response = ""
        
        # Process the streaming response
        for chunk in response_stream:
            if hasattr(chunk.choices[0].delta, "content") and chunk.choices[0].delta.content is not None:
                content = chunk.choices[0].delta.content
                assistant_response += content
                
                # Call the callback function if provided
                if callback:
                    callback(content)
        
        # Calculate processing time
        processing_time = time.time() - self.query_start_time
        
        # Add assistant response to conversation
        self.conversation_manager.add_message(
            MessageRole.ASSISTANT, 
            assistant_response,
            metadata={
                "model": self.model,
                "temperature": self.temperature,
                "processing_time": processing_time,
                "streaming": True
            }
        )
        
        result = {
            "response": assistant_response,
            "success": True,
            "processing_time": processing_time,
            "model": self.model,
            "streaming": True
        }
        
        # Log conversation if enabled
        if self.log_conversations:
            self._log_conversation()
            
        return result
    
    def _log_conversation(self) -> None:
        """Log the current conversation to a file."""
        if not self.log_conversations:
            return
            
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = os.path.join(self.log_directory, f"conversation_{timestamp}.json")
        
        try:
            self.conversation_manager.save_to_file(log_file)
        except Exception as e:
            logger.error(f"Failed to log conversation: {str(e)}")
    
    def update_system_message(self, new_system_message: str) -> None:
        """
        Update the system message.
        
        Args:
            new_system_message: The new system message
        """
        if self.conversation_manager.messages and self.conversation_manager.messages[0].role == MessageRole.SYSTEM:
            self.conversation_manager.messages[0].content = new_system_message
        else:
            self.conversation_manager.messages.insert(0, Message(MessageRole.SYSTEM, new_system_message))
            
        logger.info("System message updated")
    
    def clear_conversation(self, keep_system_message: bool = True) -> None:
        """
        Clear the conversation history.
        
        Args:
            keep_system_message: Whether to keep the system message
        """
        self.conversation_manager.clear_history(keep_system_message)
    
    def get_conversation_history(self) -> List[Dict[str, Any]]:
        """
        Get the conversation history.
        
        Returns:
            List of messages in the conversation
        """
        return [
            {
                "role": msg.role,
                "content": msg.content,
                "timestamp": msg.timestamp.isoformat(),
                "metadata": msg.metadata,
                "token_count": msg.token_count
            }
            for msg in self.conversation_manager.messages
        ]
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """
        Get performance metrics for the assistant.
        
        Returns:
            Dictionary with performance metrics
        """
        conversation_stats = self.conversation_manager.get_conversation_statistics()
        
        avg_quality = {}
        if self.response_quality_history:
            for criterion in self.response_quality_history[0].keys():
                valid_scores = [quality[criterion] for quality in self.response_quality_history if quality[criterion] is not None]
                avg_quality[criterion] = sum(valid_scores) / len(valid_scores) if valid_scores else None
        
        return {
            "conversation_statistics": conversation_stats,
            "average_quality": avg_quality,
            "model": self.model,
            "temperature": self.temperature
        }


# Example usage with advanced features
if __name__ == "__main__":
    # Initialize an advanced assistant with custom parameters
    assistant = AdvancedGPTAssistant(
        system_message="You are an expert technical consultant with deep knowledge of computer science, "
                      "programming, and software architecture. You provide detailed, accurate, and "
                      "well-structured responses to technical questions. When appropriate, include code "
                      "examples with explanations. If you're unsure about something, acknowledge the "
                      "limitations of your knowledge rather than guessing.",
        model="gpt-4o",
        temperature=0.5,
        max_context_length=6000,
        log_conversations=True
    )
    
    print("Technical Consultant Assistant (type 'exit' to quit, 'clear' to reset conversation)")
    print("-----------------------------------------------------------------------------")
    
    def stream_callback(content: str):
        """Print streaming content without newlines"""
        print(content, end="", flush=True)
    
    while True:
        user_input = input("\n\nYou: ")
        
        if user_input.lower() in ['exit', 'quit', 'bye']:
            print("\nAssistant: Goodbye! Thank you for the conversation.")
            break
            
        if user_input.lower() == 'clear':
            assistant.clear_conversation()
            print("\nConversation cleared.")
            continue
            
        if user_input.lower() == 'stats':
            metrics = assistant.get_performance_metrics()
            print("\nPerformance Metrics:")
            print(json.dumps(metrics, indent=2))
            continue
        
        print("\nAssistant: ", end="")
        
        # Use streaming response with callback
        result = assistant.ask(user_input, stream=True, callback=stream_callback, evaluate_response=True)
        
        # Print processing stats
        print(f"\n\n[Processed in {result['processing_time']:.2f}s using {result.get('model', 'unknown model')}]")
        
        if 'evaluation' in result:
            print("Response Quality Scores:")
            for criterion, score in result['evaluation'].items():
                if score is not None:
                    print(f"- {criterion}: {score}/10")
```

Let's also create a Streamlit interface with advanced functionality:

```python
import streamlit as st
import pandas as pd
import numpy as np
import json
import time
import matplotlib.pyplot as plt
from datetime import datetime
from advanced_assistant import AdvancedGPTAssistant, MessageRole
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Page configuration
st.set_page_config(
    page_title="Advanced GPT Assistant",
    page_icon="ðŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Initialize session state variables
if "assistant" not in st.session_state:
    st.session_state.assistant = AdvancedGPTAssistant(
        system_message="You are a helpful AI assistant with expertise in a wide range of topics. You provide accurate, informative, and clear responses.",
        model="gpt-4o"
    )

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "show_stats" not in st.session_state:
    st.session_state.show_stats = False

if "evaluation_history" not in st.session_state:
    st.session_state.evaluation_history = []

if "current_tab" not in st.session_state:
    st.session_state.current_tab = "Chat"

# Function to handle response evaluation visualization
def show_evaluation_stats():
    if not st.session_state.evaluation_history:
        st.info("No evaluation data available yet. Interact with the assistant to generate statistics.")
        return
    
    eval_data = pd.DataFrame(st.session_state.evaluation_history)
    
    # Show average scores
    st.subheader("Average Response Quality")
    
    avg_scores = eval_data.mean().reset_index()
    avg_scores.columns = ['Criterion', 'Average Score']
    
    col1, col2 = st.columns([2, 3])
    
    with col1:
        st.dataframe(avg_scores.style.format({'Average Score': '{:.2f}'}))
    
    with col2:
        fig, ax = plt.subplots(figsize=(8, 4))
        x = avg_scores['Criterion']
        y = avg_scores['Average Score']
        ax.bar(x, y, color='skyblue')
        ax.set_ylim(0, 10)
        ax.set_title('Average Response Quality by Criterion')
        ax.set_ylabel('Score (0-10)')
        fig.tight_layout()
        st.pyplot(fig)
    
    # Show trends over time
    st.subheader("Response Quality Trends")
    
    eval_data['index'] = range(1, len(eval_data) + 1)
    
    fig, ax = plt.subplots(figsize=(10, 6))
    for column in eval_data.columns:
        if column != 'index' and column != 'timestamp':
            ax.plot(eval_data['index'], eval_data[column], marker='o', label=column)
    
    ax.set_xlabel('Interaction Number')
    ax.set_ylabel('Score (0-10)')
    ax.set_title('Response Quality Trends Over Time')
    ax.legend()
    ax.grid(True, linestyle='--', alpha=0.7)
    fig.tight_layout()
    
    st.pyplot(fig)


# Function to reset the assistant
def reset_assistant():
    system_message = st.session_state.system_message
    model = st.session_state.model
    temperature = st.session_state.temperature
    
    st.session_state.assistant = AdvancedGPTAssistant(
        system_message=system_message,
        model=model,
        temperature=temperature
    )
    
    st.session_state.chat_history = []
    st.session_state.evaluation_history = []
    st.success("Assistant has been reset with new parameters!")


# Application header and navigation
st.title("ðŸ¤– Advanced GPT Assistant")

# Navigation tabs
tabs = ["Chat", "Analytics", "Configuration"]
st.session_state.current_tab = st.radio("Navigation", tabs, horizontal=True)

# Sidebar for configuration
with st.sidebar:
    st.header("Assistant Configuration")
    
    st.session_state.system_message = st.text_area(
        "System Message",
        "You are a helpful AI assistant with expertise in a wide range of topics. You provide accurate, informative, and clear responses."
    )
    
    st.session_state.model = st.selectbox(
        "Model",
        ["gpt-4o", "gpt-4-turbo", "gpt-3.5-turbo"]
    )
    
    st.session_state.temperature = st.slider(
        "Temperature",
        min_value=0.0,
        max_value=1.0,
        value=0.7,
        step=0.1
    )
    
    st.session_state.stream_responses = st.checkbox("Stream Responses", value=True)
    
    st.session_state.evaluate_responses = st.checkbox("Evaluate Response Quality", value=True)
    
    if st.button("Update Assistant"):
        reset_assistant()
    
    if st.button("Clear Conversation"):
        st.session_state.assistant.clear_conversation()
        st.session_state.chat_history = []
        st.success("Conversation cleared!")


# Chat Interface Tab
if st.session_state.current_tab == "Chat":
    # Chat interface
    chat_container = st.container()
    
    with chat_container:
        # Display chat history
        for message in st.session_state.chat_history:
            with st.chat_message(message["role"]):
                st.write(message["content"])
                
                # Show metadata if available and it's an assistant message
                if message["role"] == "assistant" and "metadata" in message:
                    with st.expander("Response Metadata"):
                        st.json(message["metadata"])
        
        # Chat input
        user_input = st.chat_input("Ask something...")
        
        if user_input:
            # Add user message to chat history
            st.session_state.chat_history.append({"role": "user", "content": user_input})
            
            # Display user message
            with st.chat_message("user"):
                st.write(user_input)
            
            # Get assistant response
            with st.chat_message("assistant"):
                response_container = st.empty()
                
                if st.session_state.stream_responses:
                    # For streaming response
                    message_placeholder = st.empty()
                    full_response = ""
                    
                    # Streaming callback function
                    def stream_callback(content):
                        nonlocal full_response
                        full_response += content
                        message_placeholder.markdown(full_response + "â–Œ")
                    
                    with st.spinner("Thinking..."):
                        start_time = time.time()
                        result = st.session_state.assistant.ask(
                            user_input, 
                            stream=True, 
                            callback=stream_callback,
                            evaluate_response=st.session_state.evaluate_responses
                        )
                        end_time = time.time()
                    
                    # Final update without cursor
                    message_placeholder.markdown(full_response)
                    
                    # Display metadata
                    processing_time = end_time - start_time
                    with st.expander("Response Metadata"):
                        metadata = {
                            "processing_time": f"{processing_time:.2f} seconds",
                            "model": result.get("model", "unknown"),
                            "streaming": True
                        }
                        
                        if "evaluation" in result:
                            metadata["quality_scores"] = result["evaluation"]
                            st.session_state.evaluation_history.append(result["evaluation"])
                            
                        st.json(metadata)
                        
                    # Add to chat history with metadata
                    st.session_state.chat_history.append({
                        "role": "assistant", 
                        "content": full_response,
                        "metadata": metadata
                    })
                    
                else:
                    # For non-streaming response
                    with st.spinner("Thinking..."):
                        result = st.session_state.assistant.ask(
                            user_input,
                            evaluate_response=st.session_state.evaluate_responses
                        )
                    
                    # Display response
                    st.write(result["response"])
                    
                    # Display metadata
                    with st.expander("Response Metadata"):
                        metadata = {
                            "processing_time": f"{result['processing_time']:.2f} seconds",
                            "model": result.get("model", "unknown"),
                            "finish_reason": result.get("finish_reason", "unknown")
                        }
                        
                        if "usage" in result:
                            metadata["token_usage"] = result["usage"]
                            
                        if "evaluation" in result:
                            metadata["quality_scores"] = result["evaluation"]
                            st.session_state.evaluation_history.append(result["evaluation"])
                            
                        st.json(metadata)
                    
                    # Add to chat history with metadata
                    st.session_state.chat_history.append({
                        "role": "assistant", 
                        "content": result["response"],
                        "metadata": metadata
                    })

# Analytics Tab
elif st.session_state.current_tab == "Analytics":
    st.header("Assistant Analytics")
    
    metrics = st.session_state.assistant.get_performance_metrics()
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("Conversation Statistics")
        st.json(metrics["conversation_statistics"])
    
    with col2:
        st.subheader("Configuration")
        st.json({
            "model": metrics["model"],
            "temperature": metrics["temperature"]
        })
    
    st.header("Response Quality Evaluation")
    show_evaluation_stats()
    
    # Export options
    st.header("Export Data")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("Export Conversation History"):
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            history = st.session_state.assistant.get_conversation_history()
            
            st.download_button(
                label="Download JSON",
                data=json.dumps(history, indent=2),
                file_name=f"conversation_history_{timestamp}.json",
                mime="application/json"
            )
    
    with col2:
        if st.button("Export Evaluation Data"):
            if st.session_state.evaluation_history:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                
                st.download_button(
                    label="Download CSV",
                    data=pd.DataFrame(st.session_state.evaluation_history).to_csv(index=False),
                    file_name=f"quality_evaluation_{timestamp}.csv",
                    mime="text/csv"
                )
            else:
                st.error("No evaluation data available for export.")

# Configuration Tab
elif st.session_state.current_tab == "Configuration":
    st.header("Advanced Configuration")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("Persona Templates")
        
        personas = {
            "Technical Expert": "You are a technical expert with deep knowledge in computer science, programming, and software engineering. Provide detailed technical explanations with code examples when appropriate.",
            "Creative Assistant": "You are a creative assistant specializing in generating ideas, stories, and creative content. Be imaginative and think outside the box.",
            "Academic Researcher": "You are an academic research assistant with expertise in scientific methodology and literature review. Provide well-cited information and academic perspectives.",
            "Business Consultant": "You are a business consultant with expertise in strategy, marketing, and operations. Help solve business problems with practical, actionable advice."
        }
        
        selected_persona = st.selectbox("Select a persona template", list(personas.keys()))
        
        if st.button("Apply Persona"):
            st.session_state.system_message = personas[selected_persona]
            reset_assistant()
    
    with col2:
        st.subheader("Response Behavior")
        
        response_style = st.selectbox(
            "Response Style",
            ["Balanced", "Concise", "Detailed", "Simple", "Technical"]
        )
        
        response_styles = {
            "Balanced": 0.7,
            "Concise": 0.3,
            "Detailed": 0.5,
            "Simple": 0.9,
            "Technical": 0.2
        }
        
        if st.button("Apply Style"):
            st.session_state.temperature = response_styles[response_style]
            reset_assistant()
    
    st.subheader("Custom System Message Template")
    
    template = st.text_area(
        "Create a template with placeholders",
        "You are a helpful assistant specializing in {specialization}. Your communication style is {style}. When responding, make sure to {instruction}."
    )
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        specialization = st.text_input("Specialization", "artificial intelligence")
    
    with col2:
        style = st.text_input("Style", "friendly but professional")
    
    with col3:
        instruction = st.text_input("Instruction", "provide examples when possible")
    
    if st.button("Generate System Message"):
        try:
            generated_message = template.format(
                specialization=specialization,
                style=style,
                instruction=instruction
            )
            st.session_state.system_message = generated_message
            st.success("System message updated!")
            st.code(generated_message)
        except KeyError as e:
            st.error(f"Error in template: missing placeholder {e}")
```

## Practical Exercise: Creating an Industry-Specific Assistant

Let's build a specialized assistant for a software development team that can answer questions about coding standards, architecture, and development processes:

```python
from advanced_assistant import AdvancedGPTAssistant, MessageRole
import json
import os
from dotenv import load_dotenv
import argparse

# Load environment variables
load_dotenv()

# Development team knowledge base
CODING_STANDARDS = """
## Company Coding Standards

### Python
- Follow PEP 8 style guide
- Use type hints for all function parameters and return values
- Docstrings in Google format for all public functions and classes
- Maximum line length: 100 characters
- Unit tests required for all new code with minimum 80% coverage
- Use dataclasses for data container classes
- Prefer composition over inheritance
- Use virtual environments for all projects

### JavaScript
- Use ES6+ features
- Follow Airbnb style guide
- Use TypeScript for all new projects
- Jest for unit testing with minimum 75% coverage
- React for frontend development
- Next.js for full-stack applications
- Use functional components and hooks in React

### Git Workflow
- Feature branch workflow
- Pull requests require at least one review
- Commit messages follow conventional commits standard
- Squash commits before merging
- Branch naming: feature/issue-number-short-description, bugfix/issue-number-short-description
- Main branch is protected
- CI must pass before merging

### Code Review Guidelines
- Review for functionality, not just style
- Provide constructive feedback
- Respond to PR comments within 24 hours
- Focus on the code, not the developer
- Use "suggestions" for specific change recommendations
"""

ARCHITECTURE_GUIDELINES = """
## Software Architecture Guidelines

### Microservices
- Services should be independently deployable
- Use API gateways for client-facing services
- Implement circuit breakers for service-to-service communication
- Use event-driven communication when appropriate
- Each service should have its own database
- Use containerization (Docker) for all services
- Implement health checks and metrics

### Frontend Architecture
- Component-based design
- State management with Redux for complex applications
- Use React Router for navigation
- Implement lazy loading for performance
- Follow Atomic Design principles for component structure
- Use CSS-in-JS with styled-components
- Follow accessibility (WCAG) guidelines

### Backend Architecture
- RESTful API design following OpenAPI specification
- GraphQL for complex data requirements
- Use dependency injection
- Implement proper error handling and logging
- Rate limiting for all public APIs
- Use caching strategies appropriately
- Follow separation of concerns principle

### Database Design
- Use ORM/ODM for database access
- Implement database migrations
- Follow normalization principles for relational databases
- Use indexes appropriately
- Implement soft delete when applicable
- Document database schema
- Consider read/write separation for high-load applications
"""

DEVELOPMENT_PROCESSES = """
## Development Processes

### Agile Methodology
- Two-week sprint cycles
- Daily stand-ups at 10:00 AM
- Sprint planning on Mondays
- Retrospectives on Fridays
- Story points based on Fibonacci sequence
- Definition of Done: code complete, tested, reviewed, and deployable
- User stories follow the format: "As a [role], I want [feature] so that [benefit]"

### Deployment Pipeline
- Continuous Integration with GitHub Actions
- Automated testing on all PRs
- Staging environment deployments for all merges to develop
- Production deployments require manual approval
- Blue-green deployment strategy
- Automated rollback on failure
- Deployment slots for zero-downtime deployments

### Testing Strategy
- Unit tests required for all new code
- Integration tests for critical paths
- E2E tests for user flows
- Performance testing for high-traffic endpoints
- Security testing using OWASP guidelines
- Accessibility testing for all user interfaces
- Manual QA for critical features

### Documentation Requirements
- README.md with setup instructions
- API documentation using OpenAPI/Swagger
- Architecture diagrams in repository
- Decision records for architectural choices
- User documentation for customer-facing features
- Internal wiki for development guidelines
- Update documentation as part of definition of done
"""

# Create a development team assistant
def create_dev_team_assistant():
    system_message = f"""
    You are DevTeamGPT, a specialized assistant for the software development team.
    
    You have deep knowledge of the team's:
    1. Coding standards and best practices
    2. Architecture guidelines
    3. Development processes
    
    When answering questions, reference the specific guidelines from the team's documentation.
    Be precise and technical, but explain concepts clearly. If you're unsure about specific company-specific details
    that aren't covered in your knowledge base, acknowledge the limitations and suggest where the developer might
    find that information internally.
    
    Provide code examples when relevant and format them properly. When appropriate, explain the reasoning
    behind the standards or guidelines you reference.
    
    Here is your knowledge base:
    
    {CODING_STANDARDS}
    
    {ARCHITECTURE_GUIDELINES}
    
    {DEVELOPMENT_PROCESSES}
    """
    
    return AdvancedGPTAssistant(
        system_message=system_message,
        model="gpt-4o",
        temperature=0.3,  # Lower temperature for more consistent, factual responses
        log_conversations=True
    )

def main():
    parser = argparse.ArgumentParser(description="DevTeam GPT Assistant")
    parser.add_argument("--interactive", action="store_true", help="Start interactive console mode")
    parser.add_argument("--question", type=str, help="Single question to ask the assistant")
    parser.add_argument("--export", type=str, help="Export conversation to file")
    parser.add_argument("--import", type=str, dest="import_file", help="Import conversation from file")
    
    args = parser.parse_args()
    
    assistant = create_dev_team_assistant()
    
    # Import conversation if specified
    if args.import_file:
        try:
            assistant.conversation_manager.load_from_file(args.import_file)
            print(f"Conversation loaded from {args.import_file}")
        except Exception as e:
            print(f"Error loading conversation: {e}")
    
    # Single question mode
    if args.question:
        print(f"Question: {args.question}\n")
        print("DevTeamGPT is thinking...\n")
        
        result = assistant.ask(args.question, evaluate_response=True)
        
        print(f"Answer: {result['response']}\n")
        print(f"[Processed in {result['processing_time']:.2f}s using {result.get('model', 'unknown model')}]")
        
        if 'evaluation' in result:
            print("\nResponse Quality:")
            for criterion, score in result['evaluation'].items():
                if score is not None:
                    print(f"- {criterion}: {score}/10")
    
    # Interactive mode
    elif args.interactive:
        print("DevTeam GPT Assistant")
        print("--------------------")
        print("Ask questions about coding standards, architecture, or development processes.")
        print("Type 'exit' to quit, 'clear' to reset conversation, or 'stats' to see metrics.\n")
        
        def stream_callback(content):
            """Print streaming content without newlines"""
            print(content, end="", flush=True)
        
        while True:
            user_input = input("\nYou: ")
            
            if user_input.lower() in ['exit', 'quit', 'bye']:
                print("\nDevTeamGPT: Goodbye! Happy coding!")
                break
                
            if user_input.lower() == 'clear':
                assistant.clear_conversation()
                print("\nConversation cleared.")
                continue
                
            if user_input.lower() == 'stats':
                metrics = assistant.get_performance_metrics()
                print("\nPerformance Metrics:")
                print(json.dumps(metrics, indent=2))
                continue
            
            print("\nDevTeamGPT: ", end="")
            
            # Use streaming response with callback
            result = assistant.ask(user_input, stream=True, callback=stream_callback, evaluate_response=True)
            
            # Print processing stats
            print(f"\n\n[Processed in {result['processing_time']:.2f}s using {result.get('model', 'unknown model')}]")
    
    # Export conversation if specified
    if args.export:
        try:
            assistant.conversation_manager.save_to_file(args.export)
            print(f"Conversation exported to {args.export}")
        except Exception as e:
            print(f"Error exporting conversation: {e}")

if __name__ == "__main__":
    main()
```

To run this specialized assistant:
1. From command line (single question): `python dev_team_assistant.py --question "What are our Python coding standards?"`
2. Interactive mode: `python dev_team_assistant.py --interactive`
3. Export conversation: Add `--export conversation.json` to either command
4. Import previous conversation: Add `--import conversation.json` to either command

## Conclusion

This advanced introduction to AI assistants provides a comprehensive framework for building sophisticated, production-ready GPT assistant applications. We've explored:

- The technical underpinnings of transformer architecture and attention mechanisms
- Advanced application architectures including multi-agent systems and autonomous workflows
- Sophisticated conversation management with context optimization and summarization
- Response quality evaluation and performance monitoring
- Architectural patterns for scaling and extending assistant capabilities

The implementation demonstrates professional-grade patterns including:
- Proper separation of concerns with modular components
- Robust error handling and logging
- Comprehensive telemetry and quality assessment
- Configurable behavior with clear interfaces
- Context management to optimize token usage

This foundation can be extended to build domain-specific assistants with specialized knowledge and capabilities. By understanding both the theoretical aspects and practical implementation details, you're equipped to develop AI assistants that provide significant value across various domains.

As you proceed, consider integrating external knowledge retrieval (RAG), fine-tuning for specific domains, and multi-agent architectures to further enhance capabilities while maintaining responsiveness and cost-effectiveness.