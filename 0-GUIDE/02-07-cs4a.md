<small>Claude Sonnet 4</small>
# 07. LangChain â€“ AI Application Development

## Key Terms

**LangChain Framework**: A comprehensive Python framework designed for developing applications powered by large language models, providing modular components for chaining together prompts, models, memory, and external tools into sophisticated AI workflows and applications.

**Chain Architecture**: A fundamental LangChain concept that enables the sequential or conditional execution of multiple components, allowing developers to create complex AI workflows by connecting language models, prompts, data sources, and external APIs in a structured manner.

**Retrieval-Augmented Generation (RAG)**: An advanced technique that combines information retrieval with text generation, enabling language models to access and incorporate external knowledge sources during response generation, significantly improving accuracy and reducing hallucinations.

**Vector Stores**: Specialized databases optimized for storing and querying high-dimensional vector embeddings, enabling semantic search and similarity matching for document retrieval in RAG applications with support for various backends like Chroma, Pinecone, and FAISS.

**Document Loaders**: Modular components that extract and preprocess content from various sources including PDFs, web pages, databases, and APIs, converting raw data into structured formats suitable for embedding generation and semantic search.

**Memory Management**: LangChain's system for maintaining conversation context and state across multiple interactions, supporting various memory types including buffer memory, summary memory, and vector-based memory for long-term information retention.

**Agents and Tools**: Advanced LangChain components that enable language models to interact with external systems, APIs, and databases through structured tool calling, allowing AI applications to perform actions beyond text generation.

**Prompt Templates**: Reusable and parameterizable prompt structures that enable dynamic prompt generation with variable substitution, conditional logic, and formatting to create consistent and effective model interactions.

## Comprehensive LangChain Application Development Framework

LangChain represents a paradigm shift in AI application development by providing a unified framework for orchestrating complex interactions between language models, external data sources, and computational tools. This implementation demonstrates advanced patterns for building production-ready AI applications.

### Advanced LangChain Implementation

````python
import asyncio
import json
import logging
import os
import time
import warnings
from typing import Dict, List, Any, Optional, Union, Tuple, Callable, AsyncGenerator
from dataclasses import dataclass, field
from datetime import datetime, timezone
from pathlib import Path
import hashlib
import pickle
from concurrent.futures import ThreadPoolExecutor
import tempfile
import shutil

# Core LangChain imports
import langchain
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI
from langchain.schema import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain.prompts import (
    PromptTemplate, ChatPromptTemplate, MessagesPlaceholder,
    FewShotPromptTemplate, FewShotChatMessagePromptTemplate
)
from langchain.chains import (
    LLMChain, ConversationChain, RetrievalQA, ConversationalRetrievalChain,
    SequentialChain, SimpleSequentialChain, RouterChain, MultiPromptChain
)
from langchain.memory import (
    ConversationBufferMemory, ConversationSummaryMemory, 
    ConversationBufferWindowMemory, VectorStoreRetrieverMemory
)

# Document processing and retrieval
from langchain.document_loaders import (
    PyPDFLoader, WebBaseLoader, TextLoader, CSVLoader,
    DirectoryLoader, GitLoader, NotionDirectoryLoader
)
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter, CharacterTextSplitter,
    TokenTextSplitter, SpacyTextSplitter
)
from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings
from langchain.vectorstores import (
    Chroma, FAISS, Pinecone, Qdrant, Weaviate
)
from langchain.retrievers import (
    VectorStoreRetriever, ContextualCompressionRetriever,
    MultiQueryRetriever, EnsembleRetriever
)

# Tools and agents
from langchain.agents import (
    initialize_agent, Tool, AgentType, create_react_agent,
    create_openai_functions_agent, create_structured_chat_agent
)
from langchain.tools import (
    DuckDuckGoSearchRun, WikipediaQueryRun, ShellTool,
    PythonREPLTool, RequestsGetTool, BaseTool
)
from langchain.utilities import (
    GoogleSearchAPIWrapper, SerpAPIWrapper, WikipediaAPIWrapper
)

# Evaluation and monitoring
from langchain.evaluation import (
    load_evaluator, EvaluatorType, Criteria
)
from langchain.callbacks import (
    StdOutCallbackHandler, FileCallbackHandler, 
    get_openai_callback, OpenAICallbackHandler
)

# Advanced features
from langchain.cache import InMemoryCache, SQLiteCache
from langchain.globals import set_llm_cache
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda
from langchain.schema.output_parser import StrOutputParser
from langchain.output_parsers import (
    PydanticOutputParser, JSONOutputParser, 
    StructuredOutputParser, ResponseSchema
)

# External libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics.pairwise import cosine_similarity
import requests
import sqlite3
from urllib.parse import urlparse
import PyPDF2
import docx
from bs4 import BeautifulSoup
import chromadb
import faiss

# Async support
import aiohttp
import aiofiles
from httpx import AsyncClient

# Pydantic for data validation
from pydantic import BaseModel, Field, validator

from dotenv import load_dotenv

load_dotenv()

warnings.filterwarnings("ignore", category=DeprecationWarning)
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class LangChainConfig:
    """Configuration for LangChain applications"""
    openai_api_key: str
    model_name: str = "gpt-4"
    temperature: float = 0.7
    max_tokens: int = 1000
    cache_enabled: bool = True
    verbose: bool = True
    
    # Vector store configuration
    vector_store_type: str = "chroma"  # chroma, faiss, pinecone
    embedding_model: str = "text-embedding-ada-002"
    chunk_size: int = 1000
    chunk_overlap: int = 200
    
    # Memory configuration
    memory_type: str = "buffer"  # buffer, summary, window, vector
    max_memory_length: int = 10

@dataclass
class RAGConfig:
    """Configuration for RAG applications"""
    retriever_type: str = "vector"  # vector, multi_query, contextual_compression
    search_type: str = "similarity"  # similarity, mmr, similarity_score_threshold
    search_kwargs: Dict[str, Any] = field(default_factory=lambda: {"k": 5})
    
    # Document processing
    text_splitter_type: str = "recursive"  # recursive, character, token
    
    # Chain configuration
    chain_type: str = "stuff"  # stuff, map_reduce, refine, map_rerank
    return_source_documents: bool = True

class AdvancedPromptManager:
    """Advanced prompt management with templates and optimization"""
    
    def __init__(self):
        self.templates = {}
        self.few_shot_examples = {}
        self.prompt_history = []
    
    def create_chat_prompt_template(self, system_message: str, 
                                  human_template: str,
                                  few_shot_examples: Optional[List[Dict[str, str]]] = None) -> ChatPromptTemplate:
        """Create sophisticated chat prompt template"""
        
        messages = [("system", system_message)]
        
        # Add few-shot examples if provided
        if few_shot_examples:
            for example in few_shot_examples:
                messages.append(("human", example["input"]))
                messages.append(("ai", example["output"]))
        
        # Add the main human template
        messages.append(("human", human_template))
        
        return ChatPromptTemplate.from_messages(messages)
    
    def create_conditional_prompt(self, conditions: Dict[str, str],
                                default_template: str) -> PromptTemplate:
        """Create conditional prompt based on input parameters"""
        
        template = """
        {%- if task_type == "summarization" -%}
        """ + conditions.get("summarization", default_template) + """
        {%- elif task_type == "analysis" -%}
        """ + conditions.get("analysis", default_template) + """
        {%- elif task_type == "generation" -%}
        """ + conditions.get("generation", default_template) + """
        {%- else -%}
        """ + default_template + """
        {%- endif -%}
        """
        
        return PromptTemplate(
            template=template,
            input_variables=["task_type", "input_text"]
        )
    
    def optimize_prompt_performance(self, prompt: str, test_inputs: List[str],
                                  model, evaluation_metric: str = "relevance") -> str:
        """Optimize prompt performance through iterative testing"""
        
        variations = [
            prompt,
            f"Please carefully consider the following and provide a detailed response:\n{prompt}",
            f"Think step by step:\n{prompt}",
            f"As an expert in this field:\n{prompt}",
            f"Based on the context provided:\n{prompt}"
        ]
        
        best_prompt = prompt
        best_score = 0
        
        for variation in variations:
            scores = []
            for test_input in test_inputs[:3]:  # Limit for demo
                try:
                    response = model(variation.format(input=test_input))
                    # Simple scoring based on response length and keywords
                    score = min(len(response.split()) / 50, 1.0)  # Normalized by length
                    if any(keyword in response.lower() for keyword in ["because", "therefore", "analysis", "conclusion"]):
                        score += 0.2
                    scores.append(score)
                except Exception as e:
                    logger.warning(f"Error testing prompt variation: {e}")
                    scores.append(0)
            
            avg_score = np.mean(scores) if scores else 0
            if avg_score > best_score:
                best_score = avg_score
                best_prompt = variation
        
        logger.info(f"Best prompt score: {best_score:.3f}")
        return best_prompt

class DocumentProcessor:
    """Advanced document processing and chunking"""
    
    def __init__(self, config: LangChainConfig):
        self.config = config
        self.text_splitters = self._setup_text_splitters()
        self.loaders_registry = self._setup_loaders()
    
    def _setup_text_splitters(self) -> Dict[str, Any]:
        """Setup various text splitters"""
        return {
            "recursive": RecursiveCharacterTextSplitter(
                chunk_size=self.config.chunk_size,
                chunk_overlap=self.config.chunk_overlap,
                separators=["\n\n", "\n", " ", ""]
            ),
            "character": CharacterTextSplitter(
                chunk_size=self.config.chunk_size,
                chunk_overlap=self.config.chunk_overlap,
                separator="\n"
            ),
            "token": TokenTextSplitter(
                chunk_size=self.config.chunk_size,
                chunk_overlap=self.config.chunk_overlap
            )
        }
    
    def _setup_loaders(self) -> Dict[str, Callable]:
        """Setup document loaders for different file types"""
        return {
            ".pdf": PyPDFLoader,
            ".txt": TextLoader,
            ".csv": CSVLoader,
            ".docx": lambda path: self._load_docx(path),
            ".html": lambda path: self._load_html(path),
            "url": WebBaseLoader,
            "directory": DirectoryLoader
        }
    
    def _load_docx(self, file_path: str):
        """Custom DOCX loader"""
        doc = docx.Document(file_path)
        text = "\n".join([paragraph.text for paragraph in doc.paragraphs])
        return [{"page_content": text, "metadata": {"source": file_path}}]
    
    def _load_html(self, file_path: str):
        """Custom HTML loader"""
        with open(file_path, 'r', encoding='utf-8') as file:
            soup = BeautifulSoup(file.read(), 'html.parser')
            text = soup.get_text()
        return [{"page_content": text, "metadata": {"source": file_path}}]
    
    def process_documents(self, sources: List[Union[str, Dict[str, Any]]],
                         processing_config: Optional[Dict[str, Any]] = None) -> List[Any]:
        """Process multiple document sources"""
        
        all_documents = []
        processing_config = processing_config or {}
        
        for source in sources:
            try:
                if isinstance(source, str):
                    # Handle file paths or URLs
                    if source.startswith(("http://", "https://")):
                        docs = self._process_url(source)
                    elif os.path.isfile(source):
                        docs = self._process_file(source)
                    elif os.path.isdir(source):
                        docs = self._process_directory(source)
                    else:
                        logger.warning(f"Unknown source type: {source}")
                        continue
                elif isinstance(source, dict):
                    # Handle structured data
                    docs = self._process_structured_data(source)
                else:
                    logger.warning(f"Unsupported source format: {type(source)}")
                    continue
                
                # Apply text splitting
                splitter_type = processing_config.get("splitter_type", "recursive")
                splitter = self.text_splitters[splitter_type]
                
                if docs:
                    split_docs = splitter.split_documents(docs)
                    all_documents.extend(split_docs)
                
            except Exception as e:
                logger.error(f"Error processing source {source}: {e}")
                continue
        
        logger.info(f"Processed {len(sources)} sources into {len(all_documents)} document chunks")
        return all_documents
    
    def _process_file(self, file_path: str):
        """Process individual file"""
        file_extension = Path(file_path).suffix.lower()
        
        if file_extension in self.loaders_registry:
            loader_class = self.loaders_registry[file_extension]
            if callable(loader_class):
                if file_extension == ".pdf":
                    loader = loader_class(file_path)
                    return loader.load()
                else:
                    return loader_class(file_path)
        
        # Fallback to text loader
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                content = file.read()
            return [{"page_content": content, "metadata": {"source": file_path}}]
        except Exception as e:
            logger.error(f"Error reading file {file_path}: {e}")
            return []
    
    def _process_url(self, url: str):
        """Process web URL"""
        try:
            loader = WebBaseLoader(url)
            return loader.load()
        except Exception as e:
            logger.error(f"Error loading URL {url}: {e}")
            return []
    
    def _process_directory(self, directory_path: str):
        """Process entire directory"""
        try:
            loader = DirectoryLoader(
                directory_path,
                glob="**/*",
                loader_cls=TextLoader,
                loader_kwargs={"encoding": "utf-8"}
            )
            return loader.load()
        except Exception as e:
            logger.error(f"Error loading directory {directory_path}: {e}")
            return []
    
    def _process_structured_data(self, data: Dict[str, Any]):
        """Process structured data dictionary"""
        content = data.get("content", "")
        metadata = data.get("metadata", {})
        
        return [{"page_content": content, "metadata": metadata}]

class VectorStoreManager:
    """Advanced vector store management with multiple backends"""
    
    def __init__(self, config: LangChainConfig):
        self.config = config
        self.embeddings = self._setup_embeddings()
        self.vector_stores = {}
    
    def _setup_embeddings(self):
        """Setup embedding model"""
        if "openai" in self.config.embedding_model:
            return OpenAIEmbeddings(
                openai_api_key=self.config.openai_api_key,
                model=self.config.embedding_model
            )
        else:
            return HuggingFaceEmbeddings(
                model_name=self.config.embedding_model
            )
    
    def create_vector_store(self, documents: List[Any], 
                          store_name: str = "default",
                          store_type: Optional[str] = None) -> Any:
        """Create vector store from documents"""
        
        store_type = store_type or self.config.vector_store_type
        
        try:
            if store_type == "chroma":
                vector_store = Chroma.from_documents(
                    documents=documents,
                    embedding=self.embeddings,
                    persist_directory=f"./chroma_db_{store_name}"
                )
                
            elif store_type == "faiss":
                vector_store = FAISS.from_documents(
                    documents=documents,
                    embedding=self.embeddings
                )
                # Save FAISS index
                vector_store.save_local(f"./faiss_index_{store_name}")
                
            elif store_type == "pinecone":
                # Note: Requires Pinecone API key and environment setup
                import pinecone
                
                pinecone.init(
                    api_key=os.getenv("PINECONE_API_KEY"),
                    environment=os.getenv("PINECONE_ENVIRONMENT")
                )
                
                index_name = f"langchain-{store_name}"
                vector_store = Pinecone.from_documents(
                    documents=documents,
                    embedding=self.embeddings,
                    index_name=index_name
                )
                
            else:
                raise ValueError(f"Unsupported vector store type: {store_type}")
            
            self.vector_stores[store_name] = vector_store
            logger.info(f"Created {store_type} vector store '{store_name}' with {len(documents)} documents")
            
            return vector_store
            
        except Exception as e:
            logger.error(f"Error creating vector store: {e}")
            raise
    
    def load_existing_vector_store(self, store_name: str, 
                                 store_type: Optional[str] = None) -> Any:
        """Load existing vector store"""
        
        store_type = store_type or self.config.vector_store_type
        
        try:
            if store_type == "chroma":
                vector_store = Chroma(
                    persist_directory=f"./chroma_db_{store_name}",
                    embedding_function=self.embeddings
                )
                
            elif store_type == "faiss":
                vector_store = FAISS.load_local(
                    f"./faiss_index_{store_name}",
                    self.embeddings
                )
                
            else:
                raise ValueError(f"Unsupported vector store type for loading: {store_type}")
            
            self.vector_stores[store_name] = vector_store
            logger.info(f"Loaded existing {store_type} vector store '{store_name}'")
            
            return vector_store
            
        except Exception as e:
            logger.error(f"Error loading vector store: {e}")
            raise
    
    def create_advanced_retriever(self, vector_store: Any, 
                                config: RAGConfig) -> Any:
        """Create advanced retriever with multiple strategies"""
        
        base_retriever = vector_store.as_retriever(
            search_type=config.search_type,
            search_kwargs=config.search_kwargs
        )
        
        if config.retriever_type == "multi_query":
            # Multi-query retriever generates multiple queries
            from langchain.retrievers.multi_query import MultiQueryRetriever
            from langchain.llms import OpenAI
            
            llm = OpenAI(
                openai_api_key=self.config.openai_api_key,
                temperature=0
            )
            
            retriever = MultiQueryRetriever.from_llm(
                retriever=base_retriever,
                llm=llm
            )
            
        elif config.retriever_type == "contextual_compression":
            # Contextual compression retriever
            from langchain.retrievers.document_compressors import LLMChainExtractor
            from langchain.retrievers import ContextualCompressionRetriever
            
            llm = OpenAI(
                openai_api_key=self.config.openai_api_key,
                temperature=0
            )
            
            compressor = LLMChainExtractor.from_llm(llm)
            retriever = ContextualCompressionRetriever(
                base_compressor=compressor,
                base_retriever=base_retriever
            )
            
        else:
            retriever = base_retriever
        
        return retriever

class ChainOrchestrator:
    """Advanced chain orchestration and management"""
    
    def __init__(self, config: LangChainConfig):
        self.config = config
        self.llm = self._setup_llm()
        self.memory_manager = self._setup_memory()
        self.chains = {}
        self.prompt_manager = AdvancedPromptManager()
    
    def _setup_llm(self):
        """Setup language model"""
        return ChatOpenAI(
            openai_api_key=self.config.openai_api_key,
            model_name=self.config.model_name,
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens
        )
    
    def _setup_memory(self):
        """Setup memory based on configuration"""
        if self.config.memory_type == "buffer":
            return ConversationBufferMemory(
                memory_key="chat_history",
                return_messages=True
            )
        elif self.config.memory_type == "summary":
            return ConversationSummaryMemory(
                llm=self.llm,
                memory_key="chat_history",
                return_messages=True
            )
        elif self.config.memory_type == "window":
            return ConversationBufferWindowMemory(
                k=self.config.max_memory_length,
                memory_key="chat_history",
                return_messages=True
            )
        else:
            return ConversationBufferMemory(
                memory_key="chat_history",
                return_messages=True
            )
    
    def create_rag_chain(self, retriever: Any, 
                        config: RAGConfig,
                        custom_prompt: Optional[str] = None) -> Any:
        """Create sophisticated RAG chain"""
        
        # Default RAG prompt
        if custom_prompt is None:
            custom_prompt = """
            You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. 
            If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.
            
            Context: {context}
            
            Question: {question}
            
            Answer:
            """
        
        prompt = PromptTemplate(
            template=custom_prompt,
            input_variables=["context", "question"]
        )
        
        if config.chain_type == "stuff":
            chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=retriever,
                return_source_documents=config.return_source_documents,
                chain_type_kwargs={"prompt": prompt}
            )
        else:
            # For other chain types
            chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type=config.chain_type,
                retriever=retriever,
                return_source_documents=config.return_source_documents
            )
        
        return chain
    
    def create_conversational_rag_chain(self, retriever: Any) -> Any:
        """Create conversational RAG chain with memory"""
        
        chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=retriever,
            memory=self.memory_manager,
            return_source_documents=True,
            verbose=self.config.verbose
        )
        
        return chain
    
    def create_sequential_chain(self, chain_configs: List[Dict[str, Any]]) -> Any:
        """Create sequential chain from multiple components"""
        
        chains = []
        
        for config in chain_configs:
            prompt = PromptTemplate(
                template=config["template"],
                input_variables=config["input_variables"]
            )
            
            chain = LLMChain(
                llm=self.llm,
                prompt=prompt,
                output_key=config["output_key"]
            )
            
            chains.append(chain)
        
        sequential_chain = SequentialChain(
            chains=chains,
            input_variables=chain_configs[0]["input_variables"],
            output_variables=[config["output_key"] for config in chain_configs],
            verbose=self.config.verbose
        )
        
        return sequential_chain
    
    def create_router_chain(self, destination_configs: Dict[str, Dict[str, Any]],
                          default_chain_config: Dict[str, Any]) -> Any:
        """Create router chain for dynamic prompt selection"""
        
        from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser
        from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE
        
        # Create destination chains
        destination_chains = {}
        for name, config in destination_configs.items():
            prompt = PromptTemplate(
                template=config["template"],
                input_variables=config["input_variables"]
            )
            chain = LLMChain(llm=self.llm, prompt=prompt)
            destination_chains[name] = chain
        
        # Create default chain
        default_prompt = PromptTemplate(
            template=default_chain_config["template"],
            input_variables=default_chain_config["input_variables"]
        )
        default_chain = LLMChain(llm=self.llm, prompt=default_prompt)
        
        # Create router
        destinations = [f"{name}: {config['description']}" for name, config in destination_configs.items()]
        destinations_str = "\n".join(destinations)
        router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)
        
        router_prompt = PromptTemplate(
            template=router_template,
            input_variables=["input"],
            output_parser=RouterOutputParser()
        )
        
        router_chain = LLMRouterChain.from_llm(self.llm, router_prompt)
        
        # Create multi-prompt chain
        multi_prompt_chain = MultiPromptChain(
            router_chain=router_chain,
            destination_chains=destination_chains,
            default_chain=default_chain,
            verbose=self.config.verbose
        )
        
        return multi_prompt_chain

class AgentOrchestrator:
    """Advanced agent orchestration with tools"""
    
    def __init__(self, config: LangChainConfig):
        self.config = config
        self.llm = ChatOpenAI(
            openai_api_key=config.openai_api_key,
            model_name=config.model_name,
            temperature=config.temperature
        )
        self.tools = self._setup_tools()
        self.agents = {}
    
    def _setup_tools(self) -> List[Tool]:
        """Setup various tools for agents"""
        tools = []
        
        # Search tools
        try:
            search = DuckDuckGoSearchRun()
            tools.append(Tool(
                name="Search",
                func=search.run,
                description="Useful for searching the internet for current information"
            ))
        except Exception as e:
            logger.warning(f"Could not setup search tool: {e}")
        
        # Wikipedia tool
        try:
            wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())
            tools.append(Tool(
                name="Wikipedia",
                func=wikipedia.run,
                description="Useful for getting information from Wikipedia"
            ))
        except Exception as e:
            logger.warning(f"Could not setup Wikipedia tool: {e}")
        
        # Python REPL tool
        try:
            python_repl = PythonREPLTool()
            tools.append(Tool(
                name="Python_REPL",
                func=python_repl.run,
                description="Useful for running Python code and calculations"
            ))
        except Exception as e:
            logger.warning(f"Could not setup Python REPL tool: {e}")
        
        # Custom calculator tool
        def calculator(expression: str) -> str:
            """Calculate mathematical expressions safely"""
            try:
                # Safe evaluation of mathematical expressions
                allowed_names = {
                    k: v for k, v in __builtins__.items() if k in ['abs', 'round', 'min', 'max']
                }
                allowed_names.update({"__builtins__": {}})
                
                result = eval(expression, allowed_names)
                return str(result)
            except Exception as e:
                return f"Error: {str(e)}"
        
        tools.append(Tool(
            name="Calculator",
            func=calculator,
            description="Useful for mathematical calculations. Input should be a mathematical expression."
        ))
        
        return tools
    
    def create_react_agent(self, tools: Optional[List[Tool]] = None) -> Any:
        """Create ReAct agent"""
        
        tools = tools or self.tools
        
        agent = initialize_agent(
            tools=tools,
            llm=self.llm,
            agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
            verbose=self.config.verbose,
            handle_parsing_errors=True
        )
        
        return agent
    
    def create_structured_chat_agent(self, tools: Optional[List[Tool]] = None) -> Any:
        """Create structured chat agent"""
        
        tools = tools or self.tools
        
        agent = initialize_agent(
            tools=tools,
            llm=self.llm,
            agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,
            verbose=self.config.verbose,
            handle_parsing_errors=True
        )
        
        return agent
    
    def create_custom_tool(self, name: str, description: str, 
                          func: Callable) -> Tool:
        """Create custom tool"""
        
        return Tool(
            name=name,
            func=func,
            description=description
        )

class LangChainApplicationManager:
    """Main application manager for LangChain applications"""
    
    def __init__(self, config: LangChainConfig):
        self.config = config
        self.document_processor = DocumentProcessor(config)
        self.vector_store_manager = VectorStoreManager(config)
        self.chain_orchestrator = ChainOrchestrator(config)
        self.agent_orchestrator = AgentOrchestrator(config)
        
        # Setup caching
        if config.cache_enabled:
            set_llm_cache(InMemoryCache())
        
        # Application state
        self.applications = {}
        self.metrics = {
            "queries_processed": 0,
            "documents_indexed": 0,
            "chains_created": 0,
            "agents_created": 0,
            "total_tokens_used": 0,
            "total_cost": 0.0
        }
    
    async def create_rag_application(self, app_name: str,
                                   document_sources: List[str],
                                   rag_config: RAGConfig) -> Dict[str, Any]:
        """Create comprehensive RAG application"""
        
        logger.info(f"Creating RAG application: {app_name}")
        
        try:
            # Process documents
            documents = self.document_processor.process_documents(document_sources)
            self.metrics["documents_indexed"] += len(documents)
            
            # Create vector store
            vector_store = self.vector_store_manager.create_vector_store(
                documents, 
                store_name=app_name
            )
            
            # Create retriever
            retriever = self.vector_store_manager.create_advanced_retriever(
                vector_store, 
                rag_config
            )
            
            # Create RAG chain
            rag_chain = self.chain_orchestrator.create_rag_chain(retriever, rag_config)
            
            # Create conversational RAG chain
            conversational_chain = self.chain_orchestrator.create_conversational_rag_chain(retriever)
            
            # Store application
            application = {
                "type": "rag",
                "vector_store": vector_store,
                "retriever": retriever,
                "rag_chain": rag_chain,
                "conversational_chain": conversational_chain,
                "config": rag_config,
                "created_at": datetime.now(timezone.utc),
                "document_count": len(documents)
            }
            
            self.applications[app_name] = application
            self.metrics["chains_created"] += 2
            
            logger.info(f"RAG application '{app_name}' created successfully")
            return application
            
        except Exception as e:
            logger.error(f"Error creating RAG application: {e}")
            raise
    
    async def create_agent_application(self, app_name: str,
                                     agent_type: str = "react",
                                     custom_tools: Optional[List[Tool]] = None) -> Dict[str, Any]:
        """Create agent-based application"""
        
        logger.info(f"Creating agent application: {app_name}")
        
        try:
            # Setup tools
            tools = custom_tools or self.agent_orchestrator.tools
            
            # Create agent based on type
            if agent_type == "react":
                agent = self.agent_orchestrator.create_react_agent(tools)
            elif agent_type == "structured_chat":
                agent = self.agent_orchestrator.create_structured_chat_agent(tools)
            else:
                raise ValueError(f"Unsupported agent type: {agent_type}")
            
            # Store application
            application = {
                "type": "agent",
                "agent": agent,
                "agent_type": agent_type,
                "tools": [tool.name for tool in tools],
                "created_at": datetime.now(timezone.utc)
            }
            
            self.applications[app_name] = application
            self.metrics["agents_created"] += 1
            
            logger.info(f"Agent application '{app_name}' created successfully")
            return application
            
        except Exception as e:
            logger.error(f"Error creating agent application: {e}")
            raise
    
    async def query_application(self, app_name: str, query: str,
                              callback_handler: Optional[Any] = None) -> Dict[str, Any]:
        """Query an application and return results"""
        
        if app_name not in self.applications:
            raise ValueError(f"Application '{app_name}' not found")
        
        application = self.applications[app_name]
        
        try:
            with get_openai_callback() as cb:
                start_time = time.time()
                
                if application["type"] == "rag":
                    # Use RAG chain
                    result = application["rag_chain"]({"query": query})
                    
                elif application["type"] == "agent":
                    # Use agent
                    result = application["agent"].run(query)
                    # Restructure result for consistency
                    result = {"result": result}
                    
                else:
                    raise ValueError(f"Unsupported application type: {application['type']}")
                
                end_time = time.time()
                
                # Track metrics
                self.metrics["queries_processed"] += 1
                self.metrics["total_tokens_used"] += cb.total_tokens
                self.metrics["total_cost"] += cb.total_cost
                
                # Prepare response
                response = {
                    "query": query,
                    "result": result.get("result", ""),
                    "source_documents": result.get("source_documents", []),
                    "processing_time": end_time - start_time,
                    "tokens_used": cb.total_tokens,
                    "cost": cb.total_cost,
                    "timestamp": datetime.now(timezone.utc)
                }
                
                return response
                
        except Exception as e:
            logger.error(f"Error querying application '{app_name}': {e}")
            raise
    
    def get_application_info(self, app_name: str) -> Dict[str, Any]:
        """Get information about an application"""
        
        if app_name not in self.applications:
            return {"error": "Application not found"}
        
        app = self.applications[app_name]
        
        info = {
            "name": app_name,
            "type": app["type"],
            "created_at": app["created_at"],
        }
        
        if app["type"] == "rag":
            info.update({
                "document_count": app["document_count"],
                "retriever_type": app["config"].retriever_type,
                "chain_type": app["config"].chain_type
            })
        elif app["type"] == "agent":
            info.update({
                "agent_type": app["agent_type"],
                "available_tools": app["tools"]
            })
        
        return info
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get application metrics"""
        return self.metrics.copy()

# Practical demonstration
async def demonstrate_langchain_applications():
    """Comprehensive demonstration of LangChain applications"""
    
    logger.info("=== LangChain Applications Comprehensive Demonstration ===")
    
    # Configuration
    config = LangChainConfig(
        openai_api_key=os.getenv('OPENAI_API_KEY'),
        model_name="gpt-3.5-turbo",
        temperature=0.7,
        cache_enabled=True,
        verbose=True
    )
    
    if not config.openai_api_key:
        logger.error("OpenAI API key not found. Please set OPENAI_API_KEY environment variable.")
        return
    
    # Initialize application manager
    app_manager = LangChainApplicationManager(config)
    
    # 1. Document Processing and RAG Application
    logger.info("\n1. Creating RAG Application")
    
    try:
        # Create sample documents
        sample_docs_dir = "./sample_documents"
        os.makedirs(sample_docs_dir, exist_ok=True)
        
        # Create sample text files
        sample_content = {
            "ai_basics.txt": """
            Artificial Intelligence (AI) is a branch of computer science that aims to create machines 
            capable of intelligent behavior. Machine learning is a subset of AI that enables computers 
            to learn and improve from experience without being explicitly programmed.
            
            Deep learning, a subset of machine learning, uses artificial neural networks with multiple 
            layers to model and understand complex patterns in data. These networks are inspired by 
            the structure and function of biological neural networks in animal brains.
            """,
            
            "langchain_info.txt": """
            LangChain is a framework for developing applications powered by language models. It provides 
            tools and abstractions for working with language models, including prompt management, 
            chains, memory, and agents.
            
            Key features of LangChain include:
            - Model I/O: Interface with language models
            - Data connection: Interface with application-specific data
            - Chains: Combine components together
            - Agents: Let chains choose tools based on high-level directives
            - Memory: Persist application state between chain runs
            """,
            
            "rag_explanation.txt": """
            Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval 
            with text generation. It allows language models to access and incorporate external 
            knowledge sources during response generation.
            
            The RAG process involves:
            1. Encoding documents into vector representations
            2. Storing vectors in a searchable database
            3. Retrieving relevant documents based on query similarity
            4. Providing retrieved context to the language model
            5. Generating responses based on both the query and retrieved context
            """
        }
        
        # Write sample documents
        for filename, content in sample_content.items():
            with open(os.path.join(sample_docs_dir, filename), 'w') as f:
                f.write(content)
        
        # RAG configuration
        rag_config = RAGConfig(
            retriever_type="vector",
            search_type="similarity",
            search_kwargs={"k": 3},
            chain_type="stuff",
            return_source_documents=True
        )
        
        # Create RAG application
        rag_app = await app_manager.create_rag_application(
            app_name="knowledge_base",
            document_sources=[sample_docs_dir],
            rag_config=rag_config
        )
        
        logger.info("RAG application created successfully")
        
        # Test RAG queries
        test_queries = [
            "What is artificial intelligence?",
            "How does LangChain work?",
            "Explain the RAG process",
            "What are the key features of LangChain?"
        ]
        
        rag_results = []
        for query in test_queries:
            result = await app_manager.query_application("knowledge_base", query)
            rag_results.append(result)
            
            logger.info(f"\nQuery: {query}")
            logger.info(f"Answer: {result['result'][:200]}...")
            logger.info(f"Sources: {len(result['source_documents'])} documents")
            logger.info(f"Processing time: {result['processing_time']:.2f}s")
        
    except Exception as e:
        logger.error(f"Error in RAG demonstration: {e}")
    
    # 2. Agent Application
    logger.info("\n2. Creating Agent Application")
    
    try:
        # Create agent application
        agent_app = await app_manager.create_agent_application(
            app_name="research_assistant",
            agent_type="react"
        )
        
        logger.info("Agent application created successfully")
        
        # Test agent queries
        agent_queries = [
            "What is 25 * 47?",
            "Calculate the square root of 144",
            "What is the current population of Japan?",  # This would use search
        ]
        
        agent_results = []
        for query in agent_queries[:2]:  # Limit to avoid external API calls in demo
            try:
                result = await app_manager.query_application("research_assistant", query)
                agent_results.append(result)
                
                logger.info(f"\nQuery: {query}")
                logger.info(f"Answer: {result['result']}")
                logger.info(f"Processing time: {result['processing_time']:.2f}s")
                
            except Exception as e:
                logger.warning(f"Error with agent query '{query}': {e}")
        
    except Exception as e:
        logger.error(f"Error in agent demonstration: {e}")
    
    # 3. Advanced Chain Orchestration
    logger.info("\n3. Advanced Chain Orchestration")
    
    try:
        # Create sequential chain for complex analysis
        analysis_chain_configs = [
            {
                "template": "Summarize the following text in one sentence:\n\n{text}\n\nSummary:",
                "input_variables": ["text"],
                "output_key": "summary"
            },
            {
                "template": "Identify the key topics in this summary:\n\n{summary}\n\nKey topics:",
                "input_variables": ["summary"],
                "output_key": "topics"
            },
            {
                "template": "Based on these topics: {topics}, suggest three related questions for further research:\n\nQuestions:",
                "input_variables": ["topics"],
                "output_key": "questions"
            }
        ]
        
        sequential_chain = app_manager.chain_orchestrator.create_sequential_chain(analysis_chain_configs)
        
        # Test sequential chain
        test_text = """
        Machine learning is a method of data analysis that automates analytical model building. 
        It is a branch of artificial intelligence based on the idea that systems can learn from data, 
        identify patterns and make decisions with minimal human intervention. Machine learning algorithms 
        build mathematical models based on sample data, known as training data, in order to make 
        predictions or decisions without being explicitly programmed to do so.
        """
        
        with get_openai_callback() as cb:
            chain_result = sequential_chain({"text": test_text})
        
        logger.info("Sequential Chain Results:")
        logger.info(f"Summary: {chain_result['summary']}")
        logger.info(f"Topics: {chain_result['topics']}")
        logger.info(f"Questions: {chain_result['questions']}")
        logger.info(f"Tokens used: {cb.total_tokens}")
        
    except Exception as e:
        logger.error(f"Error in chain orchestration: {e}")
    
    # 4. Custom Tools and Advanced Agents
    logger.info("\n4. Custom Tools and Advanced Agents")
    
    try:
        # Create custom tools
        def text_statistics(text: str) -> str:
            """Calculate text statistics"""
            words = text.split()
            sentences = text.split('.')
            paragraphs = text.split('\n\n')
            
            stats = {
                "words": len(words),
                "sentences": len([s for s in sentences if s.strip()]),
                "paragraphs": len([p for p in paragraphs if p.strip()]),
                "characters": len(text),
                "avg_words_per_sentence": len(words) / max(len(sentences), 1)
            }
            
            return json.dumps(stats, indent=2)
        
        def data_processor(data_str: str) -> str:
            """Process structured data"""
            try:
                data = json.loads(data_str)
                if isinstance(data, list) and all(isinstance(x, (int, float)) for x in data):
                    # Numeric data
                    result = {
                        "count": len(data),
                        "sum": sum(data),
                        "mean": sum(data) / len(data),
                        "min": min(data),
                        "max": max(data)
                    }
                    return json.dumps(result, indent=2)
                else:
                    return "Data format not supported for processing"
            except:
                return "Invalid JSON format"
        
        # Create custom tools
        custom_tools = [
            app_manager.agent_orchestrator.create_custom_tool(
                name="Text_Statistics",
                description="Calculate statistics for text input. Input should be a text string.",
                func=text_statistics
            ),
            app_manager.agent_orchestrator.create_custom_tool(
                name="Data_Processor",
                description="Process numeric data arrays. Input should be JSON array of numbers.",
                func=data_processor
            )
        ]
        
        # Create agent with custom tools
        enhanced_agent_app = await app_manager.create_agent_application(
            app_name="data_analyst",
            agent_type="structured_chat",
            custom_tools=custom_tools + app_manager.agent_orchestrator.tools
        )
        
        # Test custom tools
        custom_queries = [
            "Calculate statistics for this text: 'LangChain is a powerful framework for building AI applications. It provides many useful abstractions and tools.'",
            "Process this numeric data: [10, 20, 30, 40, 50]"
        ]
        
        for query in custom_queries:
            try:
                result = await app_manager.query_application("data_analyst", query)
                logger.info(f"\nQuery: {query}")
                logger.info(f"Result: {result['result']}")
                
            except Exception as e:
                logger.warning(f"Error with custom tool query: {e}")
        
    except Exception as e:
        logger.error(f"Error in custom tools demonstration: {e}")
    
    # 5. Performance Analysis and Monitoring
    logger.info("\n5. Performance Analysis")
    
    # Get metrics
    metrics = app_manager.get_metrics()
    
    # Application information
    applications_info = {}
    for app_name in app_manager.applications.keys():
        applications_info[app_name] = app_manager.get_application_info(app_name)
    
    # Create performance visualization
    try:
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # Query processing times
        if 'rag_results' in locals():
            rag_times = [r['processing_time'] for r in rag_results]
            axes[0, 0].bar(range(len(rag_times)), rag_times)
            axes[0, 0].set_title('RAG Query Processing Times')
            axes[0, 0].set_xlabel('Query Number')
            axes[0, 0].set_ylabel('Time (seconds)')
        
        # Token usage distribution
        if 'rag_results' in locals():
            token_usage = [r['tokens_used'] for r in rag_results]
            axes[0, 1].pie(token_usage, labels=[f'Q{i+1}' for i in range(len(token_usage))], autopct='%1.1f%%')
            axes[0, 1].set_title('Token Usage Distribution')
        
        # Application types
        app_types = [info['type'] for info in applications_info.values()]
        type_counts = {t: app_types.count(t) for t in set(app_types)}
        axes[1, 0].bar(type_counts.keys(), type_counts.values())
        axes[1, 0].set_title('Application Types')
        
        # Metrics overview
        metric_names = ['Queries', 'Documents', 'Chains', 'Agents']
        metric_values = [
            metrics['queries_processed'],
            metrics['documents_indexed'],
            metrics['chains_created'],
            metrics['agents_created']
        ]
        axes[1, 1].bar(metric_names, metric_values)
        axes[1, 1].set_title('System Metrics')
        
        plt.tight_layout()
        plt.savefig('langchain_performance_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info("Performance visualization saved")
        
    except Exception as e:
        logger.warning(f"Error creating visualizations: {e}")
    
    # 6. Generate Comprehensive Report
    comprehensive_report = {
        "demonstration_summary": {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "applications_created": len(app_manager.applications),
            "total_queries_processed": metrics["queries_processed"],
            "total_documents_indexed": metrics["documents_indexed"],
            "total_tokens_used": metrics["total_tokens_used"],
            "total_cost": metrics["total_cost"]
        },
        
        "applications": applications_info,
        
        "rag_performance": {
            "queries_tested": len(rag_results) if 'rag_results' in locals() else 0,
            "avg_processing_time": np.mean([r['processing_time'] for r in rag_results]) if 'rag_results' in locals() else 0,
            "avg_tokens_per_query": np.mean([r['tokens_used'] for r in rag_results]) if 'rag_results' in locals() else 0
        } if 'rag_results' in locals() else {},
        
        "agent_performance": {
            "queries_tested": len(agent_results) if 'agent_results' in locals() else 0,
            "avg_processing_time": np.mean([r['processing_time'] for r in agent_results]) if 'agent_results' in locals() else 0
        } if 'agent_results' in locals() else {},
        
        "features_demonstrated": [
            "Document processing and vectorization",
            "RAG application creation and querying",
            "Agent creation with tools",
            "Sequential chain orchestration",
            "Custom tool development",
            "Performance monitoring and metrics",
            "Memory management",
            "Advanced retrieval strategies"
        ],
        
        "best_practices": [
            "Use appropriate chunk sizes for document splitting",
            "Implement proper error handling for robust applications",
            "Monitor token usage and costs",
            "Cache frequently accessed data",
            "Use appropriate retrieval strategies for different use cases",
            "Implement proper memory management for conversational applications",
            "Create custom tools for domain-specific functionality",
            "Use structured agents for complex reasoning tasks"
        ],
        
        "recommendations": [
            "Start with simple RAG applications before complex chains",
            "Use vector stores for efficient document retrieval",
            "Implement proper evaluation metrics for application quality",
            "Consider multi-modal approaches for diverse data types",
            "Use agent frameworks for interactive applications",
            "Implement proper logging and monitoring for production",
            "Consider cost optimization strategies for large-scale deployment",
            "Use appropriate memory types based on conversation length"
        ]
    }
    
    # Save comprehensive report
    with open("langchain_applications_report.json", "w") as f:
        json.dump(comprehensive_report, f, indent=2, default=str)
    
    logger.info("LangChain applications demonstration completed!")
    logger.info("Check 'langchain_applications_report.json' for detailed results")
    
    # Cleanup
    try:
        if os.path.exists(sample_docs_dir):
            shutil.rmtree(sample_docs_dir)
        logger.info("Cleaned up sample documents")
    except Exception as e:
        logger.warning(f"Error cleaning up: {e}")
    
    return comprehensive_report

# Main execution
async def main():
    """Main execution for LangChain applications demonstration"""
    try:
        report = await demonstrate_langchain_applications()
        
        # Display key results
        logger.info("\n=== LangChain Applications Summary ===")
        logger.info(f"Applications created: {report['demonstration_summary']['applications_created']}")
        logger.info(f"Queries processed: {report['demonstration_summary']['total_queries_processed']}")
        logger.info(f"Documents indexed: {report['demonstration_summary']['total_documents_indexed']}")
        logger.info(f"Total cost: ${report['demonstration_summary']['total_cost']:.4f}")
        
    except Exception as e:
        logger.error(f"LangChain demonstration failed: {e}")
        import traceback
        logger.error(traceback.format_exc())

if __name__ == "__main__":
    asyncio.run(main())
````

## Conclusion

The comprehensive LangChain framework demonstrates the revolutionary capabilities of modern AI application development through sophisticated orchestration of language models, external tools, and knowledge sources. This implementation provides production-ready patterns for building intelligent applications that extend far beyond simple text generation.

**Advanced Chain Architecture** enables the creation of complex workflows by seamlessly connecting multiple components including prompts, models, memory systems, and external APIs, providing a foundation for sophisticated AI applications that can reason, retrieve information, and take actions.

**Retrieval-Augmented Generation Excellence** through comprehensive RAG implementations demonstrates how language models can effectively access and incorporate external knowledge sources, dramatically reducing hallucinations while providing accurate, contextual responses grounded in real data.

**Intelligent Document Processing** via advanced loaders, splitters, and vectorization techniques enables efficient ingestion and organization of diverse content types, creating searchable knowledge bases that support semantic retrieval and contextual understanding.

**Agent-Based Interactions** through tool-enabled agents provide powerful capabilities for creating AI systems that can interact with external services, perform calculations, search the internet, and execute complex multi-step reasoning tasks autonomously.

**Memory Management Innovation** supports various conversation patterns from simple buffer memory to sophisticated vector-based memory systems, enabling applications to maintain context, learn from interactions, and provide personalized experiences.

**Production-Ready Infrastructure** including comprehensive monitoring, cost tracking, error handling, and performance optimization ensures that LangChain applications can be deployed reliably in enterprise environments with appropriate observability and control.

**Modular Design Philosophy** allows developers to compose applications from reusable components, experiment with different approaches, and scale solutions incrementally while maintaining code clarity and maintainability.

**Vector Store Integration** provides flexible backends for semantic search and similarity matching, enabling applications to work with various storage solutions while maintaining consistent interfaces and performance characteristics.

This comprehensive LangChain framework establishes the foundation for building intelligent applications that combine the power of large language models with external knowledge sources and computational tools, enabling the development of AI systems that are both capable and reliable for real-world deployment scenarios.