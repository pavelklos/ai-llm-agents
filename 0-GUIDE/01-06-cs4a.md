<small>Claude Sonnet 4</small>
# 06. LangChain and LangGraph

## Key Terms

**LangChain**: A comprehensive framework for developing applications powered by large language models (LLMs). It provides abstractions and tools for building complex LLM-powered applications through chains, agents, and memory components.

**Chain**: A sequence of components that can be invoked together to process inputs and produce outputs. Chains can combine prompts, LLMs, parsers, and other components in a pipeline.

**Prompt Template**: A structured way to format inputs to language models, supporting variable substitution and conditional logic to create dynamic prompts.

**Tool**: An external function or API that an agent can call to perform specific tasks, such as web searches, calculations, or database queries.

**Retrieval-Augmented Generation (RAG)**: A technique that combines information retrieval with text generation, allowing models to access external knowledge bases to provide more accurate and up-to-date responses.

**LangServe**: A deployment framework for LangChain applications that provides REST API endpoints with automatic OpenAPI documentation and async support.

**LangSmith**: A monitoring and debugging platform for LangChain applications that provides tracing, evaluation, and analytics capabilities.

**LangGraph**: A framework for building stateful, multi-actor applications with LLMs, designed for creating complex workflows with cycles, conditions, and human-in-the-loop interactions.

## Working with LangChain: Prompts, Chains, and Tools

LangChain provides a sophisticated ecosystem for building LLM applications through modular components. The framework's strength lies in its ability to chain together different operations, maintain state, and integrate with external tools and data sources.

### Advanced Prompt Engineering and Templates

````python
import os
import asyncio
from typing import Dict, List, Any, Optional, Union
from datetime import datetime
from dotenv import load_dotenv

from langchain.prompts import (
    PromptTemplate, 
    ChatPromptTemplate, 
    FewShotPromptTemplate,
    PipelinePromptTemplate,
    ConditionalPromptTemplate
)
from langchain.prompts.example_selector import (
    SemanticSimilarityExampleSelector,
    LengthBasedExampleSelector,
    MaxMarginalRelevanceExampleSelector
)
from langchain.schema import BaseOutputParser
from langchain.output_parsers import (
    PydanticOutputParser,
    JsonOutputParser,
    DatetimeOutputParser,
    EnumOutputParser
)
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from pydantic import BaseModel, Field

load_dotenv()

class AnalysisResult(BaseModel):
    sentiment: str = Field(description="Sentiment of the text: positive, negative, or neutral")
    confidence: float = Field(description="Confidence score between 0 and 1")
    key_themes: List[str] = Field(description="List of key themes identified")
    summary: str = Field(description="Brief summary of the analysis")
    actionable_insights: List[str] = Field(description="List of actionable insights")

class AdvancedPromptManager:
    """Advanced prompt management system with dynamic templates"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model="gpt-4-turbo-preview",
            temperature=0.7,
            api_key=os.getenv('OPENAI_API_KEY')
        )
        self.embeddings = OpenAIEmbeddings(api_key=os.getenv('OPENAI_API_KEY'))
        self.example_vectorstore = None
        self._initialize_example_store()
        
    def _initialize_example_store(self):
        """Initialize vector store for example selection"""
        examples = [
            {
                "input": "Analyze the customer feedback about our new product launch",
                "output": "Comprehensive analysis revealing 78% positive sentiment with key themes around usability and design innovation"
            },
            {
                "input": "Review the quarterly financial performance",
                "output": "Financial analysis showing 15% revenue growth with strong performance in digital channels"
            },
            {
                "input": "Evaluate employee satisfaction survey results",
                "output": "Survey analysis indicating high satisfaction in remote work policies but concerns about career development"
            }
        ]
        
        # Create embeddings for examples
        texts = [f"{ex['input']} {ex['output']}" for ex in examples]
        self.example_vectorstore = Chroma.from_texts(
            texts, 
            self.embeddings,
            metadatas=examples
        )
    
    def create_dynamic_analysis_prompt(self, analysis_type: str, context: Dict[str, Any]) -> ChatPromptTemplate:
        """Create dynamic analysis prompt based on type and context"""
        
        # Base system prompt
        system_template = """
        You are an expert analyst specializing in {analysis_type} analysis.
        
        Current Context:
        - Date: {current_date}
        - User Role: {user_role}
        - Priority: {priority}
        - Domain: {domain}
        
        Analysis Guidelines:
        1. Provide structured, actionable insights
        2. Use data-driven reasoning
        3. Consider the specific context and domain
        4. Highlight key findings and recommendations
        5. Format output according to the specified schema
        
        {format_instructions}
        """
        
        # Dynamic human prompt based on analysis type
        analysis_prompts = {
            "sentiment": """
            Analyze the sentiment of the following text:
            
            Text: {input_text}
            
            Consider:
            - Overall emotional tone
            - Specific sentiment indicators
            - Context and domain-specific nuances
            - Confidence level in your assessment
            """,
            
            "competitive": """
            Perform competitive analysis on:
            
            Subject: {input_text}
            Competitors: {competitors}
            
            Focus on:
            - Market positioning
            - Strengths and weaknesses
            - Opportunities and threats
            - Strategic recommendations
            """,
            
            "financial": """
            Analyze the financial data:
            
            Data: {input_text}
            Period: {time_period}
            
            Examine:
            - Key performance indicators
            - Trends and patterns
            - Risk factors
            - Growth opportunities
            """
        }
        
        human_template = analysis_prompts.get(analysis_type, analysis_prompts["sentiment"])
        
        # Create output parser
        output_parser = PydanticOutputParser(pydantic_object=AnalysisResult)
        
        # Build the complete prompt
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_template),
            ("human", human_template)
        ])
        
        # Partial the prompt with context
        return prompt.partial(
            analysis_type=analysis_type,
            current_date=datetime.now().strftime("%Y-%m-%d"),
            user_role=context.get("user_role", "analyst"),
            priority=context.get("priority", "medium"),
            domain=context.get("domain", "general"),
            format_instructions=output_parser.get_format_instructions(),
            competitors=context.get("competitors", "Not specified"),
            time_period=context.get("time_period", "Current period")
        )
    
    def create_few_shot_prompt(self, task_type: str, examples: List[Dict[str, str]]) -> FewShotPromptTemplate:
        """Create few-shot learning prompt with intelligent example selection"""
        
        # Create example selector
        example_selector = SemanticSimilarityExampleSelector.from_examples(
            examples,
            self.embeddings,
            Chroma,
            k=3  # Select top 3 most relevant examples
        )
        
        # Define example format
        example_prompt = PromptTemplate(
            input_variables=["input", "output"],
            template="Input: {input}\nOutput: {output}"
        )
        
        # Create few-shot prompt
        few_shot_prompt = FewShotPromptTemplate(
            example_selector=example_selector,
            example_prompt=example_prompt,
            prefix=f"""
            You are performing {task_type} tasks. Here are some examples of high-quality outputs:
            """,
            suffix="""
            Now, process the following input:
            Input: {input}
            Output:""",
            input_variables=["input"]
        )
        
        return few_shot_prompt
    
    def create_pipeline_prompt(self, stages: List[Dict[str, Any]]) -> PipelinePromptTemplate:
        """Create multi-stage pipeline prompt"""
        
        pipeline_prompts = []
        
        for i, stage in enumerate(stages):
            stage_prompt = PromptTemplate(
                template=stage["template"],
                input_variables=stage["input_variables"]
            )
            pipeline_prompts.append((f"stage_{i}", stage_prompt))
        
        # Final prompt that combines all stages
        final_template = """
        Multi-stage Analysis Pipeline:
        
        {pipeline_output}
        
        Based on all stages above, provide a comprehensive final analysis:
        {final_instructions}
        """
        
        final_prompt = PromptTemplate(
            template=final_template,
            input_variables=["final_instructions"]
        )
        
        return PipelinePromptTemplate(
            final_prompt=final_prompt,
            pipeline_prompts=pipeline_prompts
        )

class AdvancedChainBuilder:
    """Builder for complex LangChain chains"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model="gpt-4-turbo-preview",
            temperature=0.3,
            api_key=os.getenv('OPENAI_API_KEY')
        )
        
    def create_analysis_chain(self, analysis_type: str) -> Any:
        """Create sophisticated analysis chain"""
        from langchain.chains import LLMChain, SequentialChain, TransformChain
        from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
        
        # Data preprocessing chain
        def preprocess_data(inputs: Dict[str, Any]) -> Dict[str, Any]:
            text = inputs["input_text"]
            
            # Clean and normalize text
            cleaned_text = text.strip().replace('\n', ' ').replace('\r', '')
            
            # Extract metadata
            word_count = len(cleaned_text.split())
            char_count = len(cleaned_text)
            
            return {
                **inputs,
                "cleaned_text": cleaned_text,
                "word_count": word_count,
                "char_count": char_count,
                "complexity": "high" if word_count > 500 else "medium" if word_count > 100 else "low"
            }
        
        preprocess_chain = TransformChain(
            input_variables=["input_text"],
            output_variables=["cleaned_text", "word_count", "char_count", "complexity"],
            transform=preprocess_data
        )
        
        # Main analysis chain
        analysis_prompt = PromptTemplate(
            input_variables=["cleaned_text", "analysis_type", "complexity"],
            template="""
            Perform {analysis_type} analysis on the following text.
            
            Text complexity: {complexity}
            Text: {cleaned_text}
            
            Provide detailed analysis including:
            1. Key findings
            2. Supporting evidence
            3. Confidence assessment
            4. Recommendations
            
            Format as structured JSON.
            """
        )
        
        analysis_chain = LLMChain(
            llm=self.llm,
            prompt=analysis_prompt,
            output_key="analysis_result"
        )
        
        # Post-processing chain
        def postprocess_analysis(inputs: Dict[str, Any]) -> Dict[str, Any]:
            import json
            
            try:
                analysis = json.loads(inputs["analysis_result"])
            except json.JSONDecodeError:
                analysis = {"raw_output": inputs["analysis_result"]}
            
            # Add metadata
            analysis["metadata"] = {
                "word_count": inputs["word_count"],
                "char_count": inputs["char_count"],
                "complexity": inputs["complexity"],
                "analysis_type": analysis_type,
                "timestamp": datetime.now().isoformat()
            }
            
            return {"final_analysis": analysis}
        
        postprocess_chain = TransformChain(
            input_variables=["analysis_result", "word_count", "char_count", "complexity"],
            output_variables=["final_analysis"],
            transform=postprocess_analysis
        )
        
        # Combine into sequential chain
        overall_chain = SequentialChain(
            chains=[preprocess_chain, analysis_chain, postprocess_chain],
            input_variables=["input_text"],
            output_variables=["final_analysis"],
            verbose=True
        )
        
        return overall_chain

class AdvancedToolIntegration:
    """Advanced tool integration for LangChain agents"""
    
    def __init__(self):
        self.tools = []
        self._register_tools()
    
    def _register_tools(self):
        """Register custom tools for agent use"""
        from langchain.tools import Tool, StructuredTool
        from langchain.pydantic_v1 import BaseModel, Field
        
        # Database query tool
        class DatabaseQueryInput(BaseModel):
            query: str = Field(description="SQL query to execute")
            database: str = Field(description="Database name", default="main")
            limit: int = Field(description="Maximum rows to return", default=100)
        
        database_tool = StructuredTool(
            name="database_query",
            description="Execute SQL queries against the database",
            func=self._execute_database_query,
            args_schema=DatabaseQueryInput
        )
        
        # Web search tool
        class WebSearchInput(BaseModel):
            query: str = Field(description="Search query")
            num_results: int = Field(description="Number of results", default=5)
            search_type: str = Field(description="Type of search", default="general")
        
        web_search_tool = StructuredTool(
            name="web_search",
            description="Search the web for current information",
            func=self._web_search,
            args_schema=WebSearchInput
        )
        
        # Document analysis tool
        class DocumentAnalysisInput(BaseModel):
            document_path: str = Field(description="Path to document file")
            analysis_type: str = Field(description="Type of analysis", default="summary")
            extract_entities: bool = Field(description="Extract named entities", default=True)
        
        document_tool = StructuredTool(
            name="document_analysis",
            description="Analyze documents and extract information",
            func=self._analyze_document,
            args_schema=DocumentAnalysisInput
        )
        
        # Code execution tool
        class CodeExecutionInput(BaseModel):
            code: str = Field(description="Python code to execute")
            timeout: int = Field(description="Execution timeout in seconds", default=30)
            environment: str = Field(description="Execution environment", default="sandbox")
        
        code_tool = StructuredTool(
            name="code_execution",
            description="Execute Python code in a secure sandbox",
            func=self._execute_code,
            args_schema=CodeExecutionInput
        )
        
        self.tools = [database_tool, web_search_tool, document_tool, code_tool]
    
    async def _execute_database_query(self, query: str, database: str = "main", limit: int = 100) -> str:
        """Execute database query with error handling"""
        try:
            # Simulate database connection and query execution
            # In production, use actual database connectors
            
            mock_results = [
                {"id": 1, "name": "Sample Record 1", "value": 100},
                {"id": 2, "name": "Sample Record 2", "value": 200}
            ]
            
            return f"Query executed successfully. Found {len(mock_results)} records: {mock_results[:limit]}"
            
        except Exception as e:
            return f"Database query failed: {str(e)}"
    
    async def _web_search(self, query: str, num_results: int = 5, search_type: str = "general") -> str:
        """Perform web search"""
        try:
            # Simulate web search results
            # In production, integrate with actual search APIs
            
            results = [
                {
                    "title": f"Result {i+1} for '{query}'",
                    "url": f"https://example.com/result{i+1}",
                    "snippet": f"This is a search result snippet for query: {query}"
                }
                for i in range(num_results)
            ]
            
            return f"Found {len(results)} search results: {results}"
            
        except Exception as e:
            return f"Web search failed: {str(e)}"
    
    async def _analyze_document(self, document_path: str, analysis_type: str = "summary", extract_entities: bool = True) -> str:
        """Analyze document content"""
        try:
            # Simulate document analysis
            # In production, implement actual document processing
            
            analysis_result = {
                "document": document_path,
                "analysis_type": analysis_type,
                "summary": "Document contains technical specifications and user guidelines.",
                "entities": ["Technical Specs", "User Guidelines", "API Documentation"] if extract_entities else [],
                "word_count": 1250,
                "key_topics": ["Configuration", "Setup", "Troubleshooting"]
            }
            
            return f"Document analysis completed: {analysis_result}"
            
        except Exception as e:
            return f"Document analysis failed: {str(e)}"
    
    async def _execute_code(self, code: str, timeout: int = 30, environment: str = "sandbox") -> str:
        """Execute code in secure environment"""
        try:
            # Simulate secure code execution
            # In production, use containerized execution environments
            
            # Basic security check
            dangerous_keywords = ['import os', 'import sys', 'exec', 'eval', '__import__']
            if any(keyword in code for keyword in dangerous_keywords):
                return "Code execution blocked: potentially dangerous operations detected"
            
            # Simulate execution
            execution_result = {
                "status": "success",
                "output": "Code executed successfully",
                "environment": environment,
                "execution_time": "0.15s"
            }
            
            return f"Code execution result: {execution_result}"
            
        except Exception as e:
            return f"Code execution failed: {str(e)}"
````

## Retrieval-Augmented Generation (RAG)

RAG represents a sophisticated approach to enhancing LLM capabilities by combining retrieval systems with generative models. This technique allows models to access external knowledge bases dynamically, providing more accurate and contextually relevant responses.

````python
import asyncio
import os
from typing import List, Dict, Any, Optional, Union
from datetime import datetime
import numpy as np
from dotenv import load_dotenv

from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    TokenTextSplitter,
    MarkdownHeaderTextSplitter
)
from langchain_community.document_loaders import (
    PyPDFLoader,
    UnstructuredMarkdownLoader,
    WebBaseLoader,
    DirectoryLoader
)
from langchain_community.vectorstores import Chroma, FAISS, Pinecone
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.retrievers import (
    ContextualCompressionRetriever,
    EnsembleRetriever,
    MultiQueryRetriever,
    ParentDocumentRetriever
)
from langchain.retrievers.document_compressors import (
    LLMChainExtractor,
    EmbeddingsFilter,
    DocumentCompressorPipeline
)
from langchain.chains import RetrievalQA, ConversationalRetrievalChain
from langchain.memory import ConversationBufferWindowMemory, ConversationSummaryBufferMemory
from langchain.schema import Document
from langchain.prompts import PromptTemplate
from langchain.callbacks import StreamingStdOutCallbackHandler

load_dotenv()

class AdvancedRAGSystem:
    """Sophisticated RAG implementation with multiple retrieval strategies"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-large",
            api_key=os.getenv('OPENAI_API_KEY')
        )
        self.llm = ChatOpenAI(
            model="gpt-4-turbo-preview",
            temperature=0.3,
            api_key=os.getenv('OPENAI_API_KEY'),
            streaming=True,
            callbacks=[StreamingStdOutCallbackHandler()]
        )
        self.vector_stores = {}
        self.retrievers = {}
        self.document_store = []
        
    async def ingest_documents(self, document_sources: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Ingest documents from multiple sources with advanced processing"""
        
        ingestion_results = {
            "total_documents": 0,
            "successful_ingests": 0,
            "failed_ingests": 0,
            "processing_time": 0,
            "document_stats": {}
        }
        
        start_time = datetime.now()
        
        for source in document_sources:
            try:
                documents = await self._load_documents(source)
                processed_docs = await self._process_documents(documents, source)
                
                # Store in vector database
                vector_store_name = source.get("vector_store", "default")
                await self._store_documents(processed_docs, vector_store_name)
                
                ingestion_results["successful_ingests"] += len(processed_docs)
                ingestion_results["document_stats"][source["name"]] = {
                    "document_count": len(processed_docs),
                    "total_chars": sum(len(doc.page_content) for doc in processed_docs),
                    "source_type": source["type"]
                }
                
            except Exception as e:
                print(f"Failed to ingest {source['name']}: {e}")
                ingestion_results["failed_ingests"] += 1
        
        ingestion_results["processing_time"] = (datetime.now() - start_time).total_seconds()
        ingestion_results["total_documents"] = ingestion_results["successful_ingests"]
        
        return ingestion_results
    
    async def _load_documents(self, source: Dict[str, Any]) -> List[Document]:
        """Load documents from various sources"""
        
        loaders = {
            "pdf": lambda path: PyPDFLoader(path).load(),
            "markdown": lambda path: UnstructuredMarkdownLoader(path).load(),
            "web": lambda url: WebBaseLoader(url).load(),
            "directory": lambda path: DirectoryLoader(
                path, 
                glob="**/*.{txt,md,pdf}",
                show_progress=True
            ).load()
        }
        
        source_type = source["type"]
        source_path = source["path"]
        
        if source_type in loaders:
            documents = loaders[source_type](source_path)
            
            # Add metadata
            for doc in documents:
                doc.metadata.update({
                    "source_name": source["name"],
                    "source_type": source_type,
                    "ingestion_timestamp": datetime.now().isoformat(),
                    "processing_config": source.get("config", {})
                })
            
            return documents
        else:
            raise ValueError(f"Unsupported source type: {source_type}")
    
    async def _process_documents(self, documents: List[Document], source: Dict[str, Any]) -> List[Document]:
        """Advanced document processing with multiple splitting strategies"""
        
        processing_config = source.get("config", {})
        
        # Choose text splitter based on content type
        if source["type"] == "markdown":
            # Use markdown-aware splitter
            markdown_splitter = MarkdownHeaderTextSplitter(
                headers_to_split_on=[
                    ("#", "Header 1"),
                    ("##", "Header 2"),
                    ("###", "Header 3"),
                ]
            )
            
            # First split by headers
            split_docs = []
            for doc in documents:
                header_splits = markdown_splitter.split_text(doc.page_content)
                for split in header_splits:
                    split.metadata.update(doc.metadata)
                split_docs.extend(header_splits)
            
            # Then apply recursive splitter
            recursive_splitter = RecursiveCharacterTextSplitter(
                chunk_size=processing_config.get("chunk_size", 1000),
                chunk_overlap=processing_config.get("chunk_overlap", 200),
                separators=["\n\n", "\n", " ", ""]
            )
            
            final_docs = recursive_splitter.split_documents(split_docs)
            
        else:
            # Use token-based splitter for other content
            token_splitter = TokenTextSplitter(
                chunk_size=processing_config.get("token_chunk_size", 800),
                chunk_overlap=processing_config.get("token_overlap", 100)
            )
            
            final_docs = token_splitter.split_documents(documents)
        
        # Enhance with additional metadata
        for i, doc in enumerate(final_docs):
            doc.metadata.update({
                "chunk_id": f"{source['name']}_{i}",
                "chunk_index": i,
                "total_chunks": len(final_docs),
                "word_count": len(doc.page_content.split()),
                "char_count": len(doc.page_content)
            })
        
        return final_docs
    
    async def _store_documents(self, documents: List[Document], vector_store_name: str = "default"):
        """Store documents in vector database with optimization"""
        
        if vector_store_name not in self.vector_stores:
            # Create new vector store
            self.vector_stores[vector_store_name] = await Chroma.afrom_documents(
                documents=documents,
                embedding=self.embeddings,
                persist_directory=f"./chroma_db_{vector_store_name}",
                collection_name=vector_store_name
            )
        else:
            # Add to existing vector store
            await self.vector_stores[vector_store_name].aadd_documents(documents)
        
        # Update document store
        self.document_store.extend(documents)
        
        # Create optimized retrievers
        await self._create_retrievers(vector_store_name)
    
    async def _create_retrievers(self, vector_store_name: str):
        """Create multiple retrieval strategies"""
        
        vector_store = self.vector_stores[vector_store_name]
        
        # Basic similarity retriever
        similarity_retriever = vector_store.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 6}
        )
        
        # MMR (Maximum Marginal Relevance) retriever
        mmr_retriever = vector_store.as_retriever(
            search_type="mmr",
            search_kwargs={"k": 6, "fetch_k": 20, "lambda_mult": 0.5}
        )
        
        # Multi-query retriever
        multi_query_retriever = MultiQueryRetriever.from_llm(
            retriever=similarity_retriever,
            llm=self.llm,
            verbose=True
        )
        
        # Contextual compression retriever
        compressor = LLMChainExtractor.from_llm(self.llm)
        compression_retriever = ContextualCompressionRetriever(
            base_compressor=compressor,
            base_retriever=similarity_retriever
        )
        
        # Ensemble retriever combining multiple strategies
        ensemble_retriever = EnsembleRetriever(
            retrievers=[similarity_retriever, mmr_retriever],
            weights=[0.6, 0.4]
        )
        
        self.retrievers[vector_store_name] = {
            "similarity": similarity_retriever,
            "mmr": mmr_retriever,
            "multi_query": multi_query_retriever,
            "compression": compression_retriever,
            "ensemble": ensemble_retriever
        }
    
    async def query_with_rag(self, 
                           query: str, 
                           vector_store_name: str = "default",
                           retrieval_strategy: str = "ensemble",
                           conversation_id: Optional[str] = None) -> Dict[str, Any]:
        """Perform RAG query with conversation memory"""
        
        if vector_store_name not in self.retrievers:
            raise ValueError(f"Vector store '{vector_store_name}' not found")
        
        retriever = self.retrievers[vector_store_name].get(
            retrieval_strategy, 
            self.retrievers[vector_store_name]["similarity"]
        )
        
        # Create conversation memory if conversation_id provided
        memory = None
        if conversation_id:
            memory = ConversationSummaryBufferMemory(
                llm=self.llm,
                memory_key="chat_history",
                return_messages=True,
                max_token_limit=2000
            )
        
        # Custom prompt template for RAG
        rag_prompt = PromptTemplate(
            template="""
            You are a knowledgeable assistant with access to relevant documents.
            Use the following context to answer the user's question accurately and comprehensively.
            
            Context:
            {context}
            
            Question: {question}
            
            Instructions:
            1. Base your answer primarily on the provided context
            2. If the context doesn't contain enough information, clearly state this
            3. Provide specific citations or references when possible
            4. Be precise and avoid speculation
            5. If relevant, suggest related topics the user might want to explore
            
            Answer:
            """,
            input_variables=["context", "question"]
        )
        
        # Create RAG chain
        if memory:
            rag_chain = ConversationalRetrievalChain.from_llm(
                llm=self.llm,
                retriever=retriever,
                memory=memory,
                verbose=True,
                return_source_documents=True,
                combine_docs_chain_kwargs={"prompt": rag_prompt}
            )
            
            result = await rag_chain.ainvoke({"question": query})
            
        else:
            rag_chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=retriever,
                verbose=True,
                return_source_documents=True,
                chain_type_kwargs={"prompt": rag_prompt}
            )
            
            result = await rag_chain.ainvoke({"query": query})
        
        # Format response
        response = {
            "query": query,
            "answer": result.get("answer", result.get("result")),
            "source_documents": [
                {
                    "content": doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content,
                    "metadata": doc.metadata,
                    "relevance_score": getattr(doc, 'relevance_score', None)
                }
                for doc in result.get("source_documents", [])
            ],
            "retrieval_strategy": retrieval_strategy,
            "vector_store": vector_store_name,
            "conversation_id": conversation_id,
            "timestamp": datetime.now().isoformat()
        }
        
        return response
    
    async def evaluate_rag_performance(self, test_queries: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Evaluate RAG system performance"""
        
        evaluation_results = {
            "total_queries": len(test_queries),
            "strategy_performance": {},
            "average_response_time": 0,
            "accuracy_metrics": {}
        }
        
        strategies = ["similarity", "mmr", "multi_query", "compression", "ensemble"]
        
        for strategy in strategies:
            strategy_results = []
            total_time = 0
            
            for test_query in test_queries:
                start_time = datetime.now()
                
                try:
                    result = await self.query_with_rag(
                        query=test_query["query"],
                        retrieval_strategy=strategy
                    )
                    
                    response_time = (datetime.now() - start_time).total_seconds()
                    total_time += response_time
                    
                    # Calculate relevance score if ground truth provided
                    relevance_score = 0.0
                    if "expected_answer" in test_query:
                        relevance_score = await self._calculate_relevance(
                            result["answer"], 
                            test_query["expected_answer"]
                        )
                    
                    strategy_results.append({
                        "query": test_query["query"],
                        "response_time": response_time,
                        "relevance_score": relevance_score,
                        "source_count": len(result["source_documents"])
                    })
                    
                except Exception as e:
                    strategy_results.append({
                        "query": test_query["query"],
                        "error": str(e),
                        "response_time": 0,
                        "relevance_score": 0
                    })
            
            evaluation_results["strategy_performance"][strategy] = {
                "average_response_time": total_time / len(test_queries),
                "average_relevance": np.mean([r.get("relevance_score", 0) for r in strategy_results]),
                "success_rate": len([r for r in strategy_results if "error" not in r]) / len(strategy_results),
                "detailed_results": strategy_results
            }
        
        return evaluation_results
    
    async def _calculate_relevance(self, answer: str, expected_answer: str) -> float:
        """Calculate relevance score between generated and expected answers"""
        
        # Simple relevance calculation using LLM
        relevance_prompt = f"""
        Compare the following two answers and rate their similarity on a scale of 0.0 to 1.0:
        
        Generated Answer: {answer}
        Expected Answer: {expected_answer}
        
        Consider semantic similarity, factual accuracy, and completeness.
        Respond with only a number between 0.0 and 1.0.
        """
        
        try:
            relevance_result = await self.llm.ainvoke(relevance_prompt)
            relevance_score = float(relevance_result.content.strip())
            return max(0.0, min(1.0, relevance_score))
        except:
            return 0.5  # Default neutral score if evaluation fails
````

## LangServe, LangSmith, and LangGraph for System Orchestration

### LangServe Deployment Framework

````python
import asyncio
import os
import json
from typing import Dict, List, Any, Optional, Union
from datetime import datetime
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import uvicorn
from dotenv import load_dotenv

from langserve import add_routes
from langchain.schema.runnable import Runnable, RunnablePassthrough, RunnableLambda
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import BaseMessage
from langchain.callbacks.tracers import LangChainTracer
from langsmith import Client

load_dotenv()

# Pydantic models for API
class QueryRequest(BaseModel):
    query: str = Field(description="User query")
    context: Optional[Dict[str, Any]] = Field(default={}, description="Additional context")
    session_id: Optional[str] = Field(default=None, description="Session identifier")
    parameters: Optional[Dict[str, Any]] = Field(default={}, description="Model parameters")

class QueryResponse(BaseModel):
    response: str = Field(description="Generated response")
    metadata: Dict[str, Any] = Field(description="Response metadata")
    session_id: Optional[str] = Field(description="Session identifier")
    timestamp: str = Field(description="Response timestamp")

class AnalysisRequest(BaseModel):
    text: str = Field(description="Text to analyze")
    analysis_type: str = Field(description="Type of analysis", default="general")
    options: Optional[Dict[str, Any]] = Field(default={}, description="Analysis options")

class LangServeApplication:
    """Production-ready LangServe application with monitoring"""
    
    def __init__(self):
        self.app = FastAPI(
            title="Advanced LangChain API",
            description="Production LangChain application with monitoring and orchestration",
            version="1.0.0"
        )
        
        # Configure CORS
        self.app.add_middleware(
            CORSMiddleware,
            allow_origins=["*"],
            allow_credentials=True,
            allow_methods=["*"],
            allow_headers=["*"],
        )
        
        # Initialize LangSmith client for monitoring
        self.langsmith_client = Client(
            api_key=os.getenv("LANGSMITH_API_KEY"),
            api_url="https://api.smith.langchain.com"
        )
        
        # Initialize components
        self.llm = ChatOpenAI(
            model="gpt-4-turbo-preview",
            temperature=0.7,
            api_key=os.getenv('OPENAI_API_KEY')
        )
        
        self.chains = self._create_chains()
        self._setup_routes()
        self._setup_monitoring()
    
    def _create_chains(self) -> Dict[str, Runnable]:
        """Create various chains for different use cases"""
        
        # Simple chat chain
        chat_prompt = ChatPromptTemplate.from_messages([
            ("system", "You are a helpful assistant that provides clear and accurate responses."),
            ("human", "{query}")
        ])
        
        chat_chain = chat_prompt | self.llm
        
        # Analysis chain
        analysis_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an expert analyst. Perform {analysis_type} analysis on the provided text.
            
            Analysis Guidelines:
            1. Be thorough and systematic
            2. Provide evidence-based insights
            3. Structure your response clearly
            4. Include confidence levels where appropriate
            """),
            ("human", "Analyze this text: {text}")
        ])
        
        analysis_chain = analysis_prompt | self.llm
        
        # Multi-step reasoning chain
        reasoning_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a logical reasoning assistant. Break down complex problems into steps.
            
            For each query:
            1. Identify the key components
            2. Outline your reasoning process
            3. Work through each step systematically
            4. Provide a clear conclusion
            """),
            ("human", "{query}")
        ])
        
        # Add preprocessing step
        def preprocess_reasoning(inputs: Dict[str, Any]) -> Dict[str, Any]:
            query = inputs["query"]
            
            # Extract complexity indicators
            complexity_indicators = ["analyze", "compare", "evaluate", "synthesize", "predict"]
            complexity = "high" if any(indicator in query.lower() for indicator in complexity_indicators) else "medium"
            
            return {
                **inputs,
                "complexity": complexity,
                "enhanced_query": f"[Complexity: {complexity}] {query}"
            }
        
        reasoning_chain = (
            RunnableLambda(preprocess_reasoning) |
            reasoning_prompt |
            self.llm
        )
        
        return {
            "chat": chat_chain,
            "analysis": analysis_chain,
            "reasoning": reasoning_chain
        }
    
    def _setup_routes(self):
        """Setup API routes using LangServe"""
        
        # Add chain routes
        for name, chain in self.chains.items():
            add_routes(
                self.app,
                chain,
                path=f"/{name}",
                enable_feedback_endpoint=True,
                enable_public_trace_link_endpoint=True,
                playground_type="chat" if name == "chat" else "default"
            )
        
        # Custom endpoints
        @self.app.post("/query", response_model=QueryResponse)
        async def query_endpoint(request: QueryRequest, background_tasks: BackgroundTasks):
            """Custom query endpoint with enhanced features"""
            
            try:
                # Prepare input
                chain_input = {
                    "query": request.query,
                    **request.context
                }
                
                # Select appropriate chain based on query characteristics
                chain_name = self._select_chain(request.query)
                chain = self.chains[chain_name]
                
                # Execute with tracing
                tracer = LangChainTracer(
                    project_name="langserve-production",
                    client=self.langsmith_client
                )
                
                with tracer:
                    result = await chain.ainvoke(chain_input)
                
                # Format response
                response = QueryResponse(
                    response=result.content if hasattr(result, 'content') else str(result),
                    metadata={
                        "chain_used": chain_name,
                        "model": "gpt-4-turbo-preview",
                        "processing_time": "calculated_automatically"
                    },
                    session_id=request.session_id,
                    timestamp=datetime.now().isoformat()
                )
                
                # Log metrics asynchronously
                background_tasks.add_task(
                    self._log_query_metrics,
                    request.query,
                    chain_name,
                    len(response.response)
                )
                
                return response
                
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.post("/analyze")
        async def analyze_endpoint(request: AnalysisRequest):
            """Dedicated analysis endpoint"""
            
            try:
                chain_input = {
                    "text": request.text,
                    "analysis_type": request.analysis_type
                }
                
                result = await self.chains["analysis"].ainvoke(chain_input)
                
                return {
                    "analysis_result": result.content,
                    "analysis_type": request.analysis_type,
                    "timestamp": datetime.now().isoformat()
                }
                
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
        
        # Health check endpoint
        @self.app.get("/health")
        async def health_check():
            """Health check endpoint"""
            return {
                "status": "healthy",
                "timestamp": datetime.now().isoformat(),
                "chains_loaded": list(self.chains.keys())
            }
    
    def _select_chain(self, query: str) -> str:
        """Intelligent chain selection based on query characteristics"""
        
        query_lower = query.lower()
        
        # Analysis indicators
        analysis_keywords = ["analyze", "analysis", "examine", "evaluate", "assess", "review"]
        if any(keyword in query_lower for keyword in analysis_keywords):
            return "analysis"
        
        # Reasoning indicators
        reasoning_keywords = ["why", "how", "explain", "reason", "logic", "step", "process"]
        if any(keyword in query_lower for keyword in reasoning_keywords):
            return "reasoning"
        
        # Default to chat
        return "chat"
    
    def _setup_monitoring(self):
        """Setup monitoring and metrics collection"""
        
        @self.app.middleware("http")
        async def add_monitoring(request, call_next):
            start_time = datetime.now()
            
            response = await call_next(request)
            
            process_time = (datetime.now() - start_time).total_seconds()
            
            # Log request metrics
            metrics = {
                "path": request.url.path,
                "method": request.method,
                "status_code": response.status_code,
                "process_time": process_time,
                "timestamp": start_time.isoformat()
            }
            
            # Send to monitoring system (async)
            asyncio.create_task(self._send_metrics(metrics))
            
            return response
    
    async def _log_query_metrics(self, query: str, chain_name: str, response_length: int):
        """Log query-specific metrics"""
        metrics = {
            "query_length": len(query),
            "chain_used": chain_name,
            "response_length": response_length,
            "timestamp": datetime.now().isoformat()
        }
        
        # Send to LangSmith or other monitoring service
        await self._send_metrics(metrics)
    
    async def _send_metrics(self, metrics: Dict[str, Any]):
        """Send metrics to monitoring system"""
        try:
            # In production, send to actual monitoring service
            print(f"Metrics: {json.dumps(metrics, indent=2)}")
        except Exception as e:
            print(f"Failed to send metrics: {e}")

# LangGraph orchestration
from langgraph.graph import Graph, StateGraph, END
from langgraph.prebuilt import ToolExecutor, ToolInvocation
from typing_extensions import TypedDict

class AgentState(TypedDict):
    messages: List[BaseMessage]
    context: Dict[str, Any]
    current_step: str
    iteration_count: int
    max_iterations: int

class LangGraphOrchestrator:
    """Advanced workflow orchestration using LangGraph"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model="gpt-4-turbo-preview",
            temperature=0.3,
            api_key=os.getenv('OPENAI_API_KEY')
        )
        
        self.tools = self._setup_tools()
        self.tool_executor = ToolExecutor(self.tools)
        self.workflow = self._create_workflow()
    
    def _setup_tools(self):
        """Setup tools for the graph"""
        from langchain.tools import tool
        
        @tool
        def search_knowledge_base(query: str) -> str:
            """Search the knowledge base for relevant information"""
            # Simulate knowledge base search
            return f"Knowledge base results for: {query}"
        
        @tool
        def analyze_sentiment(text: str) -> str:
            """Analyze sentiment of the given text"""
            # Simulate sentiment analysis
            return f"Sentiment analysis of '{text}': Positive (0.8 confidence)"
        
        @tool
        def generate_summary(text: str) -> str:
            """Generate a summary of the given text"""
            # Simulate text summarization
            return f"Summary: {text[:100]}..."
        
        return [search_knowledge_base, analyze_sentiment, generate_summary]
    
    def _create_workflow(self) -> StateGraph:
        """Create complex workflow with conditional logic"""
        
        def route_query(state: AgentState) -> str:
            """Route based on query type"""
            last_message = state["messages"][-1].content
            
            if "search" in last_message.lower():
                return "search_step"
            elif "sentiment" in last_message.lower():
                return "sentiment_step"
            elif "summary" in last_message.lower():
                return "summary_step"
            else:
                return "general_step"
        
        def search_step(state: AgentState) -> AgentState:
            """Execute search operation"""
            query = state["messages"][-1].content
            
            # Use search tool
            tool_invocation = ToolInvocation(
                tool="search_knowledge_base",
                tool_input={"query": query}
            )
            
            result = self.tool_executor.invoke(tool_invocation)
            
            state["context"]["search_result"] = result
            state["current_step"] = "search_completed"
            state["iteration_count"] += 1
            
            return state
        
        def sentiment_step(state: AgentState) -> AgentState:
            """Execute sentiment analysis"""
            text = state["messages"][-1].content
            
            tool_invocation = ToolInvocation(
                tool="analyze_sentiment",
                tool_input={"text": text}
            )
            
            result = self.tool_executor.invoke(tool_invocation)
            
            state["context"]["sentiment_result"] = result
            state["current_step"] = "sentiment_completed"
            state["iteration_count"] += 1
            
            return state
        
        def summary_step(state: AgentState) -> AgentState:
            """Execute summarization"""
            text = state["messages"][-1].content
            
            tool_invocation = ToolInvocation(
                tool="generate_summary",
                tool_input={"text": text}
            )
            
            result = self.tool_executor.invoke(tool_invocation)
            
            state["context"]["summary_result"] = result
            state["current_step"] = "summary_completed"
            state["iteration_count"] += 1
            
            return state
        
        def general_step(state: AgentState) -> AgentState:
            """Handle general queries"""
            query = state["messages"][-1].content
            
            # Use LLM for general response
            response = self.llm.invoke(f"Respond to: {query}")
            
            state["context"]["general_response"] = response.content
            state["current_step"] = "general_completed"
            state["iteration_count"] += 1
            
            return state
        
        def should_continue(state: AgentState) -> str:
            """Determine if workflow should continue"""
            if state["iteration_count"] >= state["max_iterations"]:
                return END
            
            current_step = state["current_step"]
            if current_step.endswith("_completed"):
                return "finalize"
            
            return "continue"
        
        def finalize_response(state: AgentState) -> AgentState:
            """Finalize and format response"""
            context = state["context"]
            
            # Combine all results
            final_response = "Combined Results:\n"
            
            if "search_result" in context:
                final_response += f"Search: {context['search_result']}\n"
            if "sentiment_result" in context:
                final_response += f"Sentiment: {context['sentiment_result']}\n"
            if "summary_result" in context:
                final_response += f"Summary: {context['summary_result']}\n"
            if "general_response" in context:
                final_response += f"Response: {context['general_response']}\n"
            
            state["context"]["final_response"] = final_response
            state["current_step"] = "finalized"
            
            return state
        
        # Build the graph
        workflow = StateGraph(AgentState)
        
        # Add nodes
        workflow.add_node("route", route_query)
        workflow.add_node("search_step", search_step)
        workflow.add_node("sentiment_step", sentiment_step)
        workflow.add_node("summary_step", summary_step)
        workflow.add_node("general_step", general_step)
        workflow.add_node("finalize", finalize_response)
        
        # Set entry point
        workflow.set_entry_point("route")
        
        # Add conditional edges
        workflow.add_conditional_edges(
            "route",
            route_query,
            {
                "search_step": "search_step",
                "sentiment_step": "sentiment_step",
                "summary_step": "summary_step",
                "general_step": "general_step"
            }
        )
        
        # Add edges to finalize
        for step in ["search_step", "sentiment_step", "summary_step", "general_step"]:
            workflow.add_edge(step, "finalize")
        
        workflow.add_edge("finalize", END)
        
        return workflow.compile()
    
    async def execute_workflow(self, query: str) -> Dict[str, Any]:
        """Execute the complete workflow"""
        
        from langchain.schema import HumanMessage
        
        initial_state = AgentState(
            messages=[HumanMessage(content=query)],
            context={},
            current_step="initial",
            iteration_count=0,
            max_iterations=5
        )
        
        result = await self.workflow.ainvoke(initial_state)
        
        return {
            "query": query,
            "final_response": result["context"].get("final_response", "No response generated"),
            "steps_executed": result["iteration_count"],
            "final_state": result["current_step"],
            "context": result["context"]
        }

# Production deployment function
def create_production_app() -> FastAPI:
    """Create production-ready FastAPI application"""
    
    langserve_app = LangServeApplication()
    
    # Add LangGraph orchestrator
    orchestrator = LangGraphOrchestrator()
    
    @langserve_app.app.post("/orchestrate")
    async def orchestrate_endpoint(request: QueryRequest):
        """Endpoint for complex workflow orchestration"""
        try:
            result = await orchestrator.execute_workflow(request.query)
            return result
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    return langserve_app.app

# Run the application
if __name__ == "__main__":
    app = create_production_app()
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        reload=True
    )
````

## Conclusion

LangChain and its ecosystem provide a comprehensive framework for building sophisticated AI applications with production-ready capabilities. The framework's modular architecture enables developers to create complex systems through composable components while maintaining flexibility and scalability.

**LangChain's core strengths** include its extensive prompt management system, which supports dynamic templates, few-shot learning, and conditional logic. The chain abstraction allows for building complex processing pipelines that can combine multiple LLMs, tools, and data sources in sophisticated workflows.

**RAG implementation** through LangChain offers powerful capabilities for knowledge-augmented generation. The framework supports multiple retrieval strategies, from basic similarity search to advanced techniques like Maximum Marginal Relevance and contextual compression. The ability to ensemble multiple retrievers and implement custom evaluation metrics makes it suitable for production knowledge systems.

**LangServe** provides production deployment capabilities with automatic API generation, monitoring integration, and scalable architecture. The FastAPI-based deployment includes built-in support for async operations, automatic documentation, and monitoring hooks that integrate seamlessly with LangSmith.

**LangGraph** represents the cutting edge of workflow orchestration, enabling the creation of stateful, multi-step applications with complex decision logic. Its support for cycles, conditions, and human-in-the-loop interactions makes it ideal for building sophisticated AI agents that can handle complex, multi-step reasoning tasks.

**LangSmith integration** provides comprehensive monitoring, debugging, and evaluation capabilities essential for production AI systems. The platform's tracing capabilities enable developers to understand system behavior, optimize performance, and ensure reliability at scale.

The ecosystem's main advantages include rapid prototyping capabilities, extensive integration support, and production-ready monitoring. However, considerations include potential vendor lock-in, complexity for simple use cases, and the need for careful performance optimization in high-throughput scenarios. For organizations building complex AI applications that require sophisticated reasoning, tool integration, and production monitoring, LangChain provides a mature and comprehensive solution.