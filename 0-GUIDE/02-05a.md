<small>Claude 3.7 Sonnet Thinking</small>
# 05. HuggingFace Introduction

## Key Terms

- **Hugging Face**: An AI community and platform that provides tools, libraries, and infrastructure for building, training, and deploying machine learning models.
- **Transformers**: A library that provides pre-trained models for natural language processing (NLP) and computer vision tasks.
- **Datasets**: A library for accessing and sharing machine learning datasets with efficient data processing capabilities.
- **Tokenizers**: Tools that convert raw text into numerical tokens that models can process.
- **Model Hub**: A central repository hosting thousands of pre-trained models shared by the community.
- **Inference API**: A hosted service that allows running inference on models without deploying them yourself.
- **Spaces**: Hosted applications for demonstrating machine learning models with custom UIs.
- **AutoClasses**: Abstract classes that automatically select the appropriate model architecture based on the checkpoint name.

## Working with Hugging Face Libraries

The Hugging Face ecosystem provides a comprehensive set of tools for working with state-of-the-art machine learning models. Let's explore the core libraries in detail:

```python
import os
from dotenv import load_dotenv
from typing import Dict, List, Union, Optional, Any, Tuple
import pandas as pd
import numpy as np
import torch
from datasets import load_dataset, Dataset, DatasetDict
from transformers import (
    AutoTokenizer, 
    AutoModel, 
    AutoModelForSequenceClassification,
    AutoModelForTokenClassification,
    AutoModelForQuestionAnswering,
    AutoModelForMaskedLM,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    pipeline
)
import logging
from tqdm.auto import tqdm

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

class HuggingFaceToolkit:
    """Comprehensive toolkit for working with Hugging Face libraries."""
    
    def __init__(
        self, 
        model_name_or_path: str = "distilbert-base-uncased",
        use_auth_token: bool = False,
        device: Optional[str] = None
    ):
        """
        Initialize the Hugging Face toolkit.
        
        Args:
            model_name_or_path: Model identifier from HF Hub or local path
            use_auth_token: Whether to use the Hugging Face auth token for private models
            device: Device to use (cuda, cpu, mps) - will auto-detect if None
        """
        # Set up authentication if needed
        self.use_auth_token = use_auth_token
        if use_auth_token:
            self.auth_token = os.getenv("HUGGINGFACE_TOKEN")
            if not self.auth_token:
                raise ValueError("HUGGINGFACE_TOKEN environment variable not set")
            
            # Set the token for libraries to use
            os.environ["HUGGINGFACE_TOKEN"] = self.auth_token
        else:
            self.auth_token = None
            
        # Set up device
        if device is None:
            self.device = "cuda" if torch.cuda.is_available() else \
                         "mps" if hasattr(torch, "has_mps") and torch.has_mps else "cpu"
        else:
            self.device = device
        
        logger.info(f"Using device: {self.device}")
        
        # Load tokenizer
        self.model_name = model_name_or_path
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name_or_path,
            token=self.auth_token if self.use_auth_token else None
        )
        
        # Model will be loaded on-demand based on the task
        self.model = None
        self.model_type = None
    
    def load_model(self, task_type: str) -> Any:
        """
        Load a model for a specific task type.
        
        Args:
            task_type: Type of task (e.g., 'sequence-classification', 'token-classification', etc.)
            
        Returns:
            Loaded model
        """
        # Map task types to model classes
        model_classes = {
            'base': AutoModel,
            'sequence-classification': AutoModelForSequenceClassification,
            'token-classification': AutoModelForTokenClassification,
            'question-answering': AutoModelForQuestionAnswering,
            'masked-lm': AutoModelForMaskedLM,
            'causal-lm': AutoModelForCausalLM
        }
        
        if task_type not in model_classes:
            raise ValueError(f"Unsupported task type: {task_type}. "
                            f"Supported types are: {list(model_classes.keys())}")
        
        logger.info(f"Loading model for task: {task_type}")
        
        # Load the model and move to device
        model_class = model_classes[task_type]
        model = model_class.from_pretrained(
            self.model_name,
            token=self.auth_token if self.use_auth_token else None
        )
        model = model.to(self.device)
        
        self.model = model
        self.model_type = task_type
        
        return model
```

### Working with the Datasets Library

The Datasets library makes it easy to access, process, and share machine learning datasets. Here's how to work with it:

```python
    def load_hf_dataset(
        self, 
        dataset_name: str, 
        subset: Optional[str] = None, 
        split: Optional[str] = None
    ) -> Union[Dataset, DatasetDict]:
        """
        Load a dataset from Hugging Face hub.
        
        Args:
            dataset_name: Name of the dataset on Hugging Face Hub
            subset: Optional subset name
            split: Optional split name (e.g., 'train', 'test')
            
        Returns:
            Dataset or DatasetDict object
        """
        logger.info(f"Loading dataset: {dataset_name}" + 
                   (f", subset: {subset}" if subset else "") + 
                   (f", split: {split}" if split else ""))
        
        try:
            dataset = load_dataset(
                dataset_name,
                subset,
                split=split,
                token=self.auth_token if self.use_auth_token else None
            )
            
            logger.info(f"Dataset loaded successfully with {len(dataset)} examples" 
                       if isinstance(dataset, Dataset) else "Dataset loaded successfully")
            
            return dataset
        
        except Exception as e:
            logger.error(f"Error loading dataset: {str(e)}")
            raise
    
    def create_dataset_from_files(
        self, 
        files: Union[str, List[str]],
        file_type: str = "csv",
        **kwargs
    ) -> Dataset:
        """
        Create a dataset from local files.
        
        Args:
            files: Path or list of paths to files
            file_type: File format (csv, json, text, parquet, etc.)
            **kwargs: Additional arguments for the loading function
            
        Returns:
            Dataset object
        """
        logger.info(f"Creating dataset from {file_type} files")
        
        try:
            dataset = load_dataset(
                file_type, 
                data_files=files,
                **kwargs
            )
            
            logger.info(f"Dataset created successfully")
            return dataset
            
        except Exception as e:
            logger.error(f"Error creating dataset: {str(e)}")
            raise
    
    def preprocess_dataset(
        self,
        dataset: Union[Dataset, DatasetDict],
        text_column: str,
        label_column: Optional[str] = None,
        max_length: int = 128,
        truncation: bool = True,
        padding: str = "max_length",
        return_tensors: Optional[str] = None
    ) -> Union[Dataset, DatasetDict]:
        """
        Preprocess a dataset for transformer models.
        
        Args:
            dataset: Dataset to preprocess
            text_column: Column name containing the text
            label_column: Column name containing labels (if any)
            max_length: Maximum sequence length
            truncation: Whether to truncate sequences
            padding: Padding strategy
            return_tensors: Return format for tensors (None, 'pt', 'tf', 'np')
            
        Returns:
            Preprocessed dataset
        """
        logger.info(f"Preprocessing dataset")
        
        def tokenize_function(examples):
            texts = examples[text_column]
            
            # Handle single strings vs lists of strings
            if isinstance(texts, str):
                texts = [texts]
                
            tokenized = self.tokenizer(
                texts,
                max_length=max_length,
                truncation=truncation,
                padding=padding,
                return_tensors=return_tensors
            )
            
            # If return_tensors is specified, we need to handle the result differently
            if return_tensors:
                result = {k: v for k, v in tokenized.items()}
            else:
                result = tokenized
            
            # Add labels if specified
            if label_column is not None and label_column in examples:
                result["labels"] = examples[label_column]
                
            return result
        
        # Apply preprocessing
        if isinstance(dataset, DatasetDict):
            # Process each split
            processed_dataset = DatasetDict({
                k: v.map(
                    tokenize_function,
                    batched=True,
                    desc=f"Tokenizing {k} split"
                ) for k, v in dataset.items()
            })
        else:
            # Process single dataset
            processed_dataset = dataset.map(
                tokenize_function,
                batched=True,
                desc="Tokenizing dataset"
            )
        
        return processed_dataset
```

### Working with Transformers and Tokenization

The Transformers library provides easy access to state-of-the-art models. Here's how to use it for various NLP tasks:

```python
    def get_pipeline(
        self, 
        task: str, 
        model: Optional[str] = None,
        tokenizer: Optional[str] = None
    ) -> pipeline:
        """
        Create a pipeline for a specific task.
        
        Args:
            task: Task name (e.g., 'sentiment-analysis', 'ner', 'text-generation')
            model: Optional model name or path (uses self.model_name if None)
            tokenizer: Optional tokenizer name or path (uses self.tokenizer if None)
            
        Returns:
            Pipeline object
        """
        model_name = model or self.model_name
        
        logger.info(f"Creating pipeline for task: {task} with model: {model_name}")
        
        return pipeline(
            task,
            model=model_name,
            tokenizer=tokenizer or self.tokenizer,
            device=0 if self.device == "cuda" else -1,  # device mapping for pipeline
            token=self.auth_token if self.use_auth_token else None
        )
    
    def extract_embeddings(
        self,
        texts: Union[str, List[str]],
        pooling_strategy: str = "mean",
        batch_size: int = 8
    ) -> np.ndarray:
        """
        Extract embeddings from text using the model.
        
        Args:
            texts: Text or list of texts to encode
            pooling_strategy: Strategy for pooling embeddings ('mean', 'cls', 'max')
            batch_size: Batch size for processing
            
        Returns:
            Numpy array of embeddings
        """
        # Ensure model is loaded
        if self.model is None:
            self.load_model('base')
        
        # Convert single text to list
        if isinstance(texts, str):
            texts = [texts]
            
        # Process in batches
        all_embeddings = []
        
        for i in tqdm(range(0, len(texts), batch_size), desc="Extracting embeddings"):
            batch_texts = texts[i:i + batch_size]
            
            # Tokenize
            inputs = self.tokenizer(
                batch_texts,
                max_length=512,
                padding=True,
                truncation=True,
                return_tensors="pt"
            ).to(self.device)
            
            # Extract embeddings
            with torch.no_grad():
                outputs = self.model(**inputs)
                
            # Apply pooling strategy
            if pooling_strategy == "cls":
                # Use [CLS] token embedding
                embeddings = outputs.last_hidden_state[:, 0].cpu().numpy()
            elif pooling_strategy == "mean":
                # Mean pooling - take attention mask into account for correct mean
                attention_mask = inputs.attention_mask
                embeddings = torch.sum(outputs.last_hidden_state * attention_mask.unsqueeze(-1), 1)
                embeddings = embeddings / torch.clamp(attention_mask.sum(-1, keepdim=True), min=1e-9)
                embeddings = embeddings.cpu().numpy()
            elif pooling_strategy == "max":
                # Max pooling
                embeddings = torch.max(outputs.last_hidden_state, dim=1).values.cpu().numpy()
            else:
                raise ValueError(f"Unsupported pooling strategy: {pooling_strategy}")
                
            all_embeddings.append(embeddings)
            
        # Concatenate all embeddings
        return np.vstack(all_embeddings)
```

## Working with Pre-trained Models

Hugging Face provides thousands of pre-trained models that can be easily accessed and used for a wide variety of tasks:

```python
    def run_text_classification(
        self,
        texts: Union[str, List[str]],
        labels: Optional[List[str]] = None
    ) -> List[Dict[str, Union[str, float]]]:
        """
        Run text classification on input texts.
        
        Args:
            texts: Text or list of texts to classify
            labels: Optional list of labels to restrict outputs
            
        Returns:
            List of classification results
        """
        # Initialize sequence classification pipeline
        classifier = self.get_pipeline('text-classification')
        
        # Run classification
        result = classifier(texts, top_k=None if not labels else len(labels))
        
        return result
    
    def run_token_classification(
        self,
        texts: Union[str, List[str]],
        aggregation_strategy: str = "simple"
    ) -> List[Dict[str, Any]]:
        """
        Run token classification (NER) on input texts.
        
        Args:
            texts: Text or list of texts for token classification
            aggregation_strategy: Strategy to aggregate tokens ('simple', 'first', 'average', 'max')
            
        Returns:
            List of token classification results
        """
        # Initialize token classification pipeline
        token_classifier = self.get_pipeline(
            'token-classification', 
            model=self.model_name
        )
        
        # Run token classification
        result = token_classifier(
            texts, 
            aggregation_strategy=aggregation_strategy
        )
        
        return result
    
    def run_text_generation(
        self,
        prompts: Union[str, List[str]],
        max_new_tokens: int = 50,
        temperature: float = 0.7,
        top_p: float = 0.9,
        top_k: int = 50,
        repetition_penalty: float = 1.1,
        do_sample: bool = True
    ) -> List[Dict[str, str]]:
        """
        Run text generation on input prompts.
        
        Args:
            prompts: Prompt or list of prompts for text generation
            max_new_tokens: Maximum number of new tokens to generate
            temperature: Temperature for sampling
            top_p: Top-p sampling probability
            top_k: Top-k sampling
            repetition_penalty: Penalty for repetition
            do_sample: Whether to use sampling (vs greedy decoding)
            
        Returns:
            List of generated text results
        """
        # Initialize text generation pipeline
        generator = self.get_pipeline('text-generation')
        
        # Run generation
        result = generator(
            prompts,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            repetition_penalty=repetition_penalty,
            do_sample=do_sample,
            return_full_text=False  # Only return the newly generated text
        )
        
        return result
    
    def run_question_answering(
        self,
        questions: Union[str, List[str]],
        contexts: Union[str, List[str]]
    ) -> List[Dict[str, Any]]:
        """
        Run question answering on input questions and contexts.
        
        Args:
            questions: Question or list of questions
            contexts: Context or list of contexts containing answers
            
        Returns:
            List of question answering results
        """
        # Initialize question answering pipeline
        qa_pipeline = self.get_pipeline('question-answering')
        
        # Prepare inputs - ensure lists of same length
        if isinstance(questions, str):
            questions = [questions]
            
        if isinstance(contexts, str):
            contexts = [contexts] * len(questions)
        
        assert len(questions) == len(contexts), "Questions and contexts must have the same length"
        
        # Process each question-context pair
        results = []
        for question, context in zip(questions, contexts):
            result = qa_pipeline(question=question, context=context)
            results.append(result)
            
        return results
```

## Model Deployment with Hugging Face Endpoints

Hugging Face provides simple ways to deploy models to production:

```python
import requests
import json
import time
from typing import Dict, Any, Optional, Union

class HuggingFaceEndpointManager:
    """Manager for deploying and interacting with Hugging Face Inference Endpoints."""
    
    def __init__(self, api_token: Optional[str] = None):
        """
        Initialize the endpoint manager.
        
        Args:
            api_token: Hugging Face API token (defaults to HUGGINGFACE_TOKEN env var)
        """
        self.api_token = api_token or os.getenv("HUGGINGFACE_TOKEN")
        if not self.api_token:
            raise ValueError("API token is required. Set it via the 'api_token' parameter or HUGGINGFACE_TOKEN env var.")
            
        self.api_url = "https://api-inference.huggingface.co/models"
        self.headers = {"Authorization": f"Bearer {self.api_token}"}
        
    def query_model(
        self, 
        model_id: str, 
        inputs: Union[str, Dict[str, Any], List],
        parameters: Optional[Dict[str, Any]] = None,
        options: Optional[Dict[str, Any]] = None,
        wait_for_model: bool = True,
        timeout: int = 30
    ) -> Dict[str, Any]:
        """
        Query a model on the Hugging Face Inference API.
        
        Args:
            model_id: Hugging Face model ID
            inputs: Input data for the model
            parameters: Model parameters
            options: Additional options for the API
            wait_for_model: Whether to wait for the model to load
            timeout: Timeout in seconds for model loading
            
        Returns:
            API response
        """
        url = f"{self.api_url}/{model_id}"
        
        payload = {"inputs": inputs}
        if parameters:
            payload["parameters"] = parameters
        if options:
            payload["options"] = options
            
        start_time = time.time()
        
        while True:
            response = requests.post(url, headers=self.headers, json=payload)
            
            # Check if model is still loading
            if response.status_code == 503 and wait_for_model:
                # Model is loading
                elapsed_time = time.time() - start_time
                
                if elapsed_time > timeout:
                    raise TimeoutError(f"Model loading timed out after {timeout} seconds")
                    
                # Parse estimated time if available
                try:
                    error_info = response.json()
                    estimated_time = error_info.get("estimated_time", 10)
                    logger.info(f"Model is loading. Waiting for {estimated_time} seconds...")
                    
                    # Wait for the estimated time or a minimum of 2 seconds
                    time.sleep(max(estimated_time, 2))
                except:
                    # If parsing fails, wait 10 seconds
                    logger.info("Model is loading. Waiting for 10 seconds...")
                    time.sleep(10)
                    
                continue  # Retry the request
                
            # Handle other errors
            if response.status_code != 200:
                logger.error(f"API request failed with status {response.status_code}")
                logger.error(f"Response: {response.text}")
                response.raise_for_status()
                
            # Successful response
            return response.json()
    
    def deploy_endpoint(
        self,
        name: str,
        model_id: str,
        instance_type: str = "cpu-basic",
        instance_size: str = "small",
        min_replicas: int = 1,
        max_replicas: int = 1,
        provider: str = "aws",
        region: str = "us-east-1",
        accelerator: Optional[str] = None,
        base_framework: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Deploy a model to a Hugging Face Inference Endpoint.
        
        Args:
            name: Endpoint name
            model_id: Model ID to deploy
            instance_type: Instance type (cpu-basic, gpu-basic, etc.)
            instance_size: Instance size (small, medium, large)
            min_replicas: Minimum number of replicas
            max_replicas: Maximum number of replicas
            provider: Cloud provider (aws, gcp, azure)
            region: Cloud region
            accelerator: Accelerator type (if needed)
            base_framework: Base framework override
            
        Returns:
            Deployment response
        """
        url = "https://api.huggingface.co/endpoints"
        
        # Construct payload
        payload = {
            "name": name,
            "repository": model_id,
            "framework": "custom" if base_framework else "pytorch",
            "task": "text-generation",
            "instance_type": instance_type,
            "instance_size": instance_size,
            "min_replicas": min_replicas,
            "max_replicas": max_replicas,
            "provider": {
                "id": provider,
                "region": region
            }
        }
        
        if accelerator:
            payload["accelerator"] = accelerator
            
        if base_framework:
            payload["base_framework"] = base_framework
        
        response = requests.post(url, headers=self.headers, json=payload)
        
        if response.status_code != 200:
            logger.error(f"Endpoint deployment failed with status {response.status_code}")
            logger.error(f"Response: {response.text}")
            response.raise_for_status()
            
        return response.json()
    
    def get_endpoint_status(self, endpoint_id: str) -> Dict[str, Any]:
        """
        Get the status of an endpoint.
        
        Args:
            endpoint_id: Endpoint ID
            
        Returns:
            Endpoint status
        """
        url = f"https://api.huggingface.co/endpoints/{endpoint_id}"
        
        response = requests.get(url, headers=self.headers)
        
        if response.status_code != 200:
            logger.error(f"Failed to get endpoint status with code {response.status_code}")
            logger.error(f"Response: {response.text}")
            response.raise_for_status()
            
        return response.json()
    
    def query_endpoint(
        self,
        endpoint_url: str,
        inputs: Union[str, Dict[str, Any], List],
        parameters: Optional[Dict[str, Any]] = None,
        options: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Query a deployed endpoint.
        
        Args:
            endpoint_url: URL of the endpoint
            inputs: Input data for the model
            parameters: Model parameters
            options: Additional options for the API
            
        Returns:
            API response
        """
        payload = {"inputs": inputs}
        if parameters:
            payload["parameters"] = parameters
        if options:
            payload["options"] = options
            
        response = requests.post(endpoint_url, headers=self.headers, json=payload)
        
        if response.status_code != 200:
            logger.error(f"Endpoint query failed with status {response.status_code}")
            logger.error(f"Response: {response.text}")
            response.raise_for_status()
            
        return response.json()
```

## Practical Example: End-to-End Workflow

Here's a complete example that demonstrates an end-to-end workflow from loading a dataset to using a model and deploying it:

```python
def huggingface_workflow_example():
    """Demonstrate complete HuggingFace workflow"""
    
    # Initialize toolkit with a sentiment analysis model
    toolkit = HuggingFaceToolkit(model_name_or_path="distilbert-base-uncased-finetuned-sst-2-english")
    
    # 1. Load and prepare a dataset
    dataset = toolkit.load_hf_dataset("imdb", split="test[:1000]")
    print(f"Loaded dataset with {len(dataset)} examples")
    print(f"Sample: {dataset[0]}")
    
    # 2. Preprocess the dataset
    processed_dataset = toolkit.preprocess_dataset(
        dataset=dataset,
        text_column="text",
        max_length=512
    )
    print(f"Processed dataset with features: {processed_dataset.column_names}")
    
    # 3. Use a pipeline for sentiment analysis
    sentiment_pipeline = toolkit.get_pipeline("sentiment-analysis")
    
    # Sample a few examples
    samples = [dataset[i]["text"] for i in range(5)]
    results = sentiment_pipeline(samples)
    
    print("\nSentiment Analysis Results:")
    for sample, result in zip(samples, results):
        print(f"Text: {sample[:100]}...")
        print(f"Sentiment: {result['label']}, Score: {result['score']:.4f}\n")
    
    # 4. Extract embeddings
    embeddings = toolkit.extract_embeddings(
        texts=samples,
        pooling_strategy="mean"
    )
    
    print(f"\nGenerated embeddings with shape: {embeddings.shape}")
    
    # 5. Demonstrate text generation
    print("\nText Generation Example:")
    generator_toolkit = HuggingFaceToolkit(model_name_or_path="gpt2")
    generator = generator_toolkit.get_pipeline("text-generation")
    
    generation_prompts = [
        "The future of artificial intelligence is",
        "Climate change will impact society by"
    ]
    
    generation_results = generator(
        generation_prompts,
        max_new_tokens=50,
        temperature=0.7,
        num_return_sequences=1
    )
    
    for prompt, results in zip(generation_prompts, generation_results):
        print(f"Prompt: {prompt}")
        print(f"Generated: {results[0]['generated_text']}\n")
    
    # 6. Work with HF Endpoints (simulated, doesn't make actual API calls)
    print("\nHugging Face Endpoint Demonstration (simulated):")
    print("To actually deploy a model, you would use the HuggingFaceEndpointManager class")
    print("Example endpoint deployment code:")
    print("""
    endpoint_manager = HuggingFaceEndpointManager()
    deployment = endpoint_manager.deploy_endpoint(
        name="my-sentiment-model",
        model_id="distilbert-base-uncased-finetuned-sst-2-english",
        instance_type="cpu-basic",
        instance_size="small"
    )
    print(f"Endpoint deployed with ID: {deployment['id']}")
    """)
    
    return {
        "dataset_size": len(dataset),
        "embedding_dimension": embeddings.shape[1],
        "sentiment_results": results
    }

if __name__ == "__main__":
    result = huggingface_workflow_example()
    print(f"\nWorkflow completed with embedding dimension: {result['embedding_dimension']}")
```

## Conclusion

Hugging Face has revolutionized the machine learning ecosystem by providing a comprehensive platform that makes state-of-the-art models accessible to developers and researchers. The libraries we've explored—Transformers, Datasets, and Tokenizers—form a powerful toolkit for working with machine learning models across various domains and tasks.

The Transformers library provides an intuitive interface for working with pre-trained models, allowing developers to leverage sophisticated architectures without having to implement them from scratch. This democratization of advanced AI capabilities has accelerated innovation and application development across industries.

The Datasets library enables efficient data handling with optimized memory usage and processing pipelines, making it easier to work with large-scale training data. Combined with the extensive model hub containing thousands of pre-trained models and datasets, Hugging Face provides all the components needed for building sophisticated AI applications.

The deployment capabilities through Hugging Face Endpoints further streamline the path to production, allowing developers to serve models without managing complex infrastructure. This end-to-end ecosystem—from model development to deployment—has made Hugging Face an essential platform for modern AI development.

As machine learning continues to evolve, Hugging Face's community-driven approach ensures that the latest research and models are quickly made available to practitioners, maintaining its position as a central hub for AI innovation and collaboration.