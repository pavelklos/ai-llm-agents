<small>Claude 3.7 Sonnet Thinking</small>
# 07. LangChain â€“ AI Application Development

## Key Terms

- **LangChain**: A framework for developing applications powered by language models through composability of components.
- **Chains**: Sequential processing pipelines that combine LLMs, prompts, and other tools to perform complex tasks.
- **Prompt Templates**: Reusable structures for generating dynamic prompts for language models.
- **Retrieval-Augmented Generation (RAG)**: A technique that enhances LLM responses by retrieving relevant information from external knowledge sources.
- **Vector Database**: A specialized database optimized for storing and querying vector embeddings.
- **Embedding Models**: Neural networks that transform text into numerical vector representations that capture semantic meaning.
- **Document Loaders**: Components that import documents from various sources for processing.
- **Text Splitters**: Utilities that divide documents into manageable chunks for embedding and retrieval.
- **Output Parsers**: Components that structure and format LLM outputs into usable formats.
- **Agents**: Systems that use LLMs to determine which actions to take and in what order.

## Chaining Models, Prompts, and Tools

LangChain's primary strength is its ability to compose different components into powerful processing pipelines. Let's explore how to build increasingly sophisticated chains:

```python
import os
from typing import List, Dict, Any, Optional, Union
from dotenv import load_dotenv
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_openai import ChatOpenAI
from langchain.chains import LLMChain, SequentialChain, TransformChain
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
import json

# Load environment variables from .env file
load_dotenv()

class LangChainOrchestrator:
    """
    Comprehensive class for building and managing LangChain applications.
    """
    
    def __init__(
        self, 
        model_name: str = "gpt-3.5-turbo", 
        temperature: float = 0.0,
        verbose: bool = False
    ):
        """
        Initialize the LangChain orchestrator.
        
        Args:
            model_name: Name of the LLM model to use
            temperature: Temperature parameter for generation
            verbose: Whether to print debugging information
        """
        self.verbose = verbose
        
        # Initialize the LLM
        self.llm = ChatOpenAI(
            model=model_name,
            temperature=temperature,
            api_key=os.getenv("OPENAI_API_KEY")
        )
        
        if self.verbose:
            print(f"Initialized with model: {model_name}, temperature: {temperature}")
    
    def create_basic_chain(self, prompt_template: str) -> LLMChain:
        """
        Create a basic LLM chain with a prompt template.
        
        Args:
            prompt_template: Template string for the prompt
            
        Returns:
            A configured LLMChain
        """
        # Create prompt template
        prompt = PromptTemplate.from_template(prompt_template)
        
        # Create and return the chain
        chain = LLMChain(llm=self.llm, prompt=prompt, verbose=self.verbose)
        return chain
    
    def create_sequential_chain(
        self, 
        prompt_templates: List[str], 
        input_variables: List[str],
        output_variables: List[str],
        chain_names: Optional[List[str]] = None
    ) -> SequentialChain:
        """
        Create a sequential chain of multiple LLM chains.
        
        Args:
            prompt_templates: List of prompt template strings
            input_variables: List of input variable names for the overall chain
            output_variables: List of output variable names to expose
            chain_names: Optional list of names for the individual chains
            
        Returns:
            A configured SequentialChain
        """
        chains = []
        
        # Create names for chains if not provided
        if chain_names is None:
            chain_names = [f"chain_{i+1}" for i in range(len(prompt_templates))]
        
        # Create individual chains
        for i, template in enumerate(prompt_templates):
            prompt = PromptTemplate.from_template(template)
            
            # Determine output key
            output_key = output_variables[i] if i < len(output_variables) else chain_names[i]
            
            chain = LLMChain(
                llm=self.llm, 
                prompt=prompt, 
                output_key=output_key,
                verbose=self.verbose
            )
            chains.append(chain)
        
        # Create and return the sequential chain
        sequential_chain = SequentialChain(
            chains=chains,
            input_variables=input_variables,
            output_variables=output_variables,
            verbose=self.verbose
        )
        
        return sequential_chain
    
    def create_router_chain(
        self, 
        condition_func: callable,
        chains: Dict[str, Any]
    ) -> callable:
        """
        Create a router chain that selects between multiple chains based on input.
        
        Args:
            condition_func: Function that determines which chain to use
            chains: Dictionary mapping condition results to chains
            
        Returns:
            A function that routes to the appropriate chain
        """
        def router_chain(inputs: Dict[str, Any]) -> Dict[str, Any]:
            # Determine which chain to use
            result = condition_func(inputs)
            
            # Check if we have a chain for this result
            if result not in chains:
                raise ValueError(f"No chain found for result: {result}")
            
            # Run the selected chain
            selected_chain = chains[result]
            return selected_chain.invoke(inputs)
        
        return router_chain
    
    def create_modern_chain(self) -> Any:
        """
        Create a chain using LangChain's modern (LCEL) interface.
        
        Returns:
            A runnable chain
        """
        # Define a prompt template
        prompt = ChatPromptTemplate.from_template(
            "Summarize the following text in {word_count} words: {text}"
        )
        
        # Create the chain
        chain = (
            prompt 
            | self.llm 
            | StrOutputParser()
        )
        
        return chain
    
    def create_branching_chain(self) -> Any:
        """
        Create a complex chain with branching logic using LCEL.
        
        Returns:
            A runnable chain
        """
        # Define a classifier prompt to determine the type of query
        classifier_prompt = ChatPromptTemplate.from_template(
            "Classify the following query into one of these categories: "
            "Technical, Business, Creative, or Other.\n\n"
            "Query: {query}\n\n"
            "Category:"
        )
        
        # Create classification chain
        classifier_chain = classifier_prompt | self.llm | StrOutputParser()
        
        # Define prompts for each category
        technical_prompt = ChatPromptTemplate.from_template(
            "You are a technical expert. Answer the following technical question with detailed explanations and code examples when appropriate:\n\n{query}"
        )
        
        business_prompt = ChatPromptTemplate.from_template(
            "You are a business consultant. Provide a strategic business analysis for the following query:\n\n{query}"
        )
        
        creative_prompt = ChatPromptTemplate.from_template(
            "You are a creative director. Provide an innovative and imaginative response to the following request:\n\n{query}"
        )
        
        general_prompt = ChatPromptTemplate.from_template(
            "Provide a helpful and informative response to the following query:\n\n{query}"
        )
        
        # Create individual response chains
        technical_chain = technical_prompt | self.llm | StrOutputParser()
        business_chain = business_prompt | self.llm | StrOutputParser()
        creative_chain = creative_prompt | self.llm | StrOutputParser()
        general_chain = general_prompt | self.llm | StrOutputParser()
        
        # Create a router function
        def route_based_on_category(inputs):
            category = inputs["category"].strip().lower()
            
            if "technical" in category:
                return technical_chain
            elif "business" in category:
                return business_chain
            elif "creative" in category:
                return creative_chain
            else:
                return general_chain
        
        # Create the full branching chain
        chain = (
            {
                "query": RunnablePassthrough(),
                "category": classifier_chain
            }
            | RunnableLambda(lambda x: {
                "query": x["query"],
                "response": route_based_on_category(x).invoke(x["query"])
            })
        )
        
        return chain
```

## Working with Retrieval-Augmented Generation (RAG)

Retrieval-Augmented Generation enhances LLM responses by incorporating external knowledge. Here's how to implement a comprehensive RAG system:

```python
from typing import List, Dict, Any, Optional, Tuple
import os
from langchain_community.document_loaders import (
    PyPDFLoader, 
    CSVLoader, 
    TextLoader,
    UnstructuredMarkdownLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma, FAISS
from langchain_core.documents import Document
from langchain.chains import RetrievalQA
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain_core.prompts import PromptTemplate

class RAGSystem:
    """
    Comprehensive RAG (Retrieval-Augmented Generation) system implementation.
    """
    
    def __init__(
        self,
        llm,
        embedding_model: str = "text-embedding-3-small",
        vector_db_type: str = "chroma",
        persist_directory: str = "./vector_db",
        enable_compression: bool = False
    ):
        """
        Initialize the RAG system.
        
        Args:
            llm: Language model for generation
            embedding_model: Model to use for embeddings
            vector_db_type: Type of vector database ("chroma" or "faiss")
            persist_directory: Directory to persist the vector database
            enable_compression: Whether to enable contextual compression
        """
        self.llm = llm
        self.embedding_model = OpenAIEmbeddings(
            model=embedding_model,
            api_key=os.getenv("OPENAI_API_KEY")
        )
        self.vector_db_type = vector_db_type
        self.persist_directory = persist_directory
        self.enable_compression = enable_compression
        
        # Initialize text splitter
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len
        )
        
        # Vector store will be initialized during document loading
        self.vector_store = None
        self.retriever = None
    
    def load_documents(self, file_paths: List[str]) -> List[Document]:
        """
        Load documents from various file formats.
        
        Args:
            file_paths: List of paths to documents
            
        Returns:
            List of loaded documents
        """
        documents = []
        
        for file_path in file_paths:
            try:
                # Determine loader based on file extension
                if file_path.endswith(".pdf"):
                    loader = PyPDFLoader(file_path)
                elif file_path.endswith(".csv"):
                    loader = CSVLoader(file_path)
                elif file_path.endswith(".md"):
                    loader = UnstructuredMarkdownLoader(file_path)
                elif file_path.endswith(".txt"):
                    loader = TextLoader(file_path)
                else:
                    print(f"Unsupported file format for {file_path}, skipping...")
                    continue
                
                # Load the document
                loaded_docs = loader.load()
                documents.extend(loaded_docs)
                print(f"Loaded {len(loaded_docs)} document(s) from {file_path}")
                
            except Exception as e:
                print(f"Error loading {file_path}: {str(e)}")
        
        return documents
    
    def process_documents(
        self, 
        documents: List[Document],
        chunk_size: Optional[int] = None,
        chunk_overlap: Optional[int] = None
    ) -> List[Document]:
        """
        Process and split documents into chunks.
        
        Args:
            documents: List of documents to process
            chunk_size: Optional override for chunk size
            chunk_overlap: Optional override for chunk overlap
            
        Returns:
            List of processed document chunks
        """
        # Update splitter parameters if provided
        if chunk_size is not None or chunk_overlap is not None:
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size or self.text_splitter.chunk_size,
                chunk_overlap=chunk_overlap or self.text_splitter.chunk_overlap,
                length_function=len
            )
        
        # Split documents
        chunked_documents = self.text_splitter.split_documents(documents)
        print(f"Split into {len(chunked_documents)} chunks")
        
        return chunked_documents
    
    def create_vector_store(
        self, 
        documents: List[Document],
        force_recreate: bool = False
    ) -> None:
        """
        Create or update the vector store with document embeddings.
        
        Args:
            documents: List of document chunks to embed
            force_recreate: Whether to recreate the vector store from scratch
        """
        # Check if vector store already exists
        if os.path.exists(self.persist_directory) and not force_recreate:
            print(f"Loading existing vector store from {self.persist_directory}")
            
            if self.vector_db_type.lower() == "chroma":
                self.vector_store = Chroma(
                    persist_directory=self.persist_directory,
                    embedding_function=self.embedding_model
                )
            else:  # FAISS
                self.vector_store = FAISS.load_local(
                    self.persist_directory,
                    self.embedding_model
                )
            
            # Check if we should add new documents
            if documents:
                print(f"Adding {len(documents)} new documents to existing vector store")
                self.vector_store.add_documents(documents)
                
                # Persist if using Chroma
                if self.vector_db_type.lower() == "chroma":
                    self.vector_store.persist()
        else:
            # Create new vector store
            print(f"Creating new vector store with {len(documents)} documents")
            
            if self.vector_db_type.lower() == "chroma":
                self.vector_store = Chroma.from_documents(
                    documents=documents,
                    embedding=self.embedding_model,
                    persist_directory=self.persist_directory
                )
                self.vector_store.persist()
            else:  # FAISS
                self.vector_store = FAISS.from_documents(
                    documents=documents,
                    embedding=self.embedding_model
                )
                # Save the FAISS index
                self.vector_store.save_local(self.persist_directory)
        
        # Create retriever
        base_retriever = self.vector_store.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 4}
        )
        
        # Apply compression if enabled
        if self.enable_compression:
            compressor = LLMChainExtractor.from_llm(self.llm)
            self.retriever = ContextualCompressionRetriever(
                base_compressor=compressor,
                base_retriever=base_retriever
            )
        else:
            self.retriever = base_retriever
    
    def setup_rag_chain(self) -> RetrievalQA:
        """
        Set up the RAG chain for question answering.
        
        Returns:
            A configured RetrievalQA chain
        """
        if not self.retriever:
            raise ValueError("Vector store not initialized. Call create_vector_store first.")
        
        # Create prompt template
        template = """
        You are an AI assistant tasked with answering questions based on the provided context.
        Use the following pieces of retrieved context to answer the question. If you don't know the
        answer based on the context, just say you don't know. Don't try to make up an answer.
        
        Context:
        {context}
        
        Question: {question}
        
        Answer:
        """
        
        qa_prompt = PromptTemplate.from_template(template)
        
        # Create retrieval QA chain
        qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.retriever,
            chain_type_kwargs={"prompt": qa_prompt},
            return_source_documents=True
        )
        
        return qa_chain
    
    def query(self, question: str) -> Dict[str, Any]:
        """
        Query the RAG system.
        
        Args:
            question: Question to ask
            
        Returns:
            Answer and source documents
        """
        # Check if we need to set up the chain
        if not hasattr(self, "qa_chain"):
            self.qa_chain = self.setup_rag_chain()
        
        # Execute query
        result = self.qa_chain.invoke({"query": question})
        
        # Format source references
        sources = []
        for i, doc in enumerate(result["source_documents"]):
            source = {
                "content": doc.page_content[:200] + "...",
                "metadata": doc.metadata
            }
            sources.append(source)
        
        return {
            "answer": result["result"],
            "sources": sources
        }
```

## Advanced RAG Techniques and Customizations

Let's explore more advanced RAG techniques including multi-query retrieval and hybrid search:

```python
from langchain.retrievers import MultiQueryRetriever
from langchain.retrievers.multi_query import TimeWeightedVectorStoreRetriever
from langchain_community.retrievers import BM25Retriever
from langchain_community.vectorstores.utils import filter_complex_metadata
from operator import itemgetter
import numpy as np
import time

class AdvancedRAGSystem(RAGSystem):
    """
    Advanced RAG system with additional retrieval techniques.
    """
    
    def create_multi_query_retriever(self) -> MultiQueryRetriever:
        """
        Create a multi-query retriever that generates multiple search queries.
        
        Returns:
            Configured MultiQueryRetriever
        """
        if not self.vector_store:
            raise ValueError("Vector store not initialized")
        
        # Create multi-query retriever
        retriever = MultiQueryRetriever.from_llm(
            retriever=self.vector_store.as_retriever(search_kwargs={"k": 3}),
            llm=self.llm
        )
        
        return retriever
    
    def create_time_weighted_retriever(self) -> TimeWeightedVectorStoreRetriever:
        """
        Create a time-weighted retriever that considers document recency.
        
        Returns:
            Configured TimeWeightedVectorStoreRetriever
        """
        if not self.vector_store:
            raise ValueError("Vector store not initialized")
        
        # Create time-weighted retriever
        retriever = TimeWeightedVectorStoreRetriever(
            vectorstore=self.vector_store,
            decay_rate=0.01,
            k=4
        )
        
        return retriever
    
    def create_hybrid_retriever(self, documents: List[Document]) -> callable:
        """
        Create a hybrid retriever combining semantic search with keyword search.
        
        Args:
            documents: Documents for BM25 initialization
            
        Returns:
            Hybrid retrieval function
        """
        if not self.vector_store:
            raise ValueError("Vector store not initialized")
        
        # Create BM25 retriever for keyword search
        bm25_retriever = BM25Retriever.from_documents(documents)
        bm25_retriever.k = 4
        
        # Create semantic search retriever
        semantic_retriever = self.vector_store.as_retriever(search_kwargs={"k": 4})
        
        # Create hybrid retrieval function
        def hybrid_retrieval(query: str, top_k: int = 4) -> List[Document]:
            # Get results from both retrievers
            semantic_docs = semantic_retriever.get_relevant_documents(query)
            bm25_docs = bm25_retriever.get_relevant_documents(query)
            
            # Create a dictionary to deduplicate documents
            seen_docs = {}
            hybrid_results = []
            
            # Process semantic docs first (prioritize them)
            for i, doc in enumerate(semantic_docs):
                doc_id = hash(doc.page_content)
                if doc_id not in seen_docs:
                    seen_docs[doc_id] = True
                    # Add a score field to track source and rank
                    doc.metadata["score"] = 1.0 - (0.1 * i)  # Semantic score
                    doc.metadata["source"] = "semantic"
                    hybrid_results.append(doc)
            
            # Process BM25 docs
            for i, doc in enumerate(bm25_docs):
                doc_id = hash(doc.page_content)
                if doc_id not in seen_docs:
                    seen_docs[doc_id] = True
                    # Add a score field to track source and rank
                    doc.metadata["score"] = 0.8 - (0.1 * i)  # BM25 score (slightly lower)
                    doc.metadata["source"] = "bm25"
                    hybrid_results.append(doc)
            
            # Sort by score and return top_k
            hybrid_results.sort(key=lambda x: x.metadata.get("score", 0), reverse=True)
            return hybrid_results[:top_k]
        
        return hybrid_retrieval
    
    def create_reranking_chain(self) -> callable:
        """
        Create a reranking chain to improve retrieval results.
        
        Returns:
            Reranking function
        """
        # Create a prompt for document relevance scoring
        rerank_prompt = ChatPromptTemplate.from_template(
            """You are an expert at evaluating document relevance to queries.
            
            Query: {query}
            
            Document: {document}
            
            On a scale of 0 to 10, how relevant is this document to the query?
            Return only a number between 0 and 10, with no explanation.
            """
        )
        
        # Create scoring chain
        scoring_chain = (
            rerank_prompt
            | self.llm
            | StrOutputParser()
        )
        
        # Create reranking function
        def rerank_documents(query: str, documents: List[Document], top_k: int = 3) -> List[Document]:
            results = []
            
            for doc in documents:
                try:
                    # Score the document
                    score_text = scoring_chain.invoke({
                        "query": query,
                        "document": doc.page_content
                    })
                    
                    # Parse the score
                    try:
                        score = float(score_text.strip())
                    except ValueError:
                        score = 0
                    
                    # Add to results
                    results.append((doc, score))
                    
                except Exception as e:
                    print(f"Error scoring document: {str(e)}")
                    results.append((doc, 0))
            
            # Sort by score and return top documents
            results.sort(key=lambda x: x[1], reverse=True)
            return [doc for doc, _ in results[:top_k]]
        
        return rerank_documents
```

## Deploying AI Applications with LangChain

Deployment is a crucial aspect of LangChain applications. Here's how to set up a production-ready deployment:

```python
from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import uvicorn
from contextlib import asynccontextmanager
import logging
import uuid
from datetime import datetime
import threading
import queue
import asyncio
from typing import List, Dict, Any, Optional

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("langchain-app")

# Global variables for the application
models = {}
rag_systems = {}
request_queue = queue.Queue()
request_results = {}

# Process requests in the background
def process_request_worker():
    while True:
        try:
            request_id, request_type, data = request_queue.get()
            logger.info(f"Processing request {request_id} of type {request_type}")
            
            try:
                if request_type == "query":
                    # Process RAG query
                    rag_system = rag_systems.get(data["system_id"])
                    if not rag_system:
                        result = {"error": "RAG system not found"}
                    else:
                        result = rag_system.query(data["question"])
                
                elif request_type == "chain":
                    # Process chain request
                    chain = models.get(data["chain_id"])
                    if not chain:
                        result = {"error": "Chain not found"}
                    else:
                        result = chain.invoke(data["inputs"])
                
                else:
                    result = {"error": "Unknown request type"}
                
                # Store the result
                request_results[request_id] = {
                    "status": "completed",
                    "result": result,
                    "timestamp": datetime.now().isoformat()
                }
                
            except Exception as e:
                logger.error(f"Error processing request {request_id}: {str(e)}")
                request_results[request_id] = {
                    "status": "error",
                    "error": str(e),
                    "timestamp": datetime.now().isoformat()
                }
            
            finally:
                request_queue.task_done()
                
        except Exception as e:
            logger.error(f"Worker error: {str(e)}")

# Start the worker thread
worker_thread = threading.Thread(target=process_request_worker, daemon=True)
worker_thread.start()

# Startup and shutdown events
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup: load models and resources
    logger.info("Loading models and initializing resources...")
    
    # Initialize an orchestrator
    orchestrator = LangChainOrchestrator(model_name="gpt-3.5-turbo", verbose=False)
    
    # Create some example chains
    models["basic_chain"] = orchestrator.create_basic_chain(
        "Summarize the following text in a professional tone: {text}"
    )
    
    models["advanced_chain"] = orchestrator.create_branching_chain()
    
    # Initialize a RAG system
    rag_system = RAGSystem(
        llm=orchestrator.llm,
        embedding_model="text-embedding-3-small",
        vector_db_type="faiss",
        persist_directory="./deployment_vector_db"
    )
    
    # You would normally initialize this with documents
    # For demo purposes, we're storing the empty system
    rag_systems["default"] = rag_system
    
    logger.info("Server startup complete")
    
    yield
    
    # Shutdown: clean up resources
    logger.info("Shutting down and cleaning up resources...")

# Define API models
class ChainRequest(BaseModel):
    chain_id: str
    inputs: Dict[str, Any]

class QueryRequest(BaseModel):
    system_id: str = "default"
    question: str

class DocumentUploadRequest(BaseModel):
    system_id: str = "default"
    document_urls: List[str]
    force_recreate: bool = False

class RequestResponse(BaseModel):
    request_id: str
    status: str

class ResultResponse(BaseModel):
    result: Dict[str, Any]
    status: str
    timestamp: str

# Create the FastAPI app
app = FastAPI(
    title="LangChain AI Application",
    description="API for interacting with LangChain powered AI application",
    version="1.0.0",
    lifespan=lifespan
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Routes
@app.post("/chain", response_model=RequestResponse)
async def run_chain(request: ChainRequest, background_tasks: BackgroundTasks):
    request_id = str(uuid.uuid4())
    
    # Store initial status
    request_results[request_id] = {
        "status": "processing",
        "timestamp": datetime.now().isoformat()
    }
    
    # Add to processing queue
    request_queue.put((request_id, "chain", request.dict()))
    
    return {"request_id": request_id, "status": "processing"}

@app.post("/query", response_model=RequestResponse)
async def run_query(request: QueryRequest, background_tasks: BackgroundTasks):
    request_id = str(uuid.uuid4())
    
    # Store initial status
    request_results[request_id] = {
        "status": "processing",
        "timestamp": datetime.now().isoformat()
    }
    
    # Add to processing queue
    request_queue.put((request_id, "query", request.dict()))
    
    return {"request_id": request_id, "status": "processing"}

@app.get("/result/{request_id}", response_model=Optional[ResultResponse])
async def get_result(request_id: str):
    result = request_results.get(request_id)
    if not result:
        raise HTTPException(status_code=404, detail="Request not found")
    
    if result["status"] == "processing":
        return {
            "status": "processing",
            "result": {},
            "timestamp": result["timestamp"]
        }
    
    return result

@app.post("/documents/upload", response_model=RequestResponse)
async def upload_documents(request: DocumentUploadRequest, background_tasks: BackgroundTasks):
    # This would normally download and process documents
    # For demo purposes, we're just returning a request ID
    request_id = str(uuid.uuid4())
    
    # Store initial status
    request_results[request_id] = {
        "status": "processing",
        "timestamp": datetime.now().isoformat()
    }
    
    # This would normally be a background task to process documents
    # For demo purposes, we're just setting a success result
    background_tasks.add_task(
        lambda: request_results.update({
            request_id: {
                "status": "completed",
                "result": {"message": "Document processing simulated"},
                "timestamp": datetime.now().isoformat()
            }
        })
    )
    
    return {"request_id": request_id, "status": "processing"}

# Run the application using uvicorn
def start_server(host="0.0.0.0", port=8000):
    """Start the FastAPI server"""
    uvicorn.run("app:app", host=host, port=port, reload=False)

if __name__ == "__main__":
    start_server()
```

## Complete End-to-End Example

Here's a complete end-to-end example of a fully functional RAG system with advanced features:

```python
def create_rag_application():
    """
    Create and demonstrate a complete RAG application.
    
    This function shows a complete workflow for creating a RAG application
    from document loading to deployment.
    """
    from langchain_community.document_loaders import WebBaseLoader
    import tempfile
    import os
    
    # Step 1: Initialize components
    print("Initializing LangChain components...")
    orchestrator = LangChainOrchestrator(model_name="gpt-3.5-turbo", verbose=False)
    
    # Step 2: Set up RAG system
    print("Setting up RAG system...")
    rag_system = RAGSystem(
        llm=orchestrator.llm,
        embedding_model="text-embedding-3-small",
        vector_db_type="faiss",
        persist_directory="./example_vector_db",
        enable_compression=True
    )
    
    # Step 3: Load sample documents from the web
    print("Loading sample documents...")
    urls = [
        "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "https://en.wikipedia.org/wiki/Machine_learning",
        "https://en.wikipedia.org/wiki/Natural_language_processing"
    ]
    
    # Create a temporary directory to store downloaded content
    with tempfile.TemporaryDirectory() as temp_dir:
        document_files = []
        
        # Download and save web content
        for i, url in enumerate(urls):
            try:
                loader = WebBaseLoader(url)
                documents = loader.load()
                
                # Save to temporary file
                file_path = os.path.join(temp_dir, f"document_{i}.txt")
                with open(file_path, "w", encoding="utf-8") as f:
                    for doc in documents:
                        f.write(doc.page_content)
                
                document_files.append(file_path)
                print(f"Downloaded and saved content from {url}")
                
            except Exception as e:
                print(f"Error loading {url}: {str(e)}")
        
        # Step 4: Process documents
        all_documents = rag_system.load_documents(document_files)
        processed_docs = rag_system.process_documents(
            all_documents,
            chunk_size=800,
            chunk_overlap=150
        )
        
        # Step 5: Create vector store
        print("Creating vector store...")
        rag_system.create_vector_store(processed_docs, force_recreate=True)
    
    # Step 6: Set up retrieval chain
    print("Setting up retrieval chain...")
    qa_chain = rag_system.setup_rag_chain()
    
    # Step 7: Create advanced retrieval techniques
    print("Creating advanced retrieval techniques...")
    try:
        multi_query_retriever = rag_system.create_multi_query_retriever()
        hybrid_retriever = rag_system.create_hybrid_retriever(processed_docs)
        reranking_chain = rag_system.create_reranking_chain()
    except Exception as e:
        print(f"Error creating advanced retrievers: {str(e)}")
    
    # Step 8: Demonstrate the RAG system
    print("\nTesting the RAG system with sample questions...")
    
    sample_questions = [
        "What is artificial intelligence and how does it relate to machine learning?",
        "Explain the role of transformers in NLP",
        "What are the ethical concerns surrounding AI development?"
    ]
    
    for question in sample_questions:
        print(f"\nQuestion: {question}")
        
        # Get answer using standard RAG
        result = rag_system.query(question)
        
        print(f"Answer: {result['answer']}")
        print("Sources:")
        for i, source in enumerate(result['sources'][:2]):
            print(f"  {i+1}. {source['content'][:100]}...")
    
    # Step 9: Demonstrate advanced chain capabilities
    print("\nDemonstrating advanced chain capabilities...")
    
    branching_chain = orchestrator.create_branching_chain()
    
    sample_queries = [
        "How do I optimize a neural network with PyTorch?",
        "What's the best strategy for market expansion in tech?",
        "Design a creative campaign for an eco-friendly product"
    ]
    
    for query in sample_queries:
        print(f"\nQuery: {query}")
        response = branching_chain.invoke(query)
        print(f"Response: {response['response'][:200]}...")
    
    # Step 10: Summary and metrics
    print("\nRAG Application Summary:")
    print(f"- Processed {len(processed_docs)} document chunks")
    print(f"- Vector store type: {rag_system.vector_db_type}")
    print(f"- LLM model: {orchestrator.llm.model_name}")
    print(f"- Advanced features: Contextual compression, multi-query retrieval, reranking")
    
    return {
        "rag_system": rag_system,
        "orchestrator": orchestrator,
        "document_count": len(processed_docs),
        "sample_questions": sample_questions
    }

if __name__ == "__main__":
    create_rag_application()
```

## Conclusion

LangChain has emerged as a powerful framework for developing sophisticated AI applications by providing composable, modular components that can be combined to create complex pipelines. Through chaining models, prompts, and tools, developers can build applications that leverage the strengths of language models while mitigating their weaknesses.

Retrieval-Augmented Generation (RAG) represents one of the most significant advancements in LLM applications, allowing models to access external knowledge beyond their training data. By implementing RAG systems, developers can create applications that provide more accurate, up-to-date, and contextually relevant responses. The ability to ground model outputs in specific documents addresses hallucination issues and enables the creation of domain-specific applications with minimal fine-tuning.

The deployment of LangChain applications requires careful consideration of performance, scalability, and monitoring. Modern architectures using asynchronous processing, background workers, and optimized retrieval techniques allow for effective scaling of these applications in production environments. By leveraging the combined power of vector databases, embedding models, and sophisticated retrieval techniques, developers can create robust AI systems that effectively balance response quality, latency, and resource utilization.

As LangChain and similar frameworks continue to evolve, we can expect further advancements in chain orchestration, retrieval techniques, and deployment strategies. The modular nature of these frameworks makes them particularly well-suited for the rapidly evolving field of AI application development, allowing developers to quickly adopt new models and techniques as they become available.