<small>Claude 3.7 Sonnet Thinking</small>
# 01. Introduction to Neural Networks and Generative AI

## Key Terms

- **Neural Network (NN)**: A computational architecture inspired by biological neural systems, consisting of interconnected nodes (neurons) organized in layers that process information.
- **Deep Learning**: A subset of machine learning using multi-layered neural networks to learn representations of data with multiple levels of abstraction.
- **Generative AI**: AI systems capable of creating new content (text, images, audio, video) that wasn't explicitly programmed.
- **Fine-tuning**: The process of taking a pre-trained model and adapting it to a specific task using domain-specific data.
- **LoRA (Low-Rank Adaptation)**: A parameter-efficient fine-tuning method that significantly reduces memory requirements.
- **QLoRA**: A quantized version of LoRA that further reduces memory requirements through weight quantization.
- **Transformer**: A neural network architecture using self-attention mechanisms, which forms the foundation of modern LLMs.

## Neural Network Fundamentals and Architectures

Neural networks consist of layers of interconnected nodes that process input data through a series of transformations. The basic building block is the artificial neuron:

```python
import numpy as np

class Neuron:
    def __init__(self, input_size):
        # Random initialization of weights and bias
        self.weights = np.random.randn(input_size)
        self.bias = np.random.randn()
        
    def forward(self, inputs):
        # Weighted sum of inputs plus bias
        z = np.dot(inputs, self.weights) + self.bias
        # Activation function (sigmoid)
        return 1 / (1 + np.exp(-z))
```

The learning process involves:
1. Forward propagation: Passing inputs through the network to generate predictions
2. Loss calculation: Measuring the error between predictions and actual targets
3. Backward propagation: Computing gradients of the loss with respect to weights
4. Optimization: Updating weights to minimize the loss

## Types of Neural Networks

### Feedforward Neural Networks

The simplest form of neural network where information travels in one direction from input to output.

```python
import torch
import torch.nn as nn

class FeedForwardNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(FeedForwardNN, self).__init__()
        self.layer1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.layer2 = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        x = self.layer1(x)
        x = self.relu(x)
        x = self.layer2(x)
        return x

# Example usage
model = FeedForwardNN(input_size=784, hidden_size=128, output_size=10)
```

### Convolutional Neural Networks (CNNs)

Specialized for processing grid-like data such as images through convolutional operations.

```python
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        # Convolutional layers
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        
        # Pooling and activation
        self.pool = nn.MaxPool2d(2, 2)
        self.relu = nn.ReLU()
        
        # Fully connected layers
        self.fc1 = nn.Linear(128 * 4 * 4, 512)
        self.fc2 = nn.Linear(512, 10)
        self.dropout = nn.Dropout(0.5)
        
    def forward(self, x):
        # Input: [batch_size, 3, 32, 32]
        x = self.pool(self.relu(self.conv1(x)))  # [batch_size, 32, 16, 16]
        x = self.pool(self.relu(self.conv2(x)))  # [batch_size, 64, 8, 8]
        x = self.pool(self.relu(self.conv3(x)))  # [batch_size, 128, 4, 4]
        
        x = x.view(-1, 128 * 4 * 4)  # Flatten
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        
        return x
```

### Recurrent Neural Networks (RNNs)

Networks designed for sequential data with loops allowing information persistence.

```python
class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(RNN, self).__init__()
        self.hidden_size = hidden_size
        
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        # x shape: [batch_size, sequence_length, input_size]
        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)
        
        # out shape: [batch_size, sequence_length, hidden_size]
        out, _ = self.rnn(x, h0)
        
        # We only want the last output
        out = self.fc(out[:, -1, :])
        return out
```

### Transformers

The architecture behind modern LLMs, using self-attention mechanisms to process sequences.

```python
class TransformerBlock(nn.Module):
    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):
        super(TransformerBlock, self).__init__()
        self.attention = nn.MultiheadAttention(embed_dim, num_heads)
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.ff = nn.Sequential(
            nn.Linear(embed_dim, ff_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(ff_dim, embed_dim),
            nn.Dropout(dropout)
        )
        
    def forward(self, x):
        # Self-attention with residual connection
        attention_output, _ = self.attention(x, x, x)
        x = self.norm1(x + attention_output)
        
        # Feed-forward with residual connection
        ff_output = self.ff(x)
        x = self.norm2(x + ff_output)
        
        return x
```

## Generative AI Applications

### Text Generation

Modern text generation uses transformer-based models:

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer

def generate_text(prompt, max_length=100):
    # Load model and tokenizer
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    model = GPT2LMHeadModel.from_pretrained("gpt2")
    
    # Encode the input and generate text
    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    output = model.generate(
        input_ids,
        max_length=max_length,
        num_return_sequences=1,
        no_repeat_ngram_size=2,
        temperature=0.7,
        top_k=50,
        top_p=0.95,
    )
    
    return tokenizer.decode(output[0], skip_special_tokens=True)
```

### Image Generation

Using modern diffusion models for high-quality image generation:

```python
from diffusers import StableDiffusionPipeline
import torch

def generate_image(prompt):
    # Load model
    model_id = "runwayml/stable-diffusion-v1-5"
    pipeline = StableDiffusionPipeline.from_pretrained(
        model_id, 
        torch_dtype=torch.float16
    )
    pipeline = pipeline.to("cuda")
    
    # Generate image
    image = pipeline(prompt).images[0]
    return image
```

### Audio Generation

Generating audio with modern neural techniques:

```python
from transformers import AutoProcessor, MusicgenForConditionalGeneration

def generate_music(prompt, duration_seconds=10):
    # Load model and processor
    processor = AutoProcessor.from_pretrained("facebook/musicgen-small")
    model = MusicgenForConditionalGeneration.from_pretrained("facebook/musicgen-small")
    
    # Process input and generate music
    inputs = processor(
        text=[prompt],
        padding=True,
        return_tensors="pt",
    )
    
    # Generate audio
    audio_values = model.generate(**inputs, max_new_tokens=int(duration_seconds * model.config.audio_encoder.frame_rate))
    sampling_rate = model.config.audio_encoder.frame_rate
    
    return audio_values[0].numpy(), sampling_rate
```

## Fine-tuning and Parameter-Efficient Training

### Standard Fine-tuning

Traditional fine-tuning updates all model parameters:

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments
import datasets

def fine_tune_model(model_name, train_dataset, eval_dataset):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)
    
    # Training arguments
    training_args = TrainingArguments(
        output_dir="./results",
        evaluation_strategy="epoch",
        learning_rate=2e-5,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        num_train_epochs=3,
        weight_decay=0.01,
    )
    
    # Create trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
    )
    
    # Train model
    trainer.train()
    
    return model, tokenizer
```

### LoRA (Low-Rank Adaptation)

LoRA reduces parameter count by using low-rank decomposition of weight updates:

```python
from peft import LoraConfig, get_peft_model, TaskType

def apply_lora(model, lora_r=8, lora_alpha=16):
    # Define LoRA config
    lora_config = LoraConfig(
        r=lora_r,
        lora_alpha=lora_alpha,
        target_modules=["q_proj", "v_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type=TaskType.CAUSAL_LM
    )
    
    # Apply LoRA to model
    peft_model = get_peft_model(model, lora_config)
    
    return peft_model
```

### QLoRA

QLoRA combines quantization with LoRA for even more memory efficiency:

```python
import torch
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model

def setup_qlora_model(model_name):
    # Configure quantization
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )
    
    # Load quantized model
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        device_map="auto"
    )
    
    # Prepare model for k-bit training
    model = prepare_model_for_kbit_training(model)
    
    # Define LoRA configuration
    lora_config = LoraConfig(
        r=16,
        lora_alpha=32,
        lora_dropout=0.05,
        target_modules=[
            "q_proj",
            "k_proj",
            "v_proj",
            "o_proj"
        ],
        bias="none",
        task_type="CAUSAL_LM"
    )
    
    # Apply LoRA
    model = get_peft_model(model, lora_config)
    
    return model
```

### Iterative Training Process

Implementing curriculum learning for progressive model improvement:

```python
def iterative_training(model, datasets_list, learning_rates, epochs_per_stage):
    """
    Implement iterative training process with progressively more complex data
    
    Args:
        model: The model to train
        datasets_list: List of datasets in increasing complexity order
        learning_rates: List of learning rates for each stage
        epochs_per_stage: List of epoch counts for each stage
    """
    assert len(datasets_list) == len(learning_rates) == len(epochs_per_stage)
    
    for stage, (dataset, lr, epochs) in enumerate(zip(datasets_list, learning_rates, epochs_per_stage)):
        print(f"Starting training stage {stage+1}/{len(datasets_list)}")
        
        # Create training arguments for this stage
        training_args = TrainingArguments(
            output_dir=f"./results/stage_{stage+1}",
            learning_rate=lr,
            num_train_epochs=epochs,
            per_device_train_batch_size=8,
            save_steps=5000,
            save_total_limit=2,
            gradient_accumulation_steps=4,
        )
        
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=dataset,
        )
        
        # Train for this stage
        trainer.train()
        
    return model
```

## Conclusion

Neural networks and generative AI represent a rapidly evolving field with diverse architectures tailored to specific data types and tasks. From fundamental feedforward networks to complex transformers, these technologies enable increasingly sophisticated AI capabilities across text, image, audio, and video generation. Modern parameter-efficient techniques like LoRA and QLoRA are revolutionizing how we adapt large pre-trained models to specific tasks with limited computational resources. As these models continue to grow in size and capability, understanding their fundamentals, architectures, and training methodologies becomes essential for AI developers looking to implement state-of-the-art solutions across various domains.