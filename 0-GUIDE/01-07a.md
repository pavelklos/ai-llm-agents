<small>Claude 3.7 Sonnet Thinking</small>
# 07. Semantic Kernel and Autogen

## Key Terms

- **Semantic Kernel**: An open-source framework by Microsoft that integrates LLMs with conventional programming languages through "skills" and "semantic functions."
- **Autogen**: A framework for building LLM applications using customizable and conversable agents that can collaborate, call tools, and interact with humans.
- **Multi-agent System**: A system composed of multiple interacting intelligent agents working together to solve problems.
- **Agent Memory**: The mechanism by which agents store, retrieve, and utilize information from past experiences or interactions.
- **Planning**: The process where an agent determines a sequence of actions to achieve specific goals.
- **Agent Orchestration**: The coordination of multiple agents to perform complex tasks through inter-agent communication.
- **Tool Calling**: The capability of agents to use external tools or functions to extend their capabilities.
- **Autogen Studio**: A visual interface for building and testing multi-agent systems based on the Autogen framework.
- **Skill**: In Semantic Kernel terminology, a collection of related functions that can be used by the framework.

## Planning, Memory, and Interaction Between Agents

### Setting up Semantic Kernel

```python
import semantic_kernel as sk
from semantic_kernel.connectors.ai.open_ai import (
    OpenAIChatCompletion,
    OpenAITextEmbedding
)
import os
from dotenv import load_dotenv
from typing import List, Dict, Any, Optional

# Load environment variables
load_dotenv()

class SemanticKernelManager:
    """Class to manage Semantic Kernel setup and operations"""
    
    def __init__(self):
        # Initialize the kernel
        self.kernel = sk.Kernel()
        
        # Add OpenAI service
        api_key = os.getenv("OPENAI_API_KEY")
        
        # Add LLM service
        self.kernel.add_service(
            OpenAIChatCompletion(
                service_id="chat_completion",
                ai_model_id="gpt-4",
                api_key=api_key
            )
        )
        
        # Add embedding service
        self.kernel.add_service(
            OpenAITextEmbedding(
                service_id="text_embedding",
                ai_model_id="text-embedding-ada-002",
                api_key=api_key
            )
        )
        
    def create_semantic_function(self, prompt_template: str, function_name: str, 
                             skill_name: str = "assistant") -> sk.SKFunctionBase:
        """Create a semantic function from a prompt template"""
        return self.kernel.create_semantic_function(
            prompt_template=prompt_template,
            function_name=function_name,
            skill_name=skill_name,
            service_id="chat_completion"
        )
    
    def register_native_function(self, func, function_name: str, 
                             skill_name: str = "tools") -> sk.SKFunctionBase:
        """Register a Python function as a native function"""
        return self.kernel.register_native_function(
            skill_name=skill_name,
            function_name=function_name,
            func=func
        )
    
    async def run_semantic_function(self, function: sk.SKFunctionBase, 
                                   variables: Optional[Dict[str, str]] = None) -> str:
        """Run a semantic function with optional variables"""
        context = self.kernel.create_new_context()
        if variables:
            for key, value in variables.items():
                context[key] = value
                
        result = await function.invoke_async(context=context)
        return result
```

### Memory Management with Semantic Kernel

```python
import semantic_kernel as sk
from semantic_kernel.memory.semantic_text_memory import SemanticTextMemory
from semantic_kernel.connectors.memory.volatile import VolatileMemoryStore
from semantic_kernel.connectors.memory.chroma import ChromaMemoryStore
import chromadb
import os
import uuid
from dotenv import load_dotenv
from datetime import datetime
from typing import List, Dict, Any, Optional

# Load environment variables
load_dotenv()

class AgentMemoryManager:
    """Class to manage agent memory using Semantic Kernel"""
    
    def __init__(self, kernel: sk.Kernel, use_persistent_storage: bool = False):
        """Initialize memory management system"""
        self.kernel = kernel
        self.use_persistent_storage = use_persistent_storage
        
        # Set up memory store - either volatile (in-memory) or persistent
        if use_persistent_storage:
            # Create a persistent client
            client = chromadb.PersistentClient(path="./data/memory_db")
            memory_store = ChromaMemoryStore(client=client)
        else:
            memory_store = VolatileMemoryStore()
            
        # Create the semantic memory
        self.memory = SemanticTextMemory(storage=memory_store)
        self.kernel.register_memory_store(memory_store=self.memory)
        
    async def save_information(self, 
                            collection: str, 
                            text: str, 
                            id: Optional[str] = None,
                            description: Optional[str] = None,
                            additional_metadata: Optional[Dict[str, str]] = None) -> str:
        """Save information to agent memory"""
        # Generate a unique ID if not provided
        memory_id = id if id else str(uuid.uuid4())
        
        # Create metadata
        metadata = {
            "timestamp": datetime.now().isoformat(),
        }
        if additional_metadata:
            metadata.update(additional_metadata)
            
        # Create description if not provided
        if not description:
            # Generate a description using the LLM
            describe_func = self.kernel.create_semantic_function(
                prompt_template="Generate a short description (max 100 chars) for this text: {{$input}}",
                function_name="describe_text"
            )
            context = self.kernel.create_new_context()
            context["input"] = text[:1000]  # Limit text length for description
            description_result = await describe_func.invoke_async(context=context)
            description = str(description_result).strip()
        
        # Save to memory
        await self.memory.save_information_async(
            collection=collection,
            id=memory_id,
            text=text,
            description=description,
            metadata=metadata
        )
        
        return memory_id
    
    async def retrieve_similar(self, collection: str, query: str, limit: int = 5) -> List[Dict[str, Any]]:
        """Retrieve information similar to the query"""
        results = await self.memory.search_async(
            collection=collection,
            query=query,
            limit=limit
        )
        
        # Format the results
        formatted_results = []
        for item in results:
            formatted_results.append({
                "id": item.id,
                "text": item.text,
                "description": item.description,
                "relevance": item.relevance,
                "metadata": item.metadata
            })
            
        return formatted_results
    
    async def get_conversation_history(self, collection: str, limit: int = 10) -> List[Dict[str, Any]]:
        """Get the most recent conversation history"""
        # In a real implementation, you would query by timestamp
        # Here we'll just get the most similar to an empty query as a dummy implementation
        results = await self.memory.search_async(
            collection=collection,
            query="",
            limit=limit
        )
        
        # Sort by timestamp if available
        formatted_results = []
        for item in results:
            formatted_results.append({
                "id": item.id,
                "text": item.text,
                "timestamp": item.metadata.get("timestamp", ""),
                "role": item.metadata.get("role", "unknown")
            })
            
        # Sort by timestamp
        formatted_results.sort(key=lambda x: x.get("timestamp", ""))
        
        return formatted_results
    
    async def clear_collection(self, collection: str) -> bool:
        """Clear a memory collection"""
        try:
            # Delete all items in the collection
            # Note: This is implementation specific
            if hasattr(self.memory.storage, "delete_collection"):
                await self.memory.storage.delete_collection_async(collection)
            return True
        except Exception as e:
            print(f"Error clearing collection: {str(e)}")
            return False
```

### Agent Planning with Semantic Kernel

```python
import semantic_kernel as sk
from semantic_kernel.planning import ActionPlanner, Plan
from semantic_kernel.planning.sequential_planner import SequentialPlanner
from typing import List, Dict, Any, Optional
import json

class AgentPlanner:
    """Class to handle agent planning capabilities"""
    
    def __init__(self, kernel: sk.Kernel):
        """Initialize the planner with a kernel"""
        self.kernel = kernel
        self.action_planner = ActionPlanner(kernel)
        self.sequential_planner = SequentialPlanner(kernel)
        
    async def create_action_plan(self, goal: str, max_tokens: int = 1500) -> Dict[str, Any]:
        """Create a plan using the ActionPlanner"""
        try:
            plan = await self.action_planner.create_plan_async(goal=goal, max_tokens=max_tokens)
            
            # Format the plan result
            return {
                "goal": goal,
                "plan_type": "action",
                "function": plan.function.name if hasattr(plan, "function") else None,
                "skill": plan.function.skill_name if hasattr(plan, "function") else None,
                "parameters": plan.parameters if hasattr(plan, "parameters") else {},
                "description": plan.description if hasattr(plan, "description") else ""
            }
        except Exception as e:
            return {
                "goal": goal,
                "error": str(e),
                "plan_type": "action",
                "success": False
            }
    
    async def create_sequential_plan(self, goal: str, max_tokens: int = 3000) -> Dict[str, Any]:
        """Create a sequential plan with multiple steps"""
        try:
            plan = await self.sequential_planner.create_plan_async(goal=goal, max_tokens=max_tokens)
            
            # Extract steps from the plan
            steps = []
            current_plan = plan
            
            # Walk through the plan to extract steps
            while current_plan and hasattr(current_plan, "steps") and current_plan.steps:
                for step in current_plan.steps:
                    step_info = {
                        "description": step.description,
                        "function": step.function.name if hasattr(step, "function") else "unknown",
                        "skill": step.function.skill_name if hasattr(step, "function") else "unknown",
                        "parameters": {}
                    }
                    
                    # Extract parameters if available
                    if hasattr(step, "parameters"):
                        for key, value in step.parameters.items():
                            step_info["parameters"][key] = value
                            
                    steps.append(step_info)
                
                # Move to next plan if this is a nested plan
                if len(current_plan.steps) == 1 and hasattr(current_plan.steps[0], "steps"):
                    current_plan = current_plan.steps[0]
                else:
                    break
            
            return {
                "goal": goal,
                "plan_type": "sequential",
                "steps": steps,
                "success": True
            }
            
        except Exception as e:
            return {
                "goal": goal,
                "error": str(e),
                "plan_type": "sequential",
                "success": False
            }
    
    async def execute_plan(self, plan: Plan, context: Optional[Dict[str, str]] = None) -> Dict[str, Any]:
        """Execute a previously created plan"""
        try:
            # Create context with variables if provided
            sk_context = self.kernel.create_new_context()
            if context:
                for key, value in context.items():
                    sk_context[key] = value
            
            # Execute the plan
            result = await plan.invoke_async(context=sk_context)
            
            # Extract result details
            return {
                "success": True,
                "result": str(result),
                "state": {key: str(value) for key, value in sk_context.variables.items()}
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
```

## Creating Multi-Agent Systems

### Creating Agents with Autogen

```python
import autogen
from typing import List, Dict, Any, Optional, Union, Callable
import os
from dotenv import load_dotenv
import json

# Load environment variables
load_dotenv()

class AutogenAgentSystem:
    """Class to manage a multi-agent system using Autogen"""
    
    def __init__(self, config_path: Optional[str] = None):
        """Initialize the multi-agent system"""
        # Load configuration or use default
        self.config = self._load_config(config_path)
        
        # Configure LLM settings
        self.llm_config = {
            "config_list": [
                {
                    "model": "gpt-4",
                    "api_key": os.getenv("OPENAI_API_KEY")
                }
            ],
            "temperature": 0.7,
            "cache_seed": 42  # For reproducibility during development
        }
        
        # Create the agents
        self.agents = self._create_agents()
        
    def _load_config(self, config_path: Optional[str]) -> Dict[str, Any]:
        """Load configuration from file or use default"""
        if config_path and os.path.exists(config_path):
            with open(config_path, "r") as f:
                return json.load(f)
        
        # Default configuration
        return {
            "agents": [
                {
                    "name": "user_proxy",
                    "type": "user_proxy",
                    "human_input_mode": "TERMINATE",
                    "is_termination_agent": True,
                    "description": "A proxy for the human user"
                },
                {
                    "name": "assistant",
                    "type": "assistant",
                    "description": "A helpful AI assistant",
                    "system_message": "You are a helpful AI assistant."
                },
                {
                    "name": "coder",
                    "type": "assistant",
                    "description": "An expert programmer",
                    "system_message": "You are an expert Python programmer. Write code that is efficient, well-documented, and follows best practices."
                },
                {
                    "name": "critic",
                    "type": "assistant",
                    "description": "A critical thinker who evaluates solutions",
                    "system_message": "You are a critical thinker who evaluates solutions. Analyze code and text for potential issues, edge cases, and improvements."
                }
            ]
        }
    
    def _create_agents(self) -> Dict[str, Any]:
        """Create agents based on configuration"""
        agents = {}
        
        for agent_config in self.config["agents"]:
            agent_type = agent_config["type"]
            
            if agent_type == "user_proxy":
                agents[agent_config["name"]] = autogen.UserProxyAgent(
                    name=agent_config["name"],
                    human_input_mode=agent_config.get("human_input_mode", "TERMINATE"),
                    is_termination_agent=agent_config.get("is_termination_agent", True),
                    system_message=agent_config.get("system_message", "")
                )
            elif agent_type == "assistant":
                agents[agent_config["name"]] = autogen.AssistantAgent(
                    name=agent_config["name"],
                    llm_config=self.llm_config,
                    system_message=agent_config.get("system_message", "You are a helpful assistant.")
                )
            elif agent_type == "function_calling":
                # This would be expanded with actual function definitions
                agents[agent_config["name"]] = autogen.AssistantAgent(
                    name=agent_config["name"],
                    llm_config=self.llm_config,
                    system_message=agent_config.get("system_message", "You are a function-calling agent."),
                    description=agent_config.get("description", "")
                )
                
        return agents
    
    def register_function(self, agent_name: str, function: Callable, function_name: Optional[str] = None):
        """Register a function with an agent"""
        if agent_name not in self.agents:
            raise ValueError(f"Agent {agent_name} not found")
            
        agent = self.agents[agent_name]
        func_name = function_name or function.__name__
        
        # Register the function with the agent
        # In newer versions of Autogen, this would use the proper function registration API
        if hasattr(agent, "register_function"):
            agent.register_function(
                function=function,
                name=func_name,
                description=function.__doc__ or f"Function {func_name}"
            )
        else:
            # Fallback for older versions
            if not hasattr(agent, "function_map"):
                agent.function_map = {}
            agent.function_map[func_name] = function
    
    def create_group_chat(self, agents_list: List[str], speaker_selection_method: str = "round_robin") -> Any:
        """Create a group chat between selected agents"""
        selected_agents = [self.agents[name] for name in agents_list if name in self.agents]
        
        if not selected_agents:
            raise ValueError("No valid agents selected for group chat")
            
        # Create a group chat
        return autogen.GroupChat(
            agents=selected_agents,
            messages=[],
            speaker_selection_method=speaker_selection_method
        )
    
    def create_group_chat_manager(self, group_chat: Any) -> Any:
        """Create a manager for a group chat"""
        return autogen.GroupChatManager(
            groupchat=group_chat,
            llm_config=self.llm_config
        )
    
    def initiate_chat(self, initiator_name: str, recipient_name: str, message: str) -> None:
        """Start a chat between two agents"""
        if initiator_name not in self.agents:
            raise ValueError(f"Initiator agent {initiator_name} not found")
            
        if recipient_name not in self.agents:
            raise ValueError(f"Recipient agent {recipient_name} not found")
            
        initiator = self.agents[initiator_name]
        recipient = self.agents[recipient_name]
        
        # Start the chat
        initiator.initiate_chat(recipient, message=message)
    
    def run_group_chat(self, manager: Any, message: str) -> None:
        """Run a group chat with the specified message"""
        # Start the group chat
        manager.run(message)
```

### Advanced Multi-Agent Collaboration

```python
import autogen
from typing import List, Dict, Any, Optional, Union, Callable
import os
from dotenv import load_dotenv
import json
import sqlite3
from datetime import datetime

# Load environment variables
load_dotenv()

# Define tools/functions that agents can use
def search_database(query: str, db_path: str = "knowledge_base.db") -> List[Dict[str, Any]]:
    """
    Search the database for information matching the query
    
    Args:
        query: SQL query to execute
        db_path: Path to the SQLite database
        
    Returns:
        List of matching records
    """
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        cursor.execute(query)
        
        # Get column names
        column_names = [description[0] for description in cursor.description]
        
        # Fetch results
        results = []
        for row in cursor.fetchall():
            results.append(dict(zip(column_names, row)))
            
        conn.close()
        return results
    except Exception as e:
        return [{"error": str(e)}]

def write_to_database(table: str, data: Dict[str, Any], db_path: str = "knowledge_base.db") -> Dict[str, Any]:
    """
    Write data to the specified database table
    
    Args:
        table: Table name to write to
        data: Dictionary of column:value pairs to insert
        db_path: Path to the SQLite database
        
    Returns:
        Result of the operation
    """
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # Generate column and value lists
        columns = ", ".join(data.keys())
        placeholders = ", ".join(["?" for _ in data])
        values = list(data.values())
        
        # Create SQL query
        query = f"INSERT INTO {table} ({columns}) VALUES ({placeholders})"
        
        # Execute query
        cursor.execute(query, values)
        conn.commit()
        
        result = {"success": True, "message": f"Inserted data into {table}", "row_id": cursor.lastrowid}
        conn.close()
        return result
    except Exception as e:
        return {"success": False, "error": str(e)}

def create_specialized_agents():
    """Create a team of specialized agents with specific roles"""
    
    # Configure LLM
    llm_config = {
        "config_list": [
            {
                "model": "gpt-4",
                "api_key": os.getenv("OPENAI_API_KEY")
            }
        ],
        "temperature": 0.7
    }
    
    # Create the user proxy agent
    user_proxy = autogen.UserProxyAgent(
        name="User",
        human_input_mode="TERMINATE",
        is_termination_agent=True,
        system_message="You are the human user working with a team of AI agents."
    )
    
    # Create a project manager agent
    manager = autogen.AssistantAgent(
        name="ProjectManager",
        llm_config=llm_config,
        system_message="""You are a project manager responsible for coordinating the efforts of a team of AI agents.
        Your responsibilities include:
        1. Breaking down complex tasks into smaller subtasks
        2. Assigning tasks to appropriate specialists
        3. Tracking progress and ensuring all parts come together
        4. Making final decisions when there are conflicting approaches
        
        You should communicate clearly, set deadlines, and ensure the team stays focused on the user's goals.
        """
    )
    
    # Create a data analyst agent
    analyst = autogen.AssistantAgent(
        name="DataAnalyst",
        llm_config=llm_config,
        system_message="""You are a data analyst specializing in querying databases and interpreting results.
        Your responsibilities include:
        1. Writing SQL queries to extract relevant information
        2. Analyzing data patterns and trends
        3. Creating summaries of data findings
        4. Recommending data-driven actions
        
        When writing SQL queries, be precise and consider performance implications.
        """
    )
    
    # Register database functions with the analyst
    analyst.register_function(
        function=search_database,
        name="search_database",
        description="Search the database with an SQL query"
    )
    
    analyst.register_function(
        function=write_to_database,
        name="write_to_database",
        description="Write data to a specified database table"
    )
    
    # Create a researcher agent
    researcher = autogen.AssistantAgent(
        name="Researcher",
        llm_config=llm_config,
        system_message="""You are a research specialist who gathers and synthesizes information.
        Your responsibilities include:
        1. Finding relevant information on given topics
        2. Evaluating the credibility and relevance of sources
        3. Summarizing key findings in a structured format
        4. Identifying gaps in available information
        
        Always provide context for your findings and cite sources when possible.
        """
    )
    
    # Create a technical writer agent
    writer = autogen.AssistantAgent(
        name="TechnicalWriter",
        llm_config=llm_config,
        system_message="""You are a technical writer who creates clear and concise documentation.
        Your responsibilities include:
        1. Writing clear explanations of complex concepts
        2. Organizing information in a logical structure
        3. Creating user-friendly documentation
        4. Ensuring consistent terminology and style
        
        Focus on clarity, accuracy, and usefulness to the intended audience.
        """
    )
    
    # Create a group chat for all agents
    groupchat = autogen.GroupChat(
        agents=[user_proxy, manager, analyst, researcher, writer],
        messages=[],
        speaker_selection_method="round_robin"
    )
    
    # Create a group chat manager
    manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)
    
    return {
        "user_proxy": user_proxy, 
        "manager": manager,
        "agents": {
            "project_manager": manager,
            "data_analyst": analyst,
            "researcher": researcher,
            "writer": writer
        },
        "groupchat": groupchat,
        "groupchat_manager": manager
    }

def run_multiagent_task(task_description: str) -> Dict[str, Any]:
    """Run a task using the multi-agent system"""
    # Create the agent system
    agent_system = create_specialized_agents()
    
    # Start the task
    agent_system["user_proxy"].initiate_chat(
        agent_system["groupchat_manager"],
        message=f"""
        Let's work together on this task: {task_description}
        
        ProjectManager, please coordinate the team to complete this task efficiently.
        """
    )
    
    # The results would be in the group chat messages
    # In a real implementation, you would extract and process these messages
    
    # For demonstration, we'll return a simple result
    return {
        "task": task_description,
        "status": "completed",
        "timestamp": datetime.now().isoformat()
    }
```

## Autogen Studio and Differences from LangChain

### Autogen Studio Interface

Autogen Studio provides a visual interface for building and testing multi-agent systems. Below is a code sample that demonstrates how to integrate with and extend Autogen Studio programmatically:

```python
import requests
import json
import os
from dotenv import load_dotenv
from typing import Dict, Any, List, Optional

# Load environment variables
load_dotenv()

class AutogenStudioClient:
    """Client for interacting with Autogen Studio"""
    
    def __init__(self, base_url: str = "http://localhost:8000"):
        """Initialize with Autogen Studio URL"""
        self.base_url = base_url
        self.api_key = os.getenv("AUTOGEN_STUDIO_API_KEY")
        
    def _make_request(self, endpoint: str, method: str = "GET", 
                     data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Make a request to the Autogen Studio API"""
        url = f"{self.base_url}/api/{endpoint}"
        headers = {"Authorization": f"Bearer {self.api_key}"}
        
        if method == "GET":
            response = requests.get(url, headers=headers)
        elif method == "POST":
            response = requests.post(url, headers=headers, json=data)
        elif method == "PUT":
            response = requests.put(url, headers=headers, json=data)
        elif method == "DELETE":
            response = requests.delete(url, headers=headers)
        else:
            raise ValueError(f"Unsupported HTTP method: {method}")
            
        response.raise_for_status()
        return response.json()
    
    def list_workflows(self) -> List[Dict[str, Any]]:
        """List available workflows"""
        return self._make_request("workflows")
    
    def get_workflow(self, workflow_id: str) -> Dict[str, Any]:
        """Get details of a specific workflow"""
        return self._make_request(f"workflows/{workflow_id}")
    
    def create_workflow(self, name: str, description: str, 
                       agents: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Create a new workflow"""
        data = {
            "name": name,
            "description": description,
            "agents": agents
        }
        return self._make_request("workflows", method="POST", data=data)
    
    def run_workflow(self, workflow_id: str, input_message: str) -> Dict[str, Any]:
        """Run a workflow with an input message"""
        data = {"input": input_message}
        return self._make_request(f"workflows/{workflow_id}/run", method="POST", data=data)
    
    def get_run_status(self, run_id: str) -> Dict[str, Any]:
        """Get the status of a workflow run"""
        return self._make_request(f"runs/{run_id}")

# Example workflow configuration that would be compatible with Autogen Studio
def create_sample_workflow():
    """Create a sample workflow configuration for Autogen Studio"""
    return {
        "name": "Research and Analysis Workflow",
        "description": "A workflow that researches a topic, analyzes data, and generates a report",
        "agents": [
            {
                "name": "user_proxy",
                "agent_type": "UserProxyAgent",
                "config": {
                    "name": "User",
                    "human_input_mode": "TERMINATE",
                    "is_termination_agent": True
                }
            },
            {
                "name": "researcher",
                "agent_type": "AssistantAgent",
                "config": {
                    "name": "Researcher",
                    "llm_config": {
                        "model": "gpt-4"
                    },
                    "system_message": "You are a research specialist who gathers information."
                }
            },
            {
                "name": "analyst",
                "agent_type": "AssistantAgent",
                "config": {
                    "name": "Analyst",
                    "llm_config": {
                        "model": "gpt-4"
                    },
                    "system_message": "You are a data analyst who processes and interprets information."
                }
            },
            {
                "name": "writer",
                "agent_type": "AssistantAgent",
                "config": {
                    "name": "Writer",
                    "llm_config": {
                        "model": "gpt-4"
                    },
                    "system_message": "You create clear, concise reports based on research and analysis."
                }
            }
        ],
        "flow": [
            {"from": "user_proxy", "to": "researcher", "condition": "start"},
            {"from": "researcher", "to": "analyst", "condition": "research_complete"},
            {"from": "analyst", "to": "writer", "condition": "analysis_complete"},
            {"from": "writer", "to": "user_proxy", "condition": "report_complete"}
        ]
    }
```

### Key Differences Between Autogen and LangChain

```python
from typing import Dict, Any, List

def framework_comparison() -> Dict[str, List[str]]:
    """Compare key features and differences between Autogen and LangChain"""
    
    comparison = {
        "architecture": [
            "Autogen: Agent-centric design with focus on multi-agent collaboration",
            "LangChain: Chain-centric design with focus on sequencing components"
        ],
        "state_management": [
            "Autogen: Built-in conversation state management between agents",
            "LangChain: Requires explicit state management through memory classes"
        ],
        "multi_agent": [
            "Autogen: Native support for multi-agent conversations and group chats",
            "LangChain: Requires custom orchestration with LangGraph for agent interaction"
        ],
        "function_calling": [
            "Autogen: Integrated function registration and calling",
            "LangChain: Tools and toolkits with structured input/output schemas"
        ],
        "human_in_the_loop": [
            "Autogen: First-class support with UserProxyAgent",
            "LangChain: Implemented through specific callbacks and chain types"
        ],
        "code_execution": [
            "Autogen: Native support for code generation and execution",
            "LangChain: Requires specific tools and sandbox setup"
        ],
        "development_maturity": [
            "Autogen: Newer but rapidly evolving, focused on research",
            "LangChain: More established ecosystem with more integrations"
        ],
        "visualization": [
            "Autogen: Autogen Studio provides visual interface for agent design",
            "LangChain: LangSmith focuses on tracing, evaluation, and debugging"
        ]
    }
    
    return comparison
```

## Practical Exercise: Building an Agent with Tools and Database Integration

Let's implement a comprehensive agent that combines Autogen with database connectivity and external tools:

```python
import autogen
import os
import json
import sqlite3
import pandas as pd
import matplotlib.pyplot as plt
from io import BytesIO
import base64
from typing import Dict, Any, List, Optional, Union
from dotenv import load_dotenv
from datetime import datetime

# Load environment variables
load_dotenv()

# Database setup function
def setup_database(db_path: str = "sales_data.db"):
    """Set up a sample SQLite database for the exercise"""
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    # Create tables
    cursor.execute('''
    CREATE TABLE IF NOT EXISTS products (
        product_id INTEGER PRIMARY KEY,
        name TEXT NOT NULL,
        category TEXT NOT NULL,
        price REAL NOT NULL
    )
    ''')
    
    cursor.execute('''
    CREATE TABLE IF NOT EXISTS sales (
        sale_id INTEGER PRIMARY KEY,
        product_id INTEGER NOT NULL,
        quantity INTEGER NOT NULL,
        sale_date TEXT NOT NULL,
        FOREIGN KEY (product_id) REFERENCES products (product_id)
    )
    ''')
    
    # Insert sample data if tables are empty
    cursor.execute("SELECT COUNT(*) FROM products")
    if cursor.fetchone()[0] == 0:
        # Add sample products
        products = [
            (1, "Laptop", "Electronics", 1200),
            (2, "Smartphone", "Electronics", 800),
            (3, "Headphones", "Accessories", 150),
            (4, "Monitor", "Electronics", 300),
            (5, "Keyboard", "Accessories", 80),
            (6, "Mouse", "Accessories", 40),
            (7, "Tablet", "Electronics", 500),
            (8, "Printer", "Electronics", 250),
            (9, "External Drive", "Accessories", 120),
            (10, "Smart Watch", "Wearables", 220)
        ]
        cursor.executemany("INSERT INTO products VALUES (?, ?, ?, ?)", products)
        
        # Add sample sales
        import random
        from datetime import datetime, timedelta
        
        sales = []
        sale_id = 1
        
        # Generate sales for the past 30 days
        for day in range(30):
            date = (datetime.now() - timedelta(days=day)).strftime("%Y-%m-%d")
            
            # Generate 1-5 sales per day
            for _ in range(random.randint(1, 5)):
                product_id = random.randint(1, 10)
                quantity = random.randint(1, 5)
                sales.append((sale_id, product_id, quantity, date))
                sale_id += 1
                
        cursor.executemany("INSERT INTO sales VALUES (?, ?, ?, ?)", sales)
    
    conn.commit()
    conn.close()
    
    return "Database setup complete"

# Tool functions for the agent
def query_database(sql_query: str, db_path: str = "sales_data.db") -> Dict[str, Any]:
    """Execute an SQL query on the database"""
    try:
        conn = sqlite3.connect(db_path)
        
        # Execute query and fetch results
        df = pd.read_sql_query(sql_query, conn)
        conn.close()
        
        # Convert DataFrame to dictionary
        result = {
            "success": True,
            "rows": df.shape[0],
            "columns": list(df.columns),
            "data": df.to_dict(orient="records")
        }
        
        return result
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }

def generate_chart(
    chart_type: str,
    data: List[Dict[str, Any]],
    x_column: str,
    y_column: str,
    title: str = "Chart",
    **kwargs
) -> Dict[str, Any]:
    """Generate a chart from data"""
    try:
        # Convert list of dicts to DataFrame
        df = pd.DataFrame(data)
        
        # Create figure and axis
        plt.figure(figsize=(10, 6))
        
        # Generate chart based on type
        if chart_type.lower() == "bar":
            plt.bar(df[x_column], df[y_column], **kwargs)
        elif chart_type.lower() == "line":
            plt.plot(df[x_column], df[y_column], **kwargs)
        elif chart_type.lower() == "scatter":
            plt.scatter(df[x_column], df[y_column], **kwargs)
        elif chart_type.lower() == "pie":
            plt.pie(df[y_column], labels=df[x_column], autopct='%1.1f%%', **kwargs)
        else:
            return {"success": False, "error": f"Chart type '{chart_type}' not supported"}
        
        # Add labels and title
        plt.xlabel(x_column)
        plt.ylabel(y_column)
        plt.title(title)
        
        # Adjust layout
        plt.tight_layout()
        
        # Save chart to bytes
        buffer = BytesIO()
        plt.savefig(buffer, format="png")
        buffer.seek(0)
        
        # Convert to base64 for easy display
        chart_data = base64.b64encode(buffer.read()).decode()
        
        # Close the figure to free memory
        plt.close()
        
        return {
            "success": True,
            "chart_type": chart_type,
            "chart_base64": chart_data,
            "title": title
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }

def analyze_sales_trend(start_date: str, end_date: str, category: Optional[str] = None, 
                      db_path: str = "sales_data.db") -> Dict[str, Any]:
    """Analyze sales trends between two dates, optionally filtered by category"""
    try:
        # Build query based on inputs
        query = """
        SELECT 
            s.sale_date, 
            SUM(p.price * s.quantity) as daily_revenue,
            SUM(s.quantity) as units_sold,
            COUNT(DISTINCT s.sale_id) as transaction_count
        FROM 
            sales s
        JOIN 
            products p ON s.product_id = p.product_id
        """
        
        # Add category filter if specified
        where_clauses = [f"s.sale_date BETWEEN '{start_date}' AND '{end_date}'"]
        if category:
            where_clauses.append(f"p.category = '{category}'")
        
        query += " WHERE " + " AND ".join(where_clauses)
        query += " GROUP BY s.sale_date ORDER BY s.sale_date"
        
        # Execute the query
        result = query_database(query, db_path)
        
        if not result["success"]:
            return result
            
        data = result["data"]
        
        # Calculate trend statistics
        if data:
            total_revenue = sum(item["daily_revenue"] for item in data)
            total_units = sum(item["units_sold"] for item in data)
            avg_revenue = total_revenue / len(data)
            
            # Calculate trend (simple linear slope)
            if len(data) > 1:
                first_day = data[0]["daily_revenue"]
                last_day = data[-1]["daily_revenue"]
                trend_direction = "increasing" if last_day > first_day else "decreasing" if last_day < first_day else "stable"
            else:
                trend_direction = "insufficient data"
                
            # Generate charts
            revenue_chart = generate_chart(
                chart_type="line",
                data=data,
                x_column="sale_date",
                y_column="daily_revenue",
                title=f"Daily Revenue {start_date} to {end_date}",
                marker='o'
            )
            
            units_chart = generate_chart(
                chart_type="bar",
                data=data,
                x_column="sale_date",
                y_column="units_sold",
                title=f"Daily Units Sold {start_date} to {end_date}"
            )
            
            return {
                "success": True,
                "period": f"{start_date} to {end_date}",
                "category": category if category else "All Categories",
                "total_revenue": total_revenue,
                "total_units_sold": total_units,
                "average_daily_revenue": avg_revenue,
                "trend_direction": trend_direction,
                "days_analyzed": len(data),
                "revenue_chart": revenue_chart,
                "units_chart": units_chart,
                "raw_data": data[:5]  # Include sample of raw data
            }
        else:
            return {
                "success": False,
                "error": "No data found for the specified period"
            }
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }

def find_top_products(start_date: str, end_date: str, limit: int = 5, 
                     by_metric: str = "revenue", db_path: str = "sales_data.db") -> Dict[str, Any]:
    """Find top performing products by revenue or quantity"""
    try:
        # Validate inputs
        if by_metric not in ["revenue", "quantity"]:
            return {"success": False, "error": "Metric must be 'revenue' or 'quantity'"}
        
        # Build the appropriate query
        metric_col = "SUM(p.price * s.quantity)" if by_metric == "revenue" else "SUM(s.quantity)"
        alias = "total_revenue" if by_metric == "revenue" else "total_quantity"
        
        query = f"""
        SELECT 
            p.product_id,
            p.name,
            p.category,
            {metric_col} as {alias}
        FROM 
            sales s
        JOIN 
            products p ON s.product_id = p.product_id
        WHERE 
            s.sale_date BETWEEN '{start_date}' AND '{end_date}'
        GROUP BY 
            p.product_id, p.name, p.category
        ORDER BY 
            {alias} DESC
        LIMIT {limit}
        """
        
        # Execute the query
        result = query_database(query, db_path)
        
        if not result["success"]:
            return result
            
        data = result["data"]
        
        # Generate visualizations
        if data:
            # Create bar chart of top products
            chart = generate_chart(
                chart_type="bar",
                data=data,
                x_column="name",
                y_column=alias,
                title=f"Top {limit} Products by {by_metric.capitalize()}"
            )
            
            # Create pie chart showing distribution
            pie_chart = generate_chart(
                chart_type="pie",
                data=data,
                x_column="name",
                y_column=alias,
                title=f"Distribution of {by_metric.capitalize()} by Product"
            )
            
            return {
                "success": True,
                "period": f"{start_date} to {end_date}",
                "metric": by_metric,
                "top_products": data,
                "bar_chart": chart,
                "pie_chart": pie_chart
            }
        else:
            return {
                "success": False,
                "error": "No data found for the specified period"
            }
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }

def create_sales_agent():
    """Create a sales analysis agent with database and visualization capabilities"""
    # Set up the database
    setup_database()
    
    # Configure LLM
    llm_config = {
        "config_list": [
            {
                "model": "gpt-4",
                "api_key": os.getenv("OPENAI_API_KEY")
            }
        ],
        "temperature": 0.2
    }
    
    # Create the user proxy agent
    user_proxy = autogen.UserProxyAgent(
        name="User",
        human_input_mode="TERMINATE",
        is_termination_agent=True,
        system_message="You are the user working with a Sales Analysis Agent."
    )
    
    # Create the sales analyst agent
    sales_analyst = autogen.AssistantAgent(
        name="SalesAnalyst",
        llm_config=llm_config,
        system_message="""You are a Sales Analysis Agent that helps analyze sales data and generate insights.
        
        You have access to a SQLite database with the following schema:
        
        - products (product_id, name, category, price)
        - sales (sale_id, product_id, quantity, sale_date)
        
        You can use the following tools:
        1. query_database - Execute SQL queries on the database
        2. generate_chart - Create visualizations from data
        3. analyze_sales_trend - Analyze sales trends between dates
        4. find_top_products - Find top performing products
        
        Always plan your approach before executing queries. When analyzing data, consider:
        - Trends over time
        - Product performance
        - Category performance
        - Revenue vs. quantity sold
        
        When presenting results, include both numerical insights and visualizations when appropriate.
        """
    )
    
    # Register functions with the sales analyst
    sales_analyst.register_function(
        function=query_database,
        name="query_database"
    )
    
    sales_analyst.register_function(
        function=generate_chart,
        name="generate_chart"
    )
    
    sales_analyst.register_function(
        function=analyze_sales_trend,
        name="analyze_sales_trend"
    )
    
    sales_analyst.register_function(
        function=find_top_products,
        name="find_top_products"
    )
    
    return user_proxy, sales_analyst

# Example usage
if __name__ == "__main__":
    user_proxy, sales_analyst = create_sales_agent()
    
    # Start interaction
    user_proxy.initiate_chat(
        sales_analyst,
        message="""
        I need a sales analysis for the past two weeks.
        Can you show me:
        1. Overall revenue trends
        2. Top 3 products by revenue
        3. Performance comparison between Electronics and Accessories categories
        """
    )
```

## Conclusion

Semantic Kernel and Autogen represent two powerful yet distinct approaches to building LLM-powered agent systems. Each framework offers unique advantages depending on the use case:

**Semantic Kernel** provides a structured approach to integrating LLM capabilities with traditional programming models through semantic functions and skills. Its planning capabilities and tight integration with mainstream programming languages make it particularly suitable for enterprise applications where integration with existing codebases is important. The framework excels in scenarios requiring complex planning and reliable orchestration of different components.

**Autogen** takes a more agent-centric approach, focusing on multi-agent collaboration and conversation. Its built-in support for agent conversations, code execution, and human-in-the-loop interactions makes it ideal for research and exploratory applications. Autogen Studio further enhances the framework by providing a visual interface for designing and testing agent systems.

When choosing between these frameworks, consider:

1. **Application complexity**: Semantic Kernel offers more structured approaches for complex enterprise applications, while Autogen excels in dynamic, conversational scenarios.

2. **Team expertise**: Semantic Kernel may be more approachable for developers experienced with traditional programming patterns, while Autogen's agent-based approach might appeal more to those focused on AI research.

3. **Integration needs**: Semantic Kernel has strong integration with existing programming paradigms, while Autogen offers superior multi-agent orchestration capabilities.

4. **Development speed**: Autogen often enables faster prototyping of multi-agent systems, while Semantic Kernel provides more robust architectural patterns for production systems.

As demonstrated in our practical exercise, both frameworks can be extended with custom tools and database integration to create powerful AI agents capable of performing complex tasks. The ability to combine structured data analysis with natural language understanding opens up new possibilities for business intelligence, automation, and decision support systems.

As the field of AI agents continues to evolve, these frameworks will likely converge on common patterns and standards, while maintaining their unique strengths in specific domains. For developers starting with AI agents today, mastering both frameworks provides the versatility needed to address a wide range of business and technical challenges.