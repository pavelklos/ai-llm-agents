<small>Claude 3.7 Sonnet Thinking</small>
# 01. Introduction to Neural Networks and Generative AI

## Key Terms

- **Neural Network (NN)**: Computational models inspired by biological neural systems, consisting of interconnected processing units (neurons) organized in layers that collectively perform complex pattern recognition tasks.
- **Activation Function**: Mathematical functions that introduce non-linearity into neural networks, enabling them to learn complex patterns. Common examples include ReLU, Sigmoid, Tanh, and GELU.
- **Backpropagation**: Algorithm for calculating gradients in neural networks by propagating error backwards through the network to update weights.
- **Gradient Descent**: Optimization technique for finding local minima of a differentiable function by iteratively moving in the direction of steepest descent.
- **Transformer**: Neural architecture utilizing self-attention mechanisms to process sequential data in parallel rather than sequentially.
- **Generative AI**: Systems that create new content resembling their training data, including language models (text), diffusion models (images), vocoding models (audio), etc.
- **Fine-tuning**: Process of further training a pre-trained model on domain-specific data to adapt it for specialized tasks.
- **LoRA**: Low-Rank Adaptation technique that significantly reduces trainable parameters by representing weight updates as low-rank decompositions.
- **QLoRA**: Quantized LoRA that combines weight quantization with low-rank adaptation for memory-efficient training.

## Neural Network Fundamentals and Advanced Architecture

Neural networks process data through interconnected layers of computational units. The modern implementation leverages computational graphs and automatic differentiation:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Optional, Tuple, Union


class AdvancedNeuralNetwork(nn.Module):
    def __init__(
        self, 
        layer_dims: List[int],
        activations: List[str] = None,
        dropout_rates: List[float] = None,
        batch_norm: bool = True,
        residual_connections: bool = False
    ):
        super(AdvancedNeuralNetwork, self).__init__()
        
        self.num_layers = len(layer_dims) - 1
        self.residual_connections = residual_connections
        
        # Default activations if not provided
        if activations is None:
            activations = ["relu"] * self.num_layers
        
        # Default dropout rates if not provided
        if dropout_rates is None:
            dropout_rates = [0.0] * self.num_layers
        
        # Dictionary of activation functions
        self.activation_funcs = {
            "relu": F.relu,
            "leaky_relu": F.leaky_relu,
            "gelu": F.gelu,
            "sigmoid": torch.sigmoid,
            "tanh": torch.tanh,
            "swish": lambda x: x * torch.sigmoid(x)
        }
        
        # Create layers
        self.layers = nn.ModuleList()
        self.batch_norms = nn.ModuleList() if batch_norm else None
        self.dropouts = nn.ModuleList()
        
        for i in range(self.num_layers):
            # Linear layer
            self.layers.append(nn.Linear(layer_dims[i], layer_dims[i+1]))
            
            # Batch normalization
            if batch_norm:
                self.batch_norms.append(nn.BatchNorm1d(layer_dims[i+1]))
            
            # Dropout layer
            self.dropouts.append(nn.Dropout(dropout_rates[i]))
        
        # Store activation names
        self.activations = activations
        
        # Xavier initialization
        self._initialize_weights()
        
    def _initialize_weights(self):
        for layer in self.layers:
            nn.init.xavier_uniform_(layer.weight)
            if layer.bias is not None:
                nn.init.zeros_(layer.bias)
                
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        for i in range(self.num_layers):
            # Store original input for residual connection if needed
            identity = x if self.residual_connections and x.shape == self.layers[i].weight.shape[0] else None
            
            # Forward through linear layer
            x = self.layers[i](x)
            
            # Apply batch normalization if enabled
            if self.batch_norms is not None:
                if len(x.shape) == 2:  # For regular batched inputs
                    x = self.batch_norms[i](x)
                else:  # For sequential inputs
                    orig_shape = x.shape
                    x = x.reshape(-1, x.shape[-1])
                    x = self.batch_norms[i](x)
                    x = x.reshape(orig_shape)
            
            # Apply activation function
            if i < self.num_layers - 1 or self.activations[-1] != "none":
                x = self.activation_funcs[self.activations[i]](x)
            
            # Apply dropout
            x = self.dropouts[i](x)
            
            # Add residual connection if applicable
            if identity is not None:
                x = x + identity
                
        return x


# Training loop with gradient accumulation and learning rate scheduling
def train_neural_network(
    model: nn.Module,
    train_loader: torch.utils.data.DataLoader,
    val_loader: Optional[torch.utils.data.DataLoader] = None,
    epochs: int = 10,
    lr: float = 0.001,
    weight_decay: float = 0.0001,
    gradient_accumulation_steps: int = 1,
    lr_scheduler_type: str = "cosine"
):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    
    # Loss function and optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.AdamW(
        model.parameters(), 
        lr=lr, 
        weight_decay=weight_decay
    )
    
    # Learning rate scheduler
    if lr_scheduler_type == "cosine":
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, 
            T_max=epochs
        )
    elif lr_scheduler_type == "linear":
        scheduler = torch.optim.lr_scheduler.LinearLR(
            optimizer,
            start_factor=1.0,
            end_factor=0.1,
            total_iters=epochs
        )
    else:
        scheduler = None
    
    # Training loop
    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        optimizer.zero_grad()
        
        for i, (inputs, labels) in enumerate(train_loader):
            inputs, labels = inputs.to(device), labels.to(device)
            
            # Forward pass
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            
            # Normalize loss for gradient accumulation
            loss = loss / gradient_accumulation_steps
            
            # Backward pass
            loss.backward()
            
            # Update weights after accumulation steps
            if (i + 1) % gradient_accumulation_steps == 0:
                optimizer.step()
                optimizer.zero_grad()
            
            running_loss += loss.item()
        
        # Validation
        if val_loader:
            model.eval()
            val_loss = 0.0
            correct = 0
            total = 0
            
            with torch.no_grad():
                for inputs, labels in val_loader:
                    inputs, labels = inputs.to(device), labels.to(device)
                    outputs = model(inputs)
                    val_loss += criterion(outputs, labels).item()
                    
                    _, predicted = outputs.max(1)
                    total += labels.size(0)
                    correct += predicted.eq(labels).sum().item()
            
            print(f'Epoch {epoch+1}/{epochs}, Train Loss: {running_loss/len(train_loader):.4f}, '
                  f'Val Loss: {val_loss/len(val_loader):.4f}, Accuracy: {100.*correct/total:.2f}%')
        else:
            print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}')
        
        # Update learning rate
        if scheduler:
            scheduler.step()
    
    return model
```

## Advanced Neural Network Architectures

### Feedforward Neural Networks with Skip Connections

Modern feedforward networks often incorporate skip connections for improved gradient flow:

```python
class ResidualFeedForwardNN(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate=0.2):
        super(ResidualFeedForwardNN, self).__init__()
        
        # Input layer
        layers = [nn.Linear(input_size, hidden_sizes[0])]
        layers.append(nn.BatchNorm1d(hidden_sizes[0]))
        layers.append(nn.GELU())
        layers.append(nn.Dropout(dropout_rate))
        
        # Hidden layers with residual connections
        for i in range(len(hidden_sizes) - 1):
            # Main path
            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))
            layers.append(nn.BatchNorm1d(hidden_sizes[i+1]))
            layers.append(nn.GELU())
            layers.append(nn.Dropout(dropout_rate))
            
            # Skip connection (if dimensions match)
            if hidden_sizes[i] == hidden_sizes[i+1]:
                layers.append(lambda x: x)  # Identity
            else:
                # 1x1 projection
                layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1], bias=False))
        
        # Output layer
        layers.append(nn.Linear(hidden_sizes[-1], output_size))
        
        self.network = nn.Sequential(*layers)
        
    def forward(self, x):
        return self.network(x)
```

### Convolutional Neural Networks with Modern Architecture

Implementation of a modern CNN with residual blocks and attention mechanisms:

```python
class AttentionBlock(nn.Module):
    def __init__(self, channels):
        super(AttentionBlock, self).__init__()
        self.channels = channels
        
        # Channel attention
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        self.fc1 = nn.Conv2d(channels, channels // 8, kernel_size=1)
        self.relu = nn.ReLU()
        self.fc2 = nn.Conv2d(channels // 8, channels, kernel_size=1)
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        # Average pooling
        avg_out = self.fc2(self.relu(self.fc1(self.avg_pool(x))))
        
        # Max pooling
        max_out = self.fc2(self.relu(self.fc1(self.max_pool(x))))
        
        # Combine and apply sigmoid
        out = self.sigmoid(avg_out + max_out)
        
        return x * out


class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super(ResidualBlock, self).__init__()
        
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.attention = AttentionBlock(out_channels)
        
        # Skip connection
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
        
    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out = self.attention(out)  # Apply attention
        out += self.shortcut(x)    # Skip connection
        out = F.relu(out)
        
        return out


class ModernCNN(nn.Module):
    def __init__(self, num_classes=10):
        super(ModernCNN, self).__init__()
        
        self.initial = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU()
        )
        
        # Residual blocks
        self.layer1 = self._make_layer(64, 64, 2, stride=1)
        self.layer2 = self._make_layer(64, 128, 2, stride=2)
        self.layer3 = self._make_layer(128, 256, 2, stride=2)
        self.layer4 = self._make_layer(256, 512, 2, stride=2)
        
        # Global average pooling and classifier
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512, num_classes)
        
        # Dropout for regularization
        self.dropout = nn.Dropout(0.2)
        
    def _make_layer(self, in_channels, out_channels, num_blocks, stride):
        layers = []
        layers.append(ResidualBlock(in_channels, out_channels, stride))
        
        for _ in range(1, num_blocks):
            layers.append(ResidualBlock(out_channels, out_channels, 1))
            
        return nn.Sequential(*layers)
    
    def forward(self, x):
        # Initial convolution
        x = self.initial(x)
        
        # Residual blocks
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        
        # Global pooling and classification
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.dropout(x)
        x = self.fc(x)
        
        return x
```

### Advanced Recurrent Neural Networks

Modern RNN implementations with GRU and bidirectional LSTM:

```python
class AdvancedRNN(nn.Module):
    def __init__(
        self, 
        input_size, 
        hidden_size, 
        num_layers=2, 
        dropout=0.2, 
        bidirectional=True, 
        rnn_type="lstm",
        attention=True
    ):
        super(AdvancedRNN, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.bidirectional = bidirectional
        self.directions = 2 if bidirectional else 1
        self.attention = attention
        
        # Select RNN type
        if rnn_type.lower() == "lstm":
            self.rnn = nn.LSTM(
                input_size=input_size,
                hidden_size=hidden_size,
                num_layers=num_layers,
                batch_first=True,
                dropout=dropout if num_layers > 1 else 0,
                bidirectional=bidirectional
            )
        elif rnn_type.lower() == "gru":
            self.rnn = nn.GRU(
                input_size=input_size,
                hidden_size=hidden_size,
                num_layers=num_layers,
                batch_first=True,
                dropout=dropout if num_layers > 1 else 0,
                bidirectional=bidirectional
            )
        else:
            raise ValueError(f"Unsupported RNN type: {rnn_type}")
        
        # Attention layer
        if attention:
            self.attention_layer = nn.Sequential(
                nn.Linear(hidden_size * self.directions, hidden_size),
                nn.Tanh(),
                nn.Linear(hidden_size, 1)
            )
        
        # Output layer
        self.fc = nn.Sequential(
            nn.Linear(hidden_size * self.directions, hidden_size),
            nn.LayerNorm(hidden_size),
            nn.Dropout(dropout),
            nn.ReLU()
        )
        
    def attention_mechanism(self, rnn_output):
        # rnn_output shape: [batch, seq_len, hidden_size * directions]
        attention_weights = self.attention_layer(rnn_output)  # [batch, seq_len, 1]
        attention_weights = F.softmax(attention_weights, dim=1)
        
        # Apply attention weights to RNN outputs
        context_vector = torch.sum(attention_weights * rnn_output, dim=1)  # [batch, hidden_size * directions]
        return context_vector
    
    def forward(self, x, lengths=None):
        batch_size, seq_len, _ = x.shape
        
        # If sequence lengths are provided, pack the sequences
        if lengths is not None:
            packed_input = nn.utils.rnn.pack_padded_sequence(
                x, lengths, batch_first=True, enforce_sorted=False
            )
            packed_output, _ = self.rnn(packed_input)
            output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)
        else:
            output, _ = self.rnn(x)
        
        # Apply attention if enabled
        if self.attention:
            context = self.attention_mechanism(output)
        else:
            # Use the last output from each direction
            if self.bidirectional:
                # Concatenate last outputs from both directions
                context = torch.cat([output[:, -1, :self.hidden_size], output[:, 0, self.hidden_size:]], dim=1)
            else:
                context = output[:, -1, :]
        
        # Final output
        output = self.fc(context)
        return output
```

### Transformer Architecture

Implementation of a modern transformer model with multi-head attention:

```python
class MultiHeadSelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads, dropout=0.1):
        super(MultiHeadSelfAttention, self).__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        
        assert self.head_dim * num_heads == embed_dim, "embed_dim must be divisible by num_heads"
        
        self.query = nn.Linear(embed_dim, embed_dim)
        self.key = nn.Linear(embed_dim, embed_dim)
        self.value = nn.Linear(embed_dim, embed_dim)
        
        self.proj = nn.Linear(embed_dim, embed_dim)
        self.attn_dropout = nn.Dropout(dropout)
        self.proj_dropout = nn.Dropout(dropout)
        
    def forward(self, x, mask=None):
        batch_size, seq_len, _ = x.shape
        
        # Linear projections and reshape for multi-head
        q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        k = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        v = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        # Scaled dot-product attention
        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)
        
        # Apply mask if provided
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # Softmax and dropout
        attn = F.softmax(scores, dim=-1)
        attn = self.attn_dropout(attn)
        
        # Get output
        out = torch.matmul(attn, v)
        
        # Reshape and project
        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)
        out = self.proj(out)
        out = self.proj_dropout(out)
        
        return out


class TransformerBlock(nn.Module):
    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):
        super(TransformerBlock, self).__init__()
        
        self.attn = MultiHeadSelfAttention(embed_dim, num_heads, dropout)
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        
        self.ffn = nn.Sequential(
            nn.Linear(embed_dim, ff_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(ff_dim, embed_dim),
            nn.Dropout(dropout)
        )
        
    def forward(self, x, mask=None):
        # Self attention with residual connection and layer norm
        attn_out = self.attn(x, mask)
        x = self.norm1(x + attn_out)
        
        # Feed forward with residual connection and layer norm
        ffn_out = self.ffn(x)
        x = self.norm2(x + ffn_out)
        
        return x


class TransformerEncoder(nn.Module):
    def __init__(
        self,
        vocab_size,
        max_seq_len,
        embed_dim,
        num_heads,
        ff_dim,
        num_layers,
        dropout=0.1
    ):
        super(TransformerEncoder, self).__init__()
        
        # Token and position embeddings
        self.token_embedding = nn.Embedding(vocab_size, embed_dim)
        self.position_embedding = nn.Parameter(torch.zeros(1, max_seq_len, embed_dim))
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
        
        # Transformer layers
        self.layers = nn.ModuleList([
            TransformerBlock(embed_dim, num_heads, ff_dim, dropout)
            for _ in range(num_layers)
        ])
        
        # Layer normalization
        self.norm = nn.LayerNorm(embed_dim)
        
    def forward(self, x, mask=None):
        seq_len = x.size(1)
        
        # Get token embeddings and add position embeddings
        token_embeds = self.token_embedding(x)
        position_embeds = self.position_embedding[:, :seq_len, :]
        x = token_embeds + position_embeds
        
        # Apply dropout
        x = self.dropout(x)
        
        # Apply transformer blocks
        for layer in self.layers:
            x = layer(x, mask)
        
        # Apply final layer normalization
        x = self.norm(x)
        
        return x
```

## Generative AI Applications

### Advanced Text Generation with Transformers

Modern text generation leveraging advanced transformer architecture:

```python
import os
from dotenv import load_dotenv
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load environment variables
load_dotenv()

class AdvancedTextGenerator:
    def __init__(
        self,
        model_name="meta-llama/Llama-2-7b-hf",
        device=None,
        use_8bit=False,
        use_4bit=False,
    ):
        # Set device
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        
        # Check if HF_TOKEN is available
        if model_name.startswith("meta-llama"):
            hf_token = os.getenv("HF_TOKEN")
            if not hf_token:
                raise ValueError("HuggingFace token not found. Please add HF_TOKEN to your .env file.")
            
            # Load tokenizer and model with HF token
            self.tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)
            
            # Configure quantization options
            kwargs = {}
            if use_8bit:
                kwargs["load_in_8bit"] = True
            elif use_4bit:
                kwargs["load_in_4bit"] = True
                kwargs["bnb_4bit_quant_type"] = "nf4"
                kwargs["bnb_4bit_compute_dtype"] = torch.bfloat16
            
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                token=hf_token,
                device_map="auto" if self.device == "cuda" else None,
                **kwargs
            )
        else:
            # For non-Llama models
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            
            kwargs = {}
            if use_8bit:
                kwargs["load_in_8bit"] = True
            elif use_4bit:
                kwargs["load_in_4bit"] = True
                kwargs["bnb_4bit_quant_type"] = "nf4"
                kwargs["bnb_4bit_compute_dtype"] = torch.bfloat16
                
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                device_map="auto" if self.device == "cuda" else None,
                **kwargs
            )
        
        # Set pad token if needed
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # Move model to device if not using device_map="auto"
        if self.device == "cuda" and not (use_8bit or use_4bit):
            self.model = self.model.to(self.device)
    
    def generate(
        self,
        prompt,
        max_new_tokens=128,
        temperature=0.7,
        top_p=0.9,
        top_k=50,
        repetition_penalty=1.2,
        do_sample=True,
        num_return_sequences=1,
        use_cache=True
    ):
        # Encode prompt
        inputs = self.tokenizer(prompt, return_tensors="pt")
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # Generate text
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                repetition_penalty=repetition_penalty,
                do_sample=do_sample,
                num_return_sequences=num_return_sequences,
                pad_token_id=self.tokenizer.pad_token_id,
                use_cache=use_cache
            )
        
        # Decode outputs
        generated_texts = []
        for output in outputs:
            text = self.tokenizer.decode(output, skip_special_tokens=True)
            # Remove prompt from output
            if text.startswith(prompt):
                text = text[len(prompt):].strip()
            generated_texts.append(text)
        
        return generated_texts if num_return_sequences > 1 else generated_texts[0]
```

### Advanced Image Generation with Diffusion Models

High-quality image synthesis using state-of-the-art diffusion models:

```python
from diffusers import StableDiffusionXLPipeline, AutoencoderKL
import torch
from PIL import Image
import os
from dotenv import load_dotenv
import requests
from io import BytesIO

load_dotenv()

class AdvancedImageGenerator:
    def __init__(
        self,
        model_id="stabilityai/stable-diffusion-xl-base-1.0",
        refiner_id="stabilityai/stable-diffusion-xl-refiner-1.0",
        lora_path=None,
        use_refiner=True,
        use_vae=True,
        vae_id="madebyollin/sdxl-vae-fp16-fix",
        device=None
    ):
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        
        # Get API token if needed
        self.api_token = os.getenv("HF_TOKEN")
        
        # Load VAE for improved performance
        if use_vae:
            self.vae = AutoencoderKL.from_pretrained(
                vae_id, 
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32
            )
        else:
            self.vae = None
        
        # Load base model
        self.pipe = StableDiffusionXLPipeline.from_pretrained(
            model_id,
            vae=self.vae,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            use_safetensors=True,
            variant="fp16" if self.device == "cuda" else None,
            token=self.api_token
        )
        
        # Load refiner if requested
        self.use_refiner = use_refiner
        if use_refiner:
            self.refiner = StableDiffusionXLPipeline.from_pretrained(
                refiner_id,
                vae=self.vae,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                use_safetensors=True,
                variant="fp16" if self.device == "cuda" else None,
                token=self.api_token
            )
            self.refiner = self.refiner.to(self.device)
        
        # Load LoRA weights if provided
        if lora_path:
            self.pipe.load_lora_weights(lora_path)
        
        # Move model to device
        self.pipe = self.pipe.to(self.device)
        
        # Enable attention slicing for lower memory usage
        self.pipe.enable_attention_slicing()
    
    def generate_image(
        self,
        prompt,
        negative_prompt=None,
        height=1024,
        width=1024,
        num_inference_steps=30,
        guidance_scale=7.5,
        strength=0.3,
        seed=None,
        refiner_steps=10,
        output_path=None,
        init_image=None,
    ):
        # Set seed for reproducibility
        if seed is not None:
            torch.manual_seed(seed)
            generator = torch.Generator(device=self.device).manual_seed(seed)
        else:
            generator = None
        
        # Process init_image if provided (for img2img)
        if init_image:
            if isinstance(init_image, str):
                if init_image.startswith('http'):
                    response = requests.get(init_image)
                    init_image = Image.open(BytesIO(response.content)).convert("RGB")
                else:
                    init_image = Image.open(init_image).convert("RGB")
            
            # Resize image
            init_image = init_image.resize((width, height))
        
        # Base generation
        if init_image:
            # Image-to-image
            image = self.pipe(
                prompt=prompt,
                negative_prompt=negative_prompt,
                image=init_image,
                strength=strength,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                generator=generator,
            ).images[0]
        else:
            # Text-to-image
            image = self.pipe(
                prompt=prompt,
                negative_prompt=negative_prompt,
                height=height,
                width=width,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                generator=generator,
            ).images[0]
        
        # Refining
        if self.use_refiner:
            image = self.refiner(
                prompt=prompt,
                negative_prompt=negative_prompt,
                image=image,
                num_inference_steps=refiner_steps,
                guidance_scale=guidance_scale,
                generator=generator,
            ).images[0]
        
        # Save image if path provided
        if output_path:
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            image.save(output_path)
        
        return image
```

### Advanced Audio Generation

State-of-the-art audio synthesis with MusicGen:

```python
import os
import torch
import numpy as np
import librosa
import soundfile as sf
from transformers import MusicgenForConditionalGeneration, AutoProcessor
from dotenv import load_dotenv

load_dotenv()

class AdvancedAudioGenerator:
    def __init__(
        self, 
        model_id="facebook/musicgen-large",
        device=None
    ):
        # Set device
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        
        # Load model and processor
        print(f"Loading model: {model_id}")
        self.processor = AutoProcessor.from_pretrained(model_id)
        
        # Load model with the right configuration
        if self.device == "cuda":
            # Use torch.bfloat16 for GPUs that support it
            dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16
            self.model = MusicgenForConditionalGeneration.from_pretrained(
                model_id, 
                torch_dtype=dtype
            ).to(self.device)
        else:
            # Use regular float32 for CPU
            self.model = MusicgenForConditionalGeneration.from_pretrained(model_id)
        
        self.sampling_rate = self.model.config.audio_encoder.sampling_rate
    
    def generate_audio(
        self,
        prompt,
        duration=10.0,
        num_samples=1,
        guidance_scale=3.0,
        output_path=None,
        seed=None,
        pooling_mode="max"
    ):
        # Set seed for reproducibility
        if seed is not None:
            torch.manual_seed(seed)
            generator = torch.Generator(device=self.device).manual_seed(seed)
        else:
            generator = None
        
        # Process prompt
        inputs = self.processor(
            text=[prompt] * num_samples,
            padding=True,
            return_tensors="pt",
        ).to(self.device)
        
        # Calculate max length based on duration
        max_new_tokens = int(duration * self.sampling_rate / self.model.config.audio_encoder.hop_length)
        
        # Generate audio
        with torch.no_grad():
            generated_audio = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                guidance_scale=guidance_scale,
                generator=generator
            )
        
        # Convert to waveform
        audio_samples = []
        for i in range(generated_audio.shape[0]):
            # Convert from model output to audio waveform
            audio = generated_audio[i].to(torch.float32).cpu().numpy()
            audio_samples.append(audio)
            
            # Save audio if path is provided
            if output_path and i == 0:  # Save only the first sample by default
                os.makedirs(os.path.dirname(output_path), exist_ok=True)
                sf.write(output_path, audio, self.sampling_rate)
        
        return audio_samples if num_samples > 1 else audio_samples[0]
    
    def generate_continuation(
        self,
        input_audio_path,
        prompt,
        continuation_duration=10.0,
        output_path=None,
        guidance_scale=3.0
    ):
        """Generate a continuation from an existing audio file with text prompt guidance"""
        
        # Load input audio
        audio, sr = librosa.load(input_audio_path, sr=self.sampling_rate, mono=True)
        
        # Prepare audio input
        audio_tensor = torch.tensor(audio).unsqueeze(0).to(self.device)
        
        # Process text prompt
        inputs = self.processor(
            text=[prompt],
            padding=True,
            return_tensors="pt",
        ).to(self.device)
        
        # Set maximum new tokens based on continuation duration
        max_new_tokens = int(continuation_duration * self.sampling_rate / self.model.config.audio_encoder.hop_length)
        
        # Generate continuation
        with torch.no_grad():
            generated_audio = self.model.generate(
                **inputs,
                audio=audio_tensor,
                max_new_tokens=max_new_tokens,
                guidance_scale=guidance_scale
            )
        
        # Convert to numpy array
        audio_output = generated_audio[0].to(torch.float32).cpu().numpy()
        
        # Save if path provided
        if output_path:
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            sf.write(output_path, audio_output, self.sampling_rate)
        
        return audio_output
```

## Advanced Fine-tuning and Parameter-Efficient Techniques

### LoRA (Low-Rank Adaptation)

Implementation of LoRA for parameter-efficient fine-tuning:

```python
from peft import (
    get_peft_model,
    LoraConfig,
    TaskType,
    prepare_model_for_kbit_training
)
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
from datasets import load_dataset
import os

def setup_lora_fine_tuning(
    base_model_name,
    dataset_name,
    output_dir="./lora-output",
    lora_r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    learning_rate=2e-4,
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8
):
    # Load tokenizer and model
    tokenizer = AutoTokenizer.from_pretrained(base_model_name)
    
    # Ensure padding token exists
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # Load model
    model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    
    # Configure LoRA
    lora_config = LoraConfig(
        r=lora_r,
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout,
        bias="none",
        task_type=TaskType.CAUSAL_LM,
        target_modules=[
            "q_proj",
            "k_proj",
            "v_proj",
            "o_proj",
            "gate_proj",
            "up_proj",
            "down_proj"
        ]
    )
    
    # Prepare model for LoRA fine-tuning
    model = prepare_model_for_kbit_training(model)
    model = get_peft_model(model, lora_config)
    
    # Print trainable parameters info
    print_trainable_parameters(model)
    
    # Load and prepare dataset
    dataset = load_dataset(dataset_name)
    dataset = prepare_dataset(dataset, tokenizer)
    
    # Set up training arguments
    training_args = TrainingArguments(
        output_dir=output_dir,
        learning_rate=learning_rate,
        num_train_epochs=num_train_epochs,
        per_device_train_batch_size=per_device_train_batch_size,
        gradient_accumulation_steps=gradient_accumulation_steps,
        save_strategy="epoch",
        evaluation_strategy="epoch" if "validation" in dataset else "no",
        logging_steps=50,
        fp16=True,
        optim="paged_adamw_8bit",
        lr_scheduler_type="cosine",
        warmup_ratio=0.05,
    )
    
    # Create trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset["train"],
        eval_dataset=dataset.get("validation", None),
        tokenizer=tokenizer,
        data_collator=lambda data: {'input_ids': torch.stack([torch.LongTensor(f) for f in data])}
    )
    
    return trainer, model, tokenizer

def print_trainable_parameters(model):
    trainable_params = 0
    all_params = 0
    
    for _, param in model.named_parameters():
        all_params += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
            
    print(
        f"Trainable parameters: {trainable_params/1e6:.2f}M ({100 * trainable_params / all_params:.2f}%)"
    )

def prepare_dataset(dataset, tokenizer, max_length=512):
    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            max_length=max_length,
            padding="max_length"
        )
    
    tokenized_dataset = dataset.map(tokenize_function, batched=True)
    return tokenized_dataset
```

### QLoRA Implementation

Advanced implementation of QLoRA for memory-efficient fine-tuning:

```python
from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model
import torch
from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer, TrainingArguments, Trainer
from datasets import load_dataset
import os
from dotenv import load_dotenv

load_dotenv()

def setup_qlora_fine_tuning(
    base_model_name,
    dataset_name_or_path,
    output_dir="./qlora-output",
    lora_r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    learning_rate=2e-4,
    num_train_epochs=3,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=16,
    max_seq_length=2048,
    nf4_config=True
):
    # Check for API token if needed
    if "llama" in base_model_name.lower():
        hf_token = os.getenv("HF_TOKEN")
        if not hf_token:
            raise ValueError("HF_TOKEN not found in .env for accessing Llama models")
        
    # Configure quantization
    compute_dtype = torch.bfloat16
    
    # 4-bit quantization configuration
    if nf4_config:
        # NF4 quantization (better for newer models)
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
            bnb_4bit_compute_dtype=compute_dtype
        )
    else:
        # FP4 quantization
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="fp4",
            bnb_4bit_use_double_quant=True,
            bnb_4bit_compute_dtype=compute_dtype
        )
    
    # Determine if token is needed
    token = os.getenv("HF_TOKEN") if "llama" in base_model_name.lower() else None

    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(base_model_name, token=token)
    
    # Ensure pad token exists
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # Load model with quantization
    model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        token=token,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True
    )
    
    # Prepare model for kbit training
    model = prepare_model_for_kbit_training(model)
    
    # Configure LoRA
    # Automatically detect target modules based on model type
    target_modules = None
    if "llama" in base_model_name.lower():
        target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    elif "mistral" in base_model_name.lower():
        target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    elif "gpt-neox" in base_model_name.lower():
        target_modules = ["query_key_value", "dense", "dense_h_to_4h", "dense_4h_to_h"]
    
    lora_config = LoraConfig(
        r=lora_r,
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout,
        bias="none",
        target_modules=target_modules,
        task_type="CAUSAL_LM"
    )
    
    # Apply LoRA
    model = get_peft_model(model, lora_config)
    
    # Print trainable parameters info
    print_trainable_parameters(model)
    
    # Load dataset
    if os.path.isfile(dataset_name_or_path) or os.path.isdir(dataset_name_or_path):
        # Local dataset
        dataset = load_dataset("json", data_files=dataset_name_or_path, split="train")
        # Split into train/validation
        dataset = dataset.train_test_split(test_size=0.1)
        train_dataset = dataset["train"]
        eval_dataset = dataset["test"]
    else:
        # HuggingFace dataset
        dataset = load_dataset(dataset_name_or_path)
        train_dataset = dataset["train"]
        eval_dataset = dataset.get("validation", None)
    
    # Prepare dataset
    train_dataset = prepare_dataset_for_qlora(train_dataset, tokenizer, max_seq_length)
    if eval_dataset:
        eval_dataset = prepare_dataset_for_qlora(eval_dataset, tokenizer, max_seq_length)
    
    # Training arguments
    training_args = TrainingArguments(
        output_dir=output_dir,
        learning_rate=learning_rate,
        num_train_epochs=num_train_epochs,
        per_device_train_batch_size=per_device_train_batch_size,
        gradient_accumulation_steps=gradient_accumulation_steps,
        save_strategy="epoch",
        evaluation_strategy="epoch" if eval_dataset else "no",
        logging_steps=10,
        save_total_limit=3,
        load_best_model_at_end=True if eval_dataset else False,
        report_to="tensorboard",
        optim="paged_adamw_8bit",
        lr_scheduler_type="cosine",
        warmup_ratio=0.05,
        bf16=torch.cuda.is_available(),
        gradient_checkpointing=True
    )
    
    # Data collator
    from transformers import DataCollatorForLanguageModeling
    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
    
    # Create trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator
    )
    
    return trainer, model, tokenizer

def prepare_dataset_for_qlora(dataset, tokenizer, max_length):
    def tokenize_and_format(examples):
        # Tokenize inputs
        tokenized = tokenizer(
            examples["text"],
            truncation=True,
            max_length=max_length,
            padding="max_length",
            return_tensors="pt"
        )
        
        return tokenized
    
    # Apply preprocessing
    tokenized_dataset = dataset.map(
        tokenize_and_format,
        batched=True,
        remove_columns=dataset.column_names
    )
    
    return tokenized_dataset

def print_trainable_parameters(model):
    trainable_params = 0
    all_params = 0
    
    for _, param in model.named_parameters():
        all_params += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
            
    print(
        f"Trainable parameters: {trainable_params/1e6:.2f}M ({100 * trainable_params / all_params:.2f}%)"
    )
```

### Iterative Training Processes

Advanced curriculum learning framework for progressively training models:

```python
import os
import torch
import json
import numpy as np
from typing import List, Dict, Any, Optional, Union
from dataclasses import dataclass
from transformers import Trainer, TrainingArguments
from peft import PeftModel, PeftConfig

@dataclass
class CurriculumStage:
    """Configuration for a curriculum learning stage"""
    dataset_name_or_path: str
    learning_rate: float
    num_epochs: int
    max_seq_length: int = 512
    batch_size: int = 8
    difficulty_score: float = 0.0  # Lower score means easier examples

class CurriculumTrainer:
    """Advanced curriculum learning trainer for LLMs"""
    
    def __init__(
        self,
        base_model,
        tokenizer,
        stages: List[CurriculumStage],
        output_dir: str = "./curriculum_training",
        gradient_accumulation_steps: int = 4,
        warmup_ratio: float = 0.05,
        evaluation_strategy: str = "steps",
        eval_steps: int = 200,
        save_total_limit: int = 3,
        logging_steps: int = 50,
        save_strategy: str = "steps",
        save_steps: int = 500,
    ):
        self.base_model = base_model
        self.tokenizer = tokenizer
        self.stages = stages
        self.output_dir = output_dir
        self.gradient_accumulation_steps = gradient_accumulation_steps
        self.warmup_ratio = warmup_ratio
        self.evaluation_strategy = evaluation_strategy
        self.eval_steps = eval_steps
        self.save_total_limit = save_total_limit
        self.logging_steps = logging_steps
        self.save_strategy = save_strategy
        self.save_steps = save_steps
        
        # Make sure output directory exists
        os.makedirs(output_dir, exist_ok=True)
        
        # Save training config
        self._save_curriculum_config()
    
    def _save_curriculum_config(self):
        """Save curriculum configuration for reproducibility"""
        config_path = os.path.join(self.output_dir, "curriculum_config.json")
        
        # Convert stages to dict for JSON serialization
        stages_dict = [
            {
                "dataset_name_or_path": stage.dataset_name_or_path,
                "learning_rate": stage.learning_rate,
                "num_epochs": stage.num_epochs,
                "max_seq_length": stage.max_seq_length,
                "batch_size": stage.batch_size,
                "difficulty_score": stage.difficulty_score
            } 
            for stage in self.stages
        ]
        
        config = {
            "stages": stages_dict,
            "gradient_accumulation_steps": self.gradient_accumulation_steps,
            "warmup_ratio": self.warmup_ratio,
            "evaluation_strategy": self.evaluation_strategy,
            "eval_steps": self.eval_steps,
            "save_total_limit": self.save_total_limit
        }
        
        with open(config_path, "w") as f:
            json.dump(config, f, indent=2)
    
    def train(self):
        """Run curriculum learning by training on progressively harder datasets"""
        model = self.base_model
        
        # Sort stages by difficulty (optional)
        sorted_stages = sorted(self.stages, key=lambda x: x.difficulty_score)
        
        from datasets import load_dataset
        from transformers import DataCollatorForLanguageModeling
        
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer, 
            mlm=False
        )
        
        # Train on each stage sequentially
        for i, stage in enumerate(sorted_stages):
            print(f"=== Training Stage {i+1}/{len(sorted_stages)} ===")
            print(f"Dataset: {stage.dataset_name_or_path}")
            print(f"Learning rate: {stage.learning_rate}")
            print(f"Epochs: {stage.num_epochs}")
            
            # Load and prepare dataset for this stage
            if os.path.exists(stage.dataset_name_or_path):
                dataset = load_dataset("json", data_files=stage.dataset_name_or_path)
                # Split dataset into train and validation
                dataset = dataset["train"].train_test_split(test_size=0.1)
                train_dataset = dataset["train"]
                eval_dataset = dataset["test"]
            else:
                dataset = load_dataset(stage.dataset_name_or_path)
                train_dataset = dataset["train"]
                eval_dataset = dataset.get("validation", None)
            
            # Tokenize dataset
            def tokenize_function(examples):
                return self.tokenizer(
                    examples["text"],
                    truncation=True,
                    max_length=stage.max_seq_length,
                    padding="max_length"
                )
            
            train_dataset = train_dataset.map(tokenize_function, batched=True)
            if eval_dataset:
                eval_dataset = eval_dataset.map(tokenize_function, batched=True)
            
            # Configure training arguments for this stage
            training_args = TrainingArguments(
                output_dir=f"{self.output_dir}/stage_{i+1}",
                learning_rate=stage.learning_rate,
                num_train_epochs=stage.num_epochs,
                per_device_train_batch_size=stage.batch_size,
                gradient_accumulation_steps=self.gradient_accumulation_steps,
                evaluation_strategy=self.evaluation_strategy if eval_dataset else "no",
                eval_steps=self.eval_steps if eval_dataset else None,
                save_strategy=self.save_strategy,
                save_steps=self.save_steps,
                save_total_limit=self.save_total_limit,
                logging_steps=self.logging_steps,
                load_best_model_at_end=True if eval_dataset else False,
                report_to="tensorboard",
                lr_scheduler_type="cosine",
                warmup_ratio=self.warmup_ratio,
                gradient_checkpointing=True,
                bf16=torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8,
                fp16=torch.cuda.is_available() and torch.cuda.get_device_capability()[0] < 8,
            )
            
            # Initialize trainer
            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=eval_dataset,
                tokenizer=self.tokenizer,
                data_collator=data_collator
            )
            
            # Train for this stage
            trainer.train()
            
            # Save checkpoint after each stage
            model_save_path = f"{self.output_dir}/stage_{i+1}_final"
            trainer.save_model(model_save_path)
            self.tokenizer.save_pretrained(model_save_path)
            
            # Update model reference for next stage
            model = trainer.model
        
        # Return the final model
        return model
```

## Conclusion

Neural networks and generative AI represent a rapidly evolving field with increasingly sophisticated architectures and methodologies. From the fundamental structures of feedforward networks to the complex attention mechanisms in transformers, we've explored how these systems learn patterns from data and generate new content. The modern landscape features advanced techniques like LoRA and QLoRA that dramatically reduce computational requirements while maintaining model quality, enabling fine-tuning of large models on consumer hardware.

Generative AI applications have revolutionized content creation across modalities, from text generation with transformer-based LLMs to high-fidelity image synthesis with diffusion models and sophisticated audio generation with specialized neural networks. These systems demonstrate remarkable capabilities in understanding context, following nuanced instructions, and producing coherent outputs that increasingly resemble human-created content.

Parameter-efficient training techniques have democratized access to state-of-the-art AI, allowing domain adaptation of large pre-trained models with minimal resources. The iterative training processes we've examined provide frameworks for progressively improving model capabilities through carefully structured learning curricula.

As these technologies continue to evolve, they'll enable increasingly sophisticated applications across industries, from creative tools for artists to specialized systems for scientific research and business analytics. Understanding the fundamental principles, architectures, and optimization techniques presented here provides a solid foundation for developing and deploying cutting-edge AI systems.