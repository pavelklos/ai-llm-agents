<small>Claude 3.7 Sonnet Thinking</small>
# 10. RL Agent - Practical Project

## Key Terms

- **Trading Bot**: An automated system that executes trades in financial markets based on predefined strategies.
- **Action Space**: The set of possible actions the trading agent can take (buy, sell, hold).
- **Market State**: The representation of financial market conditions at a given time.
- **Technical Indicators**: Mathematical calculations based on price and/or volume that help analyze market trends.
- **Reward Function**: The signal that guides the RL agent's learning, typically based on profit/loss in trading.
- **Risk-Adjusted Return**: Performance metric that accounts for both returns and risks taken.
- **Drawdown**: The measure of decline from a peak to a trough in portfolio value.
- **Overfitting**: When a model learns patterns specific to training data that don't generalize to new data.
- **Walk-Forward Testing**: A backtesting method where the model is periodically retrained on expanding windows of data.
- **Sharpe Ratio**: A measure of risk-adjusted return that considers volatility of returns.

## Building a Financial Trading Bot with RL

We'll develop a trading bot using Reinforcement Learning that can learn to make profitable trading decisions in financial markets. We'll focus on creating a practical implementation using modern RL techniques.

### Setting Up the Environment

First, let's set up our financial trading environment:

```python
import numpy as np
import pandas as pd
import gymnasium as gym
from gymnasium import spaces
from typing import Dict, List, Tuple, Optional, Any
import matplotlib.pyplot as plt
from dotenv import load_dotenv
import os

# Load environment variables
load_dotenv()

class TradingEnvironment(gym.Env):
    """
    A simulated trading environment for reinforcement learning
    """
    
    def __init__(
        self,
        df: pd.DataFrame,
        window_size: int = 30,
        initial_balance: float = 10000.0,
        transaction_fee: float = 0.001  # 0.1% transaction fee
    ):
        super(TradingEnvironment, self).__init__()
        
        self.df = df
        self.window_size = window_size
        self.initial_balance = initial_balance
        self.transaction_fee = transaction_fee
        
        # Current step in the data
        self.current_step = 0
        
        # Action space: 0 = Hold, 1 = Buy, 2 = Sell
        self.action_space = spaces.Discrete(3)
        
        # Observation space: price data + technical indicators + account info
        # We'll use 5 price points (OHLCV) + 5 technical indicators + 3 account state variables
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(self.window_size, 13), dtype=np.float32
        )
        
        # Initialize state
        self.reset()
    
    def _calculate_reward(self, action: int) -> float:
        """
        Calculate the reward based on action and resulting portfolio change
        """
        prev_portfolio_value = self.portfolio_value
        
        # Update portfolio value after action
        self._update_portfolio(action)
        
        # Calculate reward as the change in portfolio value
        reward = self.portfolio_value - prev_portfolio_value
        
        # Apply penalties for excessive trading or risky behavior
        if action != 0:  # If not holding
            # Small penalty for transaction cost
            reward -= self.transaction_fee * abs(self.shares_owned * self.current_price)
            
        # Reward for risk-adjusted return
        if self.portfolio_value > prev_portfolio_value and self.portfolio_value > self.max_portfolio_value:
            # Bonus for new high
            reward += 1.0
            
        # Penalty for drawdown
        max_drawdown = (self.max_portfolio_value - self.portfolio_value) / self.max_portfolio_value
        if max_drawdown > 0.1:  # More than 10% drawdown
            reward -= 5.0 * max_drawdown
        
        return reward
    
    def _update_portfolio(self, action: int) -> None:
        """
        Update portfolio based on action
        """
        current_price = self.df.iloc[self.current_step]['Close']
        self.current_price = current_price
        
        if action == 1:  # Buy
            # Calculate maximum shares we can buy
            max_shares = self.balance / (current_price * (1 + self.transaction_fee))
            shares_to_buy = max_shares  # Buy all we can
            
            # Update balance and shares
            cost = shares_to_buy * current_price * (1 + self.transaction_fee)
            self.balance -= cost
            self.shares_owned += shares_to_buy
            
            # Log transaction
            self.trades.append({
                'step': self.current_step,
                'type': 'buy',
                'shares': shares_to_buy,
                'price': current_price,
                'cost': cost
            })
            
        elif action == 2:  # Sell
            if self.shares_owned > 0:
                # Sell all shares
                proceeds = self.shares_owned * current_price * (1 - self.transaction_fee)
                self.balance += proceeds
                
                # Log transaction
                self.trades.append({
                    'step': self.current_step,
                    'type': 'sell',
                    'shares': self.shares_owned,
                    'price': current_price,
                    'proceeds': proceeds
                })
                
                self.shares_owned = 0
        
        # Update portfolio value
        self.portfolio_value = self.balance + (self.shares_owned * current_price)
        
        # Update maximum portfolio value
        if self.portfolio_value > self.max_portfolio_value:
            self.max_portfolio_value = self.portfolio_value
    
    def _get_observation(self) -> np.ndarray:
        """
        Get current state observation
        """
        # Get price data for the current window
        start_idx = max(0, self.current_step - self.window_size + 1)
        end_idx = self.current_step + 1
        
        # If we don't have enough data, pad with zeros
        if end_idx - start_idx < self.window_size:
            padding = self.window_size - (end_idx - start_idx)
            price_data = np.zeros((self.window_size, 5))
            price_data[padding:] = self.df.iloc[start_idx:end_idx][['Open', 'High', 'Low', 'Close', 'Volume']].values
        else:
            price_data = self.df.iloc[start_idx:end_idx][['Open', 'High', 'Low', 'Close', 'Volume']].values
        
        # Normalize price data
        price_mean = np.mean(price_data[:, :4], axis=0)
        price_std = np.std(price_data[:, :4], axis=0) + 1e-10
        normalized_prices = price_data.copy()
        normalized_prices[:, :4] = (price_data[:, :4] - price_mean) / price_std
        
        # Normalize volume data separately
        volume_mean = np.mean(price_data[:, 4])
        volume_std = np.std(price_data[:, 4]) + 1e-10
        normalized_prices[:, 4] = (price_data[:, 4] - volume_mean) / volume_std
        
        # Calculate technical indicators (simplified version)
        # Here we just create some dummy indicators for the example
        indicators = np.zeros((self.window_size, 5))
        
        # Add account state info
        account_info = np.zeros((self.window_size, 3))
        account_info[:, 0] = self.balance / self.initial_balance  # Normalized balance
        account_info[:, 1] = self.shares_owned * self.current_price / self.initial_balance  # Normalized position
        account_info[:, 2] = self.portfolio_value / self.initial_balance  # Normalized portfolio value
        
        # Combine everything
        observation = np.concatenate(
            [normalized_prices, indicators, account_info], axis=1
        )
        
        return observation.astype(np.float32)
    
    def reset(self, seed=None, options=None) -> Tuple[np.ndarray, Dict]:
        """
        Reset the environment
        """
        super().reset(seed=seed)
        
        # Reset position
        self.balance = self.initial_balance
        self.shares_owned = 0
        self.portfolio_value = self.initial_balance
        self.max_portfolio_value = self.initial_balance
        self.trades = []
        
        # Set to a random point in the data to encourage exploration
        # Keep enough buffer for the window size
        self.current_step = self.window_size - 1
        if len(self.df) > self.window_size + 100:  # If we have enough data
            self.current_step = np.random.randint(self.window_size - 1, len(self.df) - 100)
            
        self.current_price = self.df.iloc[self.current_step]['Close']
        
        return self._get_observation(), {}
    
    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict]:
        """
        Take a step in the environment
        """
        # Calculate reward based on action
        reward = self._calculate_reward(action)
        
        # Move to the next step
        self.current_step += 1
        done = self.current_step >= len(self.df) - 1
        
        # Get new observation
        observation = self._get_observation()
        
        # Additional info
        info = {
            'portfolio_value': self.portfolio_value,
            'balance': self.balance,
            'shares': self.shares_owned,
            'current_price': self.current_price
        }
        
        return observation, reward, done, False, info
    
    def render(self):
        """
        Render the environment
        """
        print(f"Step: {self.current_step}")
        print(f"Balance: ${self.balance:.2f}")
        print(f"Shares: {self.shares_owned}")
        print(f"Portfolio Value: ${self.portfolio_value:.2f}")
        print(f"Current Close: ${self.current_price:.2f}")
        
    def plot_performance(self, show_trades: bool = True):
        """
        Plot the performance of the agent
        """
        # Get price data
        prices = self.df['Close'].values
        
        # Create a figure
        fig, ax = plt.subplots(figsize=(14, 7))
        
        # Plot prices
        ax.plot(prices, color='blue', alpha=0.6, label='Close Price')
        
        # Plot buy and sell signals if trades were made
        if show_trades and self.trades:
            # Extract buy and sell points
            buys = [(t['step'], self.df.iloc[t['step']]['Close']) 
                    for t in self.trades if t['type'] == 'buy']
            sells = [(t['step'], self.df.iloc[t['step']]['Close']) 
                     for t in self.trades if t['type'] == 'sell']
            
            # Plot buy and sell points
            if buys:
                buy_x, buy_y = zip(*buys)
                ax.scatter(buy_x, buy_y, marker='^', color='green', label='Buy', s=100)
            if sells:
                sell_x, sell_y = zip(*sells)
                ax.scatter(sell_x, sell_y, marker='v', color='red', label='Sell', s=100)
        
        # Add labels and legend
        ax.set_title('Trading Performance')
        ax.set_xlabel('Time Steps')
        ax.set_ylabel('Price')
        ax.legend()
        
        # Show the plot
        plt.tight_layout()
        plt.show()
        
        return fig, ax
```

### Data Loading and Preprocessing

Now, let's create a module for loading and preprocessing financial data:

```python
import pandas as pd
import numpy as np
import yfinance as yf
from typing import List, Optional, Tuple
import ta  # Technical analysis library
from dotenv import load_dotenv
import os

# Load environment variables
load_dotenv()

class DataLoader:
    """
    Load and preprocess financial data for the trading bot
    """
    
    def __init__(self, cache_dir: str = "./data_cache"):
        """
        Initialize DataLoader
        
        Args:
            cache_dir: Directory to cache downloaded data
        """
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
    
    def load_data(
        self, 
        ticker: str, 
        start_date: str, 
        end_date: str, 
        interval: str = "1d",
        use_cache: bool = True
    ) -> pd.DataFrame:
        """
        Load financial data for a given ticker
        
        Args:
            ticker: Stock ticker symbol
            start_date: Start date in YYYY-MM-DD format
            end_date: End date in YYYY-MM-DD format
            interval: Data frequency ('1d', '1h', etc.)
            use_cache: Whether to use cached data if available
        
        Returns:
            DataFrame with OHLCV and technical indicators
        """
        # Check if data is in cache
        cache_file = os.path.join(self.cache_dir, f"{ticker}_{start_date}_{end_date}_{interval}.csv")
        
        if use_cache and os.path.exists(cache_file):
            print(f"Loading cached data for {ticker}")
            df = pd.read_csv(cache_file)
            df['Date'] = pd.to_datetime(df['Date'])
            df.set_index('Date', inplace=True)
            return df
        
        # Download data if not in cache
        print(f"Downloading data for {ticker}")
        df = yf.download(ticker, start=start_date, end=end_date, interval=interval)
        
        # Add technical indicators
        df = self.add_indicators(df)
        
        # Save to cache
        df.reset_index().to_csv(cache_file, index=False)
        
        return df
    
    def add_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Add technical indicators to the dataframe
        
        Args:
            df: DataFrame with OHLCV data
        
        Returns:
            DataFrame with added technical indicators
        """
        # Make a copy to avoid modifying the original
        df = df.copy()
        
        # Add Simple Moving Averages
        df['SMA20'] = ta.trend.sma_indicator(df['Close'], window=20)
        df['SMA50'] = ta.trend.sma_indicator(df['Close'], window=50)
        df['SMA200'] = ta.trend.sma_indicator(df['Close'], window=200)
        
        # Add Relative Strength Index
        df['RSI'] = ta.momentum.rsi(df['Close'], window=14)
        
        # Add Bollinger Bands
        bollinger = ta.volatility.BollingerBands(df['Close'], window=20, window_dev=2)
        df['BB_High'] = bollinger.bollinger_hband()
        df['BB_Low'] = bollinger.bollinger_lband()
        df['BB_Mid'] = bollinger.bollinger_mavg()
        
        # Add MACD
        macd = ta.trend.MACD(df['Close'])
        df['MACD'] = macd.macd()
        df['MACD_Signal'] = macd.macd_signal()
        df['MACD_Hist'] = macd.macd_diff()
        
        # Add Average True Range (ATR)
        df['ATR'] = ta.volatility.average_true_range(df['High'], df['Low'], df['Close'])
        
        # Add On-Balance Volume
        df['OBV'] = ta.volume.on_balance_volume(df['Close'], df['Volume'])
        
        # Drop NaN values
        df.dropna(inplace=True)
        
        return df
    
    def train_test_split(
        self, 
        df: pd.DataFrame, 
        train_ratio: float = 0.8
    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        Split data into training and testing sets
        
        Args:
            df: DataFrame to split
            train_ratio: Ratio for training data
            
        Returns:
            Tuple of (train_df, test_df)
        """
        # Calculate split point
        split_idx = int(len(df) * train_ratio)
        
        # Split data
        train_df = df.iloc[:split_idx].copy()
        test_df = df.iloc[split_idx:].copy()
        
        return train_df, test_df
```

### RL Agent for Trading

Now, let's implement an RL agent for trading using Proximal Policy Optimization (PPO):

```python
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
from typing import Dict, List, Tuple, Optional, Any
import gymnasium as gym
import os
import random
from collections import deque
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class ActorNetwork(nn.Module):
    """Policy network for the trading agent"""
    
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(ActorNetwork, self).__init__()
        
        # LSTM layer to process sequential data
        self.lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            batch_first=True
        )
        
        # Policy head
        self.policy = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim),
            nn.Softmax(dim=-1)
        )
        
    def forward(self, x):
        """Forward pass through the network"""
        # LSTM expects input of shape (batch_size, seq_len, input_dim)
        if len(x.shape) == 2:
            x = x.unsqueeze(0)  # Add batch dimension if missing
            
        # Process through LSTM
        lstm_out, _ = self.lstm(x)
        
        # Get last time step output
        lstm_out = lstm_out[:, -1, :]
        
        # Get action probabilities
        action_probs = self.policy(lstm_out)
        
        return action_probs

class CriticNetwork(nn.Module):
    """Value network for the trading agent"""
    
    def __init__(self, input_dim, hidden_dim):
        super(CriticNetwork, self).__init__()
        
        # LSTM layer to process sequential data
        self.lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            batch_first=True
        )
        
        # Value head
        self.value = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
    def forward(self, x):
        """Forward pass through the network"""
        # LSTM expects input of shape (batch_size, seq_len, input_dim)
        if len(x.shape) == 2:
            x = x.unsqueeze(0)  # Add batch dimension if missing
            
        # Process through LSTM
        lstm_out, _ = self.lstm(x)
        
        # Get last time step output
        lstm_out = lstm_out[:, -1, :]
        
        # Get state value
        value = self.value(lstm_out)
        
        return value

class TradingPPOAgent:
    """Trading agent using Proximal Policy Optimization"""
    
    def __init__(
        self,
        state_dim: int,
        action_dim: int,
        hidden_dim: int = 128,
        lr_actor: float = 0.0003,
        lr_critic: float = 0.001,
        gamma: float = 0.99,
        gae_lambda: float = 0.95,
        clip_param: float = 0.2,
        batch_size: int = 64,
        epochs: int = 10,
        value_coef: float = 0.5,
        entropy_coef: float = 0.01,
        max_grad_norm: float = 0.5
    ):
        """
        Initialize the trading agent
        """
        # Environment dimensions
        self.state_dim = state_dim
        self.action_dim = action_dim
        
        # Hyperparameters
        self.hidden_dim = hidden_dim
        self.lr_actor = lr_actor
        self.lr_critic = lr_critic
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.clip_param = clip_param
        self.batch_size = batch_size
        self.epochs = epochs
        self.value_coef = value_coef
        self.entropy_coef = entropy_coef
        self.max_grad_norm = max_grad_norm
        
        # Initialize actor and critic networks
        self.actor = ActorNetwork(state_dim, hidden_dim, action_dim).to(device)
        self.critic = CriticNetwork(state_dim, hidden_dim).to(device)
        
        # Initialize optimizers
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)
        
        # Memory buffers
        self.reset_memory()
    
    def reset_memory(self):
        """Reset memory buffers"""
        self.states = []
        self.actions = []
        self.rewards = []
        self.values = []
        self.log_probs = []
        self.dones = []
    
    def select_action(self, state, training=True):
        """Select an action based on the current state"""
        state_tensor = torch.FloatTensor(state).to(device)
        
        # Get action probabilities and state value
        with torch.no_grad():
            action_probs = self.actor(state_tensor)
            value = self.critic(state_tensor)
        
        # During training, sample from the distribution
        if training:
            dist = Categorical(action_probs)
            action = dist.sample()
            log_prob = dist.log_prob(action)
            
            # Store for training
            self.states.append(state)
            self.actions.append(action.item())
            self.values.append(value.item())
            self.log_probs.append(log_prob.item())
            
            return action.item()
        
        # During evaluation, take the most probable action
        else:
            return torch.argmax(action_probs).item()
    
    def update(self):
        """Update the policy and value networks"""
        # Convert lists to tensors
        states = torch.FloatTensor(np.array(self.states)).to(device)
        actions = torch.LongTensor(self.actions).to(device)
        rewards = torch.FloatTensor(self.rewards).to(device)
        dones = torch.FloatTensor(self.dones).to(device)
        old_log_probs = torch.FloatTensor(self.log_probs).to(device)
        
        # Compute returns and advantages
        returns = self._compute_returns(rewards, dones)
        advantages = self._compute_advantages(returns)
        
        # Normalize advantages
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # PPO update
        for _ in range(self.epochs):
            # Get mini-batches
            batch_indices = np.random.permutation(len(states))
            
            for start_idx in range(0, len(states), self.batch_size):
                # Get batch indices
                batch_idx = batch_indices[start_idx:start_idx + self.batch_size]
                
                # Get batch data
                batch_states = states[batch_idx]
                batch_actions = actions[batch_idx]
                batch_old_log_probs = old_log_probs[batch_idx]
                batch_returns = returns[batch_idx]
                batch_advantages = advantages[batch_idx]
                
                # Get current policy outputs
                action_probs = self.actor(batch_states)
                values = self.critic(batch_states).squeeze(-1)
                
                # Create distribution
                dist = Categorical(action_probs)
                
                # Get new log probs
                new_log_probs = dist.log_prob(batch_actions)
                
                # Calculate ratios
                ratios = torch.exp(new_log_probs - batch_old_log_probs)
                
                # Calculate surrogate losses
                surr1 = ratios * batch_advantages
                surr2 = torch.clamp(ratios, 1 - self.clip_param, 1 + self.clip_param) * batch_advantages
                
                # Calculate actor loss
                actor_loss = -torch.min(surr1, surr2).mean()
                
                # Calculate critic loss
                critic_loss = self.value_coef * nn.MSELoss()(values, batch_returns)
                
                # Calculate entropy bonus
                entropy = self.entropy_coef * dist.entropy().mean()
                
                # Total loss
                total_loss = actor_loss + critic_loss - entropy
                
                # Update networks
                self.actor_optimizer.zero_grad()
                self.critic_optimizer.zero_grad()
                total_loss.backward()
                
                # Clip gradients
                nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)
                nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)
                
                self.actor_optimizer.step()
                self.critic_optimizer.step()
        
        # Reset memory
        self.reset_memory()
    
    def _compute_returns(self, rewards, dones):
        """Compute returns using Generalized Advantage Estimation (GAE)"""
        returns = torch.zeros_like(rewards)
        advantages = torch.zeros_like(rewards)
        
        last_value = 0
        last_advantage = 0
        
        for t in reversed(range(len(rewards))):
            # If this is the last step of the episode
            if t == len(rewards) - 1 or dones[t]:
                next_value = 0
            else:
                next_value = self.values[t + 1]
            
            # TD error
            delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - self.values[t]
            
            # GAE
            advantages[t] = last_advantage = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * last_advantage
            
            # Returns
            returns[t] = advantages[t] + self.values[t]
        
        return returns
    
    def _compute_advantages(self, returns):
        """Compute advantages"""
        values = torch.FloatTensor(self.values).to(device)
        advantages = returns - values
        return advantages
    
    def save(self, path):
        """Save the model"""
        torch.save({
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.critic_optimizer.state_dict()
        }, path)
    
    def load(self, path):
        """Load the model"""
        checkpoint = torch.load(path)
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])
```

### Main Training Script

Now, let's create a script to train and evaluate our trading bot:

```python
import numpy as np
import pandas as pd
import torch
import matplotlib.pyplot as plt
import os
from dotenv import load_dotenv
from typing import Dict, List, Tuple
import argparse
from datetime import datetime
import json

from data_loader import DataLoader
from trading_environment import TradingEnvironment
from trading_agent import TradingPPOAgent

# Load environment variables
load_dotenv()

def train_trading_bot(
    ticker: str = "AAPL",
    start_date: str = "2018-01-01",
    end_date: str = "2023-01-01",
    window_size: int = 30,
    episodes: int = 200,
    initial_balance: float = 10000.0,
    batch_size: int = 128,
    log_dir: str = "./logs",
    model_dir: str = "./models",
):
    """
    Train a trading bot on historical data
    """
    # Create directories
    os.makedirs(log_dir, exist_ok=True)
    os.makedirs(model_dir, exist_ok=True)
    
    # Load and preprocess data
    data_loader = DataLoader()
    df = data_loader.load_data(ticker, start_date, end_date)
    
    # Split data
    train_df, test_df = data_loader.train_test_split(df)
    print(f"Training data: {len(train_df)} days")
    print(f"Testing data: {len(test_df)} days")
    
    # Create environment
    env = TradingEnvironment(
        train_df,
        window_size=window_size,
        initial_balance=initial_balance
    )
    
    # Initialize agent
    state_dim = env.observation_space.shape[1]  # Features per time step
    action_dim = env.action_space.n
    
    agent = TradingPPOAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        hidden_dim=128,
        lr_actor=0.0003,
        lr_critic=0.001,
        gamma=0.99,
        batch_size=batch_size
    )
    
    # Training loop
    best_reward = -np.inf
    rewards = []
    portfolio_values = []
    
    for episode in range(episodes):
        state, _ = env.reset()
        episode_reward = 0
        done = False
        
        # Track portfolio value over time
        episode_portfolio_values = [env.portfolio_value]
        
        while not done:
            # Select action
            action = agent.select_action(state)
            
            # Take action
            next_state, reward, done, _, info = env.step(action)
            
            # Store reward
            episode_reward += reward
            
            # Store step information
            agent.rewards.append(reward)
            agent.dones.append(done)
            
            # Track portfolio value
            episode_portfolio_values.append(info['portfolio_value'])
            
            # Update state
            state = next_state
        
        # Update the agent
        agent.update()
        
        # Track rewards and portfolio values
        rewards.append(episode_reward)
        portfolio_values.append(episode_portfolio_values[-1])
        
        # Print progress
        if (episode + 1) % 10 == 0:
            avg_reward = np.mean(rewards[-10:])
            avg_portfolio = np.mean(portfolio_values[-10:])
            
            print(f"Episode {episode+1}/{episodes} | "
                  f"Reward: {episode_reward:.2f} | "
                  f"Avg Reward: {avg_reward:.2f} | "
                  f"Portfolio: ${episode_portfolio_values[-1]:.2f} | "
                  f"Avg Portfolio: ${avg_portfolio:.2f}")
        
        # Save best model
        if episode_reward > best_reward:
            best_reward = episode_reward
            agent.save(os.path.join(model_dir, f"{ticker}_best.pth"))
        
        # Save checkpoint
        if (episode + 1) % 50 == 0:
            agent.save(os.path.join(model_dir, f"{ticker}_episode_{episode+1}.pth"))
    
    # Save final model
    agent.save(os.path.join(model_dir, f"{ticker}_final.pth"))
    
    # Plot training performance
    plt.figure(figsize=(14, 7))
    plt.subplot(2, 1, 1)
    plt.plot(rewards)
    plt.title('Episode Rewards')
    plt.xlabel('Episode')
    plt.ylabel('Reward')
    
    plt.subplot(2, 1, 2)
    plt.plot(portfolio_values)
    plt.axhline(y=initial_balance, color='r', linestyle='--', label='Initial Balance')
    plt.title('Final Portfolio Value')
    plt.xlabel('Episode')
    plt.ylabel('Portfolio Value ($)')
    plt.legend()
    
    plt.tight_layout()
    plt.savefig(os.path.join(log_dir, f"{ticker}_training_performance.png"))
    
    # Evaluate on test data
    test_env = TradingEnvironment(
        test_df,
        window_size=window_size,
        initial_balance=initial_balance
    )
    
    state, _ = test_env.reset()
    done = False
    
    # Load best model
    agent.load(os.path.join(model_dir, f"{ticker}_best.pth"))
    
    while not done:
        # Select action (no exploration)
        action = agent.select_action(state, training=False)
        
        # Take action
        next_state, _, done, _, _ = test_env.step(action)
        
        # Update state
        state = next_state
    
    # Plot test performance
    test_env.plot_performance()
    plt.savefig(os.path.join(log_dir, f"{ticker}_test_performance.png"))
    
    # Calculate metrics
    initial_price = test_df.iloc[0]['Close']
    final_price = test_df.iloc[-1]['Close']
    price_return = (final_price - initial_price) / initial_price
    
    bot_return = (test_env.portfolio_value - initial_balance) / initial_balance
    
    print(f"\nEvaluation on test data ({ticker}):")
    print(f"Buy and Hold Return: {price_return:.2%}")
    print(f"Trading Bot Return: {bot_return:.2%}")
    print(f"Initial Balance: ${initial_balance:.2f}")
    print(f"Final Portfolio Value: ${test_env.portfolio_value:.2f}")
    print(f"Number of Trades: {len(test_env.trades)}")
    
    # Save metrics
    metrics = {
        "ticker": ticker,
        "start_date": start_date,
        "end_date": end_date,
        "test_start": test_df.index[0].strftime("%Y-%m-%d"),
        "test_end": test_df.index[-1].strftime("%Y-%m-%d"),
        "buy_and_hold_return": float(price_return),
        "bot_return": float(bot_return),
        "initial_balance": float(initial_balance),
        "final_portfolio": float(test_env.portfolio_value),
        "num_trades": len(test_env.trades),
        "sharpe_ratio": calculate_sharpe_ratio(portfolio_values),
        "max_drawdown": calculate_max_drawdown(portfolio_values)
    }
    
    with open(os.path.join(log_dir, f"{ticker}_metrics.json"), "w") as f:
        json.dump(metrics, f, indent=4)
    
    return agent, metrics

def calculate_sharpe_ratio(portfolio_values, risk_free_rate=0.0):
    """Calculate Sharpe ratio"""
    if len(portfolio_values) < 2:
        return 0.0
        
    # Calculate returns
    returns = np.array([(portfolio_values[i] - portfolio_values[i-1]) / portfolio_values[i-1] 
                        for i in range(1, len(portfolio_values))])
    
    # Sharpe ratio
    excess_returns = returns - risk_free_rate
    if np.std(excess_returns) == 0:
        return 0.0
        
    sharpe = np.mean(excess_returns) / np.std(excess_returns)
    
    # Annualized (assuming daily returns)
    sharpe_annualized = sharpe * np.sqrt(252)
    
    return float(sharpe_annualized)

def calculate_max_drawdown(portfolio_values):
    """Calculate maximum drawdown"""
    peak = portfolio_values[0]
    max_dd = 0.0
    
    for value in portfolio_values:
        if value > peak:
            peak = value
        
        dd = (peak - value) / peak
        if dd > max_dd:
            max_dd = dd
    
    return float(max_dd)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Train a trading bot")
    parser.add_argument("--ticker", type=str, default="AAPL", help="Stock ticker symbol")
    parser.add_argument("--start", type=str, default="2018-01-01", help="Start date (YYYY-MM-DD)")
    parser.add_argument("--end", type=str, default="2023-01-01", help="End date (YYYY-MM-DD)")
    parser.add_argument("--episodes", type=int, default=200, help="Number of training episodes")
    parser.add_argument("--balance", type=float, default=10000.0, help="Initial balance")
    args = parser.parse_args()
    
    agent, metrics = train_trading_bot(
        ticker=args.ticker,
        start_date=args.start,
        end_date=args.end,
        episodes=args.episodes,
        initial_balance=args.balance
    )
```

### Hyperparameter Tuning

Let's create a script for hyperparameter tuning using Optuna:

```python
import optuna
import numpy as np
import pandas as pd
import os
from dotenv import load_dotenv
from typing import Dict, List, Tuple
import json
from datetime import datetime

from data_loader import DataLoader
from trading_environment import TradingEnvironment
from trading_agent import TradingPPOAgent

# Load environment variables
load_dotenv()

def objective(trial):
    """Objective function for Optuna optimization"""
    # Define hyperparameter search space
    lr_actor = trial.suggest_float("lr_actor", 1e-5, 1e-3, log=True)
    lr_critic = trial.suggest_float("lr_critic", 1e-5, 1e-3, log=True)
    hidden_dim = trial.suggest_categorical("hidden_dim", [64, 128, 256])
    gamma = trial.suggest_float("gamma", 0.9, 0.999)
    gae_lambda = trial.suggest_float("gae_lambda", 0.9, 0.99)
    clip_param = trial.suggest_float("clip_param", 0.1, 0.3)
    batch_size = trial.suggest_categorical("batch_size", [32, 64, 128])
    value_coef = trial.suggest_float("value_coef", 0.1, 1.0)
    entropy_coef = trial.suggest_float("entropy_coef", 0.001, 0.1, log=True)
    
    # Fixed parameters
    ticker = "AAPL"
    start_date = "2018-01-01"
    end_date = "2023-01-01"
    window_size = 30
    initial_balance = 10000.0
    episodes = 50  # Reduced episodes for faster tuning
    
    # Load data
    data_loader = DataLoader()
    df = data_loader.load_data(ticker, start_date, end_date)
    
    # Split data
    train_df, val_df = data_loader.train_test_split(df, train_ratio=0.7)
    val_df, _ = data_loader.train_test_split(val_df, train_ratio=0.5)
    
    # Create environment
    env = TradingEnvironment(
        train_df,
        window_size=window_size,
        initial_balance=initial_balance
    )
    
    # Initialize agent
    state_dim = env.observation_space.shape[1]  # Features per time step
    action_dim = env.action_space.n
    
    agent = TradingPPOAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        hidden_dim=hidden_dim,
        lr_actor=lr_actor,
        lr_critic=lr_critic,
        gamma=gamma,
        gae_lambda=gae_lambda,
        clip_param=clip_param,
        batch_size=batch_size,
        value_coef=value_coef,
        entropy_coef=entropy_coef
    )
    
    # Training loop
    for episode in range(episodes):
        state, _ = env.reset()
        done = False
        
        while not done:
            # Select action
            action = agent.select_action(state)
            
            # Take action
            next_state, reward, done, _, _ = env.step(action)
            
            # Store reward
            agent.rewards.append(reward)
            agent.dones.append(done)
            
            # Update state
            state = next_state
        
        # Update the agent
        agent.update()
        
        # Report intermediate values for pruning
        if episode % 5 == 0:
            # Evaluate on validation set
            val_env = TradingEnvironment(
                val_df,
                window_size=window_size,
                initial_balance=initial_balance
            )
            
            state, _ = val_env.reset()
            done = False
            
            while not done:
                # Select action (no exploration)
                action = agent.select_action(state, training=False)
                
                # Take action
                next_state, _, done, _, _ = val_env.step(action)
                
                # Update state
                state = next_state
            
            # Get validation performance
            val_return = (val_env.portfolio_value - initial_balance) / initial_balance
            
            # Report for pruning
            trial.report(val_return, episode)
            
            # Handle pruning
            if trial.should_prune():
                raise optuna.exceptions.TrialPruned()
    
    # Final evaluation on validation set
    val_env = TradingEnvironment(
        val_df,
        window_size=window_size,
        initial_balance=initial_balance
    )
    
    state, _ = val_env.reset()
    done = False
    
    while not done:
        # Select action (no exploration)
        action = agent.select_action(state, training=False)
        
        # Take action
        next_state, _, done, _, _ = val_env.step(action)
        
        # Update state
        state = next_state
    
    # Calculate return
    val_return = (val_env.portfolio_value - initial_balance) / initial_balance
    
    # Calculate other metrics
    num_trades = len(val_env.trades)
    
    # We want to maximize return
    return val_return

def run_hyperparameter_tuning(n_trials=50, study_name="trading_bot_optimization"):
    """Run hyperparameter tuning using Optuna"""
    # Create study
    study = optuna.create_study(
        direction="maximize", 
        study_name=study_name,
        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5)
    )
    
    # Run optimization
    study.optimize(objective, n_trials=n_trials)
    
    # Print results
    print("Best trial:")
    trial = study.best_trial
    
    print(f"  Value: {trial.value}")
    print("  Params: ")
    for key, value in trial.params.items():
        print(f"    {key}: {value}")
    
    # Save results
    os.makedirs("./tuning_results", exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    with open(f"./tuning_results/best_params_{timestamp}.json", "w") as f:
        json.dump(trial.params, f, indent=4)
    
    # Save study
    try:
        import joblib
        joblib.dump(study, f"./tuning_results/study_{timestamp}.pkl")
    except ImportError:
        print("Could not save study. Install joblib for saving.")
    
    return study, trial

if __name__ == "__main__":
    study, best_trial = run_hyperparameter_tuning(n_trials=50)
```

### Evaluation and Analysis

Finally, let's create a script to evaluate our trained trading bot:

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
from dotenv import load_dotenv
import json
import argparse
from typing import Dict, List, Tuple, Any
import torch
import seaborn as sns

from data_loader import DataLoader
from trading_environment import TradingEnvironment
from trading_agent import TradingPPOAgent

# Load environment variables
load_dotenv()

def evaluate_trading_bot(
    ticker: str,
    start_date: str,
    end_date: str,
    model_path: str,
    initial_balance: float = 10000.0,
    window_size: int = 30
):
    """Evaluate the trained trading bot on new data"""
    # Load data
    data_loader = DataLoader()
    df = data_loader.load_data(ticker, start_date, end_date)
    
    # Create environment
    env = TradingEnvironment(
        df,
        window_size=window_size,
        initial_balance=initial_balance
    )
    
    # Initialize agent
    state_dim = env.observation_space.shape[1]
    action_dim = env.action_space.n
    
    agent = TradingPPOAgent(
        state_dim=state_dim,
        action_dim=action_dim
    )
    
    # Load trained model
    agent.load(model_path)
    
    # Evaluate
    state, _ = env.reset()
    done = False
    
    # Track portfolio value over time
    portfolio_values = [initial_balance]
    dates = [df.index[env.current_step]]
    actions_taken = []
    
    while not done:
        # Select action (no exploration)
        action = agent.select_action(state, training=False)
        actions_taken.append(action)
        
        # Take action
        next_state, _, done, _, info = env.step(action)
        
        # Track portfolio value
        portfolio_values.append(info['portfolio_value'])
        dates.append(df.index[min(env.current_step, len(df)-1)])
        
        # Update state
        state = next_state
    
    # Create portfolio df
    portfolio_df = pd.DataFrame({
        'Date': dates,
        'Portfolio': portfolio_values
    })
    portfolio_df.set_index('Date', inplace=True)
    
    # Get price data for comparison
    price_df = df[['Close']].copy()
    
    # Normalize for comparison
    norm_portfolio = portfolio_df['Portfolio'] / portfolio_df['Portfolio'].iloc[0]
    norm_price = price_df['Close'] / price_df['Close'].iloc[0]
    
    # Calculate returns
    portfolio_return = (portfolio_values[-1] - initial_balance) / initial_balance
    price_return = (df['Close'].iloc[-1] - df['Close'].iloc[0]) / df['Close'].iloc[0]
    
    # Calculate daily returns
    portfolio_daily_returns = portfolio_df['Portfolio'].pct_change().dropna()
    
    # Calculate Sharpe ratio (assuming 0% risk-free rate)
    sharpe_ratio = np.sqrt(252) * portfolio_daily_returns.mean() / portfolio_daily_returns.std()
    
    # Calculate drawdown
    rolling_max = portfolio_df['Portfolio'].cummax()
    drawdown = (rolling_max - portfolio_df['Portfolio']) / rolling_max
    max_drawdown = drawdown.max()
    
    # Calculate win rate
    trades = env.trades
    if trades:
        buys = [t for t in trades if t['type'] == 'buy']
        sells = [t for t in trades if t['type'] == 'sell']
        
        if sells:
            profitable_trades = sum(1 for t in sells if t.get('proceeds', 0) > 0)
            win_rate = profitable_trades / len(sells)
        else:
            win_rate = 0
    else:
        win_rate = 0
    
    # Print metrics
    print(f"\nEvaluation Results for {ticker} ({start_date} to {end_date}):")
    print(f"Buy and Hold Return: {price_return:.2%}")
    print(f"Trading Bot Return: {portfolio_return:.2%}")
    print(f"Sharpe Ratio: {sharpe_ratio:.4f}")
    print(f"Max Drawdown: {max_drawdown:.2%}")
    print(f"Number of Trades: {len(trades)}")
    print(f"Win Rate: {win_rate:.2%}")
    
    # Plot results
    plt.figure(figsize=(14, 10))
    
    # Plot normalized performance
    plt.subplot(2, 1, 1)
    plt.plot(norm_portfolio.index, norm_portfolio, label='Trading Bot')
    plt.plot(norm_price.index, norm_price, label='Buy and Hold')
    plt.title(f'{ticker} Performance Comparison')
    plt.xlabel('Date')
    plt.ylabel('Normalized Value')
    plt.legend()
    plt.grid(True)
    
    # Plot trade actions
    plt.subplot(2, 1, 2)
    
    # Plot price
    plt.plot(df.index, df['Close'], color='blue', alpha=0.6)
    
    # Plot buy signals
    buy_points = [t for t in trades if t['type'] == 'buy']
    if buy_points:
        buy_dates = [df.index[t['step']] for t in buy_points]
        buy_prices = [df['Close'].iloc[t['step']] for t in buy_points]
        plt.scatter(buy_dates, buy_prices, marker='^', color='green', s=100, label='Buy')
    
    # Plot sell signals
    sell_points = [t for t in trades if t['type'] == 'sell']
    if sell_points:
        sell_dates = [df.index[t['step']] for t in sell_points]
        sell_prices = [df['Close'].iloc[t['step']] for t in sell_points]
        plt.scatter(sell_dates, sell_prices, marker='v', color='red', s=100, label='Sell')
    
    plt.title(f'{ticker} Trading Signals')
    plt.xlabel('Date')
    plt.ylabel('Price')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    
    # Save plot
    os.makedirs('./evaluation_results', exist_ok=True)
    plt.savefig(f'./evaluation_results/{ticker}_evaluation.png')
    
    # Save metrics
    metrics = {
        "ticker": ticker,
        "start_date": start_date,
        "end_date": end_date,
        "portfolio_return": float(portfolio_return),
        "buy_hold_return": float(price_return),
        "sharpe_ratio": float(sharpe_ratio),
        "max_drawdown": float(max_drawdown),
        "num_trades": len(trades),
        "win_rate": float(win_rate),
        "final_portfolio": float(portfolio_values[-1]),
        "outperformance": float(portfolio_return - price_return)
    }
    
    with open(f'./evaluation_results/{ticker}_metrics.json', 'w') as f:
        json.dump(metrics, f, indent=4)
    
    # Additional analysis
    plt.figure(figsize=(14, 10))
    
    # Plot drawdown
    plt.subplot(2, 1, 1)
    plt.fill_between(drawdown.index, drawdown, color='red', alpha=0.3)
    plt.title(f'{ticker} Drawdown')
    plt.xlabel('Date')
    plt.ylabel('Drawdown')
    plt.grid(True)
    
    # Plot daily returns histogram
    plt.subplot(2, 1, 2)
    sns.histplot(portfolio_daily_returns, bins=50, kde=True)
    plt.title('Daily Returns Distribution')
    plt.xlabel('Daily Return')
    plt.ylabel('Frequency')
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig(f'./evaluation_results/{ticker}_analysis.png')
    
    return metrics

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Evaluate a trained trading bot")
    parser.add_argument("--ticker", type=str, default="AAPL", help="Stock ticker symbol")
    parser.add_argument("--start", type=str, default="2023-01-01", help="Start date (YYYY-MM-DD)")
    parser.add_argument("--end", type=str, default="2023-12-31", help="End date (YYYY-MM-DD)")
    parser.add_argument("--model", type=str, default="./models/AAPL_best.pth", help="Path to trained model")
    parser.add_argument("--balance", type=float, default=10000.0, help="Initial balance")
    args = parser.parse_args()
    
    metrics = evaluate_trading_bot(
        ticker=args.ticker,
        start_date=args.start,
        end_date=args.end,
        model_path=args.model,
        initial_balance=args.balance
    )
```

## Conclusion

In this practical project, we've created a complete financial trading bot using reinforcement learning. Our implementation demonstrates how AI agents can learn to make profitable trading decisions through interaction with simulated market environments.

Key achievements and insights from this project:

1. **Environment Design**: We created a realistic trading environment that simulates market dynamics, transaction costs, and portfolio performance - crucial for effective RL training.

2. **Reward Engineering**: We designed a sophisticated reward function that balances immediate profit with risk management, encouraging the bot to make sustainable trading decisions.

3. **PPO Implementation**: Our implementation of Proximal Policy Optimization provided stability during training, which is essential for financial applications where variance in performance can be problematic.

4. **Model Architecture**: By using a combination of LSTM layers and standard neural networks, our agent can effectively capture temporal patterns in financial data.

5. **Evaluation Framework**: We implemented comprehensive metrics (Sharpe ratio, drawdown, win rate) that provide a holistic view of trading performance beyond simple returns.

6. **Hyperparameter Tuning**: Our Optuna-based tuning framework demonstrates how systematic optimization can significantly improve agent performance.

Financial markets remain challenging environments for RL agents due to their non-stationary nature, high noise levels, and complex dependencies. However, our trading bot implementation shows that with careful environment design, appropriate algorithm selection, and thorough evaluation, RL can be a valuable approach for algorithmic trading.

Future improvements could include incorporating additional data sources, implementing more advanced risk management strategies, and exploring multi-asset trading to enable portfolio diversification. The modular design of our implementation allows for these extensions while maintaining the core learning framework.

This project provides a solid foundation for applying reinforcement learning to financial trading, demonstrating both the power and the practical considerations of using AI for decision-making in complex, real-world domains.