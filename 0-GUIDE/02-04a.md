<small>Claude 3.7 Sonnet Thinking</small>
# 04. OpenAI Models and Fine-tuning

## Key Terms

- **OpenAI API**: A cloud-based service providing access to OpenAI's language models, embeddings, and moderation tools through REST endpoints.
- **Chat Completion API**: An endpoint that takes a series of messages as input and returns a model-generated completion as a response.
- **Embeddings**: Vector representations of text that capture semantic meaning, allowing for similarity comparisons and semantic search.
- **Moderation API**: A tool that checks content against OpenAI's usage policies, identifying potentially harmful content.
- **System Message**: A special message type that helps set the behavior and context for the model at the beginning of conversations.
- **Fine-tuning**: The process of adapting a pre-trained model to specific domains or tasks using additional training data.
- **Transfer Learning**: A machine learning technique where a model developed for one task is repurposed as the starting point for another task.
- **Epochs**: A complete pass through the entire training dataset during fine-tuning.
- **Learning Rate**: A hyperparameter that controls how much to change the model in response to the estimated error in each training iteration.

## OpenAI API Integration

The OpenAI API provides access to powerful language models and tools through standardized endpoints. Here's a comprehensive implementation showcasing the core API functionalities:

```python
import os
from typing import List, Dict, Any, Optional, Union, Tuple
from dotenv import load_dotenv
import openai
from openai import OpenAI
import json
import time
import pandas as pd
import numpy as np
import tiktoken
from tenacity import retry, wait_random_exponential, stop_after_attempt

# Load environment variables from .env file
load_dotenv()

class OpenAIManager:
    """
    Comprehensive manager for OpenAI API interactions with advanced configuration options,
    error handling, and rate limit management.
    """
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        organization: Optional[str] = None,
        default_model: str = "gpt-4-turbo",
        embedding_model: str = "text-embedding-3-large",
        max_retries: int = 5,
        timeout: int = 30,
        log_requests: bool = False
    ):
        """
        Initialize the OpenAI manager.
        
        Args:
            api_key: OpenAI API key (defaults to OPENAI_API_KEY env var)
            organization: OpenAI organization ID (defaults to OPENAI_ORG env var)
            default_model: Default model for completions
            embedding_model: Default model for embeddings
            max_retries: Maximum number of retries on rate limit or server errors
            timeout: Timeout in seconds for API requests
            log_requests: Whether to log requests and responses
        """
        # Use provided keys or load from environment variables
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.organization = organization or os.getenv("OPENAI_ORG")
        
        if not self.api_key:
            raise ValueError("OpenAI API key not provided and not found in environment variables")
        
        # Create client
        self.client = OpenAI(
            api_key=self.api_key,
            organization=self.organization if self.organization else None,
            timeout=timeout
        )
        
        # Store configuration
        self.default_model = default_model
        self.embedding_model = embedding_model
        self.max_retries = max_retries
        self.log_requests = log_requests
        
        # Initialize encoding
        try:
            self.encoding = tiktoken.encoding_for_model(default_model)
        except:
            self.encoding = tiktoken.get_encoding("cl100k_base")  # Fallback encoding
    
    @retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(5))
    def chat_completion(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        top_p: float = 1.0,
        frequency_penalty: float = 0.0,
        presence_penalty: float = 0.0,
        stop: Optional[Union[str, List[str]]] = None,
        response_format: Optional[Dict[str, str]] = None,
        seed: Optional[int] = None,
        stream: bool = False
    ) -> Any:
        """
        Generate a chat completion using the OpenAI API.
        
        Args:
            messages: List of message dictionaries with 'role' and 'content'
            model: Model to use (defaults to self.default_model)
            temperature: Controls randomness (0-2)
            max_tokens: Maximum number of tokens to generate
            top_p: Controls diversity via nucleus sampling
            frequency_penalty: Penalizes frequent tokens
            presence_penalty: Penalizes tokens already present
            stop: Sequences where the API will stop generating
            response_format: Specify response format (e.g., {"type": "json_object"})
            seed: Optional seed for reproducibility
            stream: Whether to stream the response
            
        Returns:
            OpenAI chat completion response
        """
        start_time = time.time()
        model = model or self.default_model
        
        try:
            # Log request if enabled
            if self.log_requests:
                request_data = {
                    "model": model,
                    "messages": messages,
                    "temperature": temperature,
                    "max_tokens": max_tokens,
                    "top_p": top_p,
                    "frequency_penalty": frequency_penalty,
                    "presence_penalty": presence_penalty,
                    "stop": stop,
                    "response_format": response_format,
                    "seed": seed
                }
                print(f"Request: {json.dumps(request_data, indent=2)}")
            
            # Make the API call
            response = self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=top_p,
                frequency_penalty=frequency_penalty,
                presence_penalty=presence_penalty,
                stop=stop,
                response_format=response_format,
                seed=seed,
                stream=stream
            )
            
            # Handle streaming
            if stream:
                return response  # Return stream object for caller to process
            
            # Log response if enabled
            if self.log_requests:
                elapsed = time.time() - start_time
                print(f"Response received in {elapsed:.2f}s")
                print(f"Content: {response.choices[0].message.content[:100]}...")
            
            return response
            
        except openai.RateLimitError:
            print("Rate limit exceeded, retrying after exponential backoff...")
            raise  # Let the retry decorator handle this
        except openai.APITimeoutError:
            print("API request timed out, retrying...")
            raise  # Let the retry decorator handle this
        except Exception as e:
            print(f"Error in chat completion: {str(e)}")
            raise
    
    def get_text_embedding(
        self, 
        text: Union[str, List[str]],
        model: Optional[str] = None,
        dimensions: Optional[int] = None
    ) -> Union[List[float], List[List[float]]]:
        """
        Get embedding vector(s) for text.
        
        Args:
            text: Single string or list of strings to embed
            model: Model to use (defaults to self.embedding_model)
            dimensions: Output dimensionality (depends on model capabilities)
            
        Returns:
            List of embeddings for each input text
        """
        model = model or self.embedding_model
        
        # Prepare input format
        if isinstance(text, str):
            texts = [text]
        else:
            texts = text
        
        try:
            response = self.client.embeddings.create(
                model=model,
                input=texts,
                dimensions=dimensions
            )
            
            embeddings = [embedding.embedding for embedding in response.data]
            
            # Return single embedding if input was a string
            if isinstance(text, str):
                return embeddings[0]
            
            return embeddings
            
        except Exception as e:
            print(f"Error getting embeddings: {str(e)}")
            raise
    
    def moderate_content(self, text: Union[str, List[str]]) -> Dict[str, Any]:
        """
        Check content against OpenAI's moderation endpoint.
        
        Args:
            text: Single string or list of strings to check
            
        Returns:
            Dictionary with moderation results
        """
        try:
            response = self.client.moderations.create(input=text)
            return response.model_dump()
        except Exception as e:
            print(f"Error in content moderation: {str(e)}")
            raise
    
    def count_tokens(self, text: str) -> int:
        """
        Count the number of tokens in a text string.
        
        Args:
            text: Input text
            
        Returns:
            Number of tokens
        """
        return len(self.encoding.encode(text))
    
    def count_message_tokens(self, messages: List[Dict[str, str]]) -> int:
        """
        Count tokens in a message list for chat completion.
        
        Args:
            messages: List of message dictionaries
            
        Returns:
            Total token count
        """
        token_count = 0
        
        for message in messages:
            # Count each message part
            content = message.get('content', '')
            role = message.get('role', '')
            
            token_count += self.count_tokens(content) + self.count_tokens(role) + 4  # Include overhead
        
        # Add final overhead
        token_count += 2
        
        return token_count
    
    def prepare_fine_tuning_data(
        self, 
        training_data: List[Dict[str, Any]],
        validation_split: float = 0.1,
        format_type: str = "messages",
        output_path: str = "fine_tuning_data.jsonl"
    ) -> Tuple[str, str]:
        """
        Prepare and validate data for fine-tuning.
        
        Args:
            training_data: List of examples for fine-tuning
            validation_split: Fraction of data to use for validation
            format_type: 'messages' or 'completion' format
            output_path: Path to save formatted data
            
        Returns:
            Tuple of (training_file_path, validation_file_path)
        """
        # Validate data format
        if format_type not in ["messages", "completion"]:
            raise ValueError("format_type must be 'messages' or 'completion'")
            
        # Check data structure and format properly
        formatted_data = []
        
        if format_type == "messages":
            # Message format validation
            for item in training_data:
                if 'messages' not in item:
                    raise ValueError("Each training item must contain a 'messages' field")
                
                # Validate message format
                messages = item['messages']
                if not isinstance(messages, list):
                    raise ValueError("'messages' field must be a list")
                
                for msg in messages:
                    if 'role' not in msg or 'content' not in msg:
                        raise ValueError("Each message must have 'role' and 'content'")
                    
                    if msg['role'] not in ["system", "user", "assistant"]:
                        raise ValueError(f"Invalid role: {msg['role']}")
                
                # Ensure it's a valid conversation with at least one user message and one assistant message
                roles = [msg['role'] for msg in messages]
                if "user" not in roles or "assistant" not in roles:
                    raise ValueError("Conversation must include at least one user and one assistant message")
                
                formatted_data.append(item)
        
        else:  # completion format
            # Convert to chat format
            for item in training_data:
                if 'prompt' not in item or 'completion' not in item:
                    raise ValueError("Each training item must contain 'prompt' and 'completion'")
                
                # Format as chat messages
                formatted_item = {
                    "messages": [
                        {"role": "user", "content": item['prompt']},
                        {"role": "assistant", "content": item['completion']}
                    ]
                }
                
                formatted_data.append(formatted_item)
        
        # Split data into training and validation
        np.random.shuffle(formatted_data)
        split_idx = int(len(formatted_data) * (1 - validation_split))
        
        train_data = formatted_data[:split_idx]
        val_data = formatted_data[split_idx:]
        
        # Save to JSONL files
        train_path = output_path
        val_path = output_path.replace('.jsonl', '_val.jsonl')
        
        with open(train_path, 'w') as f:
            for item in train_data:
                f.write(json.dumps(item) + '\n')
                
        with open(val_path, 'w') as f:
            for item in val_data:
                f.write(json.dumps(item) + '\n')
        
        print(f"Saved {len(train_data)} training examples to {train_path}")
        print(f"Saved {len(val_data)} validation examples to {val_path}")
        
        return train_path, val_path
    
    def start_fine_tuning_job(
        self,
        training_file_path: str,
        validation_file_path: Optional[str] = None,
        model: str = "gpt-3.5-turbo",
        suffix: Optional[str] = None,
        hyperparameters: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Start a fine-tuning job.
        
        Args:
            training_file_path: Path to training data JSONL file
            validation_file_path: Path to validation data JSONL file (optional)
            model: Base model to fine-tune
            suffix: Suffix to add to the fine-tuned model name
            hyperparameters: Dictionary of hyperparameters like n_epochs
            
        Returns:
            Fine-tuning job details
        """
        # Upload training file
        with open(training_file_path, 'rb') as f:
            training_file = self.client.files.create(
                file=f,
                purpose="fine-tune"
            )
        
        # Upload validation file if provided
        validation_file_id = None
        if validation_file_path:
            with open(validation_file_path, 'rb') as f:
                validation_file = self.client.files.create(
                    file=f,
                    purpose="fine-tune"
                )
                validation_file_id = validation_file.id
        
        # Set default hyperparameters
        if hyperparameters is None:
            hyperparameters = {
                "n_epochs": 3
            }
        
        # Create fine-tuning job
        job = self.client.fine_tuning.jobs.create(
            training_file=training_file.id,
            validation_file=validation_file_id,
            model=model,
            suffix=suffix,
            hyperparameters=hyperparameters
        )
        
        print(f"Started fine-tuning job: {job.id}")
        return job.model_dump()
    
    def monitor_fine_tuning_job(
        self, 
        job_id: str,
        poll_interval: int = 60
    ) -> Dict[str, Any]:
        """
        Monitor a fine-tuning job until it completes.
        
        Args:
            job_id: ID of the fine-tuning job
            poll_interval: How often to check status (in seconds)
            
        Returns:
            Final job details
        """
        status = "running"
        job = None
        
        print(f"Monitoring fine-tuning job {job_id}")
        
        while status in ["running", "validating_files", "queued"]:
            job = self.client.fine_tuning.jobs.retrieve(job_id)
            status = job.status
            
            print(f"Status: {status}, Created at: {job.created_at}")
            
            if hasattr(job, 'trained_tokens') and job.trained_tokens:
                print(f"Trained tokens: {job.trained_tokens}")
                
            time.sleep(poll_interval)
        
        # Get final job details
        final_job = self.client.fine_tuning.jobs.retrieve(job_id)
        
        if final_job.status == "succeeded":
            print(f"Fine-tuning completed successfully! Model: {final_job.fine_tuned_model}")
            
            # Get results if available
            if hasattr(final_job, 'result_files') and final_job.result_files:
                for file_id in final_job.result_files:
                    content = self.client.files.content(file_id)
                    print(f"Result file content: {content.read().decode('utf-8')}")
        else:
            print(f"Fine-tuning ended with status: {final_job.status}")
            if hasattr(final_job, 'error') and final_job.error:
                print(f"Error: {final_job.error}")
                
        return final_job.model_dump()
```

## System Messages and Response Configuration

System messages and response configuration are essential for controlling model behavior. Here's a detailed implementation:

```python
class ConversationManager:
    """
    Advanced manager for configuring and optimizing model responses through
    system messages and parameter tuning.
    """
    
    def __init__(
        self,
        openai_manager: OpenAIManager,
        conversation_id: Optional[str] = None
    ):
        """
        Initialize conversation manager.
        
        Args:
            openai_manager: OpenAI API manager
            conversation_id: Optional ID for the conversation
        """
        self.openai_manager = openai_manager
        self.conversation_id = conversation_id or f"conv_{int(time.time())}"
        self.messages = []
        self.persona_templates = self._load_persona_templates()
        
    def _load_persona_templates(self) -> Dict[str, str]:
        """Load predefined persona templates for system messages"""
        return {
            "default": "You are a helpful AI assistant.",
            "expert": "You are an AI expert with deep knowledge in {domain}. Provide detailed, technical responses.",
            "teacher": "You are a patient teacher who explains complex concepts simply, using analogies and examples.",
            "concise": "You are a concise AI assistant. Provide brief, direct answers with minimal elaboration.",
            "creative": "You are a creative AI assistant. Think outside the box and provide innovative solutions.",
            "socratic": "You are a Socratic tutor who helps users learn by asking thoughtful questions.",
            "balanced": "You are a balanced advisor who carefully considers multiple perspectives before responding."
        }
    
    def set_system_message(
        self,
        content: Optional[str] = None,
        persona: Optional[str] = None,
        domain: Optional[str] = None,
        clear_history: bool = False
    ) -> None:
        """
        Set or update the system message for the conversation.
        
        Args:
            content: Custom system message content
            persona: Predefined persona template key
            domain: Domain to insert into persona template
            clear_history: Whether to clear previous messages
        """
        if clear_history:
            self.clear_conversation()
        
        # Use custom content or persona template
        if content:
            system_content = content
        elif persona and persona in self.persona_templates:
            template = self.persona_templates[persona]
            system_content = template.format(domain=domain) if domain else template
        else:
            system_content = self.persona_templates["default"]
            
        # Check if there's already a system message
        if self.messages and self.messages[0]["role"] == "system":
            self.messages[0]["content"] = system_content
        else:
            # Add system message at the beginning
            self.messages.insert(0, {"role": "system", "content": system_content})
    
    def add_user_message(self, content: str) -> None:
        """Add a user message to the conversation"""
        self.messages.append({"role": "user", "content": content})
    
    def add_assistant_message(self, content: str) -> None:
        """Add an assistant message to the conversation"""
        self.messages.append({"role": "assistant", "content": content})
    
    def clear_conversation(self) -> None:
        """Clear the conversation history"""
        self.messages = []
    
    def get_response(
        self,
        user_message: Optional[str] = None,
        model: Optional[str] = None,
        temperature: float = 0.7,
        response_format: Optional[Dict[str, str]] = None,
        **kwargs
    ) -> str:
        """
        Get a response from the model given the conversation history.
        
        Args:
            user_message: Optional new user message to add before generating response
            model: Model to use
            temperature: Controls randomness (0-2)
            response_format: Specify response format (e.g., {"type": "json_object"})
            **kwargs: Additional parameters to pass to chat completion
            
        Returns:
            Assistant's response text
        """
        if user_message:
            self.add_user_message(user_message)
        
        # Ensure we have a system message
        if not self.messages or self.messages[0]["role"] != "system":
            self.set_system_message()
        
        # Get completion
        response = self.openai_manager.chat_completion(
            messages=self.messages,
            model=model,
            temperature=temperature,
            response_format=response_format,
            **kwargs
        )
        
        # Extract and add response to history
        assistant_response = response.choices[0].message.content
        self.add_assistant_message(assistant_response)
        
        return assistant_response
    
    def create_structured_response(
        self,
        user_message: str,
        structure_template: Dict[str, Any],
        temperature: float = 0.7,
        model: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Generate a structured JSON response following a specified template.
        
        Args:
            user_message: User message to respond to
            structure_template: Template dictionary showing required structure
            temperature: Controls randomness
            model: Model to use
            
        Returns:
            Structured response as a dictionary
        """
        # Create a system message that defines the required structure
        structure_description = json.dumps(structure_template, indent=2)
        system_message = (
            "You are an AI assistant that responds in structured JSON format. "
            f"Your response must follow this exact structure:\n{structure_description}\n\n"
            "Provide only valid JSON without additional text, comments, or explanations."
        )
        
        # Set system message and add user message
        self.set_system_message(content=system_message, clear_history=True)
        self.add_user_message(user_message)
        
        # Get structured response
        response = self.openai_manager.chat_completion(
            messages=self.messages,
            model=model,
            temperature=temperature,
            response_format={"type": "json_object"}
        )
        
        # Extract and parse JSON response
        json_response = response.choices[0].message.content
        try:
            structured_response = json.loads(json_response)
            self.add_assistant_message(json_response)  # Add to history
            return structured_response
        except json.JSONDecodeError:
            # Handle parsing error
            print("Error parsing JSON response")
            self.add_assistant_message("Error: Invalid JSON response")
            return {"error": "Failed to generate valid JSON response"}
    
    def generate_with_constraints(
        self, 
        user_message: str,
        constraints: List[str],
        examples: Optional[List[Dict[str, str]]] = None,
        model: Optional[str] = None
    ) -> str:
        """
        Generate a response that adheres to specific constraints.
        
        Args:
            user_message: User message to respond to
            constraints: List of constraints for the response
            examples: Optional examples of constraint-following responses
            model: Model to use
            
        Returns:
            Constrained response text
        """
        # Format constraints as a numbered list
        constraints_text = "\n".join([f"{i+1}. {constraint}" for i, constraint in enumerate(constraints)])
        
        # Create system message with constraints
        system_message = (
            "You are an AI assistant that follows these constraints exactly:\n\n"
            f"{constraints_text}\n\n"
            "Always verify that your response satisfies ALL constraints before answering."
        )
        
        # Set system message
        self.set_system_message(content=system_message, clear_history=True)
        
        # Add examples if provided
        if examples:
            for example in examples:
                if "user" in example and "assistant" in example:
                    self.add_user_message(example["user"])
                    self.add_assistant_message(example["assistant"])
        
        # Add user message and get response
        return self.get_response(user_message=user_message, model=model, temperature=0.7)
    
    def optimize_prompt(
        self,
        base_prompt: str,
        variations: int = 3,
        evaluation_criteria: List[str] = None,
        model: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Generate and evaluate variations of a prompt to find the optimal version.
        
        Args:
            base_prompt: Starting prompt to optimize
            variations: Number of variations to generate
            evaluation_criteria: List of criteria to evaluate against
            model: Model to use
            
        Returns:
            Dictionary with optimization results
        """
        if not evaluation_criteria:
            evaluation_criteria = [
                "Clarity: How clear and unambiguous is the prompt?",
                "Specificity: How well does it specify what's required?",
                "Conciseness: How efficient is the prompt?",
                "Completeness: Does it provide all necessary context?"
            ]
        
        # Create system message for generating variations
        system_message = (
            "You are an expert prompt engineer who can improve prompts. "
            "Generate variations that achieve the same goal but are more effective."
        )
        
        # Set system message
        self.set_system_message(content=system_message, clear_history=True)
        
        # Request variations
        user_message = (
            f"Generate {variations} improved versions of this prompt:\n\n"
            f"PROMPT: {base_prompt}\n\n"
            "Make each version more effective while preserving the original intent. "
            "Format each variation as VARIATION 1: [improved prompt], etc."
        )
        
        variations_response = self.get_response(user_message=user_message, model=model, temperature=0.8)
        
        # Extract variations
        variation_list = []
        for i in range(1, variations + 1):
            pattern = rf"VARIATION {i}:\s*(.*?)(?=VARIATION {i+1}:|$)"
            matches = re.findall(pattern, variations_response, re.DOTALL)
            if matches:
                variation_list.append(matches[0].strip())
            else:
                # Try alternative format
                pattern = rf"Variation {i}[.:]?\s*(.*?)(?=Variation {i+1}[.:]?|$)"
                matches = re.findall(pattern, variations_response, re.DOTALL)
                if matches:
                    variation_list.append(matches[0].strip())
        
        # Add original prompt
        all_prompts = [base_prompt] + variation_list
        
        # Evaluate all prompts
        evaluation_system_message = (
            "You are an objective prompt evaluator. "
            "Assess each prompt based on the provided criteria and assign a score from 1-10."
        )
        
        evaluation_results = {}
        
        for i, prompt in enumerate(all_prompts):
            prompt_name = "Original" if i == 0 else f"Variation {i}"
            
            # Set system message for evaluation
            self.set_system_message(content=evaluation_system_message, clear_history=True)
            
            # Create evaluation request
            eval_request = (
                f"Evaluate this prompt:\n\n{prompt}\n\n"
                f"Criteria:\n{chr(10).join(evaluation_criteria)}\n\n"
                "For each criterion, provide a score (1-10) and brief justification. "
                "Then provide an overall score and final recommendation."
            )
            
            evaluation = self.get_response(user_message=eval_request, model=model, temperature=0.3)
            evaluation_results[prompt_name] = {
                "prompt": prompt,
                "evaluation": evaluation
            }
        
        return {
            "original_prompt": base_prompt,
            "variations": variation_list,
            "evaluations": evaluation_results
        }
```

## Fine-tuning and Model Adaptation

Advanced fine-tuning process for customizing models to specific domains:

```python
class OpenAIFineTuner:
    """
    Advanced fine-tuning manager for adapting OpenAI models to specific tasks.
    """
    
    def __init__(
        self,
        openai_manager: OpenAIManager,
        project_name: str,
        base_model: str = "gpt-3.5-turbo",
        working_dir: str = "fine_tuning"
    ):
        """
        Initialize fine-tuning manager.
        
        Args:
            openai_manager: OpenAI API manager
            project_name: Name of the fine-tuning project
            base_model: Base model to fine-tune
            working_dir: Directory for working files
        """
        self.openai_manager = openai_manager
        self.project_name = project_name
        self.base_model = base_model
        self.working_dir = working_dir
        
        # Create working directory
        os.makedirs(working_dir, exist_ok=True)
    
    def prepare_data_from_conversations(
        self, 
        conversations: List[List[Dict[str, str]]],
        output_path: Optional[str] = None
    ) -> str:
        """
        Prepare fine-tuning data from conversations.
        
        Args:
            conversations: List of conversation threads (lists of messages)
            output_path: Path to save formatted data
            
        Returns:
            Path to formatted data file
        """
        if not output_path:
            output_path = f"{self.working_dir}/{self.project_name}_training_data.jsonl"
        
        # Format conversations
        formatted_data = []
        
        for conversation in conversations:
            # Validate conversation
            if not conversation:
                continue
            
            # Ensure system message exists
            if conversation[0]["role"] != "system":
                conversation.insert(0, {"role": "system", "content": "You are a helpful assistant."})
            
            # Add to formatted data
            formatted_data.append({"messages": conversation})
        
        # Save to JSONL
        with open(output_path, 'w') as f:
            for item in formatted_data:
                f.write(json.dumps(item) + '\n')
        
        print(f"Saved {len(formatted_data)} training examples to {output_path}")
        return output_path
    
    def prepare_data_from_qa_pairs(
        self,
        qa_pairs: List[Dict[str, str]],
        system_message: str = "You are a helpful assistant.",
        output_path: Optional[str] = None,
        include_context: bool = False
    ) -> str:
        """
        Prepare fine-tuning data from question-answer pairs.
        
        Args:
            qa_pairs: List of dictionaries with 'question', 'answer', and optional 'context'
            system_message: System message to use for all conversations
            output_path: Path to save formatted data
            include_context: Whether to include context in user message
            
        Returns:
            Path to formatted data file
        """
        if not output_path:
            output_path = f"{self.working_dir}/{self.project_name}_qa_data.jsonl"
        
        # Format QA pairs as conversations
        formatted_data = []
        
        for qa_item in qa_pairs:
            if 'question' not in qa_item or 'answer' not in qa_item:
                continue
            
            # Prepare user message
            user_content = qa_item['question']
            if include_context and 'context' in qa_item:
                user_content = f"Context: {qa_item['context']}\n\nQuestion: {qa_item['question']}"
            
            # Create conversation
            conversation = {
                "messages": [
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": user_content},
                    {"role": "assistant", "content": qa_item['answer']}
                ]
            }
            
            formatted_data.append(conversation)
        
        # Save to JSONL
        with open(output_path, 'w') as f:
            for item in formatted_data:
                f.write(json.dumps(item) + '\n')
        
        print(f"Saved {len(formatted_data)} QA examples to {output_path}")
        return output_path
    
    def validate_training_file(self, file_path: str) -> Tuple[bool, Dict[str, Any]]:
        """
        Validate a training file for common issues.
        
        Args:
            file_path: Path to JSONL training file
            
        Returns:
            Tuple of (is_valid, validation_results)
        """
        results = {
            "valid": True,
            "total_examples": 0,
            "total_tokens": 0,
            "max_tokens_per_example": 0,
            "issues": []
        }
        
        try:
            with open(file_path, 'r') as f:
                lines = f.readlines()
            
            results["total_examples"] = len(lines)
            
            # Check if file is empty
            if results["total_examples"] == 0:
                results["valid"] = False
                results["issues"].append("File is empty")
                return False, results
            
            # Validate each line
            for i, line in enumerate(lines):
                try:
                    data = json.loads(line)
                    
                    # Check required fields
                    if "messages" not in data:
                        results["issues"].append(f"Line {i+1}: Missing 'messages' field")
                        results["valid"] = False
                        continue
                    
                    # Validate messages
                    messages = data["messages"]
                    if not isinstance(messages, list) or len(messages) < 2:
                        results["issues"].append(f"Line {i+1}: 'messages' must be a list with at least 2 messages")
                        results["valid"] = False
                        continue
                    
                    # Check roles and content
                    roles = [msg.get("role") for msg in messages]
                    if "user" not in roles:
                        results["issues"].append(f"Line {i+1}: Missing 'user' role")
                        results["valid"] = False
                    
                    if "assistant" not in roles:
                        results["issues"].append(f"Line {i+1}: Missing 'assistant' role")
                        results["valid"] = False
                    
                    # Count tokens
                    example_tokens = self.openai_manager.count_message_tokens(messages)
                    results["total_tokens"] += example_tokens
                    
                    if example_tokens > results["max_tokens_per_example"]:
                        results["max_tokens_per_example"] = example_tokens
                    
                    if example_tokens > 4096:  # Example too long for most models
                        results["issues"].append(f"Line {i+1}: Example exceeds 4096 tokens ({example_tokens} tokens)")
                    
                except json.JSONDecodeError:
                    results["issues"].append(f"Line {i+1}: Invalid JSON")
                    results["valid"] = False
            
            # Check overall dataset size
            if results["total_examples"] < 10:
                results["issues"].append(f"Only {results['total_examples']} examples (recommended: 50+)")
            
            return results["valid"], results
            
        except Exception as e:
            results["valid"] = False
            results["issues"].append(f"Error validating file: {str(e)}")
            return False, results
    
    def run_fine_tuning(
        self,
        training_file_path: str,
        validation_file_path: Optional[str] = None,
        hyperparameters: Optional[Dict[str, Any]] = None,
        suffix: Optional[str] = None,
        auto_monitor: bool = True
    ) -> Dict[str, Any]:
        """
        Run a complete fine-tuning workflow.
        
        Args:
            training_file_path: Path to training data file
            validation_file_path: Path to validation data file (optional)
            hyperparameters: Hyperparameters for fine-tuning
            suffix: Suffix for model name
            auto_monitor: Whether to automatically monitor job progress
            
        Returns:
            Dictionary with fine-tuning job results
        """
        # Validate training file
        valid, validation_results = self.validate_training_file(training_file_path)
        if not valid:
            print("Training file validation failed:")
            for issue in validation_results["issues"]:
                print(f"- {issue}")
            
            if input("Continue anyway? (y/n): ").lower() != 'y':
                return {"status": "aborted", "validation_results": validation_results}
        
        # Validate validation file if provided
        if validation_file_path:
            valid, val_validation_results = self.validate_training_file(validation_file_path)
            if not valid:
                print("Validation file validation failed:")
                for issue in val_validation_results["issues"]:
                    print(f"- {issue}")
                
                if input("Continue anyway? (y/n): ").lower() != 'y':
                    return {
                        "status": "aborted",
                        "training_validation": validation_results,
                        "validation_validation": val_validation_results
                    }
        
        # Set default hyperparameters
        if hyperparameters is None:
            hyperparameters = {
                "n_epochs": 3
            }
        
        # Set default suffix
        if suffix is None:
            suffix = self.project_name
        
        # Start fine-tuning job
        job = self.openai_manager.start_fine_tuning_job(
            training_file_path=training_file_path,
            validation_file_path=validation_file_path,
            model=self.base_model,
            suffix=suffix,
            hyperparameters=hyperparameters
        )
        
        # Monitor job if requested
        if auto_monitor and "id" in job:
            final_job = self.openai_manager.monitor_fine_tuning_job(job["id"])
            return {
                "status": "completed",
                "job": final_job,
                "validation_results": validation_results
            }
        
        return {
            "status": "started",
            "job": job,
            "validation_results": validation_results
        }
    
    def evaluate_fine_tuned_model(
        self,
        model_id: str,
        evaluation_data: List[Dict[str, Any]],
        reference_model: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Evaluate a fine-tuned model against a reference model.
        
        Args:
            model_id: Fine-tuned model ID
            evaluation_data: List of evaluation examples
            reference_model: Model to compare against (defaults to base model)
            
        Returns:
            Evaluation results
        """
        if reference_model is None:
            reference_model = self.base_model
        
        results = {
            "model": model_id,
            "reference_model": reference_model,
            "total_examples": len(evaluation_data),
            "examples": []
        }
        
        conversation_manager = ConversationManager(self.openai_manager)
        
        for i, example in enumerate(evaluation_data):
            print(f"Evaluating example {i+1}/{len(evaluation_data)}...")
            
            # Extract prompt and expected response
            if "messages" in example:
                # Find last user message
                user_messages = [m for m in example["messages"] if m["role"] == "user"]
                if not user_messages:
                    continue
                
                user_message = user_messages[-1]["content"]
                
                # Find corresponding assistant response
                assistant_idx = example["messages"].index(user_messages[-1]) + 1
                if assistant_idx < len(example["messages"]) and example["messages"][assistant_idx]["role"] == "assistant":
                    expected = example["messages"][assistant_idx]["content"]
                else:
                    expected = None
            elif "question" in example and "answer" in example:
                user_message = example["question"]
                expected = example["answer"]
            else:
                continue
            
            # Generate responses
            try:
                # Set system message if available
                system_message = None
                if "messages" in example:
                    system_messages = [m for m in example["messages"] if m["role"] == "system"]
                    if system_messages:
                        system_message = system_messages[0]["content"]
                
                if system_message:
                    conversation_manager.set_system_message(content=system_message, clear_history=True)
                else:
                    conversation_manager.clear_conversation()
                
                # Get fine-tuned model response
                fine_tuned_response = conversation_manager.get_response(
                    user_message=user_message,
                    model=model_id,
                    temperature=0.0  # Use deterministic output for evaluation
                )
                conversation_manager.clear_conversation()
                
                # Set same system message if available
                if system_message:
                    conversation_manager.set_system_message(content=system_message, clear_history=True)
                
                # Get reference model response
                reference_response = conversation_manager.get_response(
                    user_message=user_message,
                    model=reference_model,
                    temperature=0.0  # Use deterministic output for evaluation
                )
                
                # Store results
                example_result = {
                    "prompt": user_message,
                    "fine_tuned_response": fine_tuned_response,
                    "reference_response": reference_response,
                    "expected": expected
                }
                
                results["examples"].append(example_result)
                
            except Exception as e:
                print(f"Error evaluating example {i+1}: {str(e)}")
        
        # Save results
        results_path = f"{self.working_dir}/{self.project_name}_evaluation.json"
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"Evaluation complete. Results saved to {results_path}")
        return results
```

## Implementation Example: Domain-Specific Assistant

Here's an implementation of a domain-specific assistant using the components above:

```python
def create_domain_expert_assistant():
    """
    Example implementation of a domain-specific expert assistant
    using OpenAI models and fine-tuning.
    """
    # Initialize OpenAI manager
    openai_manager = OpenAIManager()
    
    # 1. Collect training data
    print("Collecting domain-specific training data...")
    
    # Example QA pairs for a legal assistant
    legal_qa_pairs = [
        {
            "question": "What's the difference between a trademark and a copyright?",
            "answer": "Trademarks protect brand identifiers like logos, names, and slogans that distinguish goods and services in the marketplace. They primarily prevent consumer confusion about the source of products or services. Copyrights, on the other hand, protect original creative works fixed in a tangible medium, such as books, music, artwork, and software. While trademarks can potentially last indefinitely with proper maintenance, copyrights have a limited term (typically author's life plus 70 years)."
        },
        {
            "question": "How do I form a Limited Liability Company (LLC)?",
            "answer": "To form an LLC, follow these general steps: 1) Choose a business name that complies with your state's LLC rules, 2) File Articles of Organization with your state's business filing office, typically the Secretary of State, 3) Pay the required filing fee (varies by state), 4) Create an operating agreement that outlines ownership structure and operating procedures, 5) Obtain necessary business licenses and permits, 6) Apply for an EIN (Employer Identification Number) from the IRS if needed, 7) Comply with ongoing requirements like annual reports and tax filings."
        },
        # Add more QA pairs...
    ]
    
    # 2. Create fine-tuning manager
    fine_tuner = OpenAIFineTuner(
        openai_manager=openai_manager,
        project_name="legal_assistant",
        base_model="gpt-3.5-turbo",
        working_dir="legal_assistant_project"
    )
    
    # 3. Prepare training data
    system_message = "You are a knowledgeable legal assistant that provides accurate, detailed information about legal concepts, procedures, and requirements. Always clarify that you're providing general information, not legal advice, and recommend consulting with an attorney for specific situations."
    
    training_file_path = fine_tuner.prepare_data_from_qa_pairs(
        qa_pairs=legal_qa_pairs,
        system_message=system_message,
        output_path="legal_assistant_project/legal_training_data.jsonl"
    )
    
    # 4. Validate and run fine-tuning
    print("\nValidating training file...")
    valid, validation_results = fine_tuner.validate_training_file(training_file_path)
    
    if valid:
        print("Training file is valid.")
        print(f"Total examples: {validation_results['total_examples']}")
        print(f"Total tokens: {validation_results['total_tokens']}")
        
        # Run fine-tuning (commented out to avoid actual API usage)
        '''
        fine_tuning_results = fine_tuner.run_fine_tuning(
            training_file_path=training_file_path,
            hyperparameters={"n_epochs": 3},
            suffix="legal_assistant_v1"
        )
        
        if fine_tuning_results["status"] == "completed" and "job" in fine_tuning_results:
            model_id = fine_tuning_results["job"].get("fine_tuned_model")
            if model_id:
                print(f"Fine-tuned model created: {model_id}")
            else:
                print("Fine-tuning completed but model ID not found.")
        else:
            print(f"Fine-tuning status: {fine_tuning_results['status']}")
        '''
        
        # For demonstration, we'll skip actually running the fine-tuning
        # and use the base model instead
        model_id = "gpt-3.5-turbo"  # This would be the fine-tuned model ID
    else:
        print("Training file validation failed:")
        for issue in validation_results["issues"]:
            print(f"- {issue}")
        model_id = "gpt-3.5-turbo"  # Use base model as fallback
    
    # 5. Create and use the domain-specific assistant
    print("\nInitializing legal assistant...")
    conversation_manager = ConversationManager(openai_manager)
    conversation_manager.set_system_message(content=system_message)
    
    # Example interactions
    test_questions = [
        "What are the key differences between a C Corporation and an S Corporation?",
        "How does fair use apply to copyrighted materials in educational settings?",
        "What should I include in a basic employment contract?"
    ]
    
    print("\nTesting legal assistant with sample questions:")
    for question in test_questions:
        print(f"\nQ: {question}")
        response = conversation_manager.get_response(
            user_message=question,
            model=model_id,
            temperature=0.3
        )
        print(f"A: {response}")
    
    return {
        "assistant_type": "Legal Assistant",
        "fine_tuning_data_path": training_file_path,
        "model_id": model_id
    }

# Example usage
if __name__ == "__main__":
    result = create_domain_expert_assistant()
    print(f"\nCreated {result['assistant_type']} using model {result['model_id']}")
```

## Conclusion

OpenAI's models and fine-tuning capabilities provide powerful tools for building sophisticated AI systems that can be tailored to specific domains and requirements. The key components explored in this section—API integration, system message configuration, and fine-tuning processes—form a comprehensive framework for developing customized AI solutions.

The OpenAI API offers access to state-of-the-art language models through a well-designed interface, enabling applications ranging from conversational agents to content moderation and semantic search. By carefully configuring system messages and model parameters, developers can achieve precise control over model behavior, tone, and output format, creating consistent and reliable AI interactions.

Fine-tuning represents a critical capability for adapting foundation models to specialized domains. Through structured data preparation, thoughtful hyperparameter selection, and rigorous evaluation, the fine-tuning process creates models that incorporate domain-specific knowledge and terminology while maintaining the general capabilities of the foundation model.

As AI technology continues to evolve, the ability to effectively leverage these tools—combining API integrations with strategic prompt engineering and targeted fine-tuning—will be increasingly valuable across industries. Building domain-specific assistants that understand specialized knowledge domains represents just one of many applications enabled by these powerful capabilities, with substantial potential for improving productivity and unlocking new possibilities across various professional fields.