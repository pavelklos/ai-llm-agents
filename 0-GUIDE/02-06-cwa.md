<small>Claude web</small>
# 06. Advanced Fine-tuning with HuggingFace

## Key Terms and Concepts

**Fine-tuning**: The process of adapting a pre-trained model to perform specific tasks by training it on domain-specific data while preserving the knowledge learned during pre-training.

**LoRA (Low-Rank Adaptation)**: A parameter-efficient fine-tuning technique that freezes the original model weights and introduces trainable low-rank decomposition matrices, significantly reducing memory requirements and training time.

**QLoRA (Quantized LoRA)**: An extension of LoRA that combines quantization techniques with low-rank adaptation, enabling fine-tuning of large models on consumer hardware by reducing memory footprint through 4-bit quantization.

**PPO (Proximal Policy Optimization)**: A reinforcement learning algorithm used in language model training to optimize policies while maintaining stability, commonly employed in human feedback alignment.

**RLHF (Reinforcement Learning from Human Feedback)**: A training paradigm that uses human preferences to guide model behavior, typically implemented through reward modeling and policy optimization.

**Parameter-Efficient Fine-tuning (PEFT)**: Methods that update only a small subset of model parameters while keeping the majority frozen, enabling efficient adaptation with minimal computational resources.

## Advanced Fine-tuning Techniques

### LoRA Implementation

LoRA works by decomposing weight updates into low-rank matrices, allowing efficient adaptation without modifying the original model parameters. The technique introduces trainable matrices A and B such that the weight update Î”W = BA, where the rank r << min(input_dim, output_dim).

```python
import torch
import torch.nn as nn
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import (
    LoraConfig, 
    get_peft_model, 
    TaskType,
    prepare_model_for_kbit_training
)
from datasets import Dataset, load_dataset
import os
from dotenv import load_dotenv

load_dotenv()

class LoRAFineTuner:
    def __init__(self, model_name: str, task_type: TaskType = TaskType.CAUSAL_LM):
        self.model_name = model_name
        self.task_type = task_type
        self.tokenizer = None
        self.model = None
        self.peft_model = None
        
    def setup_model_and_tokenizer(self):
        """Initialize tokenizer and base model"""
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True,
            use_auth_token=os.getenv('HUGGINGFACE_TOKEN')
        )
        
        # Add padding token if not present
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
            use_auth_token=os.getenv('HUGGINGFACE_TOKEN')
        )
        
    def configure_lora(self, r: int = 16, lora_alpha: int = 32, 
                      lora_dropout: float = 0.1, target_modules: list = None):
        """Configure LoRA parameters"""
        if target_modules is None:
            # Common target modules for transformer models
            target_modules = ["q_proj", "v_proj", "k_proj", "o_proj", 
                            "gate_proj", "up_proj", "down_proj"]
        
        lora_config = LoraConfig(
            r=r,
            lora_alpha=lora_alpha,
            target_modules=target_modules,
            lora_dropout=lora_dropout,
            bias="none",
            task_type=self.task_type,
            inference_mode=False
        )
        
        self.peft_model = get_peft_model(self.model, lora_config)
        self.peft_model.print_trainable_parameters()
        
    def prepare_dataset(self, dataset_name: str = None, custom_data: list = None):
        """Prepare training dataset"""
        if custom_data:
            dataset = Dataset.from_list(custom_data)
        else:
            dataset = load_dataset(dataset_name, split="train[:1000]")  # Sample for demo
            
        def tokenize_function(examples):
            # Format for instruction following
            formatted_texts = []
            for instruction, response in zip(examples['instruction'], examples['response']):
                text = f"### Instruction:\n{instruction}\n\n### Response:\n{response}"
                formatted_texts.append(text)
            
            tokenized = self.tokenizer(
                formatted_texts,
                truncation=True,
                padding=True,
                max_length=512,
                return_tensors="pt"
            )
            tokenized["labels"] = tokenized["input_ids"].clone()
            return tokenized
            
        tokenized_dataset = dataset.map(tokenize_function, batched=True)
        return tokenized_dataset
        
    def train(self, train_dataset, output_dir: str = "./lora-finetuned-model"):
        """Execute LoRA fine-tuning"""
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=3,
            per_device_train_batch_size=4,
            gradient_accumulation_steps=4,
            warmup_steps=100,
            max_steps=500,
            learning_rate=2e-4,
            fp16=True,
            logging_steps=10,
            save_strategy="steps",
            save_steps=100,
            evaluation_strategy="no",
            remove_unused_columns=False,
            report_to="none"
        )
        
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False
        )
        
        trainer = Trainer(
            model=self.peft_model,
            args=training_args,
            train_dataset=train_dataset,
            data_collator=data_collator,
        )
        
        trainer.train()
        trainer.save_model()
        
    def inference(self, prompt: str, max_length: int = 256):
        """Generate text using fine-tuned model"""
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.peft_model.device)
        
        with torch.no_grad():
            outputs = self.peft_model.generate(
                **inputs,
                max_length=max_length,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
            
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response[len(prompt):]

# Example usage
def demonstrate_lora_finetuning():
    # Custom training data for code generation
    training_data = [
        {
            "instruction": "Write a Python function to calculate factorial",
            "response": "def factorial(n):\n    if n <= 1:\n        return 1\n    return n * factorial(n-1)"
        },
        {
            "instruction": "Create a function to reverse a string",
            "response": "def reverse_string(s):\n    return s[::-1]"
        },
        # Add more training examples...
    ]
    
    fine_tuner = LoRAFineTuner("microsoft/DialoGPT-medium")
    fine_tuner.setup_model_and_tokenizer()
    fine_tuner.configure_lora(r=8, lora_alpha=16)
    
    dataset = fine_tuner.prepare_dataset(custom_data=training_data)
    fine_tuner.train(dataset)
    
    # Test inference
    prompt = "### Instruction:\nWrite a function to find maximum in a list\n\n### Response:\n"
    response = fine_tuner.inference(prompt)
    print(f"Generated response: {response}")
```

### QLoRA Implementation

QLoRA combines 4-bit quantization with LoRA to enable training of large models on limited hardware:

```python
from transformers import BitsAndBytesConfig
from peft import prepare_model_for_kbit_training

class QLoRAFineTuner(LoRAFineTuner):
    def __init__(self, model_name: str, task_type: TaskType = TaskType.CAUSAL_LM):
        super().__init__(model_name, task_type)
        self.bnb_config = None
        
    def setup_quantization_config(self):
        """Configure 4-bit quantization"""
        self.bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
        )
        
    def setup_model_and_tokenizer(self):
        """Initialize quantized model"""
        self.setup_quantization_config()
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True,
            use_auth_token=os.getenv('HUGGINGFACE_TOKEN')
        )
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            quantization_config=self.bnb_config,
            device_map="auto",
            trust_remote_code=True,
            use_auth_token=os.getenv('HUGGINGFACE_TOKEN')
        )
        
        # Prepare model for k-bit training
        self.model = prepare_model_for_kbit_training(self.model)
        
    def configure_lora(self, r: int = 64, lora_alpha: int = 16, 
                      lora_dropout: float = 0.1, target_modules: list = None):
        """Configure LoRA for quantized model"""
        if target_modules is None:
            target_modules = ["q_proj", "v_proj", "k_proj", "o_proj"]
            
        lora_config = LoraConfig(
            r=r,
            lora_alpha=lora_alpha,
            target_modules=target_modules,
            lora_dropout=lora_dropout,
            bias="none",
            task_type=self.task_type,
        )
        
        self.peft_model = get_peft_model(self.model, lora_config)
        self.peft_model.print_trainable_parameters()
        
# Example: Fine-tune Llama model with QLoRA
def qlora_llama_example():
    qlora_tuner = QLoRAFineTuner("meta-llama/Llama-2-7b-chat-hf")
    qlora_tuner.setup_model_and_tokenizer()
    qlora_tuner.configure_lora(r=64, lora_alpha=16)
    
    # Custom dataset preparation and training would follow
    print("QLoRA model prepared for training")
```

### RLHF Implementation Framework

Implementing RLHF involves reward modeling and policy optimization:

```python
import torch.nn.functional as F
from transformers import pipeline
from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead
from trl.core import LengthSampler

class RLHFTrainer:
    def __init__(self, model_name: str, reward_model_name: str):
        self.model_name = model_name
        self.reward_model_name = reward_model_name
        self.ppo_trainer = None
        self.reward_model = None
        
    def setup_models(self):
        """Initialize policy and reward models"""
        # Policy model (the one being trained)
        self.model = AutoModelForCausalLMWithValueHead.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        # Reward model for scoring responses
        self.reward_model = pipeline(
            "text-classification",
            model=self.reward_model_name,
            device_map="auto"
        )
        
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
    def setup_ppo_trainer(self):
        """Configure PPO training"""
        ppo_config = PPOConfig(
            model_name=self.model_name,
            learning_rate=1.41e-5,
            batch_size=16,
            mini_batch_size=4,
            gradient_accumulation_steps=1,
            optimize_cuda_cache=True,
            early_stopping=False,
            target_kl=0.1,
            ppo_epochs=4,
            seed=0,
        )
        
        self.ppo_trainer = PPOTrainer(
            config=ppo_config,
            model=self.model,
            ref_model=None,  # Use the same model as reference
            tokenizer=self.tokenizer,
        )
        
    def compute_reward(self, responses: list) -> list:
        """Compute rewards for generated responses"""
        rewards = []
        for response in responses:
            # Use reward model to score response
            score = self.reward_model(response)[0]['score']
            rewards.append(torch.tensor(score, dtype=torch.float))
        return rewards
        
    def train_step(self, queries: list):
        """Execute one PPO training step"""
        query_tensors = [self.tokenizer.encode(query, return_tensors="pt")[0] 
                        for query in queries]
        
        # Generate responses
        response_tensors = []
        for query_tensor in query_tensors:
            with torch.no_grad():
                response = self.ppo_trainer.generate(
                    query_tensor.unsqueeze(0),
                    max_length=256,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
                response_tensors.append(response.squeeze()[len(query_tensor):])
        
        # Decode responses for reward computation
        responses = [self.tokenizer.decode(r, skip_special_tokens=True) 
                    for r in response_tensors]
        
        # Compute rewards
        rewards = self.compute_reward(responses)
        
        # PPO step
        stats = self.ppo_trainer.step(query_tensors, response_tensors, rewards)
        return stats
        
    def train(self, training_queries: list, num_epochs: int = 5):
        """Full RLHF training loop"""
        for epoch in range(num_epochs):
            for i in range(0, len(training_queries), 16):  # Batch size 16
                batch_queries = training_queries[i:i+16]
                stats = self.train_step(batch_queries)
                
                if i % 100 == 0:
                    print(f"Epoch {epoch}, Step {i}: {stats}")

# Example usage
def demonstrate_rlhf():
    rlhf_trainer = RLHFTrainer(
        model_name="gpt2",
        reward_model_name="sentiment-analysis-model"
    )
    
    rlhf_trainer.setup_models()
    rlhf_trainer.setup_ppo_trainer()
    
    training_queries = [
        "Write a helpful response about Python programming",
        "Explain machine learning concepts clearly",
        # Add more training queries...
    ]
    
    rlhf_trainer.train(training_queries, num_epochs=3)
```

### Advanced Optimization Techniques

````python
class AdvancedFineTuningOptimizer:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        
    def setup_gradient_checkpointing(self):
        """Enable gradient checkpointing to save memory"""
        self.model.gradient_checkpointing_enable()
        
    def setup_mixed_precision_training(self):
        """Configure automatic mixed precision"""
        from torch.cuda.amp import GradScaler, autocast
        self.scaler = GradScaler()
        return autocast()
        
    def apply_weight_decay_optimization(self, optimizer_params: list):
        """Configure optimized weight decay"""
        no_decay = ["bias", "LayerNorm.weight", "layer_norm.weight"]
        
        optimizer_grouped_parameters = [
            {
                "params": [p for n, p in self.model.named_parameters() 
                          if not any(nd in n for nd in no_decay)],
                "weight_decay": 0.01,
            },
            {
                "params": [p for n, p in self.model.named_parameters() 
                          if any(nd in n for nd in no_decay)],
                "weight_decay": 0.0,
            },
        ]
        return optimizer_grouped_parameters
        
    def setup_learning_rate_scheduling(self, optimizer, num_training_steps: int):
        """Configure learning rate scheduling"""
        from transformers import get_linear_schedule_with_warmup
        
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=num_training_steps * 0.1,  # 10% warmup
            num_training_steps=num_training_steps
        )
        return scheduler
        
    def compute_model_efficiency_metrics(self):
        """Calculate model efficiency metrics"""
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        efficiency_ratio = trainable_params / total_params
        memory_footprint = sum(p.numel() * p.element_size() for p in self.model.parameters()) / (1024**3)  # GB
        
        return {
            "total_parameters": total_params,
            "trainable_parameters": trainable_params,
            "efficiency_ratio": efficiency_ratio,
            "memory_footprint_gb": memory_footprint
        }

# Comprehensive fine-tuning pipeline
class ComprehensiveFineTuningPipeline:
    def __init__(self, base_model_name: str, technique: str = "qlora"):
        self.base_model_name = base_model_name
        self.technique = technique
        self.model = None
        self.tokenizer = None
        
    def initialize_pipeline(self):
        """Set up the complete fine-tuning pipeline"""
        if self.technique == "lora":
            self.tuner = LoRAFineTuner(self.base_model_name)
        elif self.technique == "qlora":
            self.tuner = QLoRAFineTuner(self.base_model_name)
        else:
            raise ValueError(f"Unsupported technique: {self.technique}")
            
        self.tuner.setup_model_and_tokenizer()
        self.tuner.configure_lora()
        
        # Setup optimization
        self.optimizer = AdvancedFineTuningOptimizer(
            self.tuner.peft_model, 
            self.tuner.tokenizer
        )
        
    def execute_full_training(self, dataset, evaluation_dataset=None):
        """Execute complete training with monitoring"""
        # Enable optimizations
        self.optimizer.setup_gradient_checkpointing()
        
        # Get efficiency metrics
        metrics = self.optimizer.compute_model_efficiency_metrics()
        print(f"Model efficiency metrics: {metrics}")
        
        # Execute training
        self.tuner.train(dataset)
        
        # Post-training evaluation
        if evaluation_dataset:
            self.evaluate_model(evaluation_dataset)
            
    def evaluate_model(self, eval_dataset):
        """Comprehensive model evaluation"""
        # Implementation would include BLEU, ROUGE, perplexity calculations
        print("Model evaluation completed")
        
    def save_and_export(self, output_path: str):
        """Save model and create deployment artifacts"""
        self.tuner.peft_model.save_pretrained(output_path)
        self.tuner.tokenizer.save_pretrained(output_path)
        
        # Create model card
        model_card = f"""
# Fine-tuned Model: {self.base_model_name}

## Training Details
- Base Model: {self.base_model_name}
- Fine-tuning Technique: {self.technique}
- Training Date: {torch.datetime.now()}

## Usage
```python
from peft import PeftModel
model = PeftModel.from_pretrained(base_model, "{output_path}")
```
"""
        with open(f"{output_path}/README.md", "w") as f:
            f.write(model_card)

# Example execution
def main():
    # Demonstrate comprehensive fine-tuning
    pipeline = ComprehensiveFineTuningPipeline(
        base_model_name="microsoft/DialoGPT-medium",
        technique="qlora"
    )
    
    pipeline.initialize_pipeline()
    
    # Custom training data
    training_data = [
        {"instruction": "Explain quantum computing", "response": "Quantum computing uses quantum mechanical phenomena..."},
        # Add more examples
    ]
    
    dataset = pipeline.tuner.prepare_dataset(custom_data=training_data)
    pipeline.execute_full_training(dataset)
    pipeline.save_and_export("./fine-tuned-model")
    
if __name__ == "__main__":
    main()
````

## Conclusion

Advanced fine-tuning techniques in HuggingFace enable efficient adaptation of large language models to specific tasks while managing computational constraints. LoRA and QLoRA provide parameter-efficient alternatives to full fine-tuning, reducing memory requirements by up to 90% while maintaining performance quality. RLHF integration allows alignment with human preferences through reinforcement learning, creating more helpful and harmless AI systems.

The combination of these techniques with modern optimization strategies like gradient checkpointing, mixed precision training, and advanced scheduling creates a robust framework for production-ready model customization. QLoRA particularly stands out for enabling fine-tuning of billion-parameter models on consumer hardware, democratizing access to state-of-the-art model adaptation capabilities.

Success in advanced fine-tuning requires careful consideration of the trade-offs between efficiency, quality, and computational resources, with the choice of technique depending on specific use case requirements, available hardware, and target performance metrics.

---

I've created a comprehensive markdown document for Section 06 covering advanced fine-tuning techniques in HuggingFace. The content includes:

**Key concepts explained:**
- LoRA (Low-Rank Adaptation)
- QLoRA (Quantized LoRA)  
- PPO (Proximal Policy Optimization)
- RLHF (Reinforcement Learning from Human Feedback)

**Technical implementations:**
- Complete LoRA fine-tuning class with modern Python code
- QLoRA implementation using 4-bit quantization
- RLHF training framework with PPO
- Advanced optimization techniques
- Comprehensive pipeline for production use

**Modern solutions covered:**
- Parameter-efficient fine-tuning (PEFT)
- Gradient checkpointing for memory optimization
- Mixed precision training
- Learning rate scheduling
- Model efficiency metrics

The code examples use the latest HuggingFace libraries (transformers, peft, trl) and include proper error handling, environment variable usage for API keys, and production-ready patterns. Each technique is demonstrated with complete, runnable code that can be adapted for specific use cases.

The document maintains technical depth while being practically applicable for software engineers, data engineers, and tech leaders working with large language model fine-tuning.