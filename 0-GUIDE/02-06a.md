<small>Claude 3.7 Sonnet Thinking</small>
# 06. Advanced Fine-tuning with HuggingFace

## Key Terms

- **Fine-tuning**: The process of adapting a pre-trained language model to specific tasks or domains by continued training on specialized data.
- **LoRA (Low-Rank Adaptation)**: An efficient fine-tuning technique that freezes the pre-trained model weights and injects trainable rank decomposition matrices into layers to reduce the number of trainable parameters.
- **QLoRA (Quantized Low-Rank Adaptation)**: An extension of LoRA that uses quantization to further reduce memory requirements, enabling fine-tuning of large models on consumer-grade hardware.
- **PPO (Proximal Policy Optimization)**: A reinforcement learning algorithm used to align language models with human preferences through policy optimization.
- **RLHF (Reinforcement Learning from Human Feedback)**: A training methodology that uses human preferences to guide model outputs through reinforcement learning.
- **PEFT (Parameter-Efficient Fine-Tuning)**: A collection of techniques that update only a small subset of a model's parameters during fine-tuning.
- **Adapter**: Small neural modules inserted between layers of a pre-trained model to adapt it to new tasks with minimal parameter updates.
- **Prompt Tuning**: A technique that prepends trainable continuous vectors to the input to steer the model toward specific tasks.

## Advanced Model Fine-tuning Techniques

### Implementing LoRA Fine-tuning

LoRA significantly reduces memory requirements while maintaining performance by adding low-rank matrices to the original model weights:

```python
import os
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    TaskType
)
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

class LoRAFineTuner:
    """Implement LoRA fine-tuning for causal language models."""
    
    def __init__(
        self,
        base_model_name: str,
        lora_r: int = 16,
        lora_alpha: int = 32,
        lora_dropout: float = 0.05,
        output_dir: str = "./outputs",
        device_map: str = "auto"
    ):
        """
        Initialize the LoRA fine-tuner.
        
        Args:
            base_model_name: Name of the base model from Hugging Face
            lora_r: Rank of the LoRA decomposition
            lora_alpha: LoRA alpha parameter (scaling factor)
            lora_dropout: Dropout probability for LoRA layers
            output_dir: Directory to save outputs
            device_map: Device mapping strategy
        """
        self.base_model_name = base_model_name
        self.lora_r = lora_r
        self.lora_alpha = lora_alpha
        self.lora_dropout = lora_dropout
        self.output_dir = output_dir
        self.device_map = device_map
        
        # Create output directory
        os.makedirs(output_dir, exist_ok=True)
        
        # Load tokenizer and model
        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        
        # Ensure the tokenizer has a pad token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # Load the model
        self.model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16,
            device_map=device_map
        )
        
        # Configure LoRA
        self.lora_config = LoraConfig(
            r=lora_r,
            lora_alpha=lora_alpha,
            target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "down_proj", "up_proj"],
            lora_dropout=lora_dropout,
            bias="none",
            task_type=TaskType.CAUSAL_LM
        )
        
        # Prepare the model for LoRA fine-tuning
        self.model = get_peft_model(self.model, self.lora_config)
    
    def prepare_dataset(
        self,
        dataset_name: str = None,
        dataset_path: str = None,
        text_column: str = "text",
        train_test_split: float = 0.1,
        max_length: int = 512,
        preprocessing_function = None
    ):
        """
        Prepare dataset for fine-tuning.
        
        Args:
            dataset_name: Name of the dataset on Hugging Face Hub
            dataset_path: Path to local dataset (alternative to dataset_name)
            text_column: Column containing text data
            train_test_split: Ratio of validation split
            max_length: Maximum sequence length
            preprocessing_function: Custom preprocessing function
        """
        # Load dataset
        if dataset_name:
            self.dataset = load_dataset(dataset_name)
        elif dataset_path:
            self.dataset = load_dataset(dataset_path)
        else:
            raise ValueError("Either dataset_name or dataset_path must be provided")
        
        # Apply custom preprocessing if provided
        if preprocessing_function:
            self.dataset = self.dataset.map(preprocessing_function, batched=True)
        
        # Split dataset if needed
        if "train" in self.dataset and "validation" not in self.dataset and train_test_split > 0:
            split = self.dataset["train"].train_test_split(test_size=train_test_split)
            self.dataset["train"] = split["train"]
            self.dataset["validation"] = split["test"]
        
        # Tokenize the dataset
        def tokenize_function(examples):
            return self.tokenizer(
                examples[text_column],
                padding="max_length",
                truncation=True,
                max_length=max_length,
                return_tensors="pt"
            )
        
        self.tokenized_dataset = self.dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=[col for col in self.dataset["train"].column_names if col != text_column]
        )
        
        # Create data collator
        self.data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False
        )
    
    def train(
        self,
        num_train_epochs: int = 3,
        per_device_train_batch_size: int = 4,
        per_device_eval_batch_size: int = 4,
        gradient_accumulation_steps: int = 8,
        learning_rate: float = 3e-4,
        weight_decay: float = 0.01,
        warmup_steps: int = 100,
        logging_steps: int = 10,
        eval_steps: int = 100,
        save_steps: int = 100
    ):
        """
        Train the model with LoRA.
        
        Args:
            num_train_epochs: Number of training epochs
            per_device_train_batch_size: Batch size per device during training
            per_device_eval_batch_size: Batch size per device during evaluation
            gradient_accumulation_steps: Number of steps to accumulate gradients
            learning_rate: Learning rate
            weight_decay: Weight decay
            warmup_steps: Number of warmup steps
            logging_steps: Number of steps between logging updates
            eval_steps: Number of steps between evaluations
            save_steps: Number of steps between model saves
        """
        # Set up training arguments
        training_args = TrainingArguments(
            output_dir=self.output_dir,
            num_train_epochs=num_train_epochs,
            per_device_train_batch_size=per_device_train_batch_size,
            per_device_eval_batch_size=per_device_eval_batch_size,
            gradient_accumulation_steps=gradient_accumulation_steps,
            learning_rate=learning_rate,
            weight_decay=weight_decay,
            warmup_steps=warmup_steps,
            logging_steps=logging_steps,
            evaluation_strategy="steps",
            eval_steps=eval_steps,
            save_strategy="steps",
            save_steps=save_steps,
            save_total_limit=3,
            load_best_model_at_end=True,
            report_to="tensorboard",
            fp16=True,
            optim="adamw_torch"
        )
        
        # Initialize Trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=self.tokenized_dataset["train"],
            eval_dataset=self.tokenized_dataset["validation"] if "validation" in self.tokenized_dataset else None,
            data_collator=self.data_collator,
        )
        
        # Print model parameter summary
        model_info = self.model.get_memory_footprint()
        print(f"Model memory footprint: {model_info / 1e9:.2f} GB")
        print(f"Trainable parameters: {sum(p.numel() for p in self.model.parameters() if p.requires_grad)}")
        print(f"Total parameters: {sum(p.numel() for p in self.model.parameters())}")
        
        # Train the model
        trainer.train()
        
        # Save the final model
        self.model.save_pretrained(os.path.join(self.output_dir, "final_model"))
        self.tokenizer.save_pretrained(os.path.join(self.output_dir, "final_model"))
        
        return trainer
```

### Implementing QLoRA for Memory-Efficient Fine-tuning

QLoRA combines quantization with LoRA to enable fine-tuning of large models on limited hardware:

```python
import os
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training
)
from datasets import load_dataset
from tqdm import tqdm

class QLoRAFineTuner:
    """Implement QLoRA fine-tuning for large language models."""
    
    def __init__(
        self,
        base_model_name: str,
        output_dir: str = "./outputs_qlora",
        lora_r: int = 8,
        lora_alpha: int = 16,
        lora_dropout: float = 0.05,
        quantization_bits: int = 4
    ):
        """
        Initialize the QLoRA fine-tuner.
        
        Args:
            base_model_name: Name of the base model from Hugging Face
            output_dir: Directory to save outputs
            lora_r: Rank of the LoRA decomposition
            lora_alpha: LoRA alpha parameter
            lora_dropout: Dropout probability for LoRA layers
            quantization_bits: Bits for quantization (4 or 8)
        """
        self.base_model_name = base_model_name
        self.output_dir = output_dir
        self.lora_r = lora_r
        self.lora_alpha = lora_alpha
        self.lora_dropout = lora_dropout
        
        # Create output directory
        os.makedirs(output_dir, exist_ok=True)
        
        # Configure quantization
        quant_config = BitsAndBytesConfig(
            load_in_4bit=quantization_bits == 4,
            load_in_8bit=quantization_bits == 8,
            bnb_4bit_quant_type="nf4" if quantization_bits == 4 else None,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True
        )
        
        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        
        # Ensure the tokenizer has a pad token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # Load the quantized model
        self.model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            quantization_config=quant_config,
            device_map="auto"
        )
        
        # Prepare model for k-bit training
        self.model = prepare_model_for_kbit_training(self.model)
        
        # Define LoRA config
        target_modules = self._get_target_modules()
        
        self.lora_config = LoraConfig(
            r=lora_r,
            lora_alpha=lora_alpha,
            target_modules=target_modules,
            lora_dropout=lora_dropout,
            bias="none",
            task_type="CAUSAL_LM"
        )
        
        # Apply LoRA adapter
        self.model = get_peft_model(self.model, self.lora_config)
        self.model.print_trainable_parameters()
    
    def _get_target_modules(self):
        """Determine target modules based on model architecture."""
        # This is a simplified approach - in practice, you'd want to inspect 
        # the model architecture more carefully
        if "llama" in self.base_model_name.lower():
            return ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "down_proj", "up_proj"]
        elif "mistral" in self.base_model_name.lower():
            return ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "down_proj", "up_proj"]
        elif "gpt-neox" in self.base_model_name.lower():
            return ["query_key_value", "dense", "dense_h_to_4h", "dense_4h_to_h"]
        elif "falcon" in self.base_model_name.lower():
            return ["query_key_value", "dense", "dense_h_to_4h", "dense_4h_to_h"]
        else:
            # Default for many models
            return ["query", "key", "value", "dense"]
    
    def prepare_instruction_dataset(
        self,
        dataset_path: str = None,
        instruction_column: str = "instruction",
        response_column: str = "response",
        max_length: int = 1024
    ):
        """
        Prepare dataset in instruction format.
        
        Args:
            dataset_path: Path to instruction dataset
            instruction_column: Column containing instructions
            response_column: Column containing responses
            max_length: Maximum sequence length
        """
        # Load the dataset
        dataset = load_dataset("json", data_files=dataset_path)
        
        # Define formatting function for instruction tuning
        def format_instruction(example):
            instruction = example[instruction_column]
            response = example[response_column]
            
            # Create instruction format
            text = f"### Instruction:\n{instruction}\n\n### Response:\n{response}"
            return {"text": text}
        
        # Apply formatting
        formatted_dataset = dataset["train"].map(format_instruction)
        
        # Split into train and validation
        split_dataset = formatted_dataset.train_test_split(test_size=0.05)
        
        # Tokenize the dataset
        def tokenize_function(examples):
            return self.tokenizer(
                examples["text"],
                padding="max_length",
                truncation=True,
                max_length=max_length,
                return_tensors="pt"
            )
        
        tokenized_dataset = split_dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=["text"]
        )
        
        # Add labels for causal language modeling
        def add_labels(examples):
            examples["labels"] = examples["input_ids"].copy()
            return examples
        
        self.tokenized_dataset = tokenized_dataset.map(
            add_labels,
            batched=True
        )
    
    def train(
        self,
        num_train_epochs: int = 3,
        per_device_train_batch_size: int = 2,
        gradient_accumulation_steps: int = 16,
        learning_rate: float = 2e-4,
        max_grad_norm: float = 0.3,
        warmup_ratio: float = 0.03
    ):
        """
        Train the model with QLoRA.
        
        Args:
            num_train_epochs: Number of training epochs
            per_device_train_batch_size: Batch size per device during training
            gradient_accumulation_steps: Number of steps to accumulate gradients
            learning_rate: Learning rate
            max_grad_norm: Maximum gradient norm
            warmup_ratio: Ratio of warmup steps
        """
        # Set up training arguments
        training_args = TrainingArguments(
            output_dir=self.output_dir,
            num_train_epochs=num_train_epochs,
            per_device_train_batch_size=per_device_train_batch_size,
            gradient_accumulation_steps=gradient_accumulation_steps,
            learning_rate=learning_rate,
            max_grad_norm=max_grad_norm,
            warmup_ratio=warmup_ratio,
            logging_steps=10,
            evaluation_strategy="steps",
            eval_steps=100,
            save_strategy="steps",
            save_steps=100,
            save_total_limit=3,
            load_best_model_at_end=True,
            report_to="tensorboard",
            push_to_hub=False,
            fp16=True,
            optim="paged_adamw_8bit"
        )
        
        # Initialize Trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=self.tokenized_dataset["train"],
            eval_dataset=self.tokenized_dataset["test"],
        )
        
        # Train the model
        trainer.train()
        
        # Save the final model
        self.model.save_pretrained(os.path.join(self.output_dir, "final_model"))
        self.tokenizer.save_pretrained(os.path.join(self.output_dir, "final_model"))
        
        return trainer
```

## RLHF Implementation with Hugging Face

Reinforcement Learning from Human Feedback allows models to align with human preferences:

```python
import os
import torch
import numpy as np
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    Trainer,
    TrainingArguments
)
from peft import PeftModel, PeftConfig
from trl import (
    PPOConfig,
    PPOTrainer,
    AutoModelForCausalLMWithValueHead,
    create_reference_model
)
from trl.core import respond_to_batch

class RLHFTrainer:
    """Implement Reinforcement Learning from Human Feedback."""
    
    def __init__(
        self,
        base_model_path: str,
        reward_model_path: str = None,
        output_dir: str = "./outputs_rlhf"
    ):
        """
        Initialize the RLHF trainer.
        
        Args:
            base_model_path: Path to the fine-tuned model to align
            reward_model_path: Path to the reward model
            output_dir: Directory to save outputs
        """
        self.base_model_path = base_model_path
        self.reward_model_path = reward_model_path
        self.output_dir = output_dir
        
        # Create output directory
        os.makedirs(output_dir, exist_ok=True)
        
        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(base_model_path)
        
        # Ensure the tokenizer has a pad token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
        
        # Load model with value head for PPO
        self.model = AutoModelForCausalLMWithValueHead.from_pretrained(
            base_model_path,
            device_map="auto",
            torch_dtype=torch.float16
        )
        
        # Create reference model (for KL penalty)
        self.ref_model = create_reference_model(self.model)
        
        # Load reward model if provided
        if reward_model_path:
            self.reward_model = AutoModelForCausalLM.from_pretrained(
                reward_model_path,
                device_map="auto",
                torch_dtype=torch.float16
            )
        else:
            self.reward_model = None
    
    def prepare_ppo_dataset(
        self,
        dataset_path: str,
        prompt_column: str = "prompt",
        chosen_column: str = "chosen",
        rejected_column: str = "rejected",
        max_samples: int = 1000
    ):
        """
        Prepare dataset for PPO training.
        
        Args:
            dataset_path: Path to dataset with human preferences
            prompt_column: Column containing prompts
            chosen_column: Column containing preferred responses
            rejected_column: Column containing rejected responses
            max_samples: Maximum number of samples to use
        """
        # Load the dataset
        dataset = load_dataset("json", data_files=dataset_path)["train"]
        
        # Limit dataset size if needed
        if max_samples and max_samples < len(dataset):
            dataset = dataset.select(range(max_samples))
        
        # Extract prompts
        self.prompts = dataset[prompt_column]
        
        # Store chosen and rejected responses for custom reward model training
        self.chosen_responses = dataset[chosen_column]
        self.rejected_responses = dataset[rejected_column]
        
        print(f"Loaded {len(self.prompts)} prompt-response pairs")
    
    def _compute_reward(self, prompt, response):
        """
        Compute reward for a response.
        
        Args:
            prompt: Input prompt
            response: Model generated response
            
        Returns:
            Reward score
        """
        if self.reward_model:
            # Use provided reward model
            inputs = self.tokenizer(prompt + response, return_tensors="pt").to(self.reward_model.device)
            with torch.no_grad():
                reward = self.reward_model(**inputs).logits[0, -1].cpu().numpy()
            return float(reward)
        else:
            # Simple length-based reward for demonstration purposes
            # In practice, you would use a proper reward model
            return min(len(response.split()) / 50, 1.0)
    
    def train_with_ppo(
        self,
        num_epochs: int = 1,
        batch_size: int = 8,
        mini_batch_size: int = 4,
        learning_rate: float = 1.4e-6,
        kl_penalty: float = 0.2
    ):
        """
        Train the model with PPO.
        
        Args:
            num_epochs: Number of training epochs
            batch_size: Batch size for PPO
            mini_batch_size: Mini-batch size for PPO
            learning_rate: Learning rate
            kl_penalty: KL divergence penalty coefficient
        """
        # Configure PPO
        ppo_config = PPOConfig(
            learning_rate=learning_rate,
            batch_size=batch_size,
            mini_batch_size=mini_batch_size,
            optimize_cuda_cache=True,
            gradient_accumulation_steps=1,
            kl_penalty=kl_penalty,
            ppo_epochs=num_epochs,
            seed=42
        )
        
        # Initialize PPO trainer
        ppo_trainer = PPOTrainer(
            config=ppo_config,
            model=self.model,
            ref_model=self.ref_model,
            tokenizer=self.tokenizer,
            dataset=self.prompts
        )
        
        # Training loop
        for epoch in range(num_epochs):
            print(f"Epoch {epoch+1}/{num_epochs}")
            
            for i, batch in enumerate(ppo_trainer.dataloader):
                print(f"Batch {i+1}")
                
                # Get queries
                query_tensors = [
                    self.tokenizer(prompt, return_tensors="pt").input_ids.squeeze()
                    for prompt in batch
                ]
                
                # Generate responses
                responses = []
                response_tensors = []
                
                for query in query_tensors:
                    response_tensor = respond_to_batch(
                        self.model,
                        [query.to(self.model.device)],
                        self.tokenizer,
                        max_new_tokens=128
                    )
                    response_tensors.append(response_tensor)
                    
                    response_text = self.tokenizer.decode(
                        response_tensor[0], 
                        skip_special_tokens=True
                    )
                    responses.append(response_text)
                
                # Calculate rewards
                rewards = []
                for prompt, response in zip(batch, responses):
                    reward = self._compute_reward(prompt, response)
                    rewards.append(torch.tensor(reward, device=self.model.device))
                
                # Run PPO step
                stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
                ppo_trainer.log_stats(stats, batch, rewards)
        
        # Save the final model
        self.model.save_pretrained(os.path.join(self.output_dir, "final_model"))
        self.tokenizer.save_pretrained(os.path.join(self.output_dir, "final_model"))
        
        return self.model
```

## Task-Specific Model Adaptation

Fine-tuning models for specific tasks requires careful consideration of the task structure:

```python
import os
import torch
import pandas as pd
from typing import Dict, List
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    EvalPrediction
)
from datasets import load_dataset
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import numpy as np

class TaskSpecificAdapter:
    """Adapt models for specific downstream tasks."""
    
    def __init__(
        self,
        base_model_name: str,
        task_type: str,
        num_labels: int = 2,
        output_dir: str = "./outputs_task"
    ):
        """
        Initialize the task adapter.
        
        Args:
            base_model_name: Name of the base model from Hugging Face
            task_type: Type of task (classification, regression, etc.)
            num_labels: Number of labels for classification
            output_dir: Directory to save outputs
        """
        self.base_model_name = base_model_name
        self.task_type = task_type
        self.num_labels = num_labels
        self.output_dir = output_dir
        
        # Create output directory
        os.makedirs(output_dir, exist_ok=True)
        
        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        
        # Load model based on task
        if task_type == "classification":
            self.model = AutoModelForSequenceClassification.from_pretrained(
                base_model_name,
                num_labels=num_labels,
                torch_dtype=torch.float16,
                device_map="auto"
            )
        else:
            raise ValueError(f"Unsupported task type: {task_type}")
    
    def prepare_classification_dataset(
        self,
        dataset_name: str = None,
        dataset_path: str = None,
        text_column: str = "text",
        label_column: str = "label",
        max_length: int = 512
    ):
        """
        Prepare dataset for classification tasks.
        
        Args:
            dataset_name: Name of the dataset on Hugging Face Hub
            dataset_path: Path to local dataset
            text_column: Column containing text data
            label_column: Column containing labels
            max_length: Maximum sequence length
        """
        # Load dataset
        if dataset_name:
            self.dataset = load_dataset(dataset_name)
        elif dataset_path:
            if dataset_path.endswith(".csv"):
                self.dataset = load_dataset("csv", data_files=dataset_path)
            elif dataset_path.endswith(".json") or dataset_path.endswith(".jsonl"):
                self.dataset = load_dataset("json", data_files=dataset_path)
            else:
                raise ValueError(f"Unsupported file format: {dataset_path}")
        else:
            raise ValueError("Either dataset_name or dataset_path must be provided")
        
        # Define preprocessing function
        def preprocess_function(examples):
            return self.tokenizer(
                examples[text_column],
                truncation=True,
                max_length=max_length,
                padding="max_length"
            )
        
        # Preprocess dataset
        self.processed_dataset = self.dataset.map(
            preprocess_function,
            batched=True,
            remove_columns=[col for col in self.dataset["train"].column_names if col != label_column]
        )
        
        # Rename label column if needed
        if label_column != "labels":
            self.processed_dataset = self.processed_dataset.rename_column(label_column, "labels")
        
        # Set format for PyTorch
        self.processed_dataset.set_format(
            type="torch", 
            columns=["input_ids", "attention_mask", "labels"]
        )
    
    def compute_metrics(self, pred: EvalPrediction) -> Dict:
        """
        Compute metrics for evaluation.
        
        Args:
            pred: Prediction object containing predictions and labels
            
        Returns:
            Dictionary of metrics
        """
        labels = pred.label_ids
        preds = pred.predictions.argmax(-1)
        
        precision, recall, f1, _ = precision_recall_fscore_support(
            labels, preds, average="weighted"
        )
        acc = accuracy_score(labels, preds)
        
        return {
            "accuracy": acc,
            "f1": f1,
            "precision": precision,
            "recall": recall
        }
    
    def train(
        self,
        num_train_epochs: int = 3,
        per_device_train_batch_size: int = 8,
        per_device_eval_batch_size: int = 8,
        learning_rate: float = 2e-5,
        weight_decay: float = 0.01,
        warmup_ratio: float = 0.1,
        evaluation_strategy: str = "epoch"
    ):
        """
        Train the model for the specific task.
        
        Args:
            num_train_epochs: Number of training epochs
            per_device_train_batch_size: Batch size for training
            per_device_eval_batch_size: Batch size for evaluation
            learning_rate: Learning rate
            weight_decay: Weight decay
            warmup_ratio: Ratio of steps for warmup
            evaluation_strategy: When to perform evaluation
        """
        # Set up training arguments
        training_args = TrainingArguments(
            output_dir=self.output_dir,
            num_train_epochs=num_train_epochs,
            per_device_train_batch_size=per_device_train_batch_size,
            per_device_eval_batch_size=per_device_eval_batch_size,
            learning_rate=learning_rate,
            weight_decay=weight_decay,
            warmup_ratio=warmup_ratio,
            evaluation_strategy=evaluation_strategy,
            logging_dir=os.path.join(self.output_dir, "logs"),
            logging_steps=10,
            save_strategy=evaluation_strategy,
            save_total_limit=2,
            load_best_model_at_end=True,
            metric_for_best_model="f1",
            report_to="tensorboard",
            fp16=True
        )
        
        # Initialize Trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=self.processed_dataset["train"],
            eval_dataset=self.processed_dataset["validation"] if "validation" in self.processed_dataset else None,
            compute_metrics=self.compute_metrics
        )
        
        # Train the model
        trainer.train()
        
        # Evaluate the model
        eval_results = trainer.evaluate()
        
        # Save the model
        self.model.save_pretrained(os.path.join(self.output_dir, "final_model"))
        self.tokenizer.save_pretrained(os.path.join(self.output_dir, "final_model"))
        
        return trainer, eval_results
```

## Performance Optimization Techniques

Optimizing model performance involves several techniques to improve efficiency and output quality:

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import time
from tqdm import tqdm
import numpy as np
import gc
from torch.utils.data import DataLoader, Dataset

class ModelOptimizer:
    """Optimize model performance for inference and training."""
    
    def __init__(self, model_path: str):
        """
        Initialize the model optimizer.
        
        Args:
            model_path: Path to the model
        """
        self.model_path = model_path
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = None
        
        # Ensure the tokenizer has a pad token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
    
    def load_model_optimized(
        self,
        quantization: str = None,  # None, '8bit', '4bit'
        use_bettertransformer: bool = True,
        use_flash_attention: bool = True,
        device_map: str = "auto"
    ):
        """
        Load model with optimizations.
        
        Args:
            quantization: Quantization method
            use_bettertransformer: Whether to use BetterTransformer
            use_flash_attention: Whether to use Flash Attention
            device_map: Device mapping strategy
        """
        # Set up quantization config
        kwargs = {
            "device_map": device_map,
            "torch_dtype": torch.float16
        }
        
        if quantization == "8bit":
            kwargs["load_in_8bit"] = True
        elif quantization == "4bit":
            from transformers import BitsAndBytesConfig
            
            kwargs["quantization_config"] = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
        
        # Load the model
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            **kwargs
        )
        
        # Apply BetterTransformer if requested
        if use_bettertransformer:
            try:
                from optimum.bettertransformer import BetterTransformer
                self.model = BetterTransformer.transform(self.model)
                print("BetterTransformer applied")
            except:
                print("Warning: BetterTransformer not available")
        
        # Enable Flash Attention if requested
        if use_flash_attention and hasattr(self.model.config, "use_flash_attention"):
            self.model.config.use_flash_attention = True
            print("Flash Attention enabled")
        
        # Cache model for faster inference
        self.model = self.model.eval()
        
        return self.model
    
    def benchmark_inference(
        self,
        prompt: str,
        generation_length: int = 100,
        num_runs: int = 5,
        optimize_memory: bool = True
    ):
        """
        Benchmark model inference performance.
        
        Args:
            prompt: Text prompt for generation
            generation_length: Length of generated text
            num_runs: Number of runs for benchmarking
            optimize_memory: Whether to optimize memory usage
            
        Returns:
            Dictionary with benchmark results
        """
        if self.model is None:
            raise ValueError("Model not loaded. Call load_model_optimized first.")
        
        # Prepare input
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        
        # Warm-up run
        with torch.no_grad():
            _ = self.model.generate(
                inputs.input_ids,
                max_new_tokens=10,
                do_sample=False
            )
        
        # Benchmark runs
        latencies = []
        
        for _ in tqdm(range(num_runs), desc="Benchmarking"):
            # Clear CUDA cache if optimizing memory
            if optimize_memory and torch.cuda.is_available():
                torch.cuda.empty_cache()
                gc.collect()
                
            start_time = time.time()
            
            with torch.no_grad():
                output = self.model.generate(
                    inputs.input_ids,
                    max_new_tokens=generation_length,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9
                )
            
            end_time = time.time()
            latency = end_time - start_time
            latencies.append(latency)
            
            # Decode output for validation
            decoded_output = self.tokenizer.decode(output[0], skip_special_tokens=True)
            
            # Add a small delay between runs
            time.sleep(0.5)
        
        # Calculate statistics
        avg_latency = np.mean(latencies)
        std_latency = np.std(latencies)
        tokens_per_second = generation_length / avg_latency
        
        results = {
            "avg_latency_seconds": avg_latency,
            "std_latency_seconds": std_latency,
            "tokens_per_second": tokens_per_second,
            "num_runs": num_runs,
            "generation_length": generation_length
        }
        
        print(f"Average latency: {avg_latency:.4f}s ± {std_latency:.4f}")
        print(f"Tokens per second: {tokens_per_second:.2f}")
        
        return results
    
    def optimize_prompt(self, prompt: str):
        """
        Optimize a prompt for better model performance.
        
        Args:
            prompt: Original prompt
            
        Returns:
            Optimized prompt
        """
        # Add clear instruction prefix
        if not prompt.startswith(("I want you to", "Please", "You are")):
            prompt = "Task: " + prompt
        
        # Add format instructions if needed
        if "format" not in prompt.lower() and any(word in prompt.lower() for word in ["list", "summarize", "steps"]):
            prompt += "\n\nPlease provide a well-structured response."
        
        # Add precision indicator
        if not any(word in prompt.lower() for word in ["detailed", "brief", "concise"]):
            if len(prompt.split()) > 30:
                prompt += "\n\nProvide a concise response."
            else:
                prompt += "\n\nProvide a detailed response."
        
        return prompt
```

## Practical Exercise: Fine-tuning LLM

Let's integrate these components into a practical exercise for fine-tuning a language model:

```python
def fine_tune_llm_exercise():
    """
    Practical exercise for fine-tuning a language model.
    
    This exercise covers:
    1. Data preparation
    2. LoRA fine-tuning
    3. Evaluation
    4. Optimization
    """
    import os
    from dotenv import load_dotenv
    import pandas as pd
    import torch
    from datasets import Dataset
    from tqdm import tqdm
    
    # Load environment variables
    load_dotenv()
    
    # Set up output directory
    output_dir = "./fine_tuning_exercise"
    os.makedirs(output_dir, exist_ok=True)
    
    print("Step 1: Preparing instruction data")
    
    # Create sample instruction dataset
    # In a real scenario, you would load your domain-specific data
    instructions = [
        {
            "instruction": "Explain the concept of fine-tuning in machine learning.",
            "response": "Fine-tuning in machine learning refers to the process of taking a pre-trained model that was trained on a large dataset and further training it on a smaller, more specific dataset to adapt it to a particular task or domain. This approach leverages transfer learning, where the knowledge acquired by the pre-trained model on a broad range of data is transferred and specialized for the target application. Fine-tuning typically involves unfreezing some or all of the model's parameters and updating them using a lower learning rate to prevent catastrophic forgetting while adapting to the new data distribution."
        },
        {
            "instruction": "What are the main differences between LoRA and full fine-tuning?",
            "response": "LoRA (Low-Rank Adaptation) and full fine-tuning differ in several important ways. Full fine-tuning updates all parameters of a pre-trained model, which requires significant computational resources and memory. In contrast, LoRA freezes the original model weights and injects trainable rank decomposition matrices into each layer, drastically reducing the number of trainable parameters. This makes LoRA much more memory-efficient and faster to train while often achieving comparable performance. Full fine-tuning typically requires storing a complete copy of the model, while LoRA only needs to store the much smaller adapter weights, making it practical to adapt large language models on consumer hardware."
        },
        # Add more instruction-response pairs...
    ]
    
    # Convert to DataFrame then to Hugging Face Dataset
    df = pd.DataFrame(instructions)
    instruction_dataset = Dataset.from_pandas(df)
    
    # Save dataset
    instruction_dataset.save_to_disk(os.path.join(output_dir, "instruction_dataset"))
    
    print(f"Created instruction dataset with {len(instruction_dataset)} examples")
    
    print("\nStep 2: Setting up LoRA fine-tuning")
    
    # Choose a smaller model for the exercise
    base_model_name = "mistralai/Mistral-7B-v0.1"
    
    # Initialize LoRA fine-tuner
    fine_tuner = QLoRAFineTuner(
        base_model_name=base_model_name,
        output_dir=os.path.join(output_dir, "model"),
        lora_r=16,
        lora_alpha=32,
        lora_dropout=0.05,
        quantization_bits=4
    )
    
    print("\nStep 3: Preparing the dataset")
    
    # Prepare instruction dataset
    instruction_dataset_path = os.path.join(output_dir, "instruction_dataset.jsonl")
    with open(instruction_dataset_path, 'w') as f:
        for item in instructions:
            f.write(f"{json.dumps(item)}\n")
    
    fine_tuner.prepare_instruction_dataset(
        dataset_path=instruction_dataset_path,
        instruction_column="instruction",
        response_column="response",
        max_length=1024
    )
    
    print("\nStep 4: Training the model")
    print("Note: This is a demonstration. Training would take significant time with a real model.")
    
    # For demonstration, we'll show the configuration but skip actual training
    print("Training configuration:")
    print("- Model: mistralai/Mistral-7B-v0.1")
    print("- LoRA rank: 16")
    print("- LoRA alpha: 32")
    print("- Quantization: 4-bit")
    print("- Learning rate: 2e-4")
    print("- Batch size: 2 (per device)")
    print("- Gradient accumulation steps: 16")
    print("- Training epochs: 3")
    
    # In a real scenario, you would run:
    # trainer = fine_tuner.train(
    #     num_train_epochs=3,
    #     per_device_train_batch_size=2,
    #     gradient_accumulation_steps=16,
    #     learning_rate=2e-4
    # )
    
    print("\nStep 5: Model optimization and evaluation")
    
    # For demonstration, show optimization techniques
    print("Optimization techniques:")
    print("1. 4-bit quantization: Reduces memory usage by ~4x")
    print("2. Flash Attention: Improves attention computation efficiency")
    print("3. Better prompt engineering:")
    
    example_prompt = "Compare transformer architectures"
    optimizer = ModelOptimizer(base_model_name)
    optimized_prompt = optimizer.optimize_prompt(example_prompt)
    
    print(f"   Original prompt: {example_prompt}")
    print(f"   Optimized prompt: {optimized_prompt}")
    
    print("\nStep 6: Testing the fine-tuned model")
    print("Simulated inference with the fine-tuned model:")
    
    example_results = [
        {
            "instruction": "Explain how QLoRA differs from LoRA",
            "response": "QLoRA (Quantized Low-Rank Adaptation) extends the LoRA approach by incorporating quantization techniques. While LoRA reduces parameter count by using low-rank decomposition matrices, QLoRA further reduces memory usage by quantizing the base model weights (typically to 4-bit precision). This allows for fine-tuning much larger models on consumer hardware. QLoRA maintains model quality by using a technique called Double Quantization and employing the NormalFloat 4-bit data type, which preserves the statistical properties of neural network weights better than integer quantization. This combination enables fine-tuning of models with 30B+ parameters on consumer GPUs with 24GB of VRAM while maintaining performance comparable to full 16-bit fine-tuning."
        }
    ]
    
    for example in example_results:
        print(f"\nInstruction: {example['instruction']}")
        print(f"Response: {example['response']}")
    
    print("\nFinished exercise!")
    
    return {
        "base_model": base_model_name,
        "instruction_examples": len(instructions),
        "fine_tuning_method": "QLoRA"
    }

# Run the exercise
if __name__ == "__main__":
    results = fine_tune_llm_exercise()
    print(f"\nExercise completed using {results['fine_tuning_method']} on {results['base_model']}")
```

## Conclusion

Advanced fine-tuning techniques in the Hugging Face ecosystem have democratized the ability to adapt large language models to specific domains and tasks with reasonable computational resources. LoRA, QLoRA, PPO, and RLHF represent a powerful toolkit for modern AI practitioners, enabling significant model customization without the prohibitive costs of full fine-tuning.

LoRA and QLoRA stand out as particularly transformative techniques, achieving comparable performance to full fine-tuning while reducing memory requirements by orders of magnitude. This breakthrough allows even consumer-grade hardware to fine-tune models with billions of parameters, opening up opportunities for specialized applications in various domains.

Reinforcement Learning from Human Feedback (RLHF) represents the next frontier in model alignment, enabling developers to tailor model outputs based on human preferences and ethical guidelines. Though more complex to implement, RLHF provides a powerful framework for creating AI systems that better align with human values and expectations.

The optimization techniques explored throughout this section—quantization, prompt engineering, and efficient inference strategies—play crucial roles in deploying these models in production environments. When combined with task-specific adaptation, these approaches enable development of AI applications that deliver both performance and utility.

As LLM technology continues to evolve, these fine-tuning techniques will remain essential tools for adapting foundation models to specialized domains, creating custom AI assistants, and developing applications that leverage the full potential of large language models while addressing their limitations and biases.