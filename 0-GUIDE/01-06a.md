<small>Claude 3.7 Sonnet Thinking</small>
# 06. LangChain and LangGraph

## Key Terms

- **LangChain**: An open-source framework for developing applications powered by language models, focusing on composability and modularity.
- **LLM (Large Language Model)**: AI models trained on vast amounts of text data capable of understanding and generating human-like text.
- **Prompt**: Structured input text that guides an LLM's response generation.
- **Chain**: A sequence of components (prompts, LLMs, tools) connected to perform a specific task.
- **RAG (Retrieval-Augmented Generation)**: A technique that enhances LLM outputs by retrieving relevant external information before generating responses.
- **Embeddings**: Numerical vector representations of text that capture semantic meaning.
- **Vector Database**: A specialized database optimized for storing and searching vector embeddings.
- **LangServe**: A tool for deploying LangChain chains as REST API endpoints.
- **LangSmith**: A development platform for debugging, testing, evaluating, and monitoring LLM applications.
- **LangGraph**: A library for building stateful, multi-actor applications with LLMs using computation graphs.

## Working with LangChain: Prompts, Chains and Tools

LangChain provides a structured approach to building LLM-powered applications through composable components.

### Installing Dependencies

```python
import sys
import subprocess
import os
from dotenv import load_dotenv

# Install dependencies
dependencies = [
    "langchain>=0.1.0",
    "langchain-openai>=0.0.2",
    "langchain-community>=0.0.10",
    "langserve>=0.0.30",
    "langsmith>=0.0.52",
    "langgraph>=0.0.20",
    "chromadb>=0.4.18",
    "beautifulsoup4>=4.12.2",
    "httpx>=0.25.0",
    "pypdf>=3.17.1",
    "tiktoken>=0.5.1",
    "faiss-cpu>=1.7.4"
]

def install_dependencies():
    for package in dependencies:
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])

if __name__ == "__main__":
    install_dependencies()
    print("All dependencies installed successfully!")
    
    # Load environment variables from .env file
    load_dotenv()
    
    # Verify API key is available
    if not os.getenv("OPENAI_API_KEY"):
        print("Warning: OPENAI_API_KEY not found in .env file!")
    else:
        print("Environment variables loaded successfully!")
```

### Prompting with LangChain

LangChain offers structured ways to create dynamic prompts with parameter insertion and various formats:

```python
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_openai import ChatOpenAI
from typing import List, Dict, Any
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

class AdvancedPromptManager:
    """Class to demonstrate advanced prompting techniques with LangChain"""
    
    def __init__(self, model_name="gpt-4"):
        """Initialize with model name"""
        self.llm = ChatOpenAI(
            temperature=0, 
            model=model_name,
            api_key=os.getenv("OPENAI_API_KEY")
        )
    
    def simple_prompt_chain(self, topic: str) -> str:
        """Create a simple prompt chain for summarization"""
        prompt = ChatPromptTemplate.from_template(
            "Summarize the following topic in 3 paragraphs: {topic}"
        )
        
        chain = prompt | self.llm | StrOutputParser()
        return chain.invoke({"topic": topic})
    
    def multi_variable_prompt(self, context: str, question: str, max_length: int) -> str:
        """Create a prompt with multiple variables"""
        prompt = ChatPromptTemplate.from_template("""
        Context information:
        {context}
        
        Based on the context above, answer the following question
        in no more than {max_length} words:
        
        Question: {question}
        """)
        
        chain = prompt | self.llm | StrOutputParser()
        return chain.invoke({
            "context": context,
            "question": question,
            "max_length": max_length
        })
    
    def structured_output_prompt(self, query: str) -> Dict[str, Any]:
        """Demonstrate structured output parsing"""
        # Define the output schema
        class AnalysisResult(BaseModel):
            main_points: List[str] = Field(description="Key points extracted from the query")
            sentiment: str = Field(description="Sentiment of the query (positive/negative/neutral)")
            categories: List[str] = Field(description="Categories that the query belongs to")
            follow_up_questions: List[str] = Field(description="Suggested follow-up questions")
        
        # Create the prompt
        prompt = ChatPromptTemplate.from_template("""
        Analyze the following text and provide:
        1. Main points
        2. Sentiment
        3. Categories it belongs to
        4. Suggested follow-up questions
        
        Text: {query}
        
        Provide the output as a JSON object with the following keys:
        main_points (array of strings)
        sentiment (string)
        categories (array of strings)
        follow_up_questions (array of strings)
        """)
        
        # Create the parsing chain
        parser = JsonOutputParser(pydantic_object=AnalysisResult)
        chain = prompt | self.llm | parser
        
        return chain.invoke({"query": query})
    
    def few_shot_prompting(self, query: str) -> str:
        """Demonstrate few-shot prompting technique"""
        examples = [
            {"question": "What causes rainfall?", 
             "answer": "Rainfall occurs when water droplets in clouds become heavy enough to fall. This happens through a process called coalescence where small water droplets collide and form larger drops."},
            {"question": "How do computers store data?", 
             "answer": "Computers store data in binary form (0s and 1s) on physical storage media. These binary values represent different states in electronic components like transistors in memory or magnetic orientations on hard drives."},
            {"question": "Why is the sky blue?", 
             "answer": "The sky appears blue because air molecules scatter sunlight, and blue light (which has shorter wavelengths) scatters more than other colors. This phenomenon is called Rayleigh scattering."}
        ]
        
        few_shot_prompt = ChatPromptTemplate.from_messages([
            ("system", "You are a helpful AI assistant that provides clear, concise explanations to questions."),
            ("human", "Question: {question}"),
            ("ai", "{answer}"),
            ("human", "Question: {query}")
        ])
        
        # Create a chain that formats the examples
        chain = few_shot_prompt.partial(examples=examples) | self.llm | StrOutputParser()
        
        # Format each example into the prompt
        formatted_examples = []
        for example in examples:
            formatted_examples.append({"question": example["question"], "answer": example["answer"]})
        
        # Add the actual query
        return chain.invoke({"query": query})

# Example usage
if __name__ == "__main__":
    prompt_manager = AdvancedPromptManager()
    
    # Test simple prompt
    result1 = prompt_manager.simple_prompt_chain("quantum computing")
    print("Simple Prompt Result:")
    print(result1)
    print("\n" + "="*50 + "\n")
    
    # Test multi-variable prompt
    context = """
    Machine learning (ML) is a field of study in artificial intelligence concerned with the development 
    of algorithms and statistical models that computer systems use to perform tasks without explicit 
    instructions, instead relying on patterns and inference. It is seen as a subset of artificial intelligence.
    ML algorithms build a mathematical model based on sample data, known as "training data", in order to make 
    predictions or decisions without being explicitly programmed to perform the task.
    """
    result2 = prompt_manager.multi_variable_prompt(context, "What is machine learning?", 50)
    print("Multi-Variable Prompt Result:")
    print(result2)
    print("\n" + "="*50 + "\n")
    
    # Test structured output
    result3 = prompt_manager.structured_output_prompt("I'm really excited about the potential of AI, but I'm concerned about its ethical implications.")
    print("Structured Output Result:")
    print(result3)
    print("\n" + "="*50 + "\n")
    
    # Test few-shot learning
    result4 = prompt_manager.few_shot_prompting("How do electric cars work?")
    print("Few-Shot Learning Result:")
    print(result4)
```

### Building Chains and Tools

LangChain's power comes from its ability to connect components into functional chains:

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI
from langchain_community.tools import DuckDuckGoSearchRun
from langchain.agents import tool, AgentExecutor, create_openai_tools_agent
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain.memory import ConversationBufferMemory
from langchain_core.messages import AIMessage, HumanMessage
from typing import List, Dict, Any, Optional
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

class AdvancedLangChainWorkflows:
    """Class demonstrating advanced LangChain patterns and workflows"""
    
    def __init__(self, model_name="gpt-4"):
        """Initialize with model name"""
        self.llm = ChatOpenAI(
            temperature=0, 
            model=model_name,
            api_key=os.getenv("OPENAI_API_KEY")
        )
        self.search_tool = DuckDuckGoSearchRun()
    
    def sequential_chain(self, topic: str) -> Dict[str, str]:
        """Create a sequential chain with multiple steps"""
        # Step 1: Generate research questions
        question_prompt = ChatPromptTemplate.from_template(
            "Generate 3 specific research questions about {topic}. Return only the questions in a numbered list."
        )
        question_chain = question_prompt | self.llm | StrOutputParser()
        
        # Step 2: Use questions to search for information
        def search_for_answers(questions: str) -> List[Dict[str, str]]:
            # Split the questions by line
            question_list = [q.strip() for q in questions.split("\n") if q.strip()]
            results = []
            
            for question in question_list:
                # Remove any numbering from the question
                clean_question = question
                if "." in question[:3]:  # Handle "1." format
                    clean_question = question.split(".", 1)[1].strip()
                
                # Search for information
                search_result = self.search_tool.invoke(clean_question)
                results.append({
                    "question": clean_question,
                    "search_result": search_result[:500]  # Truncate long results
                })
            
            return results
        
        # Step 3: Synthesize information into a report
        report_prompt = ChatPromptTemplate.from_template("""
        Create a comprehensive report about {topic} using the following research data:
        
        {research_data}
        
        The report should include:
        1. An executive summary
        2. Key findings for each question
        3. Implications and conclusions
        
        Format the report with clear headings and organized sections.
        """)
        
        # Connect all steps
        chain = {
            "questions": question_chain,
            "topic": RunnablePassthrough()
        } | {
            "research_data": lambda x: search_for_answers(x["questions"]),
            "topic": lambda x: x["topic"]
        } | report_prompt | self.llm | StrOutputParser()
        
        # Execute the chain
        return {"report": chain.invoke(topic)}
    
    def tool_augmented_chain(self, query: str) -> str:
        """Create a chain augmented with tools"""
        
        # Define custom tools
        @tool
        def calculate(expression: str) -> str:
            """Calculate the result of a mathematical expression"""
            try:
                return str(eval(expression))
            except Exception as e:
                return f"Error calculating: {str(e)}"
        
        @tool
        def get_current_date() -> str:
            """Get the current date and time"""
            from datetime import datetime
            return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # Create a search tool
        search = TavilySearchResults()
        
        # List available tools
        tools = [calculate, get_current_date, search]
        
        # Create a prompt template
        prompt = ChatPromptTemplate.from_messages([
            ("system", """
            You are a helpful assistant with access to tools.
            Use the tools when necessary to provide accurate information.
            Think step-by-step about how to solve the user's problem.
            """),
            ("human", "{input}")
        ])
        
        # Create the agent
        agent = create_openai_tools_agent(self.llm, tools, prompt)
        
        # Create an agent executor
        agent_executor = AgentExecutor(
            agent=agent, 
            tools=tools, 
            verbose=True,
            handle_parsing_errors=True
        )
        
        # Run the agent
        return agent_executor.invoke({"input": query})["output"]
    
    def chain_with_memory(self) -> AgentExecutor:
        """Create a chain with memory for multi-turn conversations"""
        # Set up memory
        memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True
        )
        
        # Define tools
        search = DuckDuckGoSearchRun()
        
        @tool
        def get_personal_info(info_type: str) -> str:
            """Get personal information of a specific type (name, job, location)"""
            info = {
                "name": "Alex Thompson",
                "job": "Data Scientist",
                "location": "Toronto, Canada",
                "background": "10 years of experience in ML and AI",
                "interests": "hiking, photography, and robotics"
            }
            return info.get(info_type.lower(), f"No information available for {info_type}")
        
        tools = [search, get_personal_info]
        
        # Create a prompt template with memory
        prompt = ChatPromptTemplate.from_messages([
            ("system", """
            You are a personal AI assistant named Helper. 
            You maintain context throughout the conversation.
            Use tools when needed to provide accurate information.
            """),
            ("placeholder", "{chat_history}"),
            ("human", "{input}")
        ])
        
        # Create agent
        agent = create_openai_tools_agent(self.llm, tools, prompt)
        
        # Create agent executor with memory
        return AgentExecutor(
            agent=agent,
            tools=tools,
            memory=memory,
            verbose=True
        )
    
    def router_chain(self, query: str) -> str:
        """Create a router chain that directs queries to specialized chains"""
        # Specialized chains
        math_chain = ChatPromptTemplate.from_template(
            "Solve this math problem step by step: {query}"
        ) | self.llm | StrOutputParser()
        
        history_chain = ChatPromptTemplate.from_template(
            "Provide detailed historical information about: {query}"
        ) | self.llm | StrOutputParser()
        
        science_chain = ChatPromptTemplate.from_template(
            "Explain this scientific concept in detail: {query}"
        ) | self.llm | StrOutputParser()
        
        general_chain = ChatPromptTemplate.from_template(
            "Answer this general knowledge question: {query}"
        ) | self.llm | StrOutputParser()
        
        # Router prompt
        router_prompt = ChatPromptTemplate.from_template("""
        Based on the query, determine which category it belongs to:
        - math: for mathematics problems, equations, calculations
        - history: for questions about historical events, figures, periods
        - science: for questions about scientific concepts, theories, experiments
        - general: for any other general knowledge questions
        
        Query: {query}
        
        Category (respond with just one word, lowercase):
        """)
        
        # Router chain
        router_chain = router_prompt | self.llm | StrOutputParser()
        
        # Route to the appropriate chain
        category = router_chain.invoke({"query": query})
        category = category.strip().lower()
        
        if "math" in category:
            return math_chain.invoke({"query": query})
        elif "history" in category:
            return history_chain.invoke({"query": query})
        elif "science" in category:
            return science_chain.invoke({"query": query})
        else:
            return general_chain.invoke({"query": query})

# Example usage
if __name__ == "__main__":
    workflow = AdvancedLangChainWorkflows()
    
    # Test sequential chain
    result1 = workflow.sequential_chain("renewable energy technologies")
    print("Sequential Chain Result:")
    print(result1["report"][:500] + "...\n")  # Show first 500 chars
    print("="*50 + "\n")
    
    # Test tool-augmented chain
    result2 = workflow.tool_augmented_chain("What is 255 * 15? Also, tell me the current date and some information about quantum computing.")
    print("Tool-Augmented Chain Result:")
    print(result2)
    print("\n" + "="*50 + "\n")
    
    # Test memory chain
    memory_agent = workflow.chain_with_memory()
    print("Memory Chain Conversation:")
    response1 = memory_agent.invoke({"input": "My name is John. What's the weather like in Seattle?"})
    print("Response 1:", response1["output"])
    
    response2 = memory_agent.invoke({"input": "Can you remind me of my name? Also, tell me about the Space Needle."})
    print("Response 2:", response2["output"])
    print("\n" + "="*50 + "\n")
    
    # Test router chain
    queries = [
        "Solve for x: 3x + 5 = 20",
        "Who was Alexander the Great?",
        "Explain how photosynthesis works",
        "What's the best way to learn piano?"
    ]
    
    for query in queries:
        print(f"Query: {query}")
        result = workflow.router_chain(query)
        print(f"Result: {result[:200]}...\n")
```

## Retrieval-Augmented Generation (RAG)

RAG enhances LLM responses by incorporating external knowledge retrieval:

```python
from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from typing import List, Dict, Any, Optional
import os
import uuid
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

class AdvancedRAGSystem:
    """Advanced RAG implementation with various data sources and retrieval methods"""
    
    def __init__(self, embedding_model_name="text-embedding-3-small", llm_model_name="gpt-4"):
        """Initialize the RAG system"""
        self.embeddings = OpenAIEmbeddings(
            model=embedding_model_name,
            api_key=os.getenv("OPENAI_API_KEY")
        )
        self.llm = ChatOpenAI(
            temperature=0,
            model=llm_model_name,
            api_key=os.getenv("OPENAI_API_KEY")
        )
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        self.vectorstore = None
    
    def load_documents(self, sources: List[str]):
        """Load documents from various sources (PDFs, web pages)"""
        documents = []
        
        for source in sources:
            try:
                if source.endswith('.pdf'):
                    # Load PDF
                    loader = PyPDFLoader(source)
                    documents.extend(loader.load())
                elif source.startswith('http'):
                    # Load webpage
                    loader = WebBaseLoader(source)
                    documents.extend(loader.load())
                else:
                    print(f"Unsupported source format: {source}")
            except Exception as e:
                print(f"Error loading {source}: {str(e)}")
        
        # Split documents into chunks
        splits = self.text_splitter.split_documents(documents)
        
        # Create vectorstore
        self.vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=self.embeddings,
            collection_name=f"rag_collection_{uuid.uuid4().hex[:8]}"  # Unique collection name
        )
        
        return len(splits)
    
    def query_basic(self, query: str, k: int = 4) -> str:
        """Basic RAG query implementation"""
        if not self.vectorstore:
            return "Error: No documents loaded. Please load documents first."
        
        # Retrieve relevant documents
        docs = self.vectorstore.similarity_search(query, k=k)
        
        # Create context from documents
        context = "\n\n".join([doc.page_content for doc in docs])
        
        # Create prompt
        prompt = ChatPromptTemplate.from_template("""
        Answer the question based on the following context:
        
        {context}
        
        Question: {question}
        
        Provide a detailed answer based only on the information in the context.
        If the context doesn't contain relevant information, say "I don't have enough information to answer this question."
        """)
        
        # Create chain
        chain = (
            {"context": lambda x: context, "question": RunnablePassthrough()}
            | prompt
            | self.llm
            | StrOutputParser()
        )
        
        return chain.invoke(query)
    
    def query_advanced(self, query: str, k: int = 4) -> Dict[str, Any]:
        """Advanced RAG with query transformation and source attribution"""
        if not self.vectorstore:
            return {"answer": "Error: No documents loaded. Please load documents first."}
        
        # Step 1: Transform the query for better retrieval
        query_transform_prompt = ChatPromptTemplate.from_template("""
        Given a user question, generate an improved search query that will help retrieve the most relevant documents.
        Make the query more specific, include synonyms of key terms, and focus on the core information need.
        
        Original question: {question}
        
        Improved search query:
        """)
        
        query_transform_chain = query_transform_prompt | self.llm | StrOutputParser()
        transformed_query = query_transform_chain.invoke({"question": query})
        
        # Step 2: Retrieve documents with the transformed query
        docs = self.vectorstore.similarity_search(transformed_query, k=k)
        
        # Step 3: Prepare context with source information
        contexts = []
        sources = []
        
        for i, doc in enumerate(docs):
            # Add source information
            source = doc.metadata.get("source", f"Document {i+1}")
            sources.append(source)
            
            # Add formatted context
            contexts.append(f"[Document {i+1}]: {doc.page_content}")
        
        # Combine contexts
        context_text = "\n\n".join(contexts)
        
        # Step 4: Generate the answer
        answer_prompt = ChatPromptTemplate.from_template("""
        Answer the question based on the following retrieved documents:
        
        {context}
        
        Question: {question}
        
        Provide a comprehensive answer based on the documents. Cite your sources using [Document X] notation.
        If the documents don't contain enough information, clearly state what information is missing.
        """)
        
        answer_chain = answer_prompt | self.llm | StrOutputParser()
        
        # Step 5: Execute and return results
        answer = answer_chain.invoke({
            "context": context_text,
            "question": query
        })
        
        return {
            "original_query": query,
            "transformed_query": transformed_query,
            "answer": answer,
            "sources": sources
        }
    
    def query_with_self_correction(self, query: str, k: int = 4) -> Dict[str, Any]:
        """RAG with self-correction and fact-checking"""
        if not self.vectorstore:
            return {"answer": "Error: No documents loaded. Please load documents first."}
        
        # Step 1: Retrieve documents
        docs = self.vectorstore.similarity_search(query, k=k)
        context = "\n\n".join([doc.page_content for doc in docs])
        
        # Step 2: Generate initial answer
        initial_prompt = ChatPromptTemplate.from_template("""
        Answer the question based on the following context:
        
        {context}
        
        Question: {question}
        
        Provide a detailed answer:
        """)
        
        initial_chain = initial_prompt | self.llm | StrOutputParser()
        initial_answer = initial_chain.invoke({
            "context": context,
            "question": query
        })
        
        # Step 3: Self-correct and fact-check
        correction_prompt = ChatPromptTemplate.from_template("""
        Review the following answer for factual accuracy based on the provided context:
        
        Context:
        {context}
        
        Question: {question}
        
        Initial answer: {initial_answer}
        
        Your task:
        1. Identify any factual errors or unsupported claims in the initial answer
        2. Provide corrections for any errors
        3. If the answer is accurate, improve clarity or completeness
        
        Corrected answer:
        """)
        
        correction_chain = correction_prompt | self.llm | StrOutputParser()
        corrected_answer = correction_chain.invoke({
            "context": context,
            "question": query,
            "initial_answer": initial_answer
        })
        
        # Step 4: Generate confidence score
        confidence_prompt = ChatPromptTemplate.from_template("""
        Assess the confidence level for this answer based on how well it's supported by the context:
        
        Context:
        {context}
        
        Question: {question}
        
        Final answer: {final_answer}
        
        Rate the confidence from 1-10 (where 10 is highest confidence) and explain your rating.
        Return only the numerical rating followed by a brief explanation.
        """)
        
        confidence_chain = confidence_prompt | self.llm | StrOutputParser()
        confidence_assessment = confidence_chain.invoke({
            "context": context,
            "question": query,
            "final_answer": corrected_answer
        })
        
        # Extract confidence score
        confidence_score = int(confidence_assessment.split()[0]) if confidence_assessment.split()[0].isdigit() else 0
        
        return {
            "query": query,
            "initial_answer": initial_answer,
            "final_answer": corrected_answer,
            "confidence": confidence_score,
            "confidence_explanation": confidence_assessment
        }

# Example usage
if __name__ == "__main__":
    rag_system = AdvancedRAGSystem()
    
    # Load documents
    sources = [
        "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "https://en.wikipedia.org/wiki/Machine_learning"
        # Add PDF paths like "path/to/document.pdf"
    ]
    
    num_chunks = rag_system.load_documents(sources)
    print(f"Loaded {num_chunks} document chunks")
    
    # Test basic RAG
    basic_result = rag_system.query_basic("What is the difference between supervised and unsupervised learning?")
    print("\nBasic RAG Result:")
    print(basic_result)
    print("\n" + "="*50 + "\n")
    
    # Test advanced RAG
    advanced_result = rag_system.query_advanced("How has AI impacted modern society?")
    print("Advanced RAG Result:")
    print(f"Original Query: {advanced_result['original_query']}")
    print(f"Transformed Query: {advanced_result['transformed_query']}")
    print(f"Answer: {advanced_result['answer']}")
    print(f"Sources: {advanced_result['sources']}")
    print("\n" + "="*50 + "\n")
    
    # Test RAG with self-correction
    correction_result = rag_system.query_with_self_correction("What are the ethical concerns surrounding AI development?")
    print("Self-Correcting RAG Result:")
    print(f"Initial Answer: {correction_result['initial_answer'][:150]}...")
    print(f"Final Answer: {correction_result['final_answer'][:150]}...")
    print(f"Confidence: {correction_result['confidence']}/10")
    print(f"Confidence Explanation: {correction_result['confidence_explanation']}")
```

## LangServe, LangSmith and LangGraph for Orchestration

For building and deploying complex LLM applications, LangChain offers a suite of tools for orchestration and monitoring:

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, Runnable
from langchain_community.tools import DuckDuckGoSearchRun
from typing import Dict, List, Any, Optional, Tuple, TypedDict
from langchain.agents import AgentExecutor, create_openai_tools_agent
import os
from dotenv import load_dotenv
from langgraph.graph import StateGraph, END
import operator
from typing_extensions import TypedDict
from enum import Enum
import json

# Load environment variables
load_dotenv()

# LangServe example - Creating endpoints for LangChain applications
# Note: This code would be part of a FastAPI app, showing the pattern
def create_langserve_endpoints():
    """
    Example pattern for creating LangServe endpoints
    In a real app, this would be implemented with a FastAPI server
    """
    from langserve import add_routes
    from fastapi import FastAPI
    
    app = FastAPI(
        title="LangChain Server",
        version="1.0",
        description="A simple api server using Langchain's Runnable interfaces"
    )
    
    # Create a simple chain
    model = ChatOpenAI()
    prompt = ChatPromptTemplate.from_template("Tell me a joke about {topic}")
    chain = prompt | model | StrOutputParser()
    
    # Add chain as a route
    add_routes(
        app,
        {"joke": chain},
        path="/api"
    )
    
    # Create a more complex chain with tools
    search = DuckDuckGoSearchRun()
    
    def search_and_summarize(query: str) -> Dict[str, str]:
        search_results = search.invoke(query)
        
        summarize_prompt = ChatPromptTemplate.from_template("""
        Summarize the following search results in 3 bullet points:
        
        {search_results}
        """)
        
        summarize_chain = summarize_prompt | model | StrOutputParser()
        summary = summarize_chain.invoke({"search_results": search_results})
        
        return {
            "query": query,
            "search_results": search_results[:200] + "...",  # Truncated for brevity
            "summary": summary
        }
    
    # Add second chain as a route
    add_routes(
        app,
        {"search_summarize": search_and_summarize},
        path="/api/search"
    )
    
    # In a real app, you would add:
    # uvicorn.run(app, host="0.0.0.0", port=8000)
    
    # For illustration, return the FastAPI app
    return "FastAPI app with LangServe routes added at /api/joke and /api/search/search_summarize"

# LangSmith example - Tracing and monitoring
def setup_langsmith_tracing():
    """
    Example of setting up LangSmith tracing
    Requires a LangSmith API key in .env file
    """
    # In a real app, you would set these environment variables
    os.environ["LANGCHAIN_TRACING_V2"] = "true"
    os.environ["LANGCHAIN_API_KEY"] = os.getenv("LANGSMITH_API_KEY", "your_langsmith_api_key")
    os.environ["LANGCHAIN_PROJECT"] = "My RAG Application"
    
    # Create a traced chain
    llm = ChatOpenAI()
    prompt = ChatPromptTemplate.from_template("Write a short poem about {topic}")
    chain = prompt | llm | StrOutputParser()
    
    # When this chain runs, it will be automatically traced in LangSmith
    result = chain.invoke({"topic": "artificial intelligence"})
    
    # In a real app, you would also use:
    # from langsmith import Client
    # client = Client()
    # client.create_dataset("evaluation_dataset")
    # etc.
    
    return "LangSmith tracing set up. Run chains to see traces in the LangSmith UI."

# LangGraph example - Building a multi-step agent
class AgentState(TypedDict):
    """State for the agent graph"""
    question: str
    intermediate_steps: List[Tuple[str, str]]
    retrieved_information: Optional[str]
    answer: Optional[str]

class NodeNames(str, Enum):
    """Node names for the graph"""
    RETRIEVE = "retrieve"
    THINK = "think"
    ANSWER = "answer"
    LOOKUP = "lookup"
    DONE = "done"

def build_research_agent() -> Runnable:
    """Build a multi-step research agent using LangGraph"""
    llm = ChatOpenAI(temperature=0)
    search = DuckDuckGoSearchRun()
    
    # Define the nodes
    def retrieve(state: AgentState) -> AgentState:
        """Retrieve information based on the question"""
        question = state["question"]
        print(f"🔍 Retrieving information for: {question}")
        
        search_results = search.invoke(question)
        return {"retrieved_information": search_results}
    
    def think(state: AgentState) -> AgentState:
        """Think about the question and retrieved information"""
        question = state["question"]
        retrieved_info = state.get("retrieved_information", "")
        intermediate_steps = state.get("intermediate_steps", [])
        
        print("🤔 Thinking...")
        
        think_prompt = ChatPromptTemplate.from_template("""
        Question: {question}
        
        Retrieved Information: {retrieved_information}
        
        Previous steps: {intermediate_steps}
        
        Think step by step about how to answer this question using the retrieved information.
        What additional information might you need? What conclusions can you draw?
        """)
        
        thinking = think_prompt | llm | StrOutputParser()
        thought_result = thinking.invoke({
            "question": question,
            "retrieved_information": retrieved_info,
            "intermediate_steps": str(intermediate_steps)
        })
        
        # Add the thought to intermediate steps
        new_steps = intermediate_steps + [("think", thought_result)]
        
        return {"intermediate_steps": new_steps}
    
    def answer(state: AgentState) -> AgentState:
        """Generate the final answer"""
        question = state["question"]
        retrieved_info = state.get("retrieved_information", "")
        intermediate_steps = state.get("intermediate_steps", [])
        
        print("✍️ Generating answer...")
        
        answer_prompt = ChatPromptTemplate.from_template("""
        Question: {question}
        
        Retrieved Information: {retrieved_information}
        
        Thinking Process: {intermediate_steps}
        
        Based on the above information, provide a comprehensive answer to the question.
        """)
        
        answering = answer_prompt | llm | StrOutputParser()
        final_answer = answering.invoke({
            "question": question,
            "retrieved_information": retrieved_info,
            "intermediate_steps": str(intermediate_steps)
        })
        
        return {"answer": final_answer}
    
    def lookup(state: AgentState) -> Dict[str, str]:
        """Decide whether to look up more information"""
        intermediate_steps = state.get("intermediate_steps", [])
        
        # Get the last thinking step
        last_thought = intermediate_steps[-1][1] if intermediate_steps else ""
        
        decide_prompt = ChatPromptTemplate.from_template("""
        Based on the following thinking process, decide if additional information lookup is needed.
        
        Thinking process: {thought}
        
        If more information is needed, respond with "lookup".
        If enough information is available to answer the question, respond with "answer".
        
        Your decision (just the word "lookup" or "answer"):
        """)
        
        decide_chain = decide_prompt | llm | StrOutputParser()
        decision = decide_chain.invoke({"thought": last_thought})
        
        # Clean up response to ensure it's just the decision
        decision = decision.strip().lower()
        if "lookup" in decision:
            return {"next": NodeNames.RETRIEVE}
        else:
            return {"next": NodeNames.ANSWER}
    
    # Define the workflow
    workflow = StateGraph(AgentState)
    
    # Add nodes
    workflow.add_node(NodeNames.RETRIEVE, retrieve)
    workflow.add_node(NodeNames.THINK, think)
    workflow.add_node(NodeNames.ANSWER, answer)
    
    # Add conditional edges
    workflow.add_edge(NodeNames.RETRIEVE, NodeNames.THINK)
    workflow.add_conditional_edges(
        NodeNames.THINK,
        lookup,
        {
            NodeNames.RETRIEVE: lambda x: x["next"] == NodeNames.RETRIEVE,
            NodeNames.ANSWER: lambda x: x["next"] == NodeNames.ANSWER
        }
    )
    workflow.add_edge(NodeNames.ANSWER, END)
    
    # Set entry point
    workflow.set_entry_point(NodeNames.RETRIEVE)
    
    # Compile the graph
    agent_graph = workflow.compile()
    
    return agent_graph

def run_research_agent(question: str) -> Dict[str, Any]:
    """Run the research agent on a question"""
    agent = build_research_agent()
    result = agent.invoke({"question": question, "intermediate_steps": []})
    return result

# Example usage
if __name__ == "__main__":
    # LangServe pattern example
    langserve_info = create_langserve_endpoints()
    print("LangServe Setup:")
    print(langserve_info)
    print("\n" + "="*50 + "\n")
    
    # LangSmith tracing example
    langsmith_info = setup_langsmith_tracing()
    print("LangSmith Setup:")
    print(langsmith_info)
    print("\n" + "="*50 + "\n")
    
    # LangGraph research agent example
    print("Running LangGraph Research Agent:")
    result = run_research_agent("What are the environmental benefits of electric vehicles?")
    print("\nFinal Answer:")
    print(result.get("answer", "No answer generated"))
    
    # Show intermediate steps (shortened for clarity)
    intermediate_steps = result.get("intermediate_steps", [])
    print(f"\nCompleted in {len(intermediate_steps)} thinking steps")
    for i, (step_type, content) in enumerate(intermediate_steps):
        print(f"Step {i+1} ({step_type}): {content[:100]}...")
```

## Implementing a Complex Multi-Agent System with LangGraph

Here's a more advanced example combining all the concepts into a multi-agent system:

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_community.tools import DuckDuckGoSearchRun
from typing import Dict, List, Any, Optional, Tuple, TypedDict, Union, Annotated
from langgraph.graph import StateGraph, END
from enum import Enum
import json
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Define agent types and schemas
class AgentRole(str, Enum):
    COORDINATOR = "coordinator"
    RESEARCHER = "researcher"
    CRITIC = "critic"
    WRITER = "writer"
    
class MessageType(str, Enum):
    TASK = "task"
    RESULT = "result"
    FEEDBACK = "feedback"
    REVISION = "revision"
    
class Message(BaseModel):
    sender: AgentRole
    receiver: AgentRole
    message_type: MessageType
    content: str
    
class ResearchResult(BaseModel):
    topic: str
    key_points: List[str]
    sources: List[str]
    
class WriterOutput(BaseModel):
    title: str
    content: str
    sections: List[str]
    
class CriticFeedback(BaseModel):
    overall_rating: int = Field(description="Rating from 1-10")
    strengths: List[str]
    weaknesses: List[str]
    improvement_suggestions: List[str]

class ProjectState(TypedDict):
    topic: str
    task_description: str
    messages: List[Message]
    research_results: Optional[ResearchResult]
    draft: Optional[WriterOutput]
    feedback: Optional[CriticFeedback]
    final_output: Optional[str]

class MultiAgentSystem:
    """Orchestrates a team of AI agents working together on research and content creation"""
    
    def __init__(self, model_name="gpt-4"):
        """Initialize the multi-agent system"""
        self.llm = ChatOpenAI(temperature=0.7, model=model_name)
        self.search_tool = DuckDuckGoSearchRun()
        
        # Build and compile the agent workflow
        self.workflow = self._build_workflow()
    
    def _build_workflow(self) -> StateGraph:
        """Build the agent workflow graph"""
        # Define the state graph
        graph = StateGraph(ProjectState)
        
        # Add nodes
        graph.add_node("coordinator", self._coordinator_node)
        graph.add_node("researcher", self._researcher_node)
        graph.add_node("writer", self._writer_node)
        graph.add_node("critic", self._critic_node)
        graph.add_node("finalize", self._finalize_node)
        
        # Add edges
        graph.add_edge("coordinator", "researcher")
        graph.add_edge("researcher", "writer")
        graph.add_edge("writer", "critic")
        graph.add_conditional_edges(
            "critic",
            self._should_revise,
            {
                "writer": lambda x: x["result"] == "revise",
                "finalize": lambda x: x["result"] == "finalize"
            }
        )
        graph.add_edge("finalize", END)
        
        # Set entry point
        graph.set_entry_point("coordinator")
        
        # Compile the graph
        return graph.compile()
    
    def _coordinator_node(self, state: ProjectState) -> Dict[str, Any]:
        """Coordinator agent: Plans the approach and distributes tasks"""
        topic = state["topic"]
        
        # Create a planning prompt
        planning_prompt = ChatPromptTemplate.from_template("""
        You are the Coordinator Agent responsible for planning a research project.
        
        Topic: {topic}
        
        Your task is to:
        1. Break down this topic into key research questions
        2. Create a clear plan for the Researcher Agent
        3. Specify what kind of information would be most valuable
        
        Provide your instructions in a clear, structured format.
        """)
        
        # Generate plan
        planning_chain = planning_prompt | self.llm | StrOutputParser()
        plan = planning_chain.invoke({"topic": topic})
        
        # Create task message
        messages = state.get("messages", [])
        messages.append(
            Message(
                sender=AgentRole.COORDINATOR,
                receiver=AgentRole.RESEARCHER,
                message_type=MessageType.TASK,
                content=plan
            ).dict()
        )
        
        # Update state
        return {
            "messages": messages,
            "task_description": plan
        }
    
    def _researcher_node(self, state: ProjectState) -> Dict[str, Any]:
        """Researcher agent: Gathers information from multiple sources"""
        topic = state["topic"]
        task = state["task_description"]
        
        print(f"🔍 Researcher working on: {topic}")
        
        # Get the most recent message to the researcher
        messages = state.get("messages", [])
        relevant_msgs = [
            m for m in messages 
            if m["receiver"] == AgentRole.RESEARCHER
        ]
        
        if relevant_msgs:
            latest_instructions = relevant_msgs[-1]["content"]
        else:
            latest_instructions = f"Research the topic: {topic}"
        
        # First, get search queries
        query_prompt = ChatPromptTemplate.from_template("""
        Based on the research task, generate 3 specific search queries that will help gather
        comprehensive information on the topic.
        
        Research Task: {task}
        Topic: {topic}
        
        Return only the search queries, one per line:
        """)
        
        query_chain = query_prompt | self.llm | StrOutputParser()
        search_queries = query_chain.invoke({
            "task": latest_instructions,
            "topic": topic
        })
        
        # Execute searches
        search_results = []
        sources = []
        
        for query in search_queries.strip().split('\n'):
            if query:
                print(f"  Searching: {query}")
                result = self.search_tool.invoke(query)
                search_results.append(result)
                sources.append(query)
        
        # Synthesize research findings
        synthesis_prompt = ChatPromptTemplate.from_template("""
        Synthesize the following search results into a coherent research summary:
        
        {search_results}
        
        Focus on these aspects of the topic "{topic}":
        {task}
        
        Extract the most important key points and insights.
        """)
        
        synthesis_chain = synthesis_prompt | self.llm | StrOutputParser()
        synthesis = synthesis_chain.invoke({
            "search_results": "\n\n".join(search_results),
            "topic": topic,
            "task": latest_instructions
        })
        
        # Extract key points
        key_points_prompt = ChatPromptTemplate.from_template("""
        Extract the 5-7 most important key points from this research summary:
        
        {synthesis}
        
        Return the key points in a numbered list format.
        """)
        
        key_points_chain = key_points_prompt | self.llm | StrOutputParser()
        key_points_text = key_points_chain.invoke({"synthesis": synthesis})
        
        # Parse key points into a list
        key_points = [
            point.split(".", 1)[1].strip() if "." in point else point.strip()
            for point in key_points_text.strip().split('\n')
            if point.strip()
        ]
        
        # Create research result
        research_result = ResearchResult(
            topic=topic,
            key_points=key_points,
            sources=sources
        )
        
        # Add message
        messages = state.get("messages", [])
        messages.append(
            Message(
                sender=AgentRole.RESEARCHER,
                receiver=AgentRole.WRITER,
                message_type=MessageType.RESULT,
                content=f"Research completed. Found {len(key_points)} key points on {topic}."
            ).dict()
        )
        
        # Update state
        return {
            "messages": messages,
            "research_results": research_result.dict()
        }
    
    def _writer_node(self, state: ProjectState) -> Dict[str, Any]:
        """Writer agent: Creates content based on research"""
        topic = state["topic"]
        research_results = state["research_results"]
        messages = state.get("messages", [])
        
        # Check if this is a revision
        feedback = state.get("feedback")
        is_revision = feedback is not None
        
        print(f"✍️ Writer {'revising' if is_revision else 'creating'} content on: {topic}")
        
        # Prepare writing prompt
        if is_revision:
            writing_prompt = ChatPromptTemplate.from_template("""
            You are a Writer Agent tasked with revising content based on feedback.
            
            Original Topic: {topic}
            
            Research Key Points:
            {key_points}
            
            Previous Draft:
            {previous_draft}
            
            Critic's Feedback:
            {feedback}
            
            Create a revised version that addresses the feedback while maintaining
            accuracy to the research. Include a compelling title and clear sections.
            """)
            
            # Get previous draft
            previous_draft = state.get("draft", {}).get("content", "")
            
            # Generate revised content
            writing_chain = writing_prompt | self.llm | StrOutputParser()
            content = writing_chain.invoke({
                "topic": topic,
                "key_points": "\n".join([f"- {point}" for point in research_results["key_points"]]),
                "previous_draft": previous_draft,
                "feedback": json.dumps(feedback, indent=2)
            })
        else:
            writing_prompt = ChatPromptTemplate.from_template("""
            You are a Writer Agent tasked with creating informative content.
            
            Topic: {topic}
            
            Research Key Points:
            {key_points}
            
            Create an engaging and informative piece that includes:
            1. A compelling title
            2. An introduction that hooks the reader
            3. Clearly organized sections covering the key points
            4. A conclusion that summarizes the main insights
            
            Make sure your writing is clear, engaging, and accurately reflects the research.
            """)
            
            # Generate initial content
            writing_chain = writing_prompt | self.llm | StrOutputParser()
            content = writing_chain.invoke({
                "topic": topic,
                "key_points": "\n".join([f"- {point}" for point in research_results["key_points"]])
            })
        
        # Extract title and sections
        structure_prompt = ChatPromptTemplate.from_template("""
        Analyze the following content and extract:
        1. The title
        2. The main section headings (not including introduction or conclusion)
        
        Content:
        {content}
        
        Return the information in JSON format with keys:
        - title
        - sections (as a list)
        """)
        
        structure_chain = structure_prompt | self.llm | StrOutputParser()
        structure_json = structure_chain.invoke({"content": content})
        
        # Parse the structure
        try:
            structure = json.loads(structure_json)
        except json.JSONDecodeError:
            # Fallback if JSON parsing fails
            structure = {"title": "Untitled", "sections": ["Main Content"]}
        
        # Create writer output
        draft = WriterOutput(
            title=structure.get("title", "Untitled"),
            content=content,
            sections=structure.get("sections", ["Main Content"])
        )
        
        # Add message
        messages.append(
            Message(
                sender=AgentRole.WRITER,
                receiver=AgentRole.CRITIC,
                message_type=MessageType.RESULT if not is_revision else MessageType.REVISION,
                content=f"{'Revised' if is_revision else 'Initial'} draft completed: {draft.title}"
            ).dict()
        )
        
        # Update state
        return {
            "messages": messages,
            "draft": draft.dict()
        }
    
    def _critic_node(self, state: ProjectState) -> Dict[str, Any]:
        """Critic agent: Evaluates content and provides feedback"""
        topic = state["topic"]
        research_results = state["research_results"]
        draft = state["draft"]
        messages = state.get("messages", [])
        
        print(f"🧐 Critic evaluating content on: {topic}")
        
        # Critique the content
        critique_prompt = ChatPromptTemplate.from_template("""
        You are a Critic Agent responsible for evaluating content quality, accuracy, and effectiveness.
        
        Topic: {topic}
        
        Research Key Points:
        {key_points}
        
        Draft Content:
        {draft_content}
        
        Evaluate this content on:
        1. Accuracy: Does it reflect the research key points?
        2. Clarity: Is it well-organized and easy to understand?
        3. Engagement: Is it compelling and interesting?
        4. Completeness: Does it cover all important aspects?
        
        Provide a rating from 1-10 and specific feedback for improvement.
        
        Return your evaluation in JSON format with these keys:
        - overall_rating (number 1-10)
        - strengths (list of strings)
        - weaknesses (list of strings) 
        - improvement_suggestions (list of strings)
        """)
        
        critique_chain = critique_prompt | self.llm | StrOutputParser()
        critique_json = critique_chain.invoke({
            "topic": topic,
            "key_points": "\n".join([f"- {point}" for point in research_results["key_points"]]),
            "draft_content": draft["content"]
        })
        
        # Parse feedback
        try:
            feedback_dict = json.loads(critique_json)
        except json.JSONDecodeError:
            # Fallback if JSON parsing fails
            feedback_dict = {
                "overall_rating": 5,
                "strengths": ["Content addresses the topic"],
                "weaknesses": ["Format issues detected"],
                "improvement_suggestions": ["Restructure for clarity"]
            }
        
        # Create feedback object
        feedback = CriticFeedback(**feedback_dict)
        
        # Add message
        messages.append(
            Message(
                sender=AgentRole.CRITIC,
                receiver=AgentRole.WRITER if feedback.overall_rating < 8 else AgentRole.COORDINATOR,
                message_type=MessageType.FEEDBACK,
                content=f"Evaluation complete. Rating: {feedback.overall_rating}/10"
            ).dict()
        )
        
        # Update state
        return {
            "messages": messages,
            "feedback": feedback.dict()
        }
    
    def _should_revise(self, state: ProjectState) -> Dict[str, str]:
        """Decision node: Should we revise or finalize?"""
        feedback = state["feedback"]
        
        if feedback["overall_rating"] < 8:
            return {"result": "revise"}
        else:
            return {"result": "finalize"}
    
    def _finalize_node(self, state: ProjectState) -> Dict[str, Any]:
        """Finalize the content with improvements"""
        topic = state["topic"]
        draft = state["draft"]
        feedback = state["feedback"]
        messages = state.get("messages", [])
        
        print(f"✅ Finalizing content on: {topic}")
        
        # Polish the final content
        polish_prompt = ChatPromptTemplate.from_template("""
        You are an Editor responsible for finalizing content.
        
        Title: {title}
        
        Draft Content:
        {draft_content}
        
        Strengths Identified:
        {strengths}
        
        Make final improvements to polish this content:
        1. Ensure the writing is engaging and professional
        2. Fix any grammatical or stylistic issues
        3. Enhance the flow and structure
        4. Preserve the strengths while making improvements
        
        Return the final polished version.
        """)
        
        polish_chain = polish_prompt | self.llm | StrOutputParser()
        final_content = polish_chain.invoke({
            "title": draft["title"],
            "draft_content": draft["content"],
            "strengths": "\n".join([f"- {point}" for point in feedback["strengths"]])
        })
        
        # Add final message
        messages.append(
            Message(
                sender=AgentRole.COORDINATOR,
                receiver=AgentRole.COORDINATOR,
                message_type=MessageType.RESULT,
                content=f"Project completed: {draft['title']}"
            ).dict()
        )
        
        # Update state
        return {
            "messages": messages,
            "final_output": final_content
        }
    
    def run(self, topic: str) -> Dict[str, Any]:
        """Execute the multi-agent workflow on a topic"""
        print(f"🚀 Starting multi-agent project on: {topic}")
        
        initial_state = {
            "topic": topic,
            "task_description": "",
            "messages": []
        }
        
        # Execute the workflow
        final_state = self.workflow.invoke(initial_state)
        
        return final_state

# Example usage
if __name__ == "__main__":
    # Create and run the multi-agent system
    system = MultiAgentSystem()
    
    # Choose a topic
    topic = "The impact of artificial intelligence on creative industries"
    
    # Run the system
    result = system.run(topic)
    
    # Display results
    print("\n" + "="*50)
    print("📊 MULTI-AGENT PROJECT RESULTS")
    print("="*50)
    
    print(f"\n📌 Topic: {result['topic']}")
    print(f"\n📝 Title: {result['draft']['title']}")
    
    print("\n🔑 Key Points:")
    for i, point in enumerate(result['research_results']['key_points']):
        print(f"  {i+1}. {point}")
    
    print("\n📋 Sections:")
    for i, section in enumerate(result['draft']['sections']):
        print(f"  {i+1}. {section}")
    
    print("\n⭐ Evaluation:")
    print(f"  Rating: {result['feedback']['overall_rating']}/10")
    print("  Strengths:")
    for s in result['feedback']['strengths']:
        print(f"    - {s}")
    
    print("\n📄 Final Content Preview:")
    # Show first 500 characters of the final content
    preview = result['final_output'][:500] + "..." if len(result['final_output']) > 500 else result['final_output']
    print(f"\n{preview}")
    
    print("\n✅ Project completed!")
```

## Conclusion

LangChain and LangGraph provide powerful, composable frameworks for building sophisticated LLM applications. The key advantages of these frameworks include:

1. **Composability**: LangChain's modular design enables the construction of complex workflows from simpler components.

2. **Reusability**: Common patterns like RAG, prompt templating, and tool integration are abstracted into reusable components.

3. **Orchestration**: LangGraph allows for building complex, stateful applications with multiple agents and decision points.

4. **Deployment and Monitoring**: LangServe and LangSmith provide solutions for deploying, debugging, and monitoring LLM applications.

5. **Integration**: Both frameworks offer extensive integration with databases, APIs, and other external systems.

When building systems with these frameworks, consider these best practices:

- Start with simple chains before moving to complex graphs
- Use structured outputs when possible for reliable parsing
- Implement proper error handling and fallbacks
- Leverage RAG for knowledge-intensive tasks
- Monitor performance with LangSmith for continuous improvement

As LLM applications grow more sophisticated, frameworks like LangChain and LangGraph will remain essential tools for managing complexity, improving reliability, and accelerating development. The combination of modular design patterns, powerful orchestration capabilities, and comprehensive tooling makes them ideal choices for production-ready AI applications.