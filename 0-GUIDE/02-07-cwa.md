<small>Claude web</small>
# 07. LangChain â€“ AI Application Development

## Key Terms and Concepts

**LangChain**: A comprehensive framework for building applications powered by large language models (LLMs). It provides abstractions and tools for chaining different components together to create complex AI workflows.

**Chain**: A sequence of calls to components, which can include LLMs, prompts, and other chains. Chains allow you to combine multiple processing steps into a single, reusable unit.

**Prompt Templates**: Reusable templates that format input data into prompts suitable for LLMs. They support variable substitution and conditional logic.

**Retrieval-Augmented Generation (RAG)**: A technique that combines information retrieval with text generation. It retrieves relevant documents from a knowledge base and uses them as context for generating responses.

**Vector Stores**: Databases optimized for storing and searching high-dimensional vectors, typically used for semantic similarity searches in RAG systems.

**Embeddings**: Dense vector representations of text that capture semantic meaning, enabling similarity searches and clustering.

**Document Loaders**: Components that can read and parse various document formats (PDF, HTML, CSV, etc.) for processing in LangChain pipelines.

**Text Splitters**: Tools that break down large documents into smaller, manageable chunks while preserving semantic coherence.

**Memory**: Components that allow chains to maintain context across multiple interactions, enabling conversational AI applications.

## Core LangChain Architecture

LangChain follows a modular architecture with several key components:

### 1. Model Integration and Chaining

```python
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain.chains import LLMChain, SequentialChain
from langchain.schema import Document

# Load environment variables
load_dotenv()

class AdvancedLangChainPipeline:
    def __init__(self):
        self.llm = ChatOpenAI(
            model="gpt-4-turbo",
            temperature=0.1,
            api_key=os.getenv("OPENAI_API_KEY")
        )
        self.embeddings = OpenAIEmbeddings(api_key=os.getenv("OPENAI_API_KEY"))
        
    def create_analysis_chain(self):
        """Create a complex chain for document analysis"""
        
        # Step 1: Content extraction and summarization
        summary_prompt = ChatPromptTemplate.from_template(
            """Analyze the following text and provide a structured summary:
            
            Text: {text}
            
            Please provide:
            1. Main topic
            2. Key points (3-5 bullet points)
            3. Sentiment analysis
            4. Complexity level (1-10)
            
            Format as JSON with keys: topic, key_points, sentiment, complexity
            """
        )
        
        # Step 2: Classification and categorization
        classification_prompt = ChatPromptTemplate.from_template(
            """Based on this summary, classify the content:
            
            Summary: {summary}
            
            Provide classification in these categories:
            - Domain: (technology, business, science, etc.)
            - Purpose: (informational, persuasive, educational, etc.)
            - Target audience: (general, professional, academic, etc.)
            - Action required: (none, review, implement, research)
            
            Format as JSON.
            """
        )
        
        # Step 3: Recommendation generation
        recommendation_prompt = ChatPromptTemplate.from_template(
            """Generate actionable recommendations based on:
            
            Original Summary: {summary}
            Classification: {classification}
            
            Provide 3-5 specific, actionable recommendations with:
            - Priority level (High/Medium/Low)
            - Estimated effort (hours)
            - Required resources
            - Expected outcome
            
            Format as JSON array.
            """
        )
        
        # Create individual chains
        summary_chain = summary_prompt | self.llm | JsonOutputParser()
        classification_chain = classification_prompt | self.llm | JsonOutputParser()
        recommendation_chain = recommendation_prompt | self.llm | JsonOutputParser()
        
        # Create sequential chain
        analysis_pipeline = (
            {"text": RunnablePassthrough()}
            | RunnableParallel(
                summary=summary_chain,
                original_text=lambda x: x["text"]
            )
            | RunnableParallel(
                summary=lambda x: x["summary"],
                classification=lambda x: classification_chain.invoke({"summary": x["summary"]}),
                original_text=lambda x: x["original_text"]
            )
            | RunnableParallel(
                summary=lambda x: x["summary"],
                classification=lambda x: x["classification"],
                recommendations=lambda x: recommendation_chain.invoke({
                    "summary": x["summary"],
                    "classification": x["classification"]
                }),
                original_text=lambda x: x["original_text"]
            )
        )
        
        return analysis_pipeline
    
    def create_conversation_chain(self):
        """Create a conversational chain with memory"""
        from langchain.memory import ConversationBufferWindowMemory
        
        memory = ConversationBufferWindowMemory(
            k=5,  # Remember last 5 exchanges
            return_messages=True
        )
        
        conversation_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an AI assistant that helps with document analysis and recommendations.
            You have access to previous conversation history and can reference earlier discussions.
            Always be helpful, accurate, and provide actionable insights."""),
            ("placeholder", "{chat_history}"),
            ("human", "{input}")
        ])
        
        conversation_chain = conversation_prompt | self.llm | StrOutputParser()
        
        return conversation_chain, memory

# Example usage
pipeline = AdvancedLangChainPipeline()
analysis_chain = pipeline.create_analysis_chain()

# Test the analysis chain
sample_text = """
Artificial Intelligence is revolutionizing the way businesses operate across industries. 
Machine learning algorithms are being deployed to automate decision-making processes, 
improve customer experiences, and optimize operational efficiency. Companies that fail 
to adopt AI technologies risk falling behind their competitors in terms of innovation 
and market responsiveness.
"""

result = analysis_chain.invoke({"text": sample_text})
print("Analysis Result:", result)
```

### 2. Advanced RAG Implementation

```python
from langchain.document_loaders import PyPDFLoader, WebBaseLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, TokenTextSplitter
from langchain_community.vectorstores import Chroma, FAISS
from langchain.retrievers import BM25Retriever, EnsembleRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain.retrievers import ContextualCompressionRetriever
import chromadb
from typing import List, Dict, Any

class AdvancedRAGSystem:
    def __init__(self, persist_directory: str = "./chroma_db"):
        self.llm = ChatOpenAI(
            model="gpt-4-turbo",
            temperature=0.1,
            api_key=os.getenv("OPENAI_API_KEY")
        )
        self.embeddings = OpenAIEmbeddings(api_key=os.getenv("OPENAI_API_KEY"))
        self.persist_directory = persist_directory
        self.vectorstore = None
        self.retriever = None
        
    def load_and_process_documents(self, sources: List[str], source_types: List[str]):
        """Load documents from various sources and process them"""
        documents = []
        
        for source, source_type in zip(sources, source_types):
            if source_type == "pdf":
                loader = PyPDFLoader(source)
            elif source_type == "web":
                loader = WebBaseLoader(source)
            elif source_type == "text":
                loader = TextLoader(source, encoding='utf-8')
            else:
                continue
                
            docs = loader.load()
            documents.extend(docs)
        
        # Advanced text splitting with overlap
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", " ", ""],
            keep_separator=False
        )
        
        split_documents = text_splitter.split_documents(documents)
        
        # Add metadata enrichment
        for i, doc in enumerate(split_documents):
            doc.metadata.update({
                'chunk_id': i,
                'chunk_size': len(doc.page_content),
                'source_type': source_types[0] if len(sources) == 1 else 'mixed'
            })
        
        return split_documents
    
    def create_hybrid_retriever(self, documents: List[Document]):
        """Create a hybrid retriever combining vector and keyword search"""
        
        # Create vector store
        self.vectorstore = Chroma.from_documents(
            documents=documents,
            embedding=self.embeddings,
            persist_directory=self.persist_directory
        )
        
        # Vector retriever
        vector_retriever = self.vectorstore.as_retriever(
            search_type="mmr",  # Maximum Marginal Relevance
            search_kwargs={
                "k": 6,
                "fetch_k": 20,
                "lambda_mult": 0.5
            }
        )
        
        # Keyword retriever (BM25)
        bm25_retriever = BM25Retriever.from_documents(documents)
        bm25_retriever.k = 6
        
        # Ensemble retriever combining both
        ensemble_retriever = EnsembleRetriever(
            retrievers=[vector_retriever, bm25_retriever],
            weights=[0.7, 0.3]  # Favor vector search slightly
        )
        
        # Add compression to reduce noise
        compressor = LLMChainExtractor.from_llm(self.llm)
        self.retriever = ContextualCompressionRetriever(
            base_compressor=compressor,
            base_retriever=ensemble_retriever
        )
        
        return self.retriever
    
    def create_rag_chain(self):
        """Create an advanced RAG chain with source citation"""
        
        # RAG prompt with source citation
        rag_prompt = ChatPromptTemplate.from_template(
            """You are an AI assistant that answers questions based on provided context.
            Always cite your sources and indicate confidence levels.
            
            Context:
            {context}
            
            Question: {question}
            
            Instructions:
            1. Answer the question based solely on the provided context
            2. If the context doesn't contain enough information, say so clearly
            3. Cite sources using [Source: filename/URL]
            4. Indicate confidence level (High/Medium/Low) for your answer
            5. Suggest follow-up questions if relevant
            
            Answer:"""
        )
        
        def format_documents(docs):
            """Format retrieved documents with source information"""
            formatted = []
            for i, doc in enumerate(docs):
                source = doc.metadata.get('source', 'Unknown')
                content = doc.page_content
                formatted.append(f"Document {i+1} [Source: {source}]:\n{content}\n")
            return "\n".join(formatted)
        
        # Create the RAG chain
        rag_chain = (
            {"context": self.retriever | format_documents, "question": RunnablePassthrough()}
            | rag_prompt
            | self.llm
            | StrOutputParser()
        )
        
        return rag_chain
    
    def create_conversational_rag(self):
        """Create a conversational RAG system with follow-up capability"""
        from langchain.memory import ConversationSummaryBufferMemory
        
        memory = ConversationSummaryBufferMemory(
            llm=self.llm,
            max_token_limit=1000,
            return_messages=True
        )
        
        conversational_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a helpful AI assistant with access to a knowledge base.
            Use the conversation history to provide contextually relevant answers.
            Always maintain conversation context and reference previous discussions when relevant."""),
            ("placeholder", "{chat_history}"),
            ("human", "Context: {context}\n\nQuestion: {question}"),
        ])
        
        def get_context_and_history(inputs):
            question = inputs["question"]
            chat_history = memory.chat_memory.messages
            
            # Retrieve relevant documents
            docs = self.retriever.get_relevant_documents(question)
            context = "\n".join([doc.page_content for doc in docs])
            
            return {
                "context": context,
                "question": question,
                "chat_history": chat_history
            }
        
        conversational_rag_chain = (
            RunnablePassthrough()
            | get_context_and_history
            | conversational_prompt
            | self.llm
            | StrOutputParser()
        )
        
        return conversational_rag_chain, memory

# Example usage
rag_system = AdvancedRAGSystem()

# Load sample documents (you would replace with actual file paths)
sample_documents = [
    Document(
        page_content="LangChain is a framework for developing applications powered by language models.",
        metadata={"source": "langchain_docs.pdf", "page": 1}
    ),
    Document(
        page_content="RAG combines retrieval of relevant documents with text generation for better accuracy.",
        metadata={"source": "rag_guide.txt", "page": 1}
    )
]

# Create retriever and RAG chain
retriever = rag_system.create_hybrid_retriever(sample_documents)
rag_chain = rag_system.create_rag_chain()

# Test the RAG system
question = "What is LangChain and how does it relate to RAG?"
answer = rag_chain.invoke(question)
print("RAG Answer:", answer)
```

### 3. Production Deployment with FastAPI

```python
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import Optional, List, Dict
import asyncio
import uvicorn
from datetime import datetime
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class QueryRequest(BaseModel):
    question: str = Field(..., description="The question to ask")
    session_id: Optional[str] = Field(None, description="Session ID for conversation continuity")
    include_sources: bool = Field(True, description="Whether to include source citations")

class QueryResponse(BaseModel):
    answer: str
    confidence: str
    sources: List[str]
    session_id: str
    timestamp: datetime

class DocumentUploadRequest(BaseModel):
    content: str
    filename: str
    document_type: str

class LangChainAPIService:
    def __init__(self):
        self.rag_system = AdvancedRAGSystem()
        self.pipeline = AdvancedLangChainPipeline()
        self.sessions = {}  # In production, use Redis or database
        
    async def initialize(self):
        """Initialize the service with sample data"""
        # In production, load from persistent storage
        sample_docs = [
            Document(
                page_content="LangChain provides tools for building LLM applications with chaining capabilities.",
                metadata={"source": "system_docs", "type": "documentation"}
            )
        ]
        
        self.rag_system.create_hybrid_retriever(sample_docs)
        logger.info("LangChain API Service initialized")
    
    async def process_query(self, request: QueryRequest) -> QueryResponse:
        """Process a user query using RAG"""
        try:
            # Get or create session
            session_id = request.session_id or f"session_{datetime.now().timestamp()}"
            
            if session_id not in self.sessions:
                rag_chain, memory = self.rag_system.create_conversational_rag()
                self.sessions[session_id] = {"chain": rag_chain, "memory": memory}
            
            # Process query
            chain = self.sessions[session_id]["chain"]
            answer = await asyncio.to_thread(
                chain.invoke, 
                {"question": request.question}
            )
            
            # Extract confidence and sources (simplified)
            confidence = "Medium"  # Would implement proper confidence scoring
            sources = ["system_docs"]  # Would extract from actual retrieval
            
            return QueryResponse(
                answer=answer,
                confidence=confidence,
                sources=sources if request.include_sources else [],
                session_id=session_id,
                timestamp=datetime.now()
            )
            
        except Exception as e:
            logger.error(f"Error processing query: {str(e)}")
            raise HTTPException(status_code=500, detail=str(e))
    
    async def add_document(self, request: DocumentUploadRequest):
        """Add a new document to the knowledge base"""
        try:
            doc = Document(
                page_content=request.content,
                metadata={
                    "source": request.filename,
                    "type": request.document_type,
                    "added_at": datetime.now().isoformat()
                }
            )
            
            # In production, you'd update the vector store
            # self.rag_system.add_documents([doc])
            
            return {"status": "success", "message": f"Document {request.filename} added"}
            
        except Exception as e:
            logger.error(f"Error adding document: {str(e)}")
            raise HTTPException(status_code=500, detail=str(e))

# Create FastAPI app
app = FastAPI(
    title="LangChain AI Service",
    description="Advanced AI service built with LangChain",
    version="1.0.0"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize service
service = LangChainAPIService()

@app.on_event("startup")
async def startup_event():
    await service.initialize()

@app.post("/query", response_model=QueryResponse)
async def query_endpoint(request: QueryRequest):
    """Process a user query"""
    return await service.process_query(request)

@app.post("/documents")
async def add_document_endpoint(request: DocumentUploadRequest):
    """Add a document to the knowledge base"""
    return await service.add_document(request)

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy", "timestamp": datetime.now()}

@app.get("/sessions/{session_id}")
async def get_session_info(session_id: str):
    """Get information about a session"""
    if session_id in service.sessions:
        return {
            "session_id": session_id,
            "active": True,
            "created_at": "timestamp"  # Would track in production
        }
    else:
        raise HTTPException(status_code=404, detail="Session not found")

# Production deployment configuration
if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=False,  # Set to False in production
        workers=4,     # Adjust based on your needs
        log_level="info"
    )
```

### 4. Advanced Chain Patterns and Optimization

```python
from langchain.callbacks import AsyncCallbackHandler
from langchain.callbacks.manager import AsyncCallbackManager
from langchain_core.runnables import RunnableLambda, RunnableBranch
from langchain.schema.runnable import Runnable
import asyncio
import time
from typing import Any, Dict, List
import json

class PerformanceCallback(AsyncCallbackHandler):
    """Custom callback to track performance metrics"""
    
    def __init__(self):
        self.start_time = None
        self.metrics = {}
    
    async def on_chain_start(self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs):
        self.start_time = time.time()
        self.metrics['inputs'] = inputs
    
    async def on_chain_end(self, outputs: Dict[str, Any], **kwargs):
        end_time = time.time()
        self.metrics['duration'] = end_time - self.start_time
        self.metrics['outputs'] = outputs
        print(f"Chain completed in {self.metrics['duration']:.2f} seconds")

class OptimizedChainFactory:
    """Factory for creating optimized, production-ready chains"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model="gpt-4-turbo",
            temperature=0.1,
            api_key=os.getenv("OPENAI_API_KEY"),
            max_retries=3,
            request_timeout=30
        )
    
    def create_conditional_chain(self):
        """Create a chain with conditional branching based on input"""
        
        def classify_query_type(inputs: Dict) -> str:
            """Classify the type of query"""
            query = inputs["query"].lower()
            
            if any(word in query for word in ["analyze", "analysis", "examine"]):
                return "analysis"
            elif any(word in query for word in ["summarize", "summary", "brief"]):
                return "summary"
            elif any(word in query for word in ["create", "generate", "write"]):
                return "generation"
            else:
                return "general"
        
        # Different prompts for different query types
        analysis_prompt = ChatPromptTemplate.from_template(
            """Perform a detailed analysis of: {query}
            
            Provide:
            1. Key components
            2. Relationships
            3. Implications
            4. Recommendations
            
            Analysis:"""
        )
        
        summary_prompt = ChatPromptTemplate.from_template(
            """Provide a concise summary of: {query}
            
            Focus on:
            - Main points
            - Key takeaways
            - Essential information
            
            Summary:"""
        )
        
        generation_prompt = ChatPromptTemplate.from_template(
            """Create content based on: {query}
            
            Requirements:
            - Be creative and original
            - Follow best practices
            - Provide actionable results
            
            Generated content:"""
        )
        
        general_prompt = ChatPromptTemplate.from_template(
            """Answer the following question comprehensively: {query}
            
            Answer:"""
        )
        
        # Create conditional chain
        conditional_chain = RunnableBranch(
            (lambda x: classify_query_type(x) == "analysis", analysis_prompt | self.llm),
            (lambda x: classify_query_type(x) == "summary", summary_prompt | self.llm),
            (lambda x: classify_query_type(x) == "generation", generation_prompt | self.llm),
            general_prompt | self.llm  # Default case
        )
        
        return conditional_chain
    
    def create_parallel_processing_chain(self):
        """Create a chain that processes multiple aspects in parallel"""
        
        # Define parallel processing prompts
        content_analysis_prompt = ChatPromptTemplate.from_template(
            "Analyze the content and structure of: {text}"
        )
        
        sentiment_analysis_prompt = ChatPromptTemplate.from_template(
            "Analyze the sentiment and tone of: {text}"
        )
        
        keyword_extraction_prompt = ChatPromptTemplate.from_template(
            "Extract key terms and concepts from: {text}"
        )
        
        # Create parallel chains
        parallel_chain = RunnableParallel(
            content_analysis=content_analysis_prompt | self.llm,
            sentiment_analysis=sentiment_analysis_prompt | self.llm,
            keyword_extraction=keyword_extraction_prompt | self.llm
        )
        
        # Combine results
        synthesis_prompt = ChatPromptTemplate.from_template(
            """Synthesize the following analyses into a comprehensive report:
            
            Content Analysis: {content_analysis}
            Sentiment Analysis: {sentiment_analysis}
            Keywords: {keyword_extraction}
            
            Comprehensive Report:"""
        )
        
        full_chain = (
            {"text": RunnablePassthrough()}
            | parallel_chain
            | synthesis_prompt
            | self.llm
            | StrOutputParser()
        )
        
        return full_chain
    
    def create_self_correcting_chain(self):
        """Create a chain that can self-correct its outputs"""
        
        initial_response_prompt = ChatPromptTemplate.from_template(
            """Answer the following question: {question}
            
            Answer:"""
        )
        
        validation_prompt = ChatPromptTemplate.from_template(
            """Review this answer for accuracy and completeness:
            
            Question: {question}
            Answer: {initial_answer}
            
            Issues found (if any):
            - Accuracy problems
            - Missing information
            - Logical inconsistencies
            
            Validation result (VALID/NEEDS_CORRECTION):"""
        )
        
        correction_prompt = ChatPromptTemplate.from_template(
            """Improve this answer based on the validation feedback:
            
            Original Question: {question}
            Original Answer: {initial_answer}
            Validation Issues: {validation_result}
            
            Improved Answer:"""
        )
        
        def validate_and_correct(inputs):
            """Validation and correction logic"""
            question = inputs["question"]
            
            # Get initial answer
            initial_chain = initial_response_prompt | self.llm | StrOutputParser()
            initial_answer = initial_chain.invoke({"question": question})
            
            # Validate the answer
            validation_chain = validation_prompt | self.llm | StrOutputParser()
            validation_result = validation_chain.invoke({
                "question": question,
                "initial_answer": initial_answer
            })
            
            # Correct if needed
            if "NEEDS_CORRECTION" in validation_result:
                correction_chain = correction_prompt | self.llm | StrOutputParser()
                final_answer = correction_chain.invoke({
                    "question": question,
                    "initial_answer": initial_answer,
                    "validation_result": validation_result
                })
                return {
                    "answer": final_answer,
                    "corrected": True,
                    "validation_issues": validation_result
                }
            else:
                return {
                    "answer": initial_answer,
                    "corrected": False,
                    "validation_issues": None
                }
        
        return RunnableLambda(validate_and_correct)
    
    async def create_async_batch_processor(self):
        """Create an async chain for batch processing"""
        
        async def process_batch(inputs: List[Dict]) -> List[Dict]:
            """Process multiple inputs concurrently"""
            
            async def process_single(item):
                prompt = ChatPromptTemplate.from_template(
                    "Process this item: {item}\n\nResult:"
                )
                chain = prompt | self.llm | StrOutputParser()
                
                # Add callback for monitoring
                callback = PerformanceCallback()
                result = await chain.ainvoke(
                    {"item": item["content"]},
                    config={"callbacks": [callback]}
                )
                
                return {
                    "id": item["id"],
                    "result": result,
                    "processing_time": callback.metrics.get("duration", 0)
                }
            
            # Process all items concurrently
            tasks = [process_single(item) for item in inputs]
            results = await asyncio.gather(*tasks)
            
            return results
        
        return RunnableLambda(process_batch)

# Example usage and testing
async def demonstrate_advanced_chains():
    """Demonstrate advanced chain patterns"""
    
    factory = OptimizedChainFactory()
    
    # Test conditional chain
    print("=== Conditional Chain Test ===")
    conditional_chain = factory.create_conditional_chain()
    
    test_queries = [
        {"query": "Analyze the impact of AI on healthcare"},
        {"query": "Summarize the key benefits of renewable energy"},
        {"query": "Create a marketing strategy for a new product"}
    ]
    
    for query in test_queries:
        result = conditional_chain.invoke(query)
        print(f"Query: {query['query']}")
        print(f"Result: {result.content[:100]}...\n")
    
    # Test parallel processing
    print("=== Parallel Processing Test ===")
    parallel_chain = factory.create_parallel_processing_chain()
    
    test_text = "Artificial intelligence is transforming industries worldwide."
    result = parallel_chain.invoke(test_text)
    print(f"Parallel processing result: {result[:200]}...\n")
    
    # Test self-correcting chain
    print("=== Self-Correcting Chain Test ===")
    correcting_chain = factory.create_self_correcting_chain()
    
    result = correcting_chain.invoke({"question": "What is the capital of Australia?"})
    print(f"Self-correction result: {json.dumps(result, indent=2)}\n")
    
    # Test async batch processing
    print("=== Async Batch Processing Test ===")
    batch_processor = await factory.create_async_batch_processor()
    
    batch_items = [
        {"id": 1, "content": "Process this text about machine learning"},
        {"id": 2, "content": "Analyze this content about cloud computing"},
        {"id": 3, "content": "Summarize this information about data science"}
    ]
    
    results = await batch_processor.ainvoke(batch_items)
    print("Batch processing results:")
    for result in results:
        print(f"ID {result['id']}: {result['result'][:50]}... (Time: {result['processing_time']:.2f}s)")

# Run the demonstration
if __name__ == "__main__":
    asyncio.run(demonstrate_advanced_chains())
```

## Conclusion

LangChain represents a paradigm shift in AI application development, providing a comprehensive framework that abstracts the complexity of working with large language models while offering unprecedented flexibility in creating sophisticated AI workflows. Through this section, we've explored the core concepts of chaining models, prompts, and tools, demonstrating how LangChain enables developers to build production-ready AI applications.

The implementation of Retrieval-Augmented Generation (RAG) systems showcases LangChain's strength in combining external knowledge sources with generative AI, resulting in more accurate and contextually relevant responses. The hybrid retrieval approach, combining vector similarity search with keyword-based retrieval, represents current best practices in information retrieval for AI systems.

The production deployment examples illustrate how LangChain applications can be scaled and deployed in real-world environments using modern web frameworks like FastAPI. The inclusion of advanced patterns such as conditional branching, parallel processing, and self-correction demonstrates the framework's capability to handle complex business logic and quality assurance requirements.

Key advantages of LangChain include its modular architecture, extensive integration ecosystem, and built-in support for common AI application patterns. However, developers should be mindful of performance considerations, especially when dealing with complex chains that involve multiple LLM calls, and implement appropriate caching and optimization strategies.

As AI applications continue to evolve, LangChain's abstraction layer and community-driven development make it an excellent choice for developers looking to build robust, maintainable, and scalable AI solutions.