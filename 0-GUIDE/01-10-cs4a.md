<small>Claude Sonnet 4</small>
# 10. RL Agent - Practical Project

## Key Terms

**Algorithmic Trading**: The use of computer algorithms to automatically execute trading decisions based on pre-defined rules, market data analysis, and mathematical models, eliminating human emotional bias and enabling high-frequency execution.

**Market Microstructure**: The detailed study of how financial markets operate at the transaction level, including order flow, bid-ask spreads, market impact, and the mechanisms through which prices are discovered and transactions are executed.

**Reward Engineering**: The critical process of designing reward functions in reinforcement learning that properly incentivize desired behaviors while avoiding unintended consequences such as reward hacking or suboptimal local minima.

**Portfolio Optimization**: The mathematical approach to selecting the optimal mix of assets to maximize expected returns for a given level of risk, or minimize risk for a given level of expected return, often incorporating constraints and transaction costs.

**Risk-Adjusted Returns**: Performance metrics that account for the volatility and downside risk of investment strategies, such as Sharpe ratio, Sortino ratio, and maximum drawdown, providing a more comprehensive evaluation than raw returns.

**Market Regime Detection**: The identification of different market conditions or states (bull, bear, sideways, high volatility) that may require different trading strategies, often implemented using hidden Markov models or other state-space approaches.

**Backtesting Framework**: A systematic methodology for testing trading strategies on historical data to evaluate performance, identify potential issues, and estimate future performance while avoiding look-ahead bias and overfitting.

**Transaction Cost Modeling**: The incorporation of realistic trading costs including bid-ask spreads, market impact, commission fees, and slippage into trading strategy evaluation and optimization.

## Financial Trading Bot Development

Building a sophisticated financial trading bot using reinforcement learning requires careful consideration of market dynamics, risk management, and regulatory constraints. The implementation must balance profitability with robustness while handling the non-stationary nature of financial markets.

### Advanced Trading Environment and Agent Implementation

````python
import numpy as np
import pandas as pd
import gymnasium as gym
from gymnasium import spaces
import yfinance as yf
import ta
from typing import Dict, List, Tuple, Any, Optional, Union
from dataclasses import dataclass, field
from datetime import datetime, timedelta
import json
import sqlite3
from collections import deque
import warnings
warnings.filterwarnings('ignore')

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.ensemble import IsolationForest
import matplotlib.pyplot as plt
import seaborn as sns
from dotenv import load_dotenv
import os

load_dotenv()

@dataclass
class TradingConfig:
    """Configuration for trading environment and agent"""
    initial_balance: float = 100000.0
    max_position_size: float = 0.3  # 30% of portfolio
    transaction_cost: float = 0.001  # 0.1% per trade
    lookback_window: int = 60
    max_trades_per_day: int = 10
    risk_free_rate: float = 0.02  # 2% annual
    max_drawdown_threshold: float = 0.2  # 20%
    position_holding_penalty: float = 0.0001  # Daily holding cost
    
@dataclass
class MarketState:
    """Current market state representation"""
    prices: np.ndarray
    technical_indicators: np.ndarray
    volume: np.ndarray
    volatility: float
    trend: float
    support_resistance: List[float]
    market_regime: str
    timestamp: datetime

class TechnicalIndicatorCalculator:
    """Advanced technical indicator calculation"""
    
    def __init__(self, config: TradingConfig):
        self.config = config
        self.scaler = RobustScaler()
        
    def calculate_indicators(self, df: pd.DataFrame) -> np.ndarray:
        """Calculate comprehensive technical indicators"""
        
        # Price-based indicators
        df['sma_20'] = ta.trend.sma_indicator(df['Close'], window=20)
        df['sma_50'] = ta.trend.sma_indicator(df['Close'], window=50)
        df['ema_12'] = ta.trend.ema_indicator(df['Close'], window=12)
        df['ema_26'] = ta.trend.ema_indicator(df['Close'], window=26)
        
        # Momentum indicators
        df['rsi'] = ta.momentum.rsi(df['Close'], window=14)
        df['stoch'] = ta.momentum.stoch(df['High'], df['Low'], df['Close'])
        df['macd'] = ta.trend.macd_diff(df['Close'])
        df['williams_r'] = ta.momentum.williams_r(df['High'], df['Low'], df['Close'])
        
        # Volatility indicators
        df['bb_upper'], df['bb_middle'], df['bb_lower'] = ta.volatility.bollinger_hband(df['Close']), ta.volatility.bollinger_mavg(df['Close']), ta.volatility.bollinger_lband(df['Close'])
        df['atr'] = ta.volatility.average_true_range(df['High'], df['Low'], df['Close'])
        df['keltner_upper'], df['keltner_lower'] = ta.volatility.keltner_channel_hband(df['High'], df['Low'], df['Close']), ta.volatility.keltner_channel_lband(df['High'], df['Low'], df['Close'])
        
        # Volume indicators
        df['volume_sma'] = ta.volume.volume_sma(df['Close'], df['Volume'])
        df['mfi'] = ta.volume.money_flow_index(df['High'], df['Low'], df['Close'], df['Volume'])
        df['obv'] = ta.volume.on_balance_volume(df['Close'], df['Volume'])
        
        # Trend indicators
        df['adx'] = ta.trend.adx(df['High'], df['Low'], df['Close'])
        df['cci'] = ta.trend.cci(df['High'], df['Low'], df['Close'])
        df['dpo'] = ta.trend.dpo(df['Close'])
        
        # Custom indicators
        df['price_momentum'] = df['Close'].pct_change(periods=5)
        df['volume_momentum'] = df['Volume'].pct_change(periods=5)
        df['volatility_regime'] = df['Close'].rolling(window=20).std() / df['Close'].rolling(window=60).std()
        
        # Support and resistance levels
        df['resistance_1'] = df['High'].rolling(window=20).max()
        df['support_1'] = df['Low'].rolling(window=20).min()
        df['resistance_2'] = df['High'].rolling(window=50).max()
        df['support_2'] = df['Low'].rolling(window=50).min()
        
        # Market regime indicators
        df['trend_strength'] = (df['Close'] - df['sma_50']) / df['atr']
        df['market_efficiency'] = abs(df['Close'] - df['Open']) / (df['High'] - df['Low'] + 1e-8)
        
        # Select feature columns
        feature_columns = [
            'sma_20', 'sma_50', 'ema_12', 'ema_26', 'rsi', 'stoch', 'macd', 'williams_r',
            'bb_upper', 'bb_middle', 'bb_lower', 'atr', 'keltner_upper', 'keltner_lower',
            'volume_sma', 'mfi', 'obv', 'adx', 'cci', 'dpo',
            'price_momentum', 'volume_momentum', 'volatility_regime',
            'resistance_1', 'support_1', 'resistance_2', 'support_2',
            'trend_strength', 'market_efficiency'
        ]
        
        # Normalize indicators
        indicator_data = df[feature_columns].fillna(method='ffill').fillna(0)
        
        # Apply robust scaling
        scaled_indicators = self.scaler.fit_transform(indicator_data)
        
        return scaled_indicators
    
    def detect_market_regime(self, df: pd.DataFrame) -> str:
        """Detect current market regime"""
        
        recent_data = df.tail(20)
        
        # Calculate regime indicators
        trend = (recent_data['Close'].iloc[-1] - recent_data['Close'].iloc[0]) / recent_data['Close'].iloc[0]
        volatility = recent_data['Close'].pct_change().std()
        volume_trend = recent_data['Volume'].tail(5).mean() / recent_data['Volume'].head(5).mean()
        
        # Classify regime
        if trend > 0.02 and volatility < 0.02:
            return "bull_stable"
        elif trend > 0.02 and volatility >= 0.02:
            return "bull_volatile"
        elif trend < -0.02 and volatility < 0.02:
            return "bear_stable"
        elif trend < -0.02 and volatility >= 0.02:
            return "bear_volatile"
        elif abs(trend) <= 0.02 and volatility < 0.015:
            return "sideways_stable"
        else:
            return "sideways_volatile"

class TradingEnvironment(gym.Env):
    """Advanced trading environment with realistic market simulation"""
    
    def __init__(self, data: pd.DataFrame, config: TradingConfig):
        super().__init__()
        
        self.config = config
        self.data = data.copy()
        self.indicator_calculator = TechnicalIndicatorCalculator(config)
        
        # Calculate technical indicators
        self.indicators = self.indicator_calculator.calculate_indicators(self.data)
        
        # Environment state
        self.current_step = 0
        self.balance = config.initial_balance
        self.initial_balance = config.initial_balance
        self.position = 0.0  # Current position size (-1 to 1, negative = short)
        self.position_value = 0.0
        self.trades_today = 0
        self.last_trade_day = None
        
        # Performance tracking
        self.portfolio_values = [config.initial_balance]
        self.trades = []
        self.daily_returns = []
        self.drawdowns = []
        
        # Risk management
        self.max_drawdown = 0.0
        self.peak_value = config.initial_balance
        
        # Action space: [position_change, trade_size]
        # position_change: -1 (sell/short), 0 (hold), 1 (buy/long)
        # trade_size: 0.0 to 1.0 (fraction of max position)
        self.action_space = spaces.Box(
            low=np.array([-1.0, 0.0]),
            high=np.array([1.0, 1.0]),
            dtype=np.float32
        )
        
        # Observation space: [price_history, indicators, portfolio_state, market_regime]
        n_price_features = 5  # OHLCV
        n_indicators = self.indicators.shape[1]
        n_portfolio_features = 8  # balance, position, portfolio_value, etc.
        n_regime_features = 6  # market regime encoding
        
        total_features = (n_price_features + n_indicators) * config.lookback_window + n_portfolio_features + n_regime_features
        
        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(total_features,),
            dtype=np.float32
        )
        
        # Market impact model
        self.market_impact_factor = 0.0001  # Price impact per unit traded
        
    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None):
        super().reset(seed=seed)
        
        # Reset environment state
        self.current_step = self.config.lookback_window
        self.balance = self.config.initial_balance
        self.position = 0.0
        self.position_value = 0.0
        self.trades_today = 0
        self.last_trade_day = None
        
        # Reset performance tracking
        self.portfolio_values = [self.config.initial_balance]
        self.trades = []
        self.daily_returns = []
        self.drawdowns = []
        self.max_drawdown = 0.0
        self.peak_value = self.config.initial_balance
        
        observation = self._get_observation()
        info = self._get_info()
        
        return observation, info
    
    def step(self, action: np.ndarray):
        """Execute trading action and update environment"""
        
        position_change, trade_size = action
        current_price = self.data.iloc[self.current_step]['Close']
        current_date = self.data.index[self.current_step]
        
        # Reset daily trade counter
        if self.last_trade_day != current_date.date():
            self.trades_today = 0
            self.last_trade_day = current_date.date()
        
        # Calculate desired position
        desired_position_change = np.clip(position_change, -1, 1)
        trade_size = np.clip(trade_size, 0, 1)
        
        # Check trading constraints
        can_trade = (
            self.trades_today < self.config.max_trades_per_day and
            self.max_drawdown < self.config.max_drawdown_threshold
        )
        
        executed_trade = 0.0
        trade_cost = 0.0
        
        if can_trade and abs(desired_position_change) > 0.1:  # Minimum trade threshold
            # Calculate trade size
            max_trade_value = self.config.max_position_size * (self.balance + self.position_value)
            desired_trade_value = trade_size * max_trade_value * abs(desired_position_change)
            
            # Calculate shares to trade
            shares_to_trade = desired_trade_value / current_price
            
            if desired_position_change > 0:  # Buy
                max_shares = (self.balance * 0.95) / current_price  # Keep 5% cash buffer
                shares_to_trade = min(shares_to_trade, max_shares)
            else:  # Sell/Short
                max_shares = abs(self.position) if self.position < 0 else float('inf')
                shares_to_trade = min(shares_to_trade, max_shares)
            
            if shares_to_trade > 0:
                # Calculate market impact
                market_impact = self.market_impact_factor * shares_to_trade * current_price / 1000000  # Per million
                effective_price = current_price * (1 + market_impact * np.sign(desired_position_change))
                
                # Execute trade
                trade_value = shares_to_trade * effective_price
                trade_cost = trade_value * self.config.transaction_cost
                
                if desired_position_change > 0:  # Buy
                    if self.balance >= trade_value + trade_cost:
                        self.balance -= (trade_value + trade_cost)
                        self.position += shares_to_trade
                        executed_trade = shares_to_trade
                        self.trades_today += 1
                else:  # Sell
                    self.balance += (trade_value - trade_cost)
                    self.position -= shares_to_trade
                    executed_trade = -shares_to_trade
                    self.trades_today += 1
                
                # Record trade
                self.trades.append({
                    'timestamp': current_date,
                    'action': 'buy' if desired_position_change > 0 else 'sell',
                    'shares': shares_to_trade,
                    'price': effective_price,
                    'cost': trade_cost,
                    'balance_after': self.balance,
                    'position_after': self.position
                })
        
        # Update position value
        self.position_value = self.position * current_price
        
        # Calculate total portfolio value
        total_portfolio_value = self.balance + self.position_value
        
        # Apply holding costs
        holding_cost = abs(self.position_value) * self.config.position_holding_penalty
        self.balance -= holding_cost
        total_portfolio_value -= holding_cost
        
        # Update performance metrics
        self.portfolio_values.append(total_portfolio_value)
        
        if total_portfolio_value > self.peak_value:
            self.peak_value = total_portfolio_value
        
        current_drawdown = (self.peak_value - total_portfolio_value) / self.peak_value
        self.drawdowns.append(current_drawdown)
        self.max_drawdown = max(self.max_drawdown, current_drawdown)
        
        # Calculate daily return
        if len(self.portfolio_values) > 1:
            daily_return = (total_portfolio_value - self.portfolio_values[-2]) / self.portfolio_values[-2]
            self.daily_returns.append(daily_return)
        
        # Calculate reward
        reward = self._calculate_reward(executed_trade, trade_cost, total_portfolio_value)
        
        # Check if episode is done
        self.current_step += 1
        terminated = (
            self.current_step >= len(self.data) - 1 or
            total_portfolio_value < self.config.initial_balance * 0.3 or  # 70% loss
            self.max_drawdown > self.config.max_drawdown_threshold
        )
        
        truncated = False
        
        observation = self._get_observation()
        info = self._get_info()
        
        return observation, reward, terminated, truncated, info
    
    def _calculate_reward(self, executed_trade: float, trade_cost: float, portfolio_value: float) -> float:
        """Calculate sophisticated reward incorporating multiple factors"""
        
        # Base return reward
        if len(self.portfolio_values) > 1:
            return_reward = (portfolio_value - self.portfolio_values[-2]) / self.portfolio_values[-2] * 100
        else:
            return_reward = 0.0
        
        # Risk-adjusted return (Sharpe-like)
        if len(self.daily_returns) > 10:
            volatility = np.std(self.daily_returns[-20:]) + 1e-8
            risk_adjusted_reward = return_reward / volatility
        else:
            risk_adjusted_reward = return_reward
        
        # Transaction cost penalty
        cost_penalty = -trade_cost / self.config.initial_balance * 1000
        
        # Drawdown penalty
        drawdown_penalty = -self.max_drawdown * 10
        
        # Position holding penalty (encourages active management)
        position_penalty = -abs(self.position * self.data.iloc[self.current_step]['Close']) / portfolio_value * 0.1
        
        # Diversification bonus (penalty for extreme positions)
        diversification_bonus = -abs(self.position_value / portfolio_value) * 2 if portfolio_value > 0 else 0
        
        # Market timing bonus
        if executed_trade != 0 and len(self.daily_returns) > 5:
            market_momentum = np.mean(self.daily_returns[-5:])
            if (executed_trade > 0 and market_momentum > 0) or (executed_trade < 0 and market_momentum < 0):
                timing_bonus = 1.0
            else:
                timing_bonus = -0.5
        else:
            timing_bonus = 0.0
        
        # Total reward
        total_reward = (
            risk_adjusted_reward * 0.6 +
            cost_penalty * 0.1 +
            drawdown_penalty * 0.1 +
            position_penalty * 0.05 +
            diversification_bonus * 0.05 +
            timing_bonus * 0.1
        )
        
        return total_reward
    
    def _get_observation(self) -> np.ndarray:
        """Get current environment observation"""
        
        # Price history (OHLCV)
        price_history = []
        for i in range(self.config.lookback_window):
            step_idx = self.current_step - self.config.lookback_window + i + 1
            if step_idx >= 0:
                row = self.data.iloc[step_idx]
                normalized_prices = np.array([
                    row['Open'] / row['Close'],
                    row['High'] / row['Close'],
                    row['Low'] / row['Close'],
                    1.0,  # Close/Close = 1
                    row['Volume'] / row['Volume'].rolling(20).mean() if not pd.isna(row['Volume'].rolling(20).mean()) else 1.0
                ])
                price_history.extend(normalized_prices)
            else:
                price_history.extend([1.0, 1.0, 1.0, 1.0, 1.0])  # Neutral values
        
        # Technical indicators history
        indicator_history = []
        for i in range(self.config.lookback_window):
            step_idx = self.current_step - self.config.lookback_window + i + 1
            if step_idx >= 0 and step_idx < len(self.indicators):
                indicator_history.extend(self.indicators[step_idx])
            else:
                indicator_history.extend([0.0] * self.indicators.shape[1])
        
        # Portfolio state
        current_price = self.data.iloc[self.current_step]['Close']
        portfolio_value = self.balance + self.position * current_price
        
        portfolio_state = np.array([
            self.balance / self.config.initial_balance,
            self.position * current_price / self.config.initial_balance,
            portfolio_value / self.config.initial_balance,
            self.position / (self.config.initial_balance / current_price),  # Normalized position
            self.max_drawdown,
            len(self.trades) / 1000,  # Trade count (normalized)
            self.trades_today / self.config.max_trades_per_day,
            (portfolio_value - self.config.initial_balance) / self.config.initial_balance  # Total return
        ])
        
        # Market regime encoding
        regime = self.indicator_calculator.detect_market_regime(
            self.data.iloc[max(0, self.current_step-20):self.current_step+1]
        )
        
        regime_encoding = {
            "bull_stable": [1, 0, 0, 0, 0, 0],
            "bull_volatile": [0, 1, 0, 0, 0, 0],
            "bear_stable": [0, 0, 1, 0, 0, 0],
            "bear_volatile": [0, 0, 0, 1, 0, 0],
            "sideways_stable": [0, 0, 0, 0, 1, 0],
            "sideways_volatile": [0, 0, 0, 0, 0, 1]
        }
        
        regime_features = np.array(regime_encoding.get(regime, [0, 0, 0, 0, 0, 1]))
        
        # Combine all features
        observation = np.concatenate([
            price_history,
            indicator_history,
            portfolio_state,
            regime_features
        ]).astype(np.float32)
        
        return observation
    
    def _get_info(self) -> Dict[str, Any]:
        """Get additional environment information"""
        
        current_price = self.data.iloc[self.current_step]['Close']
        portfolio_value = self.balance + self.position * current_price
        
        return {
            'timestamp': self.data.index[self.current_step],
            'current_price': current_price,
            'balance': self.balance,
            'position': self.position,
            'position_value': self.position_value,
            'portfolio_value': portfolio_value,
            'total_return': (portfolio_value - self.config.initial_balance) / self.config.initial_balance,
            'max_drawdown': self.max_drawdown,
            'trades_today': self.trades_today,
            'total_trades': len(self.trades)
        }

class AdvancedTradingAgent(nn.Module):
    """Advanced neural network for trading decisions"""
    
    def __init__(self, observation_size: int, action_size: int = 2):
        super().__init__()
        
        self.observation_size = observation_size
        self.action_size = action_size
        
        # Feature extraction layers
        self.feature_layers = nn.Sequential(
            nn.Linear(observation_size, 512),
            nn.LayerNorm(512),
            nn.ReLU(),
            nn.Dropout(0.3),
            
            nn.Linear(512, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # Separate heads for different outputs
        self.position_head = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Tanh()  # Output between -1 and 1
        )
        
        self.size_head = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()  # Output between 0 and 1
        )
        
        # Value estimation for critic
        self.value_head = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        
        # Initialize weights
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.xavier_uniform_(module.weight)
            module.bias.data.fill_(0.01)
    
    def forward(self, state):
        features = self.feature_layers(state)
        
        position_action = self.position_head(features)
        size_action = self.size_head(features)
        value = self.value_head(features)
        
        return torch.cat([position_action, size_action], dim=-1), value

class TradingRLTrainer:
    """Advanced trainer for trading RL agent"""
    
    def __init__(self, config: TradingConfig):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Training parameters
        self.learning_rate = 0.0001
        self.gamma = 0.99
        self.gae_lambda = 0.95
        self.clip_epsilon = 0.2
        self.entropy_weight = 0.01
        self.value_weight = 0.5
        
        # Training metrics
        self.training_history = {
            'episode_rewards': [],
            'portfolio_values': [],
            'max_drawdowns': [],
            'sharpe_ratios': [],
            'total_returns': [],
            'win_rates': [],
            'policy_losses': [],
            'value_losses': []
        }
    
    def prepare_data(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:
        """Download and prepare market data"""
        
        # Download data
        print(f"Downloading data for {symbol} from {start_date} to {end_date}")
        data = yf.download(symbol, start=start_date, end=end_date)
        
        if data.empty:
            raise ValueError(f"No data found for symbol {symbol}")
        
        # Clean data
        data = data.dropna()
        
        # Add additional features
        data['Returns'] = data['Close'].pct_change()
        data['Volatility'] = data['Returns'].rolling(window=20).std()
        data['Volume_MA'] = data['Volume'].rolling(window=20).mean()
        
        return data
    
    def create_train_test_split(self, data: pd.DataFrame, test_ratio: float = 0.2) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Split data into train and test sets"""
        
        split_idx = int(len(data) * (1 - test_ratio))
        train_data = data.iloc[:split_idx]
        test_data = data.iloc[split_idx:]
        
        return train_data, test_data
    
    def train_agent(self, train_data: pd.DataFrame, episodes: int = 1000) -> Dict[str, Any]:
        """Train the trading agent using PPO algorithm"""
        
        # Create environment
        env = TradingEnvironment(train_data, self.config)
        
        # Create agent
        observation_size = env.observation_space.shape[0]
        agent = AdvancedTradingAgent(observation_size).to(self.device)
        optimizer = optim.Adam(agent.parameters(), lr=self.learning_rate)
        
        print(f"Training agent for {episodes} episodes on {len(train_data)} days of data")
        
        best_performance = -np.inf
        
        for episode in range(episodes):
            # Collect trajectory
            states, actions, rewards, values, log_probs = self._collect_trajectory(env, agent)
            
            if len(states) == 0:
                continue
            
            # Calculate advantages using GAE
            advantages, returns = self._calculate_advantages(rewards, values)
            
            # Update policy using PPO
            policy_loss, value_loss = self._update_policy(
                agent, optimizer, states, actions, log_probs, advantages, returns
            )
            
            # Calculate episode metrics
            episode_reward = sum(rewards)
            final_portfolio_value = env.portfolio_values[-1]
            total_return = (final_portfolio_value - self.config.initial_balance) / self.config.initial_balance
            max_drawdown = env.max_drawdown
            
            # Calculate Sharpe ratio
            if len(env.daily_returns) > 1:
                excess_returns = np.array(env.daily_returns) - self.config.risk_free_rate / 252
                sharpe_ratio = np.mean(excess_returns) / (np.std(excess_returns) + 1e-8) * np.sqrt(252)
            else:
                sharpe_ratio = 0.0
            
            # Calculate win rate
            positive_trades = sum(1 for trade in env.trades if self._calculate_trade_pnl(trade, env.data) > 0)
            win_rate = positive_trades / len(env.trades) if env.trades else 0.0
            
            # Store metrics
            self.training_history['episode_rewards'].append(episode_reward)
            self.training_history['portfolio_values'].append(final_portfolio_value)
            self.training_history['max_drawdowns'].append(max_drawdown)
            self.training_history['sharpe_ratios'].append(sharpe_ratio)
            self.training_history['total_returns'].append(total_return)
            self.training_history['win_rates'].append(win_rate)
            self.training_history['policy_losses'].append(policy_loss)
            self.training_history['value_losses'].append(value_loss)
            
            # Save best model
            current_performance = sharpe_ratio - max_drawdown
            if current_performance > best_performance:
                best_performance = current_performance
                torch.save(agent.state_dict(), 'best_trading_agent.pth')
            
            # Print progress
            if episode % 100 == 0:
                avg_return = np.mean(self.training_history['total_returns'][-100:])
                avg_sharpe = np.mean(self.training_history['sharpe_ratios'][-100:])
                avg_drawdown = np.mean(self.training_history['max_drawdowns'][-100:])
                
                print(f"Episode {episode}: Return={total_return:.4f}, "
                      f"Avg Return={avg_return:.4f}, Sharpe={sharpe_ratio:.4f}, "
                      f"Avg Sharpe={avg_sharpe:.4f}, Max DD={max_drawdown:.4f}")
        
        return self.training_history
    
    def _collect_trajectory(self, env, agent):
        """Collect trajectory data from environment"""
        
        states, actions, rewards, values, log_probs = [], [], [], [], []
        
        state, _ = env.reset()
        done = False
        
        while not done:
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                action_dist, value = agent(state_tensor)
                
            # Sample action
            action_mean = action_dist.squeeze(0)
            action_std = torch.ones_like(action_mean) * 0.1  # Fixed std for simplicity
            dist = torch.distributions.Normal(action_mean, action_std)
            action = dist.sample()
            action = torch.clamp(action, -1, 1)  # Ensure valid action range
            
            log_prob = dist.log_prob(action).sum()
            
            # Execute action
            next_state, reward, terminated, truncated, _ = env.step(action.cpu().numpy())
            done = terminated or truncated
            
            # Store trajectory data
            states.append(state)
            actions.append(action.cpu().numpy())
            rewards.append(reward)
            values.append(value.item())
            log_probs.append(log_prob.item())
            
            state = next_state
        
        return states, actions, rewards, values, log_probs
    
    def _calculate_advantages(self, rewards, values):
        """Calculate advantages using Generalized Advantage Estimation"""
        
        advantages = []
        returns = []
        
        gae = 0
        for i in reversed(range(len(rewards))):
            if i == len(rewards) - 1:
                next_value = 0
            else:
                next_value = values[i + 1]
            
            delta = rewards[i] + self.gamma * next_value - values[i]
            gae = delta + self.gamma * self.gae_lambda * gae
            advantages.insert(0, gae)
            returns.insert(0, gae + values[i])
        
        # Normalize advantages
        advantages = np.array(advantages)
        advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-8)
        
        return advantages, returns
    
    def _update_policy(self, agent, optimizer, states, actions, old_log_probs, advantages, returns):
        """Update policy using PPO algorithm"""
        
        states_tensor = torch.FloatTensor(np.array(states)).to(self.device)
        actions_tensor = torch.FloatTensor(np.array(actions)).to(self.device)
        old_log_probs_tensor = torch.FloatTensor(old_log_probs).to(self.device)
        advantages_tensor = torch.FloatTensor(advantages).to(self.device)
        returns_tensor = torch.FloatTensor(returns).to(self.device)
        
        # Forward pass
        action_dist, values = agent(states_tensor)
        
        # Calculate new log probabilities
        action_std = torch.ones_like(action_dist) * 0.1
        dist = torch.distributions.Normal(action_dist, action_std)
        new_log_probs = dist.log_prob(actions_tensor).sum(dim=1)
        
        # Calculate ratio
        ratio = torch.exp(new_log_probs - old_log_probs_tensor)
        
        # Calculate surrogate losses
        surr1 = ratio * advantages_tensor
        surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages_tensor
        policy_loss = -torch.min(surr1, surr2).mean()
        
        # Value loss
        value_loss = F.mse_loss(values.squeeze(), returns_tensor)
        
        # Entropy bonus
        entropy = dist.entropy().mean()
        
        # Total loss
        total_loss = policy_loss + self.value_weight * value_loss - self.entropy_weight * entropy
        
        # Update
        optimizer.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(agent.parameters(), max_norm=0.5)
        optimizer.step()
        
        return policy_loss.item(), value_loss.item()
    
    def _calculate_trade_pnl(self, trade, data):
        """Calculate PnL for a single trade"""
        # Simplified PnL calculation - would need more sophisticated logic for real implementation
        return 0.0
    
    def evaluate_agent(self, test_data: pd.DataFrame, model_path: str = 'best_trading_agent.pth') -> Dict[str, Any]:
        """Evaluate trained agent on test data"""
        
        # Create test environment
        env = TradingEnvironment(test_data, self.config)
        
        # Load trained agent
        observation_size = env.observation_space.shape[0]
        agent = AdvancedTradingAgent(observation_size).to(self.device)
        agent.load_state_dict(torch.load(model_path, map_location=self.device))
        agent.eval()
        
        # Run evaluation
        state, _ = env.reset()
        done = False
        
        while not done:
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                action_dist, _ = agent(state_tensor)
                action = action_dist.squeeze(0).cpu().numpy()
            
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            state = next_state
        
        # Calculate performance metrics
        final_value = env.portfolio_values[-1]
        total_return = (final_value - self.config.initial_balance) / self.config.initial_balance
        
        # Sharpe ratio
        if len(env.daily_returns) > 1:
            excess_returns = np.array(env.daily_returns) - self.config.risk_free_rate / 252
            sharpe_ratio = np.mean(excess_returns) / (np.std(excess_returns) + 1e-8) * np.sqrt(252)
        else:
            sharpe_ratio = 0.0
        
        # Sortino ratio
        downside_returns = [r for r in env.daily_returns if r < 0]
        if downside_returns:
            sortino_ratio = np.mean(env.daily_returns) / (np.std(downside_returns) + 1e-8) * np.sqrt(252)
        else:
            sortino_ratio = float('inf')
        
        # Maximum drawdown
        max_drawdown = max(env.drawdowns)
        
        # Win rate
        positive_trades = sum(1 for trade in env.trades if self._calculate_trade_pnl(trade, env.data) > 0)
        win_rate = positive_trades / len(env.trades) if env.trades else 0.0
        
        # Benchmark comparison (buy and hold)
        benchmark_return = (test_data['Close'].iloc[-1] - test_data['Close'].iloc[0]) / test_data['Close'].iloc[0]
        
        return {
            'total_return': total_return,
            'sharpe_ratio': sharpe_ratio,
            'sortino_ratio': sortino_ratio,
            'max_drawdown': max_drawdown,
            'win_rate': win_rate,
            'total_trades': len(env.trades),
            'final_portfolio_value': final_value,
            'benchmark_return': benchmark_return,
            'alpha': total_return - benchmark_return,
            'portfolio_values': env.portfolio_values,
            'trades': env.trades,
            'daily_returns': env.daily_returns
        }
    
    def plot_performance(self, evaluation_results: Dict[str, Any]):
        """Plot comprehensive performance analysis"""
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        
        # Portfolio value over time
        ax1.plot(evaluation_results['portfolio_values'], label='RL Agent')
        ax1.axhline(y=self.config.initial_balance, color='r', linestyle='--', label='Initial Capital')
        ax1.set_title('Portfolio Value Over Time')
        ax1.set_xlabel('Days')
        ax1.set_ylabel('Portfolio Value ($)')
        ax1.legend()
        ax1.grid(True)
        
        # Daily returns distribution
        ax2.hist(evaluation_results['daily_returns'], bins=50, alpha=0.7, edgecolor='black')
        ax2.axvline(x=np.mean(evaluation_results['daily_returns']), color='r', linestyle='--', label=f'Mean: {np.mean(evaluation_results["daily_returns"]):.4f}')
        ax2.set_title('Daily Returns Distribution')
        ax2.set_xlabel('Daily Return')
        ax2.set_ylabel('Frequency')
        ax2.legend()
        ax2.grid(True)
        
        # Drawdown over time
        portfolio_values = np.array(evaluation_results['portfolio_values'])
        peak_values = np.maximum.accumulate(portfolio_values)
        drawdowns = (peak_values - portfolio_values) / peak_values
        
        ax3.fill_between(range(len(drawdowns)), drawdowns, alpha=0.3, color='red')
        ax3.plot(drawdowns, color='red')
        ax3.set_title('Drawdown Over Time')
        ax3.set_xlabel('Days')
        ax3.set_ylabel('Drawdown (%)')
        ax3.grid(True)
        
        # Performance metrics
        metrics = {
            'Total Return': f"{evaluation_results['total_return']:.2%}",
            'Sharpe Ratio': f"{evaluation_results['sharpe_ratio']:.3f}",
            'Sortino Ratio': f"{evaluation_results['sortino_ratio']:.3f}",
            'Max Drawdown': f"{evaluation_results['max_drawdown']:.2%}",
            'Win Rate': f"{evaluation_results['win_rate']:.2%}",
            'Total Trades': f"{evaluation_results['total_trades']}",
            'Alpha vs Benchmark': f"{evaluation_results['alpha']:.2%}"
        }
        
        ax4.axis('off')
        y_pos = 0.9
        for metric, value in metrics.items():
            ax4.text(0.1, y_pos, f"{metric}: {value}", fontsize=12, transform=ax4.transAxes)
            y_pos -= 0.12
        
        ax4.set_title('Performance Metrics', fontsize=14, weight='bold')
        
        plt.tight_layout()
        plt.savefig('trading_performance_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()

# Usage example
async def main():
    """Example usage of trading RL bot"""
    
    # Configuration
    config = TradingConfig(
        initial_balance=100000.0,
        max_position_size=0.5,
        transaction_cost=0.001,
        lookback_window=30,
        max_trades_per_day=5,
        risk_free_rate=0.02
    )
    
    # Create trainer
    trainer = TradingRLTrainer(config)
    
    # Prepare data
    symbol = "AAPL"
    start_date = "2020-01-01"
    end_date = "2024-01-01"
    
    print("Preparing market data...")
    data = trainer.prepare_data(symbol, start_date, end_date)
    train_data, test_data = trainer.create_train_test_split(data, test_ratio=0.3)
    
    print(f"Training data: {len(train_data)} days")
    print(f"Test data: {len(test_data)} days")
    
    # Train agent
    print("\nTraining RL trading agent...")
    training_history = trainer.train_agent(train_data, episodes=500)
    
    # Evaluate agent
    print("\nEvaluating trained agent...")
    evaluation_results = trainer.evaluate_agent(test_data)
    
    print("\nPerformance Results:")
    print(f"Total Return: {evaluation_results['total_return']:.2%}")
    print(f"Sharpe Ratio: {evaluation_results['sharpe_ratio']:.3f}")
    print(f"Max Drawdown: {evaluation_results['max_drawdown']:.2%}")
    print(f"Win Rate: {evaluation_results['win_rate']:.2%}")
    print(f"Alpha vs Benchmark: {evaluation_results['alpha']:.2%}")
    
    # Plot results
    trainer.plot_performance(evaluation_results)

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
````

## Conclusion

The development of a financial trading bot using reinforcement learning represents the intersection of advanced AI techniques with practical financial applications. This implementation demonstrates the complexity required to create robust trading systems that can navigate real-world market conditions while managing risk and transaction costs.

**Reward Engineering** emerges as the most critical component, requiring careful balance between profitability, risk management, and transaction efficiency. The multi-faceted reward function incorporating risk-adjusted returns, drawdown penalties, and transaction costs creates incentives for sustainable trading behaviors rather than short-term optimization.

**Environment Design** showcases the importance of realistic market simulation, including transaction costs, market impact, position limits, and regime detection. The sophisticated observation space combining price history, technical indicators, portfolio state, and market regime information provides the agent with comprehensive market context.

**Neural Network Architecture** employs modern deep learning techniques including layer normalization, dropout regularization, and separate heads for different decision types. The actor-critic approach enables stable learning while the multi-output design allows for nuanced trading decisions regarding both position direction and sizing.

**Risk Management Integration** throughout the system ensures that the agent operates within acceptable risk parameters. Maximum drawdown limits, position size constraints, and daily trade limits prevent catastrophic losses while encouraging disciplined trading behavior.

**Performance Evaluation** encompasses multiple metrics beyond simple returns, including Sharpe ratio, Sortino ratio, maximum drawdown, and win rate. The comparison with benchmark strategies provides context for evaluating the agent's value-added performance.

**Market Regime Awareness** enables the agent to adapt its strategy to different market conditions, recognizing that optimal trading behavior varies between bull markets, bear markets, and sideways trends. This adaptability is crucial for long-term performance stability.

**Technical Challenges** include handling non-stationary market data, managing exploration-exploitation trade-offs in live markets, and ensuring robustness against market anomalies. The implementation addresses these through sophisticated preprocessing, adaptive learning rates, and comprehensive backtesting frameworks.

**Real-World Considerations** encompass regulatory compliance, data quality management, execution infrastructure, and continuous monitoring systems. While this implementation provides a strong foundation, production deployment requires additional considerations around market access, order management, and regulatory reporting.

The field's future development points toward more sophisticated reward functions, better handling of market microstructure effects, integration of alternative data sources, and ensemble methods combining multiple specialized agents. The growing importance of ESG factors and regulatory constraints will also influence algorithm design and deployment strategies.