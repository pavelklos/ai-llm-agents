<small>Claude Sonnet 4</small>
# 03. Vector Databases and Their Applications

## Key Terms

**Vector Database**: A specialized database system designed to store, index, and query high-dimensional vector embeddings that represent semantic relationships between data objects, enabling efficient similarity search and retrieval operations based on mathematical distance calculations rather than exact keyword matching.

**Embedding Model**: A neural network architecture that transforms unstructured data (text, images, audio) into dense numerical vector representations capturing semantic meaning and contextual relationships, allowing machines to understand and compare conceptual similarity between different data elements.

**Similarity Search**: A computational process that identifies semantically related content by calculating mathematical distances (cosine, euclidean, dot product) between vector embeddings in high-dimensional space, enabling retrieval of relevant information based on meaning rather than literal text matching.

**RAG (Retrieval-Augmented Generation)**: An advanced AI architecture pattern that combines vector-based information retrieval with large language model generation capabilities, enabling AI systems to access external knowledge bases and provide factually grounded responses with improved accuracy and reduced hallucination.

**Semantic Chunking**: An intelligent text segmentation strategy that divides documents into meaningful, contextually coherent segments while preserving semantic relationships and ensuring optimal embedding representation for accurate retrieval and generation tasks.

**Vector Index**: A sophisticated data structure (HNSW, IVF, LSH) that organizes high-dimensional vectors for efficient approximate nearest neighbor search, enabling sub-linear query performance even with millions of embedded documents or data points.

**Contextual Retrieval**: An advanced technique that enhances basic similarity search by incorporating conversation history, user intent, and query context to retrieve the most relevant information for specific user questions and scenarios.

## Comprehensive Vector Database and RAG Implementation Framework

Vector databases represent a paradigm shift in information storage and retrieval, enabling AI systems to understand and process unstructured data through semantic similarity rather than traditional keyword-based approaches, forming the foundation for intelligent document analysis and contextual question-answering systems.

### Advanced Vector Database and RAG Architecture

````python
import asyncio
import json
import logging
import os
import time
import warnings
from typing import Dict, List, Any, Optional, Union, Callable, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timezone
from pathlib import Path
import uuid
import re
from enum import Enum
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import pickle
import hashlib

# OpenAI and embedding libraries
import openai
from openai import OpenAI, AsyncOpenAI
import tiktoken

# Vector database libraries
import chromadb
from chromadb.config import Settings
import pinecone
import weaviate
import qdrant_client
from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, Range

# LangChain components for RAG
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma, Pinecone, Weaviate, Qdrant
from langchain.text_splitter import RecursiveCharacterTextSplitter, TokenTextSplitter
from langchain.document_loaders import TextLoader, PyPDFLoader, UnstructuredFileLoader
from langchain.schema import Document, BaseRetriever
from langchain.vectorstores.base import VectorStore
from langchain.retrievers import ContextualCompressionRetriever, EnsembleRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor, EmbeddingsFilter
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.retrievers.self_query.base import SelfQueryRetriever

# Advanced RAG components
from langchain.chains import RetrievalQA, ConversationalRetrievalChain
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate, ChatPromptTemplate
from langchain.memory import ConversationBufferMemory, ConversationSummaryBufferMemory
from langchain.callbacks import get_openai_callback

# Data processing and analysis
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

# Document processing
import PyPDF2
from docx import Document as DocxDocument
import markdown
from bs4 import BeautifulSoup
import chardet

# Performance monitoring
import structlog
from prometheus_client import Counter, Histogram, Gauge
import psutil
import memory_profiler

# Utilities
from dotenv import load_dotenv
import requests
from urllib.parse import urlparse
import validators
from collections import defaultdict, deque
import heapq

load_dotenv()

warnings.filterwarnings("ignore", category=DeprecationWarning)

# Setup structured logging
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    wrapper_class=structlog.stdlib.BoundLogger,
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()

# Metrics
vector_operations = Counter('vector_operations_total', 'Total vector operations', ['operation_type', 'status'])
retrieval_time = Histogram('retrieval_time_seconds', 'Time spent on retrieval operations')
embedding_time = Histogram('embedding_time_seconds', 'Time spent on embedding generation')
active_queries = Gauge('active_queries_count', 'Number of active queries')

class VectorDatabaseType(Enum):
    """Supported vector database types"""
    CHROMADB = "chromadb"
    PINECONE = "pinecone"
    WEAVIATE = "weaviate"
    QDRANT = "qdrant"
    FAISS = "faiss"

class EmbeddingProvider(Enum):
    """Supported embedding providers"""
    OPENAI = "openai"
    HUGGINGFACE = "huggingface"
    SENTENCE_TRANSFORMERS = "sentence_transformers"
    COHERE = "cohere"

class ChunkingStrategy(Enum):
    """Document chunking strategies"""
    RECURSIVE_CHARACTER = "recursive_character"
    TOKEN_BASED = "token_based"
    SEMANTIC_SIMILARITY = "semantic_similarity"
    SLIDING_WINDOW = "sliding_window"
    PARAGRAPH_BASED = "paragraph_based"

class RetrievalStrategy(Enum):
    """Information retrieval strategies"""
    SIMILARITY_SEARCH = "similarity_search"
    MAXIMAL_MARGINAL_RELEVANCE = "mmr"
    MULTI_QUERY = "multi_query"
    CONTEXTUAL_COMPRESSION = "contextual_compression"
    ENSEMBLE_RETRIEVAL = "ensemble"
    SELF_QUERY = "self_query"

@dataclass
class VectorDatabaseConfig:
    """Configuration for vector database setup"""
    database_type: VectorDatabaseType
    embedding_provider: EmbeddingProvider
    embedding_model: str
    collection_name: str
    dimension: int
    distance_metric: str = "cosine"
    index_type: str = "hnsw"
    connection_params: Dict[str, Any] = field(default_factory=dict)
    performance_params: Dict[str, Any] = field(default_factory=dict)

@dataclass
class ChunkingConfig:
    """Configuration for document chunking"""
    strategy: ChunkingStrategy
    chunk_size: int = 1000
    chunk_overlap: int = 200
    separators: List[str] = field(default_factory=lambda: ["\n\n", "\n", " ", ""])
    semantic_threshold: float = 0.8
    metadata_fields: List[str] = field(default_factory=list)

@dataclass
class RetrievalConfig:
    """Configuration for information retrieval"""
    strategy: RetrievalStrategy
    k: int = 5
    score_threshold: float = 0.7
    fetch_k: int = 20
    lambda_mult: float = 0.5
    use_reranking: bool = True
    context_window: int = 4000

@dataclass
class DocumentMetadata:
    """Metadata for processed documents"""
    document_id: str
    source: str
    title: str
    author: Optional[str] = None
    created_date: Optional[datetime] = None
    document_type: str = "text"
    language: str = "en"
    tags: List[str] = field(default_factory=list)
    custom_fields: Dict[str, Any] = field(default_factory=dict)

@dataclass
class RetrievalResult:
    """Result from vector retrieval operation"""
    documents: List[Document]
    scores: List[float]
    query: str
    retrieval_time: float
    total_documents_searched: int
    metadata: Dict[str, Any] = field(default_factory=dict)

class AdvancedDocumentProcessor:
    """Advanced document processing with multiple format support"""
    
    def __init__(self, chunking_config: ChunkingConfig):
        self.chunking_config = chunking_config
        self.supported_formats = {
            '.txt': self._process_text,
            '.pdf': self._process_pdf,
            '.docx': self._process_docx,
            '.md': self._process_markdown,
            '.html': self._process_html,
            '.json': self._process_json
        }
        
        # Initialize text splitters
        self.recursive_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunking_config.chunk_size,
            chunk_overlap=chunking_config.chunk_overlap,
            separators=chunking_config.separators
        )
        
        self.token_splitter = TokenTextSplitter(
            chunk_size=chunking_config.chunk_size,
            chunk_overlap=chunking_config.chunk_overlap
        )
    
    async def process_documents(self, file_paths: List[str], 
                              metadata_list: Optional[List[DocumentMetadata]] = None) -> List[Document]:
        """Process multiple documents with parallel processing"""
        
        if metadata_list and len(metadata_list) != len(file_paths):
            raise ValueError("Metadata list length must match file paths length")
        
        tasks = []
        for i, file_path in enumerate(file_paths):
            metadata = metadata_list[i] if metadata_list else None
            tasks.append(self._process_single_document(file_path, metadata))
        
        # Process documents in parallel
        with ThreadPoolExecutor(max_workers=4) as executor:
            future_to_path = {
                executor.submit(self._process_document_sync, path, meta): path 
                for path, meta in zip(file_paths, metadata_list or [None] * len(file_paths))
            }
            
            all_documents = []
            for future in as_completed(future_to_path):
                file_path = future_to_path[future]
                try:
                    documents = future.result()
                    all_documents.extend(documents)
                    logger.info(f"Processed {file_path}: {len(documents)} chunks")
                except Exception as e:
                    logger.error(f"Error processing {file_path}: {e}")
        
        return all_documents
    
    def _process_document_sync(self, file_path: str, metadata: Optional[DocumentMetadata] = None) -> List[Document]:
        """Synchronous document processing for thread execution"""
        
        file_path = Path(file_path)
        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")
        
        # Detect file encoding
        with open(file_path, 'rb') as f:
            raw_data = f.read()
            encoding = chardet.detect(raw_data)['encoding']
        
        # Get file extension
        ext = file_path.suffix.lower()
        
        if ext not in self.supported_formats:
            raise ValueError(f"Unsupported file format: {ext}")
        
        # Process document based on format
        try:
            content = self.supported_formats[ext](file_path, encoding)
            
            # Create base metadata
            doc_metadata = {
                "source": str(file_path),
                "file_name": file_path.name,
                "file_extension": ext,
                "file_size": file_path.stat().st_size,
                "processed_date": datetime.now(timezone.utc).isoformat()
            }
            
            # Add custom metadata if provided
            if metadata:
                doc_metadata.update({
                    "document_id": metadata.document_id,
                    "title": metadata.title,
                    "author": metadata.author,
                    "created_date": metadata.created_date.isoformat() if metadata.created_date else None,
                    "document_type": metadata.document_type,
                    "language": metadata.language,
                    "tags": metadata.tags,
                    **metadata.custom_fields
                })
            
            # Chunk the document
            chunks = self._chunk_content(content, doc_metadata)
            
            return chunks
            
        except Exception as e:
            logger.error(f"Error processing {file_path}: {e}")
            return []
    
    def _process_text(self, file_path: Path, encoding: str) -> str:
        """Process plain text files"""
        with open(file_path, 'r', encoding=encoding) as f:
            return f.read()
    
    def _process_pdf(self, file_path: Path, encoding: str) -> str:
        """Process PDF files"""
        try:
            with open(file_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                text = ""
                for page in reader.pages:
                    text += page.extract_text() + "\n"
                return text
        except Exception as e:
            logger.error(f"Error processing PDF {file_path}: {e}")
            return ""
    
    def _process_docx(self, file_path: Path, encoding: str) -> str:
        """Process DOCX files"""
        try:
            doc = DocxDocument(file_path)
            text = ""
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            return text
        except Exception as e:
            logger.error(f"Error processing DOCX {file_path}: {e}")
            return ""
    
    def _process_markdown(self, file_path: Path, encoding: str) -> str:
        """Process Markdown files"""
        with open(file_path, 'r', encoding=encoding) as f:
            md_content = f.read()
            # Convert to HTML and then extract text
            html = markdown.markdown(md_content)
            soup = BeautifulSoup(html, 'html.parser')
            return soup.get_text()
    
    def _process_html(self, file_path: Path, encoding: str) -> str:
        """Process HTML files"""
        with open(file_path, 'r', encoding=encoding) as f:
            soup = BeautifulSoup(f.read(), 'html.parser')
            return soup.get_text()
    
    def _process_json(self, file_path: Path, encoding: str) -> str:
        """Process JSON files"""
        with open(file_path, 'r', encoding=encoding) as f:
            data = json.load(f)
            return json.dumps(data, indent=2)
    
    def _chunk_content(self, content: str, metadata: Dict[str, Any]) -> List[Document]:
        """Chunk content based on configured strategy"""
        
        if self.chunking_config.strategy == ChunkingStrategy.RECURSIVE_CHARACTER:
            chunks = self.recursive_splitter.split_text(content)
        elif self.chunking_config.strategy == ChunkingStrategy.TOKEN_BASED:
            chunks = self.token_splitter.split_text(content)
        elif self.chunking_config.strategy == ChunkingStrategy.SEMANTIC_SIMILARITY:
            chunks = self._semantic_chunking(content)
        elif self.chunking_config.strategy == ChunkingStrategy.SLIDING_WINDOW:
            chunks = self._sliding_window_chunking(content)
        elif self.chunking_config.strategy == ChunkingStrategy.PARAGRAPH_BASED:
            chunks = self._paragraph_chunking(content)
        else:
            chunks = self.recursive_splitter.split_text(content)
        
        # Create Document objects with metadata
        documents = []
        for i, chunk in enumerate(chunks):
            chunk_metadata = metadata.copy()
            chunk_metadata.update({
                "chunk_id": f"{metadata.get('document_id', 'doc')}_{i}",
                "chunk_index": i,
                "chunk_size": len(chunk),
                "total_chunks": len(chunks)
            })
            
            documents.append(Document(
                page_content=chunk,
                metadata=chunk_metadata
            ))
        
        return documents
    
    def _semantic_chunking(self, content: str) -> List[str]:
        """Perform semantic-based chunking using sentence similarity"""
        
        # Split into sentences
        sentences = re.split(r'[.!?]+', content)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if len(sentences) <= 1:
            return [content]
        
        # Simple sentence grouping based on length
        chunks = []
        current_chunk = ""
        current_length = 0
        
        for sentence in sentences:
            sentence_length = len(sentence)
            
            if current_length + sentence_length > self.chunking_config.chunk_size and current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = sentence
                current_length = sentence_length
            else:
                current_chunk += " " + sentence if current_chunk else sentence
                current_length += sentence_length
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def _sliding_window_chunking(self, content: str) -> List[str]:
        """Perform sliding window chunking"""
        
        words = content.split()
        chunks = []
        
        if len(words) <= self.chunking_config.chunk_size:
            return [content]
        
        step_size = self.chunking_config.chunk_size - self.chunking_config.chunk_overlap
        
        for i in range(0, len(words), step_size):
            chunk_words = words[i:i + self.chunking_config.chunk_size]
            chunk = " ".join(chunk_words)
            chunks.append(chunk)
            
            if i + self.chunking_config.chunk_size >= len(words):
                break
        
        return chunks
    
    def _paragraph_chunking(self, content: str) -> List[str]:
        """Perform paragraph-based chunking"""
        
        paragraphs = content.split('\n\n')
        chunks = []
        current_chunk = ""
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue
            
            if len(current_chunk) + len(paragraph) > self.chunking_config.chunk_size and current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = paragraph
            else:
                current_chunk += "\n\n" + paragraph if current_chunk else paragraph
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks

class UniversalVectorStore:
    """Universal vector store supporting multiple vector database backends"""
    
    def __init__(self, config: VectorDatabaseConfig):
        self.config = config
        self.embeddings = self._initialize_embeddings()
        self.vector_store = None
        self.client = None
        
        # Performance tracking
        self.operation_stats = {
            "inserts": 0,
            "queries": 0,
            "updates": 0,
            "deletes": 0
        }
        
        self._initialize_vector_store()
    
    def _initialize_embeddings(self):
        """Initialize embedding model based on provider"""
        
        if self.config.embedding_provider == EmbeddingProvider.OPENAI:
            return OpenAIEmbeddings(
                model=self.config.embedding_model,
                openai_api_key=os.getenv('OPENAI_API_KEY')
            )
        else:
            raise ValueError(f"Unsupported embedding provider: {self.config.embedding_provider}")
    
    def _initialize_vector_store(self):
        """Initialize vector store based on database type"""
        
        if self.config.database_type == VectorDatabaseType.CHROMADB:
            self._initialize_chromadb()
        elif self.config.database_type == VectorDatabaseType.PINECONE:
            self._initialize_pinecone()
        elif self.config.database_type == VectorDatabaseType.QDRANT:
            self._initialize_qdrant()
        else:
            raise ValueError(f"Unsupported vector database: {self.config.database_type}")
    
    def _initialize_chromadb(self):
        """Initialize ChromaDB"""
        
        settings = Settings(
            chroma_db_impl="duckdb+parquet",
            persist_directory=self.config.connection_params.get("persist_directory", "./chroma_db")
        )
        
        self.client = chromadb.Client(settings)
        
        try:
            collection = self.client.get_collection(self.config.collection_name)
        except:
            collection = self.client.create_collection(
                name=self.config.collection_name,
                metadata={"hnsw:space": self.config.distance_metric}
            )
        
        self.vector_store = Chroma(
            client=self.client,
            collection_name=self.config.collection_name,
            embedding_function=self.embeddings
        )
    
    def _initialize_pinecone(self):
        """Initialize Pinecone"""
        
        import pinecone
        
        pinecone.init(
            api_key=os.getenv('PINECONE_API_KEY'),
            environment=self.config.connection_params.get("environment", "us-west1-gcp")
        )
        
        # Create index if it doesn't exist
        if self.config.collection_name not in pinecone.list_indexes():
            pinecone.create_index(
                name=self.config.collection_name,
                dimension=self.config.dimension,
                metric=self.config.distance_metric
            )
        
        self.vector_store = Pinecone.from_existing_index(
            self.config.collection_name,
            self.embeddings
        )
    
    def _initialize_qdrant(self):
        """Initialize Qdrant"""
        
        self.client = qdrant_client.QdrantClient(
            host=self.config.connection_params.get("host", "localhost"),
            port=self.config.connection_params.get("port", 6333),
            api_key=self.config.connection_params.get("api_key")
        )
        
        # Create collection if it doesn't exist
        try:
            self.client.get_collection(self.config.collection_name)
        except:
            self.client.create_collection(
                collection_name=self.config.collection_name,
                vectors_config=VectorParams(
                    size=self.config.dimension,
                    distance=Distance.COSINE if self.config.distance_metric == "cosine" else Distance.EUCLIDEAN
                )
            )
        
        self.vector_store = Qdrant(
            client=self.client,
            collection_name=self.config.collection_name,
            embeddings=self.embeddings
        )
    
    async def add_documents(self, documents: List[Document]) -> List[str]:
        """Add documents to vector store"""
        
        start_time = time.time()
        
        try:
            # Generate embeddings and add to vector store
            document_ids = self.vector_store.add_documents(documents)
            
            # Update statistics
            self.operation_stats["inserts"] += len(documents)
            
            # Log metrics
            embedding_time.observe(time.time() - start_time)
            vector_operations.labels(operation_type="insert", status="success").inc()
            
            logger.info(f"Added {len(documents)} documents to vector store")
            
            return document_ids
            
        except Exception as e:
            vector_operations.labels(operation_type="insert", status="error").inc()
            logger.error(f"Error adding documents: {e}")
            raise
    
    async def similarity_search(self, query: str, k: int = 5, 
                              filter_dict: Optional[Dict] = None,
                              score_threshold: Optional[float] = None) -> RetrievalResult:
        """Perform similarity search"""
        
        start_time = time.time()
        active_queries.inc()
        
        try:
            # Perform similarity search with scores
            docs_and_scores = self.vector_store.similarity_search_with_score(
                query=query,
                k=k,
                filter=filter_dict
            )
            
            # Filter by score threshold if provided
            if score_threshold:
                docs_and_scores = [
                    (doc, score) for doc, score in docs_and_scores 
                    if score >= score_threshold
                ]
            
            # Separate documents and scores
            documents = [doc for doc, score in docs_and_scores]
            scores = [score for doc, score in docs_and_scores]
            
            # Update statistics
            self.operation_stats["queries"] += 1
            retrieval_time.observe(time.time() - start_time)
            vector_operations.labels(operation_type="query", status="success").inc()
            
            result = RetrievalResult(
                documents=documents,
                scores=scores,
                query=query,
                retrieval_time=time.time() - start_time,
                total_documents_searched=self._get_total_documents(),
                metadata={
                    "k": k,
                    "filter": filter_dict,
                    "score_threshold": score_threshold
                }
            )
            
            logger.info(f"Retrieved {len(documents)} documents for query")
            
            return result
            
        except Exception as e:
            vector_operations.labels(operation_type="query", status="error").inc()
            logger.error(f"Error performing similarity search: {e}")
            raise
        finally:
            active_queries.dec()
    
    def _get_total_documents(self) -> int:
        """Get total number of documents in the vector store"""
        
        try:
            if self.config.database_type == VectorDatabaseType.CHROMADB:
                return self.client.get_collection(self.config.collection_name).count()
            elif self.config.database_type == VectorDatabaseType.QDRANT:
                info = self.client.get_collection(self.config.collection_name)
                return info.points_count
            else:
                return 0  # Default for unsupported databases
        except:
            return 0
    
    def get_statistics(self) -> Dict[str, Any]:
        """Get vector store statistics"""
        
        return {
            "database_type": self.config.database_type.value,
            "collection_name": self.config.collection_name,
            "total_documents": self._get_total_documents(),
            "operation_stats": self.operation_stats,
            "embedding_model": self.config.embedding_model,
            "dimension": self.config.dimension,
            "distance_metric": self.config.distance_metric
        }

class AdvancedRAGSystem:
    """Advanced Retrieval-Augmented Generation system with multiple strategies"""
    
    def __init__(self, vector_store: UniversalVectorStore, 
                 retrieval_config: RetrievalConfig,
                 model_name: str = "gpt-4"):
        
        self.vector_store = vector_store
        self.retrieval_config = retrieval_config
        self.llm = ChatOpenAI(
            model_name=model_name,
            temperature=0.1,
            openai_api_key=os.getenv('OPENAI_API_KEY')
        )
        
        # Initialize retrievers
        self.retrievers = self._initialize_retrievers()
        
        # Initialize memory for conversational RAG
        self.memory = ConversationSummaryBufferMemory(
            llm=self.llm,
            max_token_limit=2000,
            return_messages=True,
            memory_key="chat_history"
        )
        
        # Performance tracking
        self.query_stats = {
            "total_queries": 0,
            "successful_queries": 0,
            "average_response_time": 0.0,
            "average_relevance_score": 0.0
        }
    
    def _initialize_retrievers(self) -> Dict[str, BaseRetriever]:
        """Initialize different retrieval strategies"""
        
        retrievers = {}
        
        # Basic similarity search retriever
        retrievers["similarity"] = self.vector_store.vector_store.as_retriever(
            search_type="similarity",
            search_kwargs={
                "k": self.retrieval_config.k,
                "score_threshold": self.retrieval_config.score_threshold
            }
        )
        
        # MMR retriever for diversity
        retrievers["mmr"] = self.vector_store.vector_store.as_retriever(
            search_type="mmr",
            search_kwargs={
                "k": self.retrieval_config.k,
                "fetch_k": self.retrieval_config.fetch_k,
                "lambda_mult": self.retrieval_config.lambda_mult
            }
        )
        
        # Multi-query retriever for query expansion
        retrievers["multi_query"] = MultiQueryRetriever.from_llm(
            retriever=retrievers["similarity"],
            llm=self.llm
        )
        
        # Contextual compression retriever
        compressor = LLMChainExtractor.from_llm(self.llm)
        retrievers["contextual_compression"] = ContextualCompressionRetriever(
            base_compressor=compressor,
            base_retriever=retrievers["similarity"]
        )
        
        return retrievers
    
    async def query(self, question: str, 
                   retrieval_strategy: Optional[RetrievalStrategy] = None,
                   conversation_id: Optional[str] = None,
                   custom_prompt: Optional[str] = None) -> Dict[str, Any]:
        """Query the RAG system with advanced retrieval and generation"""
        
        start_time = time.time()
        self.query_stats["total_queries"] += 1
        
        try:
            # Select retrieval strategy
            strategy = retrieval_strategy or self.retrieval_config.strategy
            retriever = self._get_retriever_for_strategy(strategy)
            
            # Retrieve relevant documents
            relevant_docs = await self._retrieve_documents(question, retriever)
            
            # Generate response using retrieved context
            response = await self._generate_response(
                question, relevant_docs, custom_prompt
            )
            
            # Calculate metrics
            response_time = time.time() - start_time
            relevance_score = self._calculate_relevance_score(question, relevant_docs)
            
            # Update statistics
            self.query_stats["successful_queries"] += 1
            self._update_performance_stats(response_time, relevance_score)
            
            # Prepare result
            result = {
                "question": question,
                "answer": response["answer"],
                "source_documents": [
                    {
                        "content": doc.page_content,
                        "metadata": doc.metadata,
                        "relevance_score": score
                    }
                    for doc, score in zip(relevant_docs["documents"], relevant_docs.get("scores", []))
                ],
                "retrieval_strategy": strategy.value,
                "response_time": response_time,
                "relevance_score": relevance_score,
                "token_usage": response.get("token_usage", {}),
                "conversation_id": conversation_id,
                "metadata": {
                    "total_documents_searched": relevant_docs.get("total_documents_searched", 0),
                    "documents_retrieved": len(relevant_docs["documents"]),
                    "model_used": self.llm.model_name
                }
            }
            
            logger.info(f"RAG query completed in {response_time:.2f}s with {len(relevant_docs['documents'])} documents")
            
            return result
            
        except Exception as e:
            logger.error(f"Error in RAG query: {e}")
            raise
    
    def _get_retriever_for_strategy(self, strategy: RetrievalStrategy) -> BaseRetriever:
        """Get retriever based on strategy"""
        
        strategy_mapping = {
            RetrievalStrategy.SIMILARITY_SEARCH: "similarity",
            RetrievalStrategy.MAXIMAL_MARGINAL_RELEVANCE: "mmr",
            RetrievalStrategy.MULTI_QUERY: "multi_query",
            RetrievalStrategy.CONTEXTUAL_COMPRESSION: "contextual_compression"
        }
        
        retriever_key = strategy_mapping.get(strategy, "similarity")
        return self.retrievers[retriever_key]
    
    async def _retrieve_documents(self, query: str, retriever: BaseRetriever) -> Dict[str, Any]:
        """Retrieve relevant documents using specified retriever"""
        
        try:
            # Get documents from retriever
            docs = retriever.get_relevant_documents(query)
            
            # If using vector store directly, get scores
            if hasattr(retriever, 'vectorstore'):
                docs_with_scores = retriever.vectorstore.similarity_search_with_score(
                    query, k=self.retrieval_config.k
                )
                documents = [doc for doc, score in docs_with_scores]
                scores = [score for doc, score in docs_with_scores]
            else:
                documents = docs
                scores = [1.0] * len(docs)  # Default scores
            
            return {
                "documents": documents,
                "scores": scores,
                "total_documents_searched": self.vector_store._get_total_documents()
            }
            
        except Exception as e:
            logger.error(f"Error retrieving documents: {e}")
            return {"documents": [], "scores": [], "total_documents_searched": 0}
    
    async def _generate_response(self, question: str, 
                               relevant_docs: Dict[str, Any],
                               custom_prompt: Optional[str] = None) -> Dict[str, Any]:
        """Generate response using LLM with retrieved context"""
        
        # Prepare context from retrieved documents
        context = self._prepare_context(relevant_docs["documents"])
        
        # Use custom prompt or default
        if custom_prompt:
            prompt_template = custom_prompt
        else:
            prompt_template = """
You are a helpful AI assistant that answers questions based on the provided context. 
Use the context information to provide accurate and relevant answers.

Context:
{context}

Question: {question}

Instructions:
1. Base your answer primarily on the provided context
2. If the context doesn't contain enough information, clearly state this
3. Provide specific references to the source documents when possible
4. Be concise but comprehensive in your response
5. If you're uncertain about any information, express this clearly

Answer:
"""
        
        # Create prompt
        prompt = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        # Generate response with token tracking
        with get_openai_callback() as cb:
            formatted_prompt = prompt.format(context=context, question=question)
            
            response = await self.llm.agenerate([[formatted_prompt]])
            answer = response.generations[0][0].text
            
            token_usage = {
                "prompt_tokens": cb.prompt_tokens,
                "completion_tokens": cb.completion_tokens,
                "total_tokens": cb.total_tokens,
                "total_cost": cb.total_cost
            }
        
        return {
            "answer": answer,
            "token_usage": token_usage,
            "context_used": context
        }
    
    def _prepare_context(self, documents: List[Document]) -> str:
        """Prepare context string from retrieved documents"""
        
        context_parts = []
        
        for i, doc in enumerate(documents):
            # Add document metadata for reference
            source = doc.metadata.get("source", "Unknown")
            title = doc.metadata.get("title", "Untitled")
            
            context_part = f"Document {i+1} (Source: {source}, Title: {title}):\n{doc.page_content}"
            context_parts.append(context_part)
        
        return "\n\n---\n\n".join(context_parts)
    
    def _calculate_relevance_score(self, question: str, relevant_docs: Dict[str, Any]) -> float:
        """Calculate average relevance score"""
        
        scores = relevant_docs.get("scores", [])
        if not scores:
            return 0.0
        
        return float(np.mean(scores))
    
    def _update_performance_stats(self, response_time: float, relevance_score: float):
        """Update performance statistics"""
        
        total_queries = self.query_stats["total_queries"]
        
        # Update average response time
        current_avg_time = self.query_stats["average_response_time"]
        new_avg_time = ((current_avg_time * (total_queries - 1)) + response_time) / total_queries
        self.query_stats["average_response_time"] = new_avg_time
        
        # Update average relevance score
        current_avg_relevance = self.query_stats["average_relevance_score"]
        new_avg_relevance = ((current_avg_relevance * (total_queries - 1)) + relevance_score) / total_queries
        self.query_stats["average_relevance_score"] = new_avg_relevance
    
    def get_performance_statistics(self) -> Dict[str, Any]:
        """Get RAG system performance statistics"""
        
        return {
            "query_statistics": self.query_stats,
            "vector_store_stats": self.vector_store.get_statistics(),
            "retrieval_config": {
                "strategy": self.retrieval_config.strategy.value,
                "k": self.retrieval_config.k,
                "score_threshold": self.retrieval_config.score_threshold
            },
            "model_info": {
                "llm_model": self.llm.model_name,
                "embedding_model": self.vector_store.config.embedding_model
            }
        }

# Demonstration and testing functions
async def comprehensive_vector_rag_demonstration():
    """Comprehensive demonstration of vector database and RAG capabilities"""
    
    logger.info("=== Comprehensive Vector Database and RAG Demonstration ===")
    
    # Configuration
    vector_config = VectorDatabaseConfig(
        database_type=VectorDatabaseType.CHROMADB,
        embedding_provider=EmbeddingProvider.OPENAI,
        embedding_model="text-embedding-ada-002",
        collection_name="demo_documents",
        dimension=1536,
        distance_metric="cosine",
        connection_params={"persist_directory": "./demo_chroma_db"}
    )
    
    chunking_config = ChunkingConfig(
        strategy=ChunkingStrategy.RECURSIVE_CHARACTER,
        chunk_size=800,
        chunk_overlap=100,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    
    retrieval_config = RetrievalConfig(
        strategy=RetrievalStrategy.SIMILARITY_SEARCH,
        k=5,
        score_threshold=0.7,
        fetch_k=20,
        lambda_mult=0.5,
        use_reranking=True
    )
    
    # Create sample documents for demonstration
    sample_documents = [
        {
            "content": """
# Artificial Intelligence and Machine Learning

Artificial Intelligence (AI) is a branch of computer science that aims to create intelligent machines 
that can perform tasks that typically require human intelligence. Machine Learning (ML) is a subset 
of AI that enables computers to learn and improve from experience without being explicitly programmed.

## Key Concepts in AI:
- Natural Language Processing (NLP)
- Computer Vision
- Robotics
- Expert Systems
- Neural Networks

## Machine Learning Types:
1. Supervised Learning: Uses labeled training data
2. Unsupervised Learning: Finds patterns in unlabeled data
3. Reinforcement Learning: Learns through interaction with environment

The field has seen rapid advancement with deep learning technologies and large language models 
like GPT, which have revolutionized how we interact with AI systems.
            """,
            "metadata": DocumentMetadata(
                document_id="ai_ml_intro",
                source="ai_handbook.md",
                title="Introduction to AI and ML",
                author="Tech Expert",
                document_type="educational",
                tags=["AI", "ML", "technology", "education"]
            )
        },
        {
            "content": """
# Vector Databases and Embeddings

Vector databases are specialized database systems designed to store, index, and query 
high-dimensional vector embeddings. These databases are essential for modern AI applications, 
particularly in similarity search and retrieval-augmented generation (RAG) systems.

## Key Features:
- High-dimensional vector storage
- Efficient similarity search algorithms
- Scalable indexing structures (HNSW, IVF, LSH)
- Integration with machine learning workflows

## Popular Vector Databases:
- Pinecone: Cloud-native vector database
- Chroma: Open-source embedding database
- Weaviate: Vector search engine with GraphQL
- Qdrant: Vector similarity search engine
- FAISS: Facebook AI Similarity Search

## Use Cases:
1. Semantic search and information retrieval
2. Recommendation systems
3. Image and document similarity
4. Question-answering systems
5. Chatbots and virtual assistants

Vector embeddings capture semantic meaning, allowing for more intelligent search 
and retrieval compared to traditional keyword-based approaches.
            """,
            "metadata": DocumentMetadata(
                document_id="vector_db_guide",
                source="vector_database_guide.md",
                title="Vector Databases Guide",
                author="Data Engineer",
                document_type="technical",
                tags=["vector", "database", "embeddings", "search"]
            )
        },
        {
            "content": """
# Retrieval-Augmented Generation (RAG)

RAG is an advanced AI architecture that combines information retrieval with text generation 
to produce more accurate and contextually relevant responses. It addresses the limitation 
of large language models by providing them with external knowledge sources.

## RAG Architecture Components:
1. Document Ingestion and Preprocessing
2. Vector Embedding and Storage
3. Query Processing and Retrieval
4. Context Integration and Generation
5. Response Synthesis and Delivery

## Benefits of RAG:
- Reduced hallucination in AI responses
- Access to up-to-date information
- Domain-specific knowledge integration
- Improved factual accuracy
- Transparent source attribution

## Implementation Strategies:
- Naive RAG: Basic retrieval and generation
- Advanced RAG: Query enhancement and reranking
- Modular RAG: Specialized components for different tasks
- Adaptive RAG: Dynamic strategy selection

## Best Practices:
1. Quality document chunking strategies
2. Appropriate embedding model selection
3. Effective retrieval algorithms
4. Robust context management
5. Continuous evaluation and optimization

RAG systems are becoming the standard for building reliable AI applications 
that require factual accuracy and domain expertise.
            """,
            "metadata": DocumentMetadata(
                document_id="rag_architecture",
                source="rag_implementation.md",
                title="RAG Architecture and Implementation",
                author="AI Architect",
                document_type="technical",
                tags=["RAG", "architecture", "LLM", "retrieval"]
            )
        },
        {
            "content": """
# Python Libraries for AI Development

Python has become the dominant language for AI and machine learning development, 
with a rich ecosystem of libraries and frameworks that make building AI applications accessible.

## Core ML Libraries:
- **NumPy**: Fundamental package for numerical computing
- **Pandas**: Data manipulation and analysis
- **Scikit-learn**: Machine learning algorithms and tools
- **Matplotlib/Seaborn**: Data visualization

## Deep Learning Frameworks:
- **TensorFlow**: Google's machine learning platform
- **PyTorch**: Facebook's dynamic neural network framework
- **Keras**: High-level neural networks API
- **JAX**: NumPy-compatible library for ML research

## NLP and Language Models:
- **Transformers**: Hugging Face's transformer models
- **spaCy**: Industrial-strength NLP
- **NLTK**: Natural Language Toolkit
- **LangChain**: Framework for developing LLM applications

## Vector Database Integration:
- **Chroma**: Open-source embedding database
- **Pinecone**: Managed vector database service
- **Weaviate**: Vector search engine
- **FAISS**: Similarity search library

## Development Tools:
- **Jupyter**: Interactive development environment
- **MLflow**: ML lifecycle management
- **Weights & Biases**: Experiment tracking
- **Docker**: Containerization for deployment

The Python ecosystem continues to evolve rapidly, with new tools and libraries 
constantly emerging to support the growing AI development community.
            """,
            "metadata": DocumentMetadata(
                document_id="python_ai_libs",
                source="python_ai_libraries.md",
                title="Python Libraries for AI Development",
                author="Python Developer",
                document_type="reference",
                tags=["Python", "libraries", "AI", "development"]
            )
        }
    ]
    
    # Initialize components
    logger.info("1. Initializing Vector Database and RAG System")
    
    # Initialize document processor
    doc_processor = AdvancedDocumentProcessor(chunking_config)
    
    # Create temporary files for demonstration
    temp_files = []
    for i, doc_data in enumerate(sample_documents):
        temp_file = f"temp_doc_{i}.md"
        with open(temp_file, 'w', encoding='utf-8') as f:
            f.write(doc_data["content"])
        temp_files.append(temp_file)
    
    # Process documents
    logger.info("2. Processing and Chunking Documents")
    metadata_list = [doc_data["metadata"] for doc_data in sample_documents]
    processed_docs = await doc_processor.process_documents(temp_files, metadata_list)
    
    logger.info(f"Processed {len(processed_docs)} document chunks")
    
    # Initialize vector store
    vector_store = UniversalVectorStore(vector_config)
    
    # Add documents to vector store
    logger.info("3. Adding Documents to Vector Store")
    document_ids = await vector_store.add_documents(processed_docs)
    
    logger.info(f"Added {len(document_ids)} documents to vector store")
    
    # Initialize RAG system
    rag_system = AdvancedRAGSystem(vector_store, retrieval_config)
    
    # Test queries
    logger.info("4. Testing RAG System with Various Queries")
    
    test_queries = [
        "What is machine learning and what are its main types?",
        "How do vector databases work and what are their key features?",
        "Explain the RAG architecture and its benefits",
        "What Python libraries are commonly used for AI development?",
        "What are the differences between supervised and unsupervised learning?",
        "How does semantic search differ from keyword-based search?",
        "What are the main components of a RAG system?",
        "Which vector databases are most popular and why?"
    ]
    
    query_results = []
    
    for query in test_queries:
        logger.info(f"Processing query: {query}")
        
        # Test different retrieval strategies
        strategies_to_test = [
            RetrievalStrategy.SIMILARITY_SEARCH,
            RetrievalStrategy.MAXIMAL_MARGINAL_RELEVANCE,
            RetrievalStrategy.MULTI_QUERY
        ]
        
        strategy_results = {}
        
        for strategy in strategies_to_test:
            try:
                result = await rag_system.query(
                    question=query,
                    retrieval_strategy=strategy,
                    conversation_id=f"demo_{uuid.uuid4()}"
                )
                
                strategy_results[strategy.value] = {
                    "answer": result["answer"],
                    "response_time": result["response_time"],
                    "relevance_score": result["relevance_score"],
                    "documents_retrieved": len(result["source_documents"]),
                    "token_usage": result["token_usage"]
                }
                
            except Exception as e:
                logger.error(f"Error with strategy {strategy.value}: {e}")
                strategy_results[strategy.value] = {"error": str(e)}
        
        query_results.append({
            "query": query,
            "strategy_results": strategy_results
        })
    
    # Performance analysis
    logger.info("5. Analyzing Performance and Results")
    
    performance_stats = rag_system.get_performance_statistics()
    
    # Create comprehensive analysis report
    analysis_report = {
        "demonstration_timestamp": datetime.now(timezone.utc).isoformat(),
        "configuration": {
            "vector_database": vector_config.database_type.value,
            "embedding_model": vector_config.embedding_model,
            "chunking_strategy": chunking_config.strategy.value,
            "chunk_size": chunking_config.chunk_size,
            "retrieval_strategies_tested": [s.value for s in strategies_to_test]
        },
        "document_processing": {
            "total_documents": len(sample_documents),
            "total_chunks": len(processed_docs),
            "average_chunk_size": np.mean([len(doc.page_content) for doc in processed_docs]),
            "document_types": list(set([doc.metadata.get("document_type") for doc in processed_docs]))
        },
        "vector_store_stats": performance_stats["vector_store_stats"],
        "rag_performance": performance_stats["query_statistics"],
        "query_results": query_results,
        "strategy_comparison": {},
        "insights_and_recommendations": []
    }
    
    # Analyze strategy performance
    strategy_performance = defaultdict(list)
    
    for result in query_results:
        for strategy, metrics in result["strategy_results"].items():
            if "error" not in metrics:
                strategy_performance[strategy].append({
                    "response_time": metrics["response_time"],
                    "relevance_score": metrics["relevance_score"],
                    "documents_retrieved": metrics["documents_retrieved"]
                })
    
    # Calculate strategy averages
    for strategy, performances in strategy_performance.items():
        if performances:
            analysis_report["strategy_comparison"][strategy] = {
                "average_response_time": np.mean([p["response_time"] for p in performances]),
                "average_relevance_score": np.mean([p["relevance_score"] for p in performances]),
                "average_documents_retrieved": np.mean([p["documents_retrieved"] for p in performances]),
                "queries_processed": len(performances)
            }
    
    # Generate insights
    insights = [
        f"Processed {len(processed_docs)} document chunks from {len(sample_documents)} source documents",
        f"Vector store successfully indexed {vector_config.dimension}-dimensional embeddings",
        f"RAG system achieved average relevance score of {performance_stats['query_statistics']['average_relevance_score']:.3f}",
        f"Average query response time: {performance_stats['query_statistics']['average_response_time']:.3f} seconds"
    ]
    
    # Strategy-specific insights
    if "similarity_search" in analysis_report["strategy_comparison"]:
        similarity_perf = analysis_report["strategy_comparison"]["similarity_search"]
        insights.append(
            f"Similarity search strategy: {similarity_perf['average_response_time']:.3f}s avg response time, "
            f"{similarity_perf['average_relevance_score']:.3f} avg relevance"
        )
    
    if "mmr" in analysis_report["strategy_comparison"]:
        mmr_perf = analysis_report["strategy_comparison"]["mmr"]
        insights.append(
            f"MMR strategy provides more diverse results with {mmr_perf['average_relevance_score']:.3f} avg relevance"
        )
    
    analysis_report["insights_and_recommendations"] = insights
    
    # Save results
    with open("vector_rag_demonstration_results.json", "w") as f:
        json.dump(analysis_report, f, indent=2, default=str)
    
    # Create visualizations
    try:
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # Strategy performance comparison
        strategies = list(analysis_report["strategy_comparison"].keys())
        response_times = [analysis_report["strategy_comparison"][s]["average_response_time"] for s in strategies]
        relevance_scores = [analysis_report["strategy_comparison"][s]["average_relevance_score"] for s in strategies]
        
        axes[0, 0].bar(strategies, response_times, color='skyblue')
        axes[0, 0].set_title('Average Response Time by Strategy')
        axes[0, 0].set_ylabel('Response Time (seconds)')
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        axes[0, 1].bar(strategies, relevance_scores, color='lightgreen')
        axes[0, 1].set_title('Average Relevance Score by Strategy')
        axes[0, 1].set_ylabel('Relevance Score')
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # Document chunk size distribution
        chunk_sizes = [len(doc.page_content) for doc in processed_docs]
        axes[1, 0].hist(chunk_sizes, bins=15, color='lightcoral', alpha=0.7)
        axes[1, 0].set_title('Document Chunk Size Distribution')
        axes[1, 0].set_xlabel('Chunk Size (characters)')
        axes[1, 0].set_ylabel('Frequency')
        
        # Query performance over time
        query_indices = range(len(query_results))
        similarity_times = []
        
        for result in query_results:
            if "similarity_search" in result["strategy_results"] and "error" not in result["strategy_results"]["similarity_search"]:
                similarity_times.append(result["strategy_results"]["similarity_search"]["response_time"])
            else:
                similarity_times.append(0)
        
        axes[1, 1].plot(query_indices, similarity_times, marker='o', color='purple')
        axes[1, 1].set_title('Query Response Times')
        axes[1, 1].set_xlabel('Query Index')
        axes[1, 1].set_ylabel('Response Time (seconds)')
        
        plt.tight_layout()
        plt.savefig('vector_rag_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
    except Exception as e:
        logger.warning(f"Error creating visualizations: {e}")
    
    # Cleanup temporary files
    for temp_file in temp_files:
        try:
            os.remove(temp_file)
        except:
            pass
    
    logger.info("Vector Database and RAG demonstration completed!")
    logger.info("Check 'vector_rag_demonstration_results.json' for detailed results")
    
    return analysis_report

# Main execution
if __name__ == "__main__":
    asyncio.run(comprehensive_vector_rag_demonstration())
````

## Conclusion

This comprehensive vector database and RAG implementation framework establishes the foundational architecture for building intelligent information retrieval systems that combine semantic search capabilities with advanced language generation, enabling AI applications to access and leverage external knowledge sources effectively while maintaining high accuracy and contextual relevance.

**Advanced Vector Database Integration** through universal storage abstraction supports multiple backend systems including ChromaDB, Pinecone, and Qdrant, providing flexibility in deployment scenarios while maintaining consistent performance characteristics and enabling seamless migration between different vector database technologies based on scalability and operational requirements.

**Sophisticated Document Processing Pipeline** with multi-format support and intelligent chunking strategies ensures optimal embedding representation of diverse content types, preserving semantic coherence while managing context window limitations and enabling efficient retrieval operations across large document collections.

**Multi-Strategy Retrieval Architecture** implementing similarity search, maximal marginal relevance, multi-query expansion, and contextual compression provides adaptive information retrieval capabilities that can be optimized for different use cases, from factual question-answering to creative content generation requiring diverse source materials.

**Production-Ready RAG System** with comprehensive performance monitoring, conversation memory management, and token usage tracking enables deployment of scalable question-answering systems that maintain context across interactions while providing transparent source attribution and confidence scoring for improved user trust.

**Intelligent Context Management** through semantic chunking, relevance scoring, and adaptive retrieval strategies ensures that language models receive the most pertinent information within context window constraints, significantly reducing hallucination while improving response accuracy and factual grounding.

**Comprehensive Performance Analytics** with detailed metrics collection, strategy comparison, and optimization insights enables continuous improvement of RAG systems through data-driven decision making, ensuring optimal balance between response quality, retrieval speed, and computational efficiency.

This framework empowers developers to build sophisticated AI applications that combine the reasoning capabilities of large language models with the precision and reliability of structured knowledge retrieval, creating systems that can provide accurate, contextual, and well-sourced responses across diverse domains and use cases.