<small>Claude 3.7 Sonnet Thinking</small>
# 03. Model Context Protocol (MCP)

## Key Terms

- **Model Context Protocol (MCP)**: A specification for standardizing the format of messages and function calls across different AI model providers.
- **Interoperability**: The ability of different systems or components to work together seamlessly.
- **Message**: Basic unit of communication in MCP containing role and content information.
- **Tool**: External function definition that can be called by a model.
- **Tool Use/Call**: A request from a model to execute a specific tool with parameters.
- **Source**: Reference to information used in generating a response.
- **Adapter Pattern**: Design pattern that allows incompatible interfaces to work together.
- **Schema**: Formal description of data structures and their constraints.

## What is MCP and Why It Matters

The Model Context Protocol (MCP) is a specification that standardizes the format of messages, tools, and other components across different AI model providers. As the AI ecosystem continues to expand with various proprietary APIs (OpenAI, Anthropic, Google, Mistral, etc.), the need for a common interface becomes critical for building provider-agnostic applications.

MCP serves as a translation layer between your application and various AI providers, enabling:

1. **Provider Flexibility**: Switch between model providers without rewriting your application logic
2. **Future-Proofing**: Insulate your codebase from API changes by providers
3. **Unified Development Experience**: Consistent interface regardless of the underlying model
4. **Ecosystem Compatibility**: Enable tools built for one provider to work with others

## MCP Structure and Components

The MCP specification defines standardized formats for the key components of AI interactions:

### Messages

Messages represent communications between users, assistants, and system components:

```json
{
  "role": "user|assistant|system|tool",
  "content": [
    {
      "type": "text",
      "text": "Message content here"
    }
  ],
  "name": "optional_name",
  "tool_calls": [],
  "tool_call_id": "optional_id_if_responding_to_tool_call"
}
```

### Tools

Tools define functions that models can call:

```json
{
  "type": "function",
  "function": {
    "name": "tool_name",
    "description": "What the tool does",
    "parameters": {
      "type": "object",
      "properties": {
        "param1": {
          "type": "string",
          "description": "Description of parameter"
        }
      },
      "required": ["param1"]
    }
  }
}
```

### Tool Calls

Tool calls are requests from the model to execute a defined tool:

```json
{
  "id": "call_xyz123",
  "type": "function",
  "function": {
    "name": "tool_name",
    "arguments": "{\"param1\":\"value\"}"
  }
}
```

### Sources

Sources reference information used in responses:

```json
{
  "type": "file|search|database",
  "source_id": "unique_identifier",
  "metadata": {"additional": "information"}
}
```

## Implementing MCP in Python

Let's implement a comprehensive MCP framework that can adapt between various AI providers:

```python
from typing import Dict, List, Any, Optional, Union, Literal
from pydantic import BaseModel, Field, validator
import json
from enum import Enum
import os
from dotenv import load_dotenv
from openai import OpenAI
from anthropic import Anthropic
import requests

# Load environment variables
load_dotenv()

class ContentType(str, Enum):
    text = "text"
    image = "image"
    tool_use = "tool_use"
    tool_result = "tool_result"

class TextContent(BaseModel):
    """Text content in a message"""
    type: Literal["text"] = "text"
    text: str

class ImageContent(BaseModel):
    """Image content in a message"""
    type: Literal["image"] = "image"
    image_url: dict

class ToolParameterDefinition(BaseModel):
    """Parameter definition for a tool"""
    type: str
    description: Optional[str] = None
    enum: Optional[List[Any]] = None
    properties: Optional[Dict[str, 'ToolParameterDefinition']] = None
    items: Optional[Dict[str, Any]] = None
    required: Optional[List[str]] = None

class ToolDefinition(BaseModel):
    """Definition of a tool that models can use"""
    name: str
    description: Optional[str] = None
    parameters: ToolParameterDefinition

class Tool(BaseModel):
    """Tool that can be called by the model"""
    type: Literal["function"] = "function"
    function: ToolDefinition

class ToolCall(BaseModel):
    """A call to a tool by the model"""
    id: str
    type: Literal["function"] = "function" 
    function: Dict[str, Any]
    
    @property
    def name(self) -> str:
        return self.function.get("name", "")
    
    @property
    def arguments(self) -> Dict[str, Any]:
        args = self.function.get("arguments", "{}")
        if isinstance(args, str):
            return json.loads(args)
        return args

class ToolUseContent(BaseModel):
    """Tool use content in a message"""
    type: Literal["tool_use"] = "tool_use"
    tool_use_id: str
    name: str
    input: Dict[str, Any]

class ToolResultContent(BaseModel):
    """Tool result content in a message"""
    type: Literal["tool_result"] = "tool_result"
    tool_use_id: str
    content: str

class Source(BaseModel):
    """Source of information used in a response"""
    type: str
    source_id: str
    metadata: Optional[Dict[str, Any]] = None

class Role(str, Enum):
    user = "user"
    assistant = "assistant"
    system = "system"
    tool = "tool"

class MCPMessage(BaseModel):
    """Standard MCP message format"""
    role: Role
    content: Union[str, List[Union[TextContent, ImageContent, ToolUseContent, ToolResultContent]]]
    name: Optional[str] = None
    tool_calls: Optional[List[ToolCall]] = None
    tool_call_id: Optional[str] = None
    
    def __init__(self, **data):
        # Handle string content by converting to TextContent list
        if "content" in data and isinstance(data["content"], str):
            data["content"] = [TextContent(text=data["content"])]
        super().__init__(**data)
    
    @validator("content", pre=True)
    def validate_content(cls, v):
        if isinstance(v, str):
            return [TextContent(text=v)]
        return v
    
    def get_text_content(self) -> str:
        """Extract text content from the message"""
        if isinstance(self.content, str):
            return self.content
            
        text_parts = []
        for item in self.content:
            if isinstance(item, dict) and item.get("type") == "text":
                text_parts.append(item["text"])
            elif hasattr(item, "type") and item.type == "text":
                text_parts.append(item.text)
        
        return "\n".join(text_parts)

class MCPChatCompletion(BaseModel):
    """Standard MCP chat completion format"""
    model: str
    messages: List[MCPMessage]
    tools: Optional[List[Tool]] = None
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = None
    stream: bool = False

class MCPResponse(BaseModel):
    """Standard MCP response format"""
    message: MCPMessage
    usage: Optional[Dict[str, int]] = None
    sources: Optional[List[Source]] = None

class ModelProviderAdapter:
    """Base adapter class for model providers"""
    
    def format_prompt(self, mcp_completion: MCPChatCompletion) -> Any:
        """Convert MCP format to provider-specific format"""
        raise NotImplementedError("Subclasses must implement this")
    
    def parse_response(self, provider_response: Any) -> MCPResponse:
        """Convert provider-specific response to MCP format"""
        raise NotImplementedError("Subclasses must implement this")
    
    def complete(self, mcp_completion: MCPChatCompletion) -> MCPResponse:
        """Complete a prompt using the provider's API"""
        raise NotImplementedError("Subclasses must implement this")

class OpenAIAdapter(ModelProviderAdapter):
    """Adapter for OpenAI API"""
    
    def __init__(self, model: str = "gpt-4o"):
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.model = model
    
    def format_prompt(self, mcp_completion: MCPChatCompletion) -> Dict:
        """Convert MCP format to OpenAI format"""
        messages = []
        
        for msg in mcp_completion.messages:
            openai_msg = {"role": msg.role}
            
            # Handle content
            if isinstance(msg.content, str):
                openai_msg["content"] = msg.content
            else:
                # Convert MCP content format to OpenAI format
                openai_content = []
                for content_item in msg.content:
                    if content_item.type == "text":
                        openai_content.append({"type": "text", "text": content_item.text})
                    elif content_item.type == "image":
                        openai_content.append({"type": "image", "image_url": content_item.image_url})
                openai_msg["content"] = openai_content
            
            # Add optional fields
            if msg.name:
                openai_msg["name"] = msg.name
                
            if msg.tool_call_id:
                openai_msg["tool_call_id"] = msg.tool_call_id
                
            if msg.tool_calls:
                openai_msg["tool_calls"] = [tool_call.dict() for tool_call in msg.tool_calls]
                
            messages.append(openai_msg)
        
        request = {
            "model": mcp_completion.model or self.model,
            "messages": messages,
            "temperature": mcp_completion.temperature
        }
        
        if mcp_completion.max_tokens:
            request["max_tokens"] = mcp_completion.max_tokens
            
        if mcp_completion.tools:
            request["tools"] = [tool.dict() for tool in mcp_completion.tools]
            
        return request
    
    def parse_response(self, provider_response: Any) -> MCPResponse:
        """Convert OpenAI response to MCP format"""
        if hasattr(provider_response, "choices"):
            # For OpenAI API response object
            message = provider_response.choices[0].message
            
            # Convert content based on type
            mcp_content = []
            if hasattr(message, "content") and message.content:
                if isinstance(message.content, str):
                    mcp_content = [TextContent(text=message.content)]
                elif isinstance(message.content, list):
                    # Handle multi-modal content
                    for content_item in message.content:
                        if content_item.get("type") == "text":
                            mcp_content.append(TextContent(text=content_item["text"]))
                        # Add other content type handlers as needed
            
            # Build MCP message
            mcp_message = MCPMessage(
                role=message.role,
                content=mcp_content,
                name=message.name if hasattr(message, "name") else None
            )
            
            # Handle tool calls
            if hasattr(message, "tool_calls") and message.tool_calls:
                mcp_message.tool_calls = []
                for tool_call in message.tool_calls:
                    mcp_tool_call = ToolCall(
                        id=tool_call.id,
                        type="function",
                        function={
                            "name": tool_call.function.name,
                            "arguments": tool_call.function.arguments
                        }
                    )
                    mcp_message.tool_calls.append(mcp_tool_call)
            
            # Get usage statistics if available
            usage = None
            if hasattr(provider_response, "usage"):
                usage = {
                    "prompt_tokens": provider_response.usage.prompt_tokens,
                    "completion_tokens": provider_response.usage.completion_tokens,
                    "total_tokens": provider_response.usage.total_tokens
                }
            
            return MCPResponse(message=mcp_message, usage=usage)
        else:
            # For dictionary format
            message = provider_response["choices"][0]["message"]
            
            # Convert message to MCP format
            mcp_message = MCPMessage(
                role=message["role"],
                content=message.get("content", "")
            )
            
            # Handle tool calls
            if "tool_calls" in message:
                mcp_tool_calls = []
                for tool_call in message["tool_calls"]:
                    mcp_tool_call = ToolCall(
                        id=tool_call["id"],
                        type="function",
                        function={
                            "name": tool_call["function"]["name"],
                            "arguments": tool_call["function"]["arguments"]
                        }
                    )
                    mcp_tool_calls.append(mcp_tool_call)
                mcp_message.tool_calls = mcp_tool_calls
            
            # Get usage statistics if available
            usage = None
            if "usage" in provider_response:
                usage = provider_response["usage"]
            
            return MCPResponse(message=mcp_message, usage=usage)
    
    def complete(self, mcp_completion: MCPChatCompletion) -> MCPResponse:
        """Complete a prompt using OpenAI API"""
        formatted_request = self.format_prompt(mcp_completion)
        response = self.client.chat.completions.create(**formatted_request)
        return self.parse_response(response)

class AnthropicAdapter(ModelProviderAdapter):
    """Adapter for Anthropic API"""
    
    def __init__(self, model: str = "claude-3-opus-20240229"):
        self.client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        self.model = model
    
    def format_prompt(self, mcp_completion: MCPChatCompletion) -> Dict:
        """Convert MCP format to Anthropic format"""
        # Extract system message
        system = None
        messages = []
        
        for msg in mcp_completion.messages:
            if msg.role == Role.system:
                system = msg.get_text_content()
            else:
                # Convert MCP message to Anthropic message
                anthropic_msg = {"role": msg.role}
                
                # Handle content
                if isinstance(msg.content, str):
                    anthropic_msg["content"] = msg.content
                else:
                    # Convert MCP content format to Anthropic format
                    anthropic_content = []
                    for content_item in msg.content:
                        if content_item.type == "text":
                            anthropic_content.append({
                                "type": "text", 
                                "text": content_item.text
                            })
                        elif content_item.type == "image":
                            anthropic_content.append({
                                "type": "image",
                                "source": content_item.image_url
                            })
                    anthropic_msg["content"] = anthropic_content
                
                messages.append(anthropic_msg)
        
        request = {
            "model": mcp_completion.model or self.model,
            "messages": messages,
            "temperature": mcp_completion.temperature
        }
        
        if system:
            request["system"] = system
            
        if mcp_completion.max_tokens:
            request["max_tokens"] = mcp_completion.max_tokens
            
        # Convert tools to Anthropic format if present
        if mcp_completion.tools:
            anthropic_tools = []
            for tool in mcp_completion.tools:
                if tool.type == "function":
                    anthropic_tool = {
                        "name": tool.function.name,
                        "description": tool.function.description or "",
                        "input_schema": tool.function.parameters.dict()
                    }
                    anthropic_tools.append(anthropic_tool)
            
            if anthropic_tools:
                request["tools"] = anthropic_tools
                
        return request
    
    def parse_response(self, provider_response: Any) -> MCPResponse:
        """Convert Anthropic response to MCP format"""
        # Extract content blocks
        mcp_content = []
        tool_calls = []
        
        # Parse different content types
        for content_block in provider_response.content:
            if content_block.type == "text":
                mcp_content.append(TextContent(text=content_block.text))
            elif content_block.type == "tool_use":
                # Anthropic's tool_use is equivalent to tool_calls in OpenAI
                tool_use_id = content_block.id
                tool_calls.append(ToolCall(
                    id=tool_use_id,
                    type="function",
                    function={
                        "name": content_block.name,
                        "arguments": json.dumps(content_block.input)
                    }
                ))
                
                # Also include in content for completeness
                mcp_content.append(ToolUseContent(
                    tool_use_id=tool_use_id,
                    name=content_block.name,
                    input=content_block.input
                ))
        
        # Create MCP message
        mcp_message = MCPMessage(
            role="assistant",
            content=mcp_content
        )
        
        # Add tool calls if any
        if tool_calls:
            mcp_message.tool_calls = tool_calls
        
        # Get usage statistics if available
        usage = None
        if hasattr(provider_response, "usage"):
            usage = {
                "prompt_tokens": provider_response.usage.input_tokens,
                "completion_tokens": provider_response.usage.output_tokens,
                "total_tokens": provider_response.usage.input_tokens + provider_response.usage.output_tokens
            }
        
        return MCPResponse(message=mcp_message, usage=usage)
    
    def complete(self, mcp_completion: MCPChatCompletion) -> MCPResponse:
        """Complete a prompt using Anthropic API"""
        formatted_request = self.format_prompt(mcp_completion)
        response = self.client.messages.create(**formatted_request)
        return self.parse_response(response)

class OllamaAdapter(ModelProviderAdapter):
    """Adapter for Ollama API"""
    
    def __init__(self, model: str = "llama2", base_url: str = "http://localhost:11434"):
        self.model = model
        self.base_url = base_url
    
    def format_prompt(self, mcp_completion: MCPChatCompletion) -> Dict:
        """Convert MCP format to Ollama format"""
        system = None
        messages = []
        
        # Extract system message and prepare chat messages
        for msg in mcp_completion.messages:
            if msg.role == Role.system:
                system = msg.get_text_content()
            else:
                ollama_msg = {
                    "role": msg.role,
                    "content": msg.get_text_content()
                }
                messages.append(ollama_msg)
        
        request = {
            "model": mcp_completion.model or self.model,
            "messages": messages,
            "temperature": mcp_completion.temperature,
            "stream": mcp_completion.stream
        }
        
        if system:
            request["system"] = system
            
        if mcp_completion.max_tokens:
            request["max_tokens"] = mcp_completion.max_tokens
            
        return request
    
    def parse_response(self, provider_response: Dict) -> MCPResponse:
        """Convert Ollama response to MCP format"""
        message_content = provider_response.get("message", {}).get("content", "")
        
        # Create MCP message
        mcp_message = MCPMessage(
            role="assistant",
            content=[TextContent(text=message_content)]
        )
        
        # Ollama doesn't fully support tool calls yet, but we can parse them from the response
        # if they appear in JSON format
        try:
            # Look for JSON blocks that might represent tool calls
            import re
            json_blocks = re.findall(r'```json\n(.*?)\n```', message_content, re.DOTALL)
            if not json_blocks:
                # Try with different markdown code block syntax
                json_blocks = re.findall(r'```\n(.*?)\n```', message_content, re.DOTALL)
                
            # If we found JSON blocks, try to parse them as tool calls
            tool_calls = []
            for block in json_blocks:
                try:
                    data = json.loads(block)
                    if isinstance(data, dict) and "tool" in data and "parameters" in data:
                        tool_calls.append(ToolCall(
                            id=f"call_{len(tool_calls)+1}",
                            type="function",
                            function={
                                "name": data["tool"],
                                "arguments": json.dumps(data["parameters"])
                            }
                        ))
                except json.JSONDecodeError:
                    continue
                    
            if tool_calls:
                mcp_message.tool_calls = tool_calls
        except Exception:
            # Silently handle parsing errors
            pass
            
        # Get usage statistics if available
        usage = None
        if "prompt_eval_count" in provider_response and "eval_count" in provider_response:
            usage = {
                "prompt_tokens": provider_response.get("prompt_eval_count", 0),
                "completion_tokens": provider_response.get("eval_count", 0),
                "total_tokens": provider_response.get("prompt_eval_count", 0) + provider_response.get("eval_count", 0)
            }
        
        return MCPResponse(message=mcp_message, usage=usage)
    
    def complete(self, mcp_completion: MCPChatCompletion) -> MCPResponse:
        """Complete a prompt using Ollama API"""
        formatted_request = self.format_prompt(mcp_completion)
        
        response = requests.post(
            f"{self.base_url}/api/chat",
            json=formatted_request
        )
        response.raise_for_status()
        
        return self.parse_response(response.json())
```

## Creating Custom MCP-Compatible Components

Now let's implement custom MCP-compatible components for a real-world agent application:

```python
from typing import Dict, List, Any, Optional, Union, Callable, TypeVar, Type
from pydantic import BaseModel, Field
import json
import uuid
from enum import Enum
from .core import MCPMessage, MCPChatCompletion, MCPResponse, Tool, ToolDefinition, ToolParameterDefinition, ModelProviderAdapter, OpenAIAdapter, AnthropicAdapter, OllamaAdapter, Role

T = TypeVar('T', bound=BaseModel)

class ToolRegistry:
    """Registry for tools that can be called by the agent"""
    
    def __init__(self):
        self.tools: Dict[str, Callable] = {}
        self.definitions: Dict[str, Tool] = {}
    
    def register(self, func: Callable, description: str = None, parameter_model: Type[BaseModel] = None):
        """Register a function as a tool"""
        tool_name = func.__name__
        
        # If a Pydantic model is provided, use it for parameters
        if parameter_model:
            schema = parameter_model.schema()
            parameters = ToolParameterDefinition(**schema)
        else:
            # Create a basic parameter schema from function signature
            import inspect
            sig = inspect.signature(func)
            properties = {}
            required = []
            
            for name, param in sig.parameters.items():
                # Skip self for methods
                if name == "self":
                    continue
                
                # Determine parameter type
                param_type = "string"  # Default type
                if param.annotation != inspect.Parameter.empty:
                    if param.annotation == int:
                        param_type = "integer"
                    elif param.annotation == float:
                        param_type = "number"
                    elif param.annotation == bool:
                        param_type = "boolean"
                    elif param.annotation == list or param.annotation == List:
                        param_type = "array"
                        
                properties[name] = {
                    "type": param_type,
                    "description": f"Parameter: {name}"
                }
                
                # Add to required if no default value
                if param.default == inspect.Parameter.empty:
                    required.append(name)
            
            parameters = ToolParameterDefinition(
                type="object",
                properties=properties,
                required=required
            )
        
        # Create tool definition
        tool_def = Tool(
            type="function",
            function=ToolDefinition(
                name=tool_name,
                description=description or func.__doc__ or f"Tool: {tool_name}",
                parameters=parameters
            )
        )
        
        # Register tool and definition
        self.tools[tool_name] = func
        self.definitions[tool_name] = tool_def
    
    def get_tool_list(self) -> List[Tool]:
        """Get list of all tool definitions"""
        return list(self.definitions.values())
    
    def execute_tool(self, name: str, arguments: Dict[str, Any]) -> str:
        """Execute a registered tool"""
        if name not in self.tools:
            return f"Error: Tool '{name}' not found"
        
        try:
            tool_func = self.tools[name]
            result = tool_func(**arguments)
            return json.dumps(result) if not isinstance(result, str) else result
        except Exception as e:
            return f"Error executing tool '{name}': {str(e)}"

class MCPAgent:
    """MCP-compatible agent that can use tools and different model providers"""
    
    def __init__(self, 
                provider_adapter: ModelProviderAdapter = None,
                system_message: str = None):
        """Initialize the agent with a provider adapter"""
        self.adapter = provider_adapter or OpenAIAdapter()
        self.tool_registry = ToolRegistry()
        self.messages: List[MCPMessage] = []
        
        # Add system message if provided
        if system_message:
            self.messages.append(MCPMessage(role="system", content=system_message))
    
    def register_tool(self, func: Callable, description: str = None, parameter_model: Type[BaseModel] = None):
        """Register a tool that the agent can use"""
        self.tool_registry.register(func, description, parameter_model)
    
    def add_message(self, role: str, content: str):
        """Add a message to the conversation"""
        self.messages.append(MCPMessage(role=role, content=content))
    
    def run(self, 
           query: str = None, 
           temperature: float = 0.7,
           max_iterations: int = 5) -> str:
        """Run the agent with a query"""
        if query:
            self.add_message("user", query)
        
        tools = self.tool_registry.get_tool_list()
        iterations = 0
        
        while iterations < max_iterations:
            iterations += 1
            
            # Create MCP chat completion
            mcp_completion = MCPChatCompletion(
                model=self.adapter.model,
                messages=self.messages,
                tools=tools if tools else None,
                temperature=temperature
            )
            
            # Get response from the model
            response = self.adapter.complete(mcp_completion)
            response_message = response.message
            
            # Add response to messages
            self.messages.append(response_message)
            
            # Check if response contains tool calls
            if response_message.tool_calls:
                for tool_call in response_message.tool_calls:
                    tool_name = tool_call.name
                    tool_args = tool_call.arguments
                    
                    # Execute the tool
                    result = self.tool_registry.execute_tool(tool_name, tool_args)
                    
                    # Add tool result to messages
                    self.messages.append(MCPMessage(
                        role="tool",
                        content=result,
                        tool_call_id=tool_call.id
                    ))
                
                # Continue loop to process tool results
                continue
            else:
                # No tool calls, return the response
                return response_message.get_text_content()
        
        return "Maximum iterations reached without final response."
    
    def switch_provider(self, provider_name: str, model: str = None):
        """Switch to a different provider"""
        if provider_name.lower() == "openai":
            self.adapter = OpenAIAdapter(model=model or "gpt-4o")
        elif provider_name.lower() == "anthropic":
            self.adapter = AnthropicAdapter(model=model or "claude-3-opus-20240229")
        elif provider_name.lower() == "ollama":
            self.adapter = OllamaAdapter(model=model or "llama2")
        else:
            raise ValueError(f"Unsupported provider: {provider_name}")
    
    def clear_conversation(self, keep_system: bool = True):
        """Clear the conversation history"""
        if keep_system:
            # Keep only system messages
            self.messages = [msg for msg in self.messages if msg.role == "system"]
        else:
            # Clear all messages
            self.messages = []
```

## Practical Example: Multi-Provider Agent

Let's create a practical implementation of an MCP-compatible agent that can work with multiple providers and custom tools:

```python
import os
import json
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from dotenv import load_dotenv
import requests
from datetime import datetime
import sys
import logging

# Add parent directory to path to import our MCP modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from mcp.core import OpenAIAdapter, AnthropicAdapter, OllamaAdapter
from mcp.agent import MCPAgent

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# Load environment variables
load_dotenv()

# Define tool parameter models using Pydantic
class WeatherParams(BaseModel):
    location: str
    units: Optional[str] = "metric"

class StockPriceParams(BaseModel):
    symbol: str

class CalculatorParams(BaseModel):
    operation: str
    x: float
    y: float

# Define tool implementations
def get_weather(location: str, units: str = "metric") -> str:
    """Get current weather for a location"""
    # Simulate API call
    logging.info(f"Getting weather for {location} in {units} units")
    
    # In a real implementation, call a weather API
    weather_data = {
        "New York": {"temp": 22, "conditions": "Partly Cloudy"},
        "London": {"temp": 18, "conditions": "Rainy"},
        "Tokyo": {"temp": 28, "conditions": "Sunny"},
        "Sydney": {"temp": 24, "conditions": "Clear"}
    }
    
    if location in weather_data:
        data = weather_data[location]
        temp = data["temp"]
        if units == "imperial":
            temp = temp * 9/5 + 32
        
        return f"Temperature in {location}: {temp}°{'C' if units == 'metric' else 'F'}, Conditions: {data['conditions']}"
    else:
        return f"Weather data for {location} not found."

def get_stock_price(symbol: str) -> Dict[str, Any]:
    """Get current stock price for a given symbol"""
    # Simulate API call
    logging.info(f"Getting stock price for {symbol}")
    
    # In a real implementation, call a stock API
    stock_data = {
        "AAPL": 182.52,
        "MSFT": 415.33,
        "GOOGL": 172.92,
        "AMZN": 186.21,
        "META": 474.33
    }
    
    if symbol.upper() in stock_data:
        price = stock_data[symbol.upper()]
        return {
            "symbol": symbol.upper(),
            "price": price,
            "currency": "USD",
            "timestamp": datetime.now().isoformat()
        }
    else:
        return {"error": f"Stock data for {symbol} not found."}

def calculator(operation: str, x: float, y: float) -> Dict[str, Any]:
    """Perform simple calculations"""
    logging.info(f"Calculating {x} {operation} {y}")
    
    result = None
    if operation == "add":
        result = x + y
    elif operation == "subtract":
        result = x - y
    elif operation == "multiply":
        result = x * y
    elif operation == "divide":
        if y == 0:
            return {"error": "Division by zero error"}
        result = x / y
    else:
        return {"error": f"Unsupported operation: {operation}"}
    
    return {
        "operation": operation,
        "x": x,
        "y": y,
        "result": result
    }

class FinancialAssistant:
    """A financial assistant agent using MCP for provider interoperability"""
    
    def __init__(self, default_provider: str = "openai"):
        """Initialize with default provider"""
        # Set up system message
        system_message = """
        You are a helpful financial assistant. You can help users with:
        - Getting current weather information
        - Looking up stock prices
        - Performing calculations
        - Answering general questions about finance and investments
        
        Always use the tools provided when asked for specific data like weather or stock prices.
        When giving financial advice, make it clear you're providing general information, not financial advice.
        """
        
        # Create agent with the default provider
        self.agent = MCPAgent(system_message=system_message)
        self.set_provider(default_provider)
        
        # Register tools
        self.agent.register_tool(get_weather, "Get current weather for a location", WeatherParams)
        self.agent.register_tool(get_stock_price, "Get current stock price for a given symbol", StockPriceParams)
        self.agent.register_tool(calculator, "Perform basic calculations", CalculatorParams)
    
    def set_provider(self, provider_name: str):
        """Change the provider being used"""
        self.agent.switch_provider(provider_name)
        logging.info(f"Switched to provider: {provider_name}")
    
    def ask(self, query: str) -> str:
        """Ask the agent a question"""
        logging.info(f"User query: {query}")
        
        # Check if it's a provider switch command
        if query.startswith("!use "):
            provider = query[5:].strip().lower()
            self.set_provider(provider)
            return f"Switched to {provider} provider"
        
        # Process the query
        response = self.agent.run(query)
        logging.info(f"Agent response complete")
        
        return response

if __name__ == "__main__":
    print("Financial Assistant Demo (MCP-Compatible)")
    print("----------------------------------------")
    print("Available commands:")
    print("  !use openai - Switch to OpenAI provider")
    print("  !use anthropic - Switch to Anthropic provider")
    print("  !use ollama - Switch to Ollama provider (local)")
    print("  !exit - Exit the program")
    
    assistant = FinancialAssistant()
    
    while True:
        query = input("\nYou: ")
        
        if query.lower() == "!exit":
            print("Goodbye!")
            break
            
        try:
            response = assistant.ask(query)
            print(f"\nAssistant: {response}")
        except Exception as e:
            print(f"\nError: {str(e)}")
```

## Converting Between Provider Formats

Another essential part of working with MCP is converting between different provider formats:

```python
from typing import Dict, List, Any, Union
from .core import MCPMessage, MCPResponse, Tool, ToolCall

def openai_to_mcp(openai_message: Dict[str, Any]) -> MCPMessage:
    """Convert OpenAI message format to MCP"""
    mcp_message = MCPMessage(
        role=openai_message["role"],
        content=openai_message.get("content", "")
    )
    
    # Add optional fields
    if "name" in openai_message:
        mcp_message.name = openai_message["name"]
        
    if "tool_call_id" in openai_message:
        mcp_message.tool_call_id = openai_message["tool_call_id"]
        
    if "tool_calls" in openai_message:
        tool_calls = []
        for tc in openai_message["tool_calls"]:
            tool_calls.append(ToolCall(
                id=tc["id"],
                type="function",
                function={
                    "name": tc["function"]["name"],
                    "arguments": tc["function"]["arguments"]
                }
            ))
        mcp_message.tool_calls = tool_calls
        
    return mcp_message

def anthropic_to_mcp(anthropic_message: Dict[str, Any]) -> MCPMessage:
    """Convert Anthropic message format to MCP"""
    # Extract content based on type
    content = []
    tool_calls = []
    
    if isinstance(anthropic_message.get("content"), list):
        for item in anthropic_message["content"]:
            if item["type"] == "text":
                content.append({"type": "text", "text": item["text"]})
            elif item["type"] == "tool_use":
                # Create tool call
                tool_calls.append(ToolCall(
                    id=item["id"],
                    type="function",
                    function={
                        "name": item["name"],
                        "arguments": item["input"]
                    }
                ))
    else:
        # Simple text content
        content = anthropic_message.get("content", "")
    
    # Create MCP message
    mcp_message = MCPMessage(
        role=anthropic_message["role"],
        content=content
    )
    
    # Add tool calls if present
    if tool_calls:
        mcp_message.tool_calls = tool_calls
        
    return mcp_message

def mcp_to_openai(message: MCPMessage) -> Dict[str, Any]:
    """Convert MCP message to OpenAI format"""
    openai_msg = {"role": message.role}
    
    # Convert content
    if isinstance(message.content, str):
        openai_msg["content"] = message.content
    else:
        content_list = []
        for item in message.content:
            if item.type == "text":
                content_list.append({"type": "text", "text": item.text})
            elif item.type == "image":
                content_list.append({"type": "image", "image_url": item.image_url})
        openai_msg["content"] = content_list
    
    # Add optional fields
    if message.name:
        openai_msg["name"] = message.name
        
    if message.tool_call_id:
        openai_msg["tool_call_id"] = message.tool_call_id
        
    if message.tool_calls:
        openai_msg["tool_calls"] = []
        for tc in message.tool_calls:
            openai_msg["tool_calls"].append({
                "id": tc.id,
                "type": "function",
                "function": {
                    "name": tc.name,
                    "arguments": tc.function["arguments"]
                }
            })
    
    return openai_msg

def mcp_to_anthropic(message: MCPMessage) -> Dict[str, Any]:
    """Convert MCP message to Anthropic format"""
    anthropic_msg = {"role": "user" if message.role == "user" else "assistant"}
    
    # Convert content
    if isinstance(message.content, str):
        anthropic_msg["content"] = [{"type": "text", "text": message.content}]
    else:
        content_list = []
        for item in message.content:
            if item.type == "text":
                content_list.append({"type": "text", "text": item.text})
            elif item.type == "image":
                content_list.append({
                    "type": "image",
                    "source": {"type": "url", "url": item.image_url["url"]}
                })
        anthropic_msg["content"] = content_list
        
    return anthropic_msg

def convert_tools_to_anthropic(tools: List[Tool]) -> List[Dict[str, Any]]:
    """Convert MCP tools to Anthropic format"""
    anthropic_tools = []
    
    for tool in tools:
        if tool.type == "function":
            anthropic_tools.append({
                "name": tool.function.name,
                "description": tool.function.description or "",
                "input_schema": tool.function.parameters.dict()
            })
    
    return anthropic_tools

def convert_tools_to_openai(tools: List[Tool]) -> List[Dict[str, Any]]:
    """Convert MCP tools to OpenAI format"""
    return [tool.dict() for tool in tools]
```

## Conclusion

The Model Context Protocol (MCP) represents a crucial advancement in AI system interoperability. By establishing a standardized interface for communicating with different LLM providers, MCP enables developers to build provider-agnostic applications that can easily switch between models without extensive code changes.

In this comprehensive exploration of MCP, we've:

1. Defined the core components of the protocol including messages, tools, and sources
2. Implemented adapter classes for major providers (OpenAI, Anthropic, Ollama)
3. Built a flexible agent framework that leverages MCP for seamless provider switching
4. Demonstrated practical tools for converting between different provider formats
5. Created a complete multi-provider agent with custom tool implementations

The benefits of adopting MCP in your AI architecture include:

- **Reduced Vendor Lock-in**: Applications can switch between providers as needed
- **Future-proofing**: Code remains compatible as provider APIs evolve
- **Simplified Development**: Standardized interfaces reduce cognitive load
- **Enhanced Experimentation**: Easily test the same agent with different models
- **Component Reusability**: Tools built for one provider work with others

As the AI ecosystem continues to evolve, MCP-compatible components will become increasingly important for building sustainable and flexible applications. Whether you're developing simple chatbots or complex multi-agent systems, implementing the Model Context Protocol provides a solid foundation for interoperable AI development.

When designing your own MCP-compatible components, focus on clear separation of concerns between the protocol layer and provider-specific implementations. This approach ensures your application can adapt to new providers and API changes with minimal disruption.