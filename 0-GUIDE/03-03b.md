<small>Claude 3.7 Sonnet Thinking</small>
# 03. Vector Databases and Their Applications

## Key Terms

- **Vector Database**: Specialized database optimized for storing and querying high-dimensional vector data
- **Embedding**: Dense vector representation of data (text, images, etc.) capturing semantic meaning
- **Vector Similarity**: Measurement of closeness between vectors, typically using cosine similarity or Euclidean distance
- **Dimensionality Reduction**: Techniques to reduce the number of dimensions while preserving semantic relationships
- **ANN (Approximate Nearest Neighbors)**: Algorithms for efficiently finding similar vectors without exhaustive search
- **RAG (Retrieval-Augmented Generation)**: Technique combining retrieval of relevant information with generative AI
- **Semantic Search**: Finding information based on meaning rather than keyword matching
- **Quantization**: Process of reducing vector precision to optimize storage and query speed

## Introduction to Vector Databases

Vector databases represent a fundamental shift in how we store and retrieve information, moving beyond traditional structured query models to semantic similarity search. Unlike relational databases that excel at exact matches and predefined relationships, vector databases are designed to capture and query based on conceptual similarity and semantic meaning.

Vector databases store data as high-dimensional numerical vectors (typically ranging from 128 to 1536 dimensions) and provide specialized indexing structures to efficiently search for similar vectors. This capability is critical for modern AI applications where understanding semantic relationships is essential.

Core characteristics of vector databases include:

1. **Efficient similarity search**: Using specialized algorithms like HNSW (Hierarchical Navigable Small World), IVF (Inverted File Index), or PQ (Product Quantization) to enable sub-linear time complexity for search operations

2. **Hybrid search capabilities**: Combining vector similarity with metadata filtering for contextually relevant results

3. **Scalability**: Handling billions of vectors while maintaining query performance through distributed architectures

4. **Persistence and durability**: Maintaining vector data integrity across system restarts and failures

5. **Real-time updates**: Supporting dynamic addition and removal of vectors without requiring full reindexing

Popular vector database implementations include:

- **Pinecone**: Fully managed vector database with simple API
- **Weaviate**: Open-source vector search engine with GraphQL interface
- **Milvus**: Distributed vector database designed for massive scale
- **Qdrant**: Open-source vector similarity engine with rich filtering
- **Chroma**: Lightweight embeddings database designed for RAG applications
- **FAISS (Facebook AI Similarity Search)**: Library for efficient similarity search

## Converting Data to Vectors and Finding Similarity

The process of leveraging vector databases begins with transforming raw data into meaningful vector representations through embedding models.

### Embedding Generation

Embedding models convert text, images, or other data types into dense vector representations that capture semantic properties. For text, these models analyze context and relationships between words to position semantically similar concepts closer together in vector space.

```python
import os
from dotenv import load_dotenv
from openai import OpenAI
import numpy as np
from typing import List, Dict, Union, Optional
import time
from tqdm import tqdm

# Load environment variables
load_dotenv()

class EmbeddingGenerator:
    """Class for generating and working with embeddings."""
    
    def __init__(self, 
                model: str = "text-embedding-3-small", 
                dimensions: int = 1536,
                batch_size: int = 100,
                rate_limit_pause: float = 0.5):
        """
        Initialize the embedding generator.
        
        Args:
            model: The embedding model to use
            dimensions: The dimensionality of the embedding vectors
            batch_size: Maximum number of texts to embed in a single API call
            rate_limit_pause: Pause between batches to respect API rate limits
        """
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.model = model
        self.dimensions = dimensions
        self.batch_size = batch_size
        self.rate_limit_pause = rate_limit_pause
    
    def _get_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """Get embeddings for a batch of texts."""
        try:
            response = self.client.embeddings.create(
                model=self.model,
                input=texts,
                dimensions=self.dimensions
            )
            # Extract embeddings from response
            embeddings = [item.embedding for item in response.data]
            return embeddings
        except Exception as e:
            print(f"Error generating embeddings: {str(e)}")
            # Return empty embeddings in case of error
            return [np.zeros(self.dimensions).tolist() for _ in range(len(texts))]
    
    def get_embeddings(self, texts: Union[str, List[str]]) -> Union[List[float], List[List[float]]]:
        """
        Get embeddings for one or more texts.
        
        Args:
            texts: Single text string or list of text strings
            
        Returns:
            Single embedding vector or list of embedding vectors
        """
        # Handle single text input
        if isinstance(texts, str):
            return self._get_embeddings_batch([texts])[0]
        
        # Handle list of texts with batching
        all_embeddings = []
        for i in tqdm(range(0, len(texts), self.batch_size), desc="Generating embeddings"):
            batch_texts = texts[i:i+self.batch_size]
            batch_embeddings = self._get_embeddings_batch(batch_texts)
            all_embeddings.extend(batch_embeddings)
            
            # Pause to avoid rate limits if not the last batch
            if i + self.batch_size < len(texts):
                time.sleep(self.rate_limit_pause)
                
        return all_embeddings
    
    @staticmethod
    def cosine_similarity(vector_a: List[float], vector_b: List[float]) -> float:
        """
        Calculate the cosine similarity between two vectors.
        
        Args:
            vector_a: First vector
            vector_b: Second vector
            
        Returns:
            Cosine similarity (1 = identical, 0 = orthogonal, -1 = opposite)
        """
        dot_product = sum(a * b for a, b in zip(vector_a, vector_b))
        magnitude_a = sum(a * a for a in vector_a) ** 0.5
        magnitude_b = sum(b * b for b in vector_b) ** 0.5
        
        if magnitude_a == 0 or magnitude_b == 0:
            return 0
            
        return dot_product / (magnitude_a * magnitude_b)
    
    @staticmethod
    def euclidean_distance(vector_a: List[float], vector_b: List[float]) -> float:
        """
        Calculate the Euclidean distance between two vectors.
        
        Args:
            vector_a: First vector
            vector_b: Second vector
            
        Returns:
            Euclidean distance (lower values indicate higher similarity)
        """
        return sum((a - b) ** 2 for a, b in zip(vector_a, vector_b)) ** 0.5
    
    @staticmethod
    def normalize_vector(vector: List[float]) -> List[float]:
        """
        Normalize a vector to unit length.
        
        Args:
            vector: Input vector
            
        Returns:
            Normalized vector
        """
        magnitude = sum(v * v for v in vector) ** 0.5
        if magnitude == 0:
            return vector
        return [v / magnitude for v in vector]
```

### Vector Similarity Search

Once data is converted to vectors, the core operation becomes finding the most similar vectors to a query vector. Common similarity metrics include:

1. **Cosine Similarity**: Measures the cosine of the angle between vectors (range: -1 to 1)
2. **Euclidean Distance**: Measures straight-line distance between vector points
3. **Dot Product**: For normalized vectors, equivalent to cosine similarity
4. **Manhattan Distance**: Sum of absolute differences between vector dimensions

For efficient similarity search at scale, approximate nearest neighbor (ANN) algorithms are used:

```python
import os
from typing import List, Dict, Tuple, Optional, Any, Union
import numpy as np
import qdrant_client
from qdrant_client.http import models
from dotenv import load_dotenv
from embedding_generator import EmbeddingGenerator

# Load environment variables
load_dotenv()

class VectorDatabase:
    """Interface for vector database operations."""
    
    def __init__(self, 
                collection_name: str,
                embedding_generator: Optional[EmbeddingGenerator] = None,
                dimension: int = 1536):
        """
        Initialize the vector database.
        
        Args:
            collection_name: Name of the collection to use
            embedding_generator: Instance of EmbeddingGenerator
            dimension: Dimensionality of embedding vectors
        """
        self.collection_name = collection_name
        self.dimension = dimension
        
        # Initialize Qdrant client
        self.client = qdrant_client.QdrantClient(
            url=os.getenv("QDRANT_URL", "http://localhost:6333"),
            api_key=os.getenv("QDRANT_API_KEY", None)
        )
        
        # Create embedding generator if not provided
        self.embedding_generator = embedding_generator or EmbeddingGenerator(dimensions=dimension)
        
        # Ensure collection exists
        self._create_collection_if_not_exists()
    
    def _create_collection_if_not_exists(self) -> None:
        """Create the vector collection if it doesn't exist."""
        collections = self.client.get_collections().collections
        collection_names = [collection.name for collection in collections]
        
        if self.collection_name not in collection_names:
            self.client.create_collection(
                collection_name=self.collection_name,
                vectors_config=models.VectorParams(
                    size=self.dimension, 
                    distance=models.Distance.COSINE
                )
            )
            print(f"Created collection: {self.collection_name}")
    
    def add_documents(self, 
                     texts: List[str], 
                     metadatas: Optional[List[Dict[str, Any]]] = None,
                     ids: Optional[List[Union[str, int]]] = None,
                     batch_size: int = 100) -> List[str]:
        """
        Add documents to the vector database.
        
        Args:
            texts: List of text documents to embed and store
            metadatas: List of metadata dictionaries for each document
            ids: List of IDs for the documents (auto-generated if not provided)
            batch_size: Number of documents to process in each batch
            
        Returns:
            List of IDs for the added documents
        """
        if not texts:
            return []
            
        # Generate embeddings for all texts
        embeddings = self.embedding_generator.get_embeddings(texts)
        
        # Prepare IDs (generate if not provided)
        if ids is None:
            ids = [str(i) for i in range(len(texts))]
        elif len(ids) != len(texts):
            raise ValueError("Number of IDs must match number of texts")
            
        # Prepare metadata (empty dict if not provided)
        if metadatas is None:
            metadatas = [{} for _ in range(len(texts))]
        elif len(metadatas) != len(texts):
            raise ValueError("Number of metadata items must match number of texts")
            
        # Add documents in batches
        for i in range(0, len(texts), batch_size):
            batch_end = min(i + batch_size, len(texts))
            
            # Create points for this batch
            points = [
                models.PointStruct(
                    id=ids[j],
                    vector=embeddings[j],
                    payload={
                        "text": texts[j],
                        **metadatas[j]
                    }
                )
                for j in range(i, batch_end)
            ]
            
            # Upsert points to collection
            self.client.upsert(
                collection_name=self.collection_name,
                points=points
            )
            
        return ids
    
    def similarity_search(self, 
                         query: str,
                         k: int = 5,
                         filter_condition: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        Perform similarity search with a text query.
        
        Args:
            query: Text query to search for
            k: Number of results to return
            filter_condition: Filter to apply to search results
            
        Returns:
            List of search results with text, metadata, and similarity score
        """
        # Generate embedding for query
        query_embedding = self.embedding_generator.get_embeddings(query)
        
        # Prepare filter if provided
        filter_param = None
        if filter_condition:
            filter_param = models.Filter(
                must=[
                    models.FieldCondition(
                        key=key,
                        match=models.MatchValue(value=value)
                    )
                    for key, value in filter_condition.items()
                ]
            )
        
        # Perform search
        search_result = self.client.search(
            collection_name=self.collection_name,
            query_vector=query_embedding,
            limit=k,
            with_payload=True,
            filter=filter_param
        )
        
        # Format results
        results = []
        for scored_point in search_result:
            payload = scored_point.payload
            text = payload.pop("text")
            
            results.append({
                "text": text,
                "metadata": payload,
                "id": scored_point.id,
                "score": scored_point.score
            })
            
        return results
    
    def delete_by_ids(self, ids: List[Union[str, int]]) -> None:
        """Delete documents by their IDs."""
        self.client.delete(
            collection_name=self.collection_name,
            points_selector=models.PointIdsList(
                points=ids
            )
        )
    
    def get_collection_info(self) -> Dict[str, Any]:
        """Get information about the collection."""
        collection_info = self.client.get_collection(self.collection_name)
        return {
            "name": collection_info.name,
            "vectors_count": collection_info.vectors_count,
            "dimension": collection_info.config.params.vectors.size,
            "distance": collection_info.config.params.vectors.distance,
            "index": collection_info.config.hnsw_config.dict() if hasattr(collection_info.config, 'hnsw_config') else None
        }
```

## RAG (Retrieval-Augmented Generation) Principle and Its Application in Chatbots

RAG combines the strengths of retrieval-based and generative approaches to create AI systems that can access external knowledge for more accurate, relevant, and current responses.

### Core RAG Components

1. **Retriever**: Finds relevant information from a knowledge base using semantic search
2. **Generator**: Creates coherent responses incorporating retrieved information
3. **Knowledge Base**: Vector database storing embeddings of reference documents
4. **Orchestration Logic**: Manages the flow between retrieval and generation

The RAG process follows these steps:
1. Convert a user query into an embedding vector
2. Find semantically similar documents in the vector database
3. Retrieve the most relevant documents
4. Augment the LLM prompt with retrieved information
5. Generate a contextually informed response

### Implementing RAG for a Document-Based Chatbot

Let's implement a comprehensive RAG system for answering questions based on document content:

```python
import os
from dotenv import load_dotenv
from openai import OpenAI
from typing import List, Dict, Any, Optional, Union
import pathlib
import time
import textwrap
from tqdm import tqdm

from embedding_generator import EmbeddingGenerator
from vector_search import VectorDatabase

# Load environment variables
load_dotenv()

class DocumentProcessor:
    """Process documents for RAG system."""
    
    @staticmethod
    def load_text_files(directory_path: str, file_extensions: List[str] = ['.txt', '.md']) -> List[Dict[str, str]]:
        """
        Load text files from a directory.
        
        Args:
            directory_path: Path to directory containing documents
            file_extensions: List of file extensions to include
            
        Returns:
            List of document dictionaries with text content and metadata
        """
        documents = []
        directory = pathlib.Path(directory_path)
        
        for file_path in directory.glob('**/*'):
            if file_path.is_file() and file_path.suffix.lower() in file_extensions:
                try:
                    with open(file_path, 'r', encoding='utf-8') as file:
                        content = file.read()
                        
                    documents.append({
                        'text': content,
                        'metadata': {
                            'source': str(file_path.relative_to(directory)),
                            'filename': file_path.name,
                            'extension': file_path.suffix,
                            'created': time.ctime(file_path.stat().st_ctime),
                            'modified': time.ctime(file_path.stat().st_mtime)
                        }
                    })
                except Exception as e:
                    print(f"Error loading {file_path}: {e}")
        
        return documents
    
    @staticmethod
    def chunk_document(document: Dict[str, str], 
                      chunk_size: int = 1000, 
                      chunk_overlap: int = 200) -> List[Dict[str, Any]]:
        """
        Split document into smaller chunks for processing.
        
        Args:
            document: Document dictionary with text and metadata
            chunk_size: Maximum number of characters per chunk
            chunk_overlap: Number of characters to overlap between chunks
            
        Returns:
            List of chunk dictionaries with text and metadata
        """
        text = document['text']
        metadata = document['metadata']
        
        if len(text) <= chunk_size:
            return [{'text': text, 'metadata': metadata}]
            
        chunks = []
        start = 0
        
        while start < len(text):
            # Find a good break point
            end = min(start + chunk_size, len(text))
            
            # Try to break at paragraph or sentence if possible
            if end < len(text):
                # Look for paragraph break
                paragraph_break = text.rfind('\n\n', start, end)
                if paragraph_break != -1 and paragraph_break > start + chunk_size // 2:
                    end = paragraph_break + 2
                else:
                    # Look for sentence break (period followed by space)
                    sentence_break = text.rfind('. ', start, end)
                    if sentence_break != -1 and sentence_break > start + chunk_size // 2:
                        end = sentence_break + 2
                    else:
                        # Look for any whitespace
                        space_break = text.rfind(' ', start, end)
                        if space_break != -1 and space_break > start + chunk_size // 2:
                            end = space_break + 1
            
            # Create chunk with original metadata plus chunk-specific info
            chunk_metadata = metadata.copy()
            chunk_metadata['chunk_index'] = len(chunks)
            chunk_metadata['chunk_text_range'] = f"{start}-{end}"
            
            chunks.append({
                'text': text[start:end],
                'metadata': chunk_metadata
            })
            
            # Move start position for next chunk, accounting for overlap
            start = max(start + 1, end - chunk_overlap)
        
        return chunks

class RAGChatbot:
    """Retrieval-Augmented Generation chatbot for document Q&A."""
    
    def __init__(self, 
                collection_name: str,
                embedding_model: str = "text-embedding-3-small",
                llm_model: str = "gpt-4o",
                chunk_size: int = 1000,
                chunk_overlap: int = 200,
                similarity_top_k: int = 5,
                temperature: float = 0.7):
        """
        Initialize the RAG chatbot.
        
        Args:
            collection_name: Name of the vector collection to use
            embedding_model: Model to use for generating embeddings
            llm_model: LLM model to use for generation
            chunk_size: Size of document chunks
            chunk_overlap: Overlap between chunks
            similarity_top_k: Number of similar documents to retrieve
            temperature: Temperature parameter for the LLM
        """
        self.collection_name = collection_name
        self.embedding_model = embedding_model
        self.llm_model = llm_model
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.similarity_top_k = similarity_top_k
        self.temperature = temperature
        
        # Initialize components
        self.embedding_generator = EmbeddingGenerator(model=embedding_model)
        self.vector_db = VectorDatabase(
            collection_name=collection_name,
            embedding_generator=self.embedding_generator
        )
        self.document_processor = DocumentProcessor()
        self.openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        # Conversation history
        self.conversation_history = []
        
        # System prompt template
        self.system_prompt_template = """
        You are a helpful assistant that answers questions based on the provided context from documents.
        If the context doesn't contain the information needed to answer the question, 
        acknowledge that and don't make up information.
        
        When citing information, mention the source document if that information is available.
        Respond in a clear, concise, and helpful manner.
        """
    
    def ingest_documents(self, 
                        directory_path: str,
                        file_extensions: List[str] = ['.txt', '.md'],
                        show_progress: bool = True) -> int:
        """
        Ingest documents from a directory into the vector database.
        
        Args:
            directory_path: Path to directory containing documents
            file_extensions: List of file extensions to include
            show_progress: Whether to show progress bars
            
        Returns:
            Number of chunks ingested
        """
        # Load documents
        print(f"Loading documents from {directory_path}...")
        documents = self.document_processor.load_text_files(directory_path, file_extensions)
        print(f"Loaded {len(documents)} documents")
        
        # Chunk documents
        chunks = []
        if show_progress:
            for doc in tqdm(documents, desc="Chunking documents"):
                chunks.extend(self.document_processor.chunk_document(
                    doc, 
                    chunk_size=self.chunk_size, 
                    chunk_overlap=self.chunk_overlap
                ))
        else:
            for doc in documents:
                chunks.extend(self.document_processor.chunk_document(
                    doc, 
                    chunk_size=self.chunk_size, 
                    chunk_overlap=self.chunk_overlap
                ))
                
        print(f"Created {len(chunks)} chunks")
        
        # Prepare data for vectorization
        texts = [chunk['text'] for chunk in chunks]
        metadatas = [chunk['metadata'] for chunk in chunks]
        
        # Add to vector database
        print("Adding to vector database...")
        ids = self.vector_db.add_documents(texts, metadatas)
        
        print(f"Ingested {len(ids)} chunks into vector database")
        return len(ids)
    
    def format_retrieved_context(self, retrieved_docs: List[Dict[str, Any]]) -> str:
        """Format retrieved documents into a context string."""
        context_parts = []
        
        for i, doc in enumerate(retrieved_docs):
            source_info = f"[Source: {doc['metadata'].get('source', 'Unknown')}]" if 'metadata' in doc else ""
            context_parts.append(f"Document {i+1} {source_info}:\n{doc['text']}\n")
            
        return "\n".join(context_parts)
    
    def answer_question(self, 
                       question: str, 
                       filter_condition: Optional[Dict[str, Any]] = None,
                       use_conversation_history: bool = True,
                       max_history_turns: int = 3) -> Dict[str, Any]:
        """
        Answer a question using RAG.
        
        Args:
            question: User's question
            filter_condition: Optional filter for vector search
            use_conversation_history: Whether to include conversation history
            max_history_turns: Maximum number of previous turns to include
            
        Returns:
            Dictionary with answer and supporting information
        """
        # Find relevant documents
        retrieved_docs = self.vector_db.similarity_search(
            query=question,
            k=self.similarity_top_k,
            filter_condition=filter_condition
        )
        
        # Format context from retrieved documents
        context = self.format_retrieved_context(retrieved_docs)
        
        # Prepare conversation history if needed
        history_text = ""
        if use_conversation_history and self.conversation_history:
            history_turns = self.conversation_history[-max_history_turns:]
            history_formatted = "\n".join([
                f"User: {turn['question']}\nAssistant: {turn['answer']}"
                for turn in history_turns
            ])
            history_text = f"\nPrevious conversation:\n{history_formatted}\n"
        
        # Create prompt for LLM
        user_prompt = f"""
        {history_text}
        
        Context information for answering the user's question:
        {context}
        
        User question: {question}
        
        Answer the question based on the provided context. If the context doesn't contain enough information, 
        acknowledge the limitations of your knowledge.
        """
        
        # Get response from LLM
        response = self.openai_client.chat.completions.create(
            model=self.llm_model,
            messages=[
                {"role": "system", "content": self.system_prompt_template},
                {"role": "user", "content": user_prompt}
            ],
            temperature=self.temperature
        )
        
        answer = response.choices[0].message.content
        
        # Store in conversation history
        self.conversation_history.append({
            "question": question,
            "answer": answer,
            "retrieved_docs": retrieved_docs,
            "timestamp": time.time()
        })
        
        return {
            "answer": answer,
            "retrieved_docs": retrieved_docs,
            "context_used": context,
            "model_used": self.llm_model
        }
    
    def clear_conversation_history(self) -> None:
        """Clear the conversation history."""
        self.conversation_history = []
    
    def update_system_prompt(self, new_prompt: str) -> None:
        """Update the system prompt template."""
        self.system_prompt_template = new_prompt


# Example usage with a simple CLI interface
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="RAG Chatbot for document Q&A")
    parser.add_argument("--ingest", help="Directory path to ingest documents from")
    parser.add_argument("--collection", default="documents", help="Vector collection name")
    parser.add_argument("--interactive", action="store_true", help="Start interactive chat mode")
    
    args = parser.parse_args()
    
    # Initialize chatbot
    chatbot = RAGChatbot(
        collection_name=args.collection,
        llm_model="gpt-4o",
        temperature=0.5
    )
    
    # Ingest documents if specified
    if args.ingest:
        chatbot.ingest_documents(args.ingest)
    
    # Interactive mode
    if args.interactive:
        print("RAG Chatbot is ready! Ask questions about your documents (type 'exit' to quit)")
        print("=" * 80)
        
        while True:
            user_input = input("\nYou: ")
            
            if user_input.lower() in ["exit", "quit"]:
                break
                
            if user_input.lower() == "clear":
                chatbot.clear_conversation_history()
                print("Conversation history cleared")
                continue
                
            print("\nThinking...")
            result = chatbot.answer_question(user_input)
            
            print(f"\nAnswer: {result['answer']}")
            
            # Optionally show sources
            sources = set()
            for doc in result['retrieved_docs']:
                if 'metadata' in doc and 'source' in doc['metadata']:
                    sources.add(doc['metadata']['source'])
            
            if sources:
                print("\nSources:")
                for source in sources:
                    print(f"- {source}")
```

## Practical Exercise: Working with Vector Databases for Document Q&A

Let's create a complete example implementation that brings together all the concepts we've covered. This exercise demonstrates building a web interface for a document-based question answering system using Streamlit:

```python
import os
import streamlit as st
import time
import pandas as pd
from dotenv import load_dotenv
from rag_chatbot import RAGChatbot

# Load environment variables
load_dotenv()

# Set page configuration
st.set_page_config(
    page_title="Document Q&A System",
    page_icon="ðŸ“š",
    layout="wide"
)

# Initialize session state variables
if "chatbot" not in st.session_state:
    st.session_state.chatbot = RAGChatbot(
        collection_name="streamlit_docs",
        llm_model="gpt-4o",
        temperature=0.5
    )

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "documents_ingested" not in st.session_state:
    st.session_state.documents_ingested = False

if "collection_info" not in st.session_state:
    st.session_state.collection_info = None

# Add message to chat history
def add_message(role, content):
    st.session_state.chat_history.append({"role": role, "content": content, "timestamp": time.time()})

# Helper function to ingest documents
def ingest_documents(directory_path):
    with st.spinner("Ingesting documents... This may take a while."):
        num_chunks = st.session_state.chatbot.ingest_documents(directory_path)
        st.session_state.documents_ingested = True
        st.session_state.collection_info = st.session_state.chatbot.vector_db.get_collection_info()
        return num_chunks

# Display chat history
def display_chat_history():
    for message in st.session_state.chat_history:
        with st.chat_message(message["role"]):
            st.write(message["content"])
            # Show timestamp in small text if not the last message
            if message != st.session_state.chat_history[-1]:
                st.caption(f"{time.strftime('%H:%M:%S', time.localtime(message['timestamp']))}")

# Process query and show response
def process_query(query):
    # Add user message
    add_message("user", query)
    
    # Get response from chatbot
    with st.spinner("Thinking..."):
        result = st.session_state.chatbot.answer_question(query)
    
    # Add assistant response
    add_message("assistant", result["answer"])
    
    # Store retrieved docs in session state for inspection
    st.session_state.last_retrieved_docs = result["retrieved_docs"]
    
    return result

# Main app layout
def main():
    st.title("ðŸ“š Document Q&A System with RAG")
    
    # Sidebar for document ingestion and settings
    with st.sidebar:
        st.header("Documents & Settings")
        
        # Document ingestion section
        st.subheader("Document Ingestion")
        doc_dir = st.text_input("Document directory path", "docs")
        
        if st.button("Ingest Documents"):
            if os.path.exists(doc_dir):
                num_chunks = ingest_documents(doc_dir)
                st.success(f"Successfully ingested {num_chunks} document chunks!")
            else:
                st.error(f"Directory not found: {doc_dir}")
        
        # Display collection info if available
        if st.session_state.collection_info:
            st.subheader("Collection Info")
            st.write(f"Collection: {st.session_state.collection_info['name']}")
            st.write(f"Vectors: {st.session_state.collection_info['vectors_count']}")
            st.write(f"Dimensions: {st.session_state.collection_info['dimension']}")
        
        # Settings section
        st.subheader("Settings")
        
        model = st.selectbox(
            "LLM Model",
            ["gpt-4o", "gpt-3.5-turbo", "gpt-4-turbo"],
            index=0
        )
        
        temperature = st.slider(
            "Temperature",
            min_value=0.0,
            max_value=1.0,
            value=0.5,
            step=0.1
        )
        
        top_k = st.slider(
            "Number of relevant chunks",
            min_value=1,
            max_value=10,
            value=5
        )
        
        if st.button("Apply Settings"):
            st.session_state.chatbot.llm_model = model
            st.session_state.chatbot.temperature = temperature
            st.session_state.chatbot.similarity_top_k = top_k
            st.success("Settings applied!")
        
        if st.button("Clear Chat History"):
            st.session_state.chat_history = []
            st.session_state.chatbot.clear_conversation_history()
            st.success("Chat history cleared!")
    
    # Main chat interface
    col1, col2 = st.columns([3, 1])
    
    with col1:
        # Display warning if no documents ingested
        if not st.session_state.documents_ingested:
            st.warning("No documents have been ingested yet. Please ingest documents in the sidebar.")
        
        # Chat interface
        display_chat_history()
        
        # User input
        if user_query := st.chat_input("Ask a question about your documents..."):
            if not st.session_state.documents_ingested:
                with st.chat_message("assistant"):
                    st.write("Please ingest documents first before asking questions.")
                add_message("assistant", "Please ingest documents first before asking questions.")
            else:
                result = process_query(user_query)
    
    # Show sources and context in the sidebar
    with col2:
        st.subheader("Source Documents")
        
        if "last_retrieved_docs" in st.session_state and st.session_state.last_retrieved_docs:
            # Create a dataframe of sources for better display
            sources_data = []
            for i, doc in enumerate(st.session_state.last_retrieved_docs):
                source = doc['metadata'].get('source', 'Unknown')
                similarity = doc['score'] if 'score' in doc else 0
                
                sources_data.append({
                    "Source": source,
                    "Similarity": f"{similarity:.2f}",
                    "Index": i
                })
            
            # Display sources table
            st.dataframe(pd.DataFrame(sources_data), use_container_width=True)
            
            # Allow user to view content of each source
            selected_doc_idx = st.selectbox(
                "View document content",
                range(len(st.session_state.last_retrieved_docs)),
                format_func=lambda x: sources_data[x]["Source"]
            )
            
            if selected_doc_idx is not None:
                selected_doc = st.session_state.last_retrieved_docs[selected_doc_idx]
                st.text_area(
                    "Document content",
                    selected_doc['text'],
                    height=400
                )

if __name__ == "__main__":
    main()
```

To run the application:
1. Create a `.env` file with your API keys:
```
OPENAI_API_KEY=your_openai_api_key
QDRANT_URL=your_qdrant_url_or_localhost
QDRANT_API_KEY=your_qdrant_api_key
```

2. Install dependencies:
```
pip install openai python-dotenv streamlit qdrant-client numpy pandas tqdm
```

3. Create a document directory (e.g., "docs") and add text files

4. Run the Streamlit app:
```
streamlit run rag_app.py
```

## Conclusion

Vector databases represent a transformative technology in the AI landscape, enabling semantic search and retrieval capabilities that dramatically enhance the utility of large language models. By converting textual data into high-dimensional vector representations, these specialized databases allow systems to understand and retrieve information based on meaning rather than keyword matching.

Through the RAG (Retrieval-Augmented Generation) paradigm, we've seen how vector databases can address critical limitations of standalone LLMs by providing access to external knowledge that is:
1. More current than the model's training cutoff
2. Domain-specific and proprietary
3. Factually grounded with clear provenance

The implementation examples demonstrate a complete workflow for creating document-based question answering systems, from embedding generation to vector storage and retrieval, culminating in contextually informed responses. This approach offers several key advantages:

- **Accuracy improvement**: By grounding responses in retrieved information
- **Hallucination reduction**: Through direct access to factual source material
- **Knowledge extension**: Beyond the LLM's internal parameters
- **Source transparency**: With clear attribution to information sources
- **Dynamic information**: Ability to update knowledge without retraining

As vector databases continue to evolve, we can expect improvements in scalability, query performance, and integration capabilities, further enhancing the power of RAG-based systems across a wide range of applications from customer support to research assistance and knowledge management.

The combination of vector databases with LLMs through the RAG architecture represents one of the most practical and immediately valuable applications of modern AI technology, enabling systems that intelligently leverage both parametric and non-parametric knowledge.