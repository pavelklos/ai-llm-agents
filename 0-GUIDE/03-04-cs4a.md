<small>Claude Sonnet 4</small>
# 04. Multi-Agent Orchestration with LangGraph

## Key Terms

**LangGraph**: A revolutionary graph-based framework for building stateful, multi-actor applications with large language models, enabling complex agent orchestration through directed acyclic graphs (DAGs) where nodes represent computational steps and edges define conditional workflow transitions between different AI agents and tools.

**Agent Orchestration**: The systematic coordination and management of multiple specialized AI agents working collaboratively to solve complex problems, where each agent has distinct capabilities, roles, and responsibilities, requiring sophisticated workflow management and state synchronization mechanisms.

**Stateful Workflow Management**: Advanced computational pattern that maintains persistent state information across multiple interaction steps, enabling agents to remember previous decisions, intermediate results, and context throughout complex multi-step problem-solving processes.

**Conditional Routing**: Dynamic decision-making mechanism in agent workflows that determines the next execution path based on current state, agent outputs, or external conditions, allowing for adaptive and intelligent flow control through complex decision trees.

**Graph-Based Execution**: Computational paradigm where workflow logic is represented as nodes (agents/tools) and edges (transitions), enabling parallel execution, conditional branching, loop handling, and sophisticated control flow patterns that traditional linear chains cannot achieve.

**Role-Based Agent Specialization**: Design pattern where individual agents are optimized for specific domain expertise, tasks, or capabilities, creating a collaborative ecosystem where specialized knowledge and skills are distributed across multiple focused components.

**Workflow State Management**: Comprehensive system for tracking, persisting, and synchronizing shared state information across multiple agents, ensuring data consistency, conflict resolution, and proper information flow throughout complex multi-agent interactions.

**Inter-Agent Communication Protocol**: Standardized mechanisms and data structures that enable agents to exchange information, coordinate actions, pass intermediate results, and maintain coherent collaboration throughout distributed problem-solving workflows.

## Advanced Multi-Agent Orchestration with LangGraph

LangGraph represents a paradigm shift from linear chain-based AI workflows to sophisticated graph-based orchestration systems that enable multiple specialized agents to collaborate intelligently on complex tasks requiring diverse expertise, dynamic decision-making, and adaptive workflow management.

### Comprehensive LangGraph Multi-Agent Framework

````python
import asyncio
import json
import logging
import os
import time
import warnings
from typing import Dict, List, Any, Optional, Union, Callable, Tuple, TypedDict, Annotated
from dataclasses import dataclass, field
from datetime import datetime, timezone
from pathlib import Path
import uuid
import re
from enum import Enum
import threading
from concurrent.futures import ThreadPoolExecutor
import pickle
import hashlib

# LangGraph and LangChain imports
from langgraph.graph import Graph, StateGraph, END, START
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolExecutor, ToolInvocation
from langgraph.checkpoint.memory import MemorySaver
from langgraph.checkpoint.sqlite import SqliteSaver

# LangChain components
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage
from langchain_core.tools import BaseTool, tool
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import JsonOutputParser, PydanticOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field

# Additional LangChain tools and utilities
from langchain_community.tools import DuckDuckGoSearchRun, WikipediaQueryRun
from langchain_community.utilities import WikipediaAPIWrapper
from langchain_experimental.tools import PythonREPLTool
from langchain.agents import initialize_agent, AgentType
from langchain.memory import ConversationBufferMemory, ConversationSummaryBufferMemory
from langchain.callbacks import get_openai_callback

# Data processing and analysis
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import seaborn as sns

# Web scraping and API tools
import requests
from bs4 import BeautifulSoup
import yfinance as yf
from datetime import timedelta

# Performance monitoring
import structlog
from prometheus_client import Counter, Histogram, Gauge
import psutil
import memory_profiler

# Utilities
from dotenv import load_dotenv
from collections import defaultdict, deque
import operator
from functools import reduce

load_dotenv()

warnings.filterwarnings("ignore", category=DeprecationWarning)

# Setup structured logging
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    wrapper_class=structlog.stdlib.BoundLogger,
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()

# Metrics
agent_operations = Counter('agent_operations_total', 'Total agent operations', ['agent_type', 'operation', 'status'])
workflow_execution_time = Histogram('workflow_execution_time_seconds', 'Time spent on workflow execution')
active_workflows = Gauge('active_workflows_count', 'Number of active workflows')
agent_message_count = Counter('agent_messages_total', 'Total messages processed by agents', ['agent_type'])

class AgentRole(Enum):
    """Defined roles for specialized agents"""
    COORDINATOR = "coordinator"
    RESEARCHER = "researcher"
    ANALYST = "analyst"
    WRITER = "writer"
    CRITIC = "critic"
    PLANNER = "planner"
    EXECUTOR = "executor"
    VALIDATOR = "validator"
    SYNTHESIZER = "synthesizer"

class WorkflowState(TypedDict):
    """Comprehensive state structure for multi-agent workflows"""
    messages: Annotated[List[BaseMessage], add_messages]
    current_task: str
    workflow_id: str
    step_count: int
    agent_outputs: Dict[str, Any]
    shared_context: Dict[str, Any]
    execution_path: List[str]
    error_log: List[str]
    performance_metrics: Dict[str, float]
    final_result: Optional[str]

class AgentDecision(BaseModel):
    """Structured decision output from agents"""
    decision: str = Field(description="The decision made by the agent")
    confidence: float = Field(description="Confidence score between 0 and 1")
    reasoning: str = Field(description="Explanation of the decision")
    next_action: Optional[str] = Field(description="Suggested next action")
    required_tools: List[str] = Field(description="Tools needed for next steps")

class TaskComplexity(BaseModel):
    """Assessment of task complexity"""
    complexity_score: float = Field(description="Complexity score between 0 and 1")
    required_agents: List[str] = Field(description="List of required agent types")
    estimated_steps: int = Field(description="Estimated number of steps")
    risk_factors: List[str] = Field(description="Potential risk factors")

@dataclass
class AgentConfig:
    """Configuration for individual agents"""
    role: AgentRole
    name: str
    system_prompt: str
    tools: List[str]
    temperature: float = 0.1
    max_tokens: int = 1000
    specialization_keywords: List[str] = field(default_factory=list)
    interaction_patterns: Dict[str, Any] = field(default_factory=dict)

class AdvancedLangGraphAgent:
    """Advanced individual agent with specialized capabilities"""
    
    def __init__(self, config: AgentConfig, model_name: str = "gpt-4"):
        self.config = config
        self.llm = ChatOpenAI(
            model_name=model_name,
            temperature=config.temperature,
            max_tokens=config.max_tokens,
            openai_api_key=os.getenv('OPENAI_API_KEY')
        )
        
        # Initialize tools
        self.available_tools = self._initialize_tools()
        self.tool_executor = ToolExecutor(self.available_tools)
        
        # Agent-specific memory and context
        self.local_memory = []
        self.expertise_context = {}
        
        # Performance tracking
        self.operation_count = 0
        self.total_processing_time = 0.0
        
        logger.info(f"Initialized agent: {self.config.name} ({self.config.role.value})")
    
    def _initialize_tools(self) -> List[BaseTool]:
        """Initialize tools based on agent configuration"""
        
        tool_registry = {
            "search": DuckDuckGoSearchRun(),
            "wikipedia": WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper()),
            "python": PythonREPLTool(),
            "web_scraper": self._create_web_scraper_tool(),
            "data_analyzer": self._create_data_analyzer_tool(),
            "financial_data": self._create_financial_tool(),
            "text_analyzer": self._create_text_analyzer_tool()
        }
        
        # Return tools specified in configuration
        selected_tools = []
        for tool_name in self.config.tools:
            if tool_name in tool_registry:
                selected_tools.append(tool_registry[tool_name])
        
        return selected_tools
    
    def _create_web_scraper_tool(self) -> BaseTool:
        """Create web scraping tool"""
        
        @tool
        def web_scraper(url: str) -> str:
            """Scrape content from a web URL and return text content."""
            try:
                response = requests.get(url, timeout=10)
                response.raise_for_status()
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Remove script and style elements
                for script in soup(["script", "style"]):
                    script.decompose()
                
                # Get text content
                text = soup.get_text()
                
                # Clean up text
                lines = (line.strip() for line in text.splitlines())
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                text = ' '.join(chunk for chunk in chunks if chunk)
                
                return text[:2000]  # Limit length
                
            except Exception as e:
                return f"Error scraping {url}: {str(e)}"
        
        return web_scraper
    
    def _create_data_analyzer_tool(self) -> BaseTool:
        """Create data analysis tool"""
        
        @tool
        def data_analyzer(data_description: str, analysis_type: str) -> str:
            """Analyze data based on description and analysis type."""
            
            try:
                # Generate sample data based on description
                if "sales" in data_description.lower():
                    data = pd.DataFrame({
                        'month': pd.date_range('2023-01-01', periods=12, freq='M'),
                        'sales': np.random.normal(10000, 2000, 12),
                        'customers': np.random.normal(500, 100, 12)
                    })
                elif "stock" in data_description.lower():
                    data = pd.DataFrame({
                        'date': pd.date_range('2023-01-01', periods=30, freq='D'),
                        'price': np.random.normal(100, 10, 30),
                        'volume': np.random.normal(1000000, 200000, 30)
                    })
                else:
                    data = pd.DataFrame({
                        'value': np.random.normal(0, 1, 100)
                    })
                
                # Perform analysis
                if analysis_type == "summary":
                    return str(data.describe())
                elif analysis_type == "correlation":
                    return str(data.corr())
                elif analysis_type == "trend":
                    return f"Data shows trend: {data.iloc[:, -1].diff().mean():.2f} average change"
                else:
                    return str(data.head())
                    
            except Exception as e:
                return f"Analysis error: {str(e)}"
        
        return data_analyzer
    
    def _create_financial_tool(self) -> BaseTool:
        """Create financial data tool"""
        
        @tool
        def financial_data(symbol: str, period: str = "1mo") -> str:
            """Get financial data for a stock symbol."""
            try:
                ticker = yf.Ticker(symbol)
                hist = ticker.history(period=period)
                
                if hist.empty:
                    return f"No data found for symbol: {symbol}"
                
                current_price = hist['Close'][-1]
                price_change = hist['Close'][-1] - hist['Close'][0]
                percent_change = (price_change / hist['Close'][0]) * 100
                
                return f"Symbol: {symbol}, Current Price: ${current_price:.2f}, Change: {price_change:.2f} ({percent_change:.1f}%)"
                
            except Exception as e:
                return f"Error getting financial data: {str(e)}"
        
        return financial_data
    
    def _create_text_analyzer_tool(self) -> BaseTool:
        """Create text analysis tool"""
        
        @tool
        def text_analyzer(text: str, analysis_type: str) -> str:
            """Analyze text for various properties."""
            try:
                if analysis_type == "sentiment":
                    # Simple sentiment analysis
                    positive_words = ["good", "great", "excellent", "positive", "amazing", "wonderful"]
                    negative_words = ["bad", "terrible", "awful", "negative", "horrible", "worst"]
                    
                    text_lower = text.lower()
                    pos_count = sum(1 for word in positive_words if word in text_lower)
                    neg_count = sum(1 for word in negative_words if word in text_lower)
                    
                    if pos_count > neg_count:
                        return "Positive sentiment detected"
                    elif neg_count > pos_count:
                        return "Negative sentiment detected"
                    else:
                        return "Neutral sentiment detected"
                        
                elif analysis_type == "keywords":
                    words = re.findall(r'\w+', text.lower())
                    word_freq = {}
                    for word in words:
                        if len(word) > 3:  # Only words longer than 3 chars
                            word_freq[word] = word_freq.get(word, 0) + 1
                    
                    top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]
                    return f"Top keywords: {', '.join([word for word, count in top_words])}"
                    
                elif analysis_type == "statistics":
                    words = len(text.split())
                    chars = len(text)
                    sentences = len(re.split(r'[.!?]+', text))
                    return f"Words: {words}, Characters: {chars}, Sentences: {sentences}"
                    
                else:
                    return f"Text length: {len(text)} characters"
                    
            except Exception as e:
                return f"Text analysis error: {str(e)}"
        
        return text_analyzer
    
    async def process_message(self, state: WorkflowState, task_context: Dict[str, Any]) -> Dict[str, Any]:
        """Process a message with agent-specific logic"""
        
        start_time = time.time()
        agent_message_count.labels(agent_type=self.config.role.value).inc()
        
        try:
            # Create agent-specific prompt
            system_message = SystemMessage(content=self.config.system_prompt)
            
            # Add context about current task and agent role
            context_message = HumanMessage(content=f"""
Current Task: {state['current_task']}
Agent Role: {self.config.role.value}
Workflow Step: {state['step_count']}

Task Context: {json.dumps(task_context, indent=2)}

Previous Agent Outputs: {json.dumps(state['agent_outputs'], indent=2)}

Please provide your analysis, recommendations, and any required actions based on your role and expertise.
""")
            
            # Prepare messages for LLM
            messages = [system_message, context_message] + state['messages'][-3:]  # Keep recent context
            
            # Get agent response
            with get_openai_callback() as cb:
                response = await self.llm.ainvoke(messages)
            
            # Process response and determine if tools are needed
            response_content = response.content
            
            # Check if agent wants to use tools
            tool_calls = []
            if any(tool_name in response_content.lower() for tool_name in self.config.tools):
                tool_calls = await self._process_tool_usage(response_content, task_context)
            
            # Update local memory
            self.local_memory.append({
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "task": state['current_task'],
                "response": response_content,
                "tool_calls": tool_calls,
                "context": task_context
            })
            
            # Prepare agent output
            agent_output = {
                "agent_name": self.config.name,
                "agent_role": self.config.role.value,
                "response": response_content,
                "tool_results": tool_calls,
                "confidence": self._calculate_confidence(response_content),
                "processing_time": time.time() - start_time,
                "token_usage": {
                    "prompt_tokens": cb.prompt_tokens,
                    "completion_tokens": cb.completion_tokens,
                    "total_cost": cb.total_cost
                }
            }
            
            # Update performance metrics
            self.operation_count += 1
            self.total_processing_time += time.time() - start_time
            
            # Log metrics
            agent_operations.labels(
                agent_type=self.config.role.value,
                operation="process_message",
                status="success"
            ).inc()
            
            logger.info(f"Agent {self.config.name} completed processing in {time.time() - start_time:.2f}s")
            
            return agent_output
            
        except Exception as e:
            agent_operations.labels(
                agent_type=self.config.role.value,
                operation="process_message",
                status="error"
            ).inc()
            
            logger.error(f"Error in agent {self.config.name}: {e}")
            
            return {
                "agent_name": self.config.name,
                "agent_role": self.config.role.value,
                "error": str(e),
                "processing_time": time.time() - start_time
            }
    
    async def _process_tool_usage(self, response_content: str, context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Process tool usage based on agent response"""
        
        tool_results = []
        
        # Simple tool invocation logic based on response content
        if "search" in response_content.lower() and "search" in self.config.tools:
            search_query = self._extract_search_query(response_content, context)
            if search_query:
                try:
                    search_tool = next(tool for tool in self.available_tools if tool.name == "duckduckgo_search")
                    result = search_tool.run(search_query)
                    tool_results.append({
                        "tool": "search",
                        "query": search_query,
                        "result": result[:500]  # Limit result length
                    })
                except Exception as e:
                    tool_results.append({
                        "tool": "search",
                        "error": str(e)
                    })
        
        if "analyze" in response_content.lower() and "data_analyzer" in self.config.tools:
            analysis_request = self._extract_analysis_request(response_content, context)
            if analysis_request:
                try:
                    data_tool = next(tool for tool in self.available_tools if tool.name == "data_analyzer")
                    result = data_tool.run(analysis_request)
                    tool_results.append({
                        "tool": "data_analyzer",
                        "request": analysis_request,
                        "result": result
                    })
                except Exception as e:
                    tool_results.append({
                        "tool": "data_analyzer",
                        "error": str(e)
                    })
        
        return tool_results
    
    def _extract_search_query(self, response: str, context: Dict[str, Any]) -> Optional[str]:
        """Extract search query from agent response"""
        
        # Simple extraction logic
        search_patterns = [
            r"search for [\"']([^\"']+)[\"']",
            r"search: ([^\n]+)",
            r"query: ([^\n]+)"
        ]
        
        for pattern in search_patterns:
            match = re.search(pattern, response, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        # Fallback to task context
        if "search_query" in context:
            return context["search_query"]
        
        return None
    
    def _extract_analysis_request(self, response: str, context: Dict[str, Any]) -> Optional[str]:
        """Extract analysis request from agent response"""
        
        # Simple extraction logic for data analysis
        if "sales" in response.lower():
            return "sales data summary"
        elif "financial" in response.lower():
            return "financial analysis trend"
        elif "correlation" in response.lower():
            return "data correlation"
        
        return "general summary"
    
    def _calculate_confidence(self, response: str) -> float:
        """Calculate confidence score based on response characteristics"""
        
        confidence_indicators = {
            "high": ["definitely", "certainly", "confident", "sure", "clear"],
            "medium": ["likely", "probably", "seems", "appears", "suggests"],
            "low": ["maybe", "possibly", "uncertain", "unclear", "might"]
        }
        
        response_lower = response.lower()
        
        high_count = sum(1 for word in confidence_indicators["high"] if word in response_lower)
        medium_count = sum(1 for word in confidence_indicators["medium"] if word in response_lower)
        low_count = sum(1 for word in confidence_indicators["low"] if word in response_lower)
        
        if high_count > medium_count and high_count > low_count:
            return 0.9
        elif medium_count > low_count:
            return 0.7
        elif low_count > 0:
            return 0.4
        else:
            return 0.6  # Default confidence
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """Get agent performance statistics"""
        
        avg_processing_time = (
            self.total_processing_time / self.operation_count 
            if self.operation_count > 0 else 0.0
        )
        
        return {
            "agent_name": self.config.name,
            "agent_role": self.config.role.value,
            "operation_count": self.operation_count,
            "total_processing_time": self.total_processing_time,
            "average_processing_time": avg_processing_time,
            "tools_available": len(self.available_tools),
            "memory_entries": len(self.local_memory)
        }

class MultiAgentOrchestrator:
    """Advanced multi-agent orchestrator using LangGraph"""
    
    def __init__(self, agents: List[AdvancedLangGraphAgent]):
        self.agents = {agent.config.name: agent for agent in agents}
        self.agent_roles = {agent.config.role: agent for agent in agents}
        
        # Initialize graph and workflow components
        self.workflow_graph = None
        self.checkpointer = MemorySaver()
        
        # Workflow statistics
        self.workflow_stats = {
            "total_workflows": 0,
            "successful_workflows": 0,
            "average_execution_time": 0.0,
            "agent_utilization": defaultdict(int)
        }
        
        self._build_workflow_graph()
        
        logger.info(f"Initialized orchestrator with {len(self.agents)} agents")
    
    def _build_workflow_graph(self):
        """Build the LangGraph workflow"""
        
        # Create state graph
        workflow = StateGraph(WorkflowState)
        
        # Add nodes for each agent
        workflow.add_node("start", self._start_workflow)
        workflow.add_node("coordinator", self._coordinator_node)
        workflow.add_node("researcher", self._researcher_node)
        workflow.add_node("analyst", self._analyst_node)
        workflow.add_node("writer", self._writer_node)
        workflow.add_node("critic", self._critic_node)
        workflow.add_node("synthesizer", self._synthesizer_node)
        workflow.add_node("end", self._end_workflow)
        
        # Define workflow edges and routing logic
        workflow.set_entry_point("start")
        
        # Start to coordinator
        workflow.add_edge("start", "coordinator")
        
        # Coordinator decides next steps
        workflow.add_conditional_edges(
            "coordinator",
            self._coordinator_routing,
            {
                "research": "researcher",
                "analyze": "analyst",
                "write": "writer",
                "end": "end"
            }
        )
        
        # Researcher to analyst
        workflow.add_edge("researcher", "analyst")
        
        # Analyst to writer
        workflow.add_edge("analyst", "writer")
        
        # Writer to critic for review
        workflow.add_edge("writer", "critic")
        
        # Critic decides: revise or synthesize
        workflow.add_conditional_edges(
            "critic",
            self._critic_routing,
            {
                "revise": "writer",
                "synthesize": "synthesizer",
                "end": "end"
            }
        )
        
        # Synthesizer to end
        workflow.add_edge("synthesizer", "end")
        
        # Compile the graph
        self.workflow_graph = workflow.compile(checkpointer=self.checkpointer)
        
        logger.info("Workflow graph built successfully")
    
    async def _start_workflow(self, state: WorkflowState) -> WorkflowState:
        """Initialize workflow state"""
        
        state["step_count"] = 0
        state["agent_outputs"] = {}
        state["execution_path"] = ["start"]
        state["error_log"] = []
        state["performance_metrics"] = {}
        
        logger.info(f"Started workflow: {state['workflow_id']}")
        
        return state
    
    async def _coordinator_node(self, state: WorkflowState) -> WorkflowState:
        """Coordinator agent node"""
        
        if AgentRole.COORDINATOR in self.agent_roles:
            coordinator = self.agent_roles[AgentRole.COORDINATOR]
            
            task_context = {
                "workflow_stage": "coordination",
                "available_agents": list(self.agents.keys()),
                "task_complexity": self._assess_task_complexity(state["current_task"])
            }
            
            result = await coordinator.process_message(state, task_context)
            state["agent_outputs"]["coordinator"] = result
            state["execution_path"].append("coordinator")
            
            self.workflow_stats["agent_utilization"]["coordinator"] += 1
        
        state["step_count"] += 1
        return state
    
    async def _researcher_node(self, state: WorkflowState) -> WorkflowState:
        """Researcher agent node"""
        
        if AgentRole.RESEARCHER in self.agent_roles:
            researcher = self.agent_roles[AgentRole.RESEARCHER]
            
            task_context = {
                "workflow_stage": "research",
                "research_focus": self._extract_research_focus(state["current_task"]),
                "previous_findings": state["agent_outputs"].get("coordinator", {})
            }
            
            result = await researcher.process_message(state, task_context)
            state["agent_outputs"]["researcher"] = result
            state["execution_path"].append("researcher")
            
            self.workflow_stats["agent_utilization"]["researcher"] += 1
        
        state["step_count"] += 1
        return state
    
    async def _analyst_node(self, state: WorkflowState) -> WorkflowState:
        """Analyst agent node"""
        
        if AgentRole.ANALYST in self.agent_roles:
            analyst = self.agent_roles[AgentRole.ANALYST]
            
            task_context = {
                "workflow_stage": "analysis",
                "research_data": state["agent_outputs"].get("researcher", {}),
                "analysis_requirements": self._determine_analysis_requirements(state["current_task"])
            }
            
            result = await analyst.process_message(state, task_context)
            state["agent_outputs"]["analyst"] = result
            state["execution_path"].append("analyst")
            
            self.workflow_stats["agent_utilization"]["analyst"] += 1
        
        state["step_count"] += 1
        return state
    
    async def _writer_node(self, state: WorkflowState) -> WorkflowState:
        """Writer agent node"""
        
        if AgentRole.WRITER in self.agent_roles:
            writer = self.agent_roles[AgentRole.WRITER]
            
            task_context = {
                "workflow_stage": "writing",
                "analysis_results": state["agent_outputs"].get("analyst", {}),
                "writing_requirements": self._determine_writing_requirements(state["current_task"])
            }
            
            result = await writer.process_message(state, task_context)
            state["agent_outputs"]["writer"] = result
            state["execution_path"].append("writer")
            
            self.workflow_stats["agent_utilization"]["writer"] += 1
        
        state["step_count"] += 1
        return state
    
    async def _critic_node(self, state: WorkflowState) -> WorkflowState:
        """Critic agent node"""
        
        if AgentRole.CRITIC in self.agent_roles:
            critic = self.agent_roles[AgentRole.CRITIC]
            
            task_context = {
                "workflow_stage": "criticism",
                "written_content": state["agent_outputs"].get("writer", {}),
                "quality_criteria": self._get_quality_criteria(state["current_task"])
            }
            
            result = await critic.process_message(state, task_context)
            state["agent_outputs"]["critic"] = result
            state["execution_path"].append("critic")
            
            self.workflow_stats["agent_utilization"]["critic"] += 1
        
        state["step_count"] += 1
        return state
    
    async def _synthesizer_node(self, state: WorkflowState) -> WorkflowState:
        """Synthesizer agent node"""
        
        if AgentRole.SYNTHESIZER in self.agent_roles:
            synthesizer = self.agent_roles[AgentRole.SYNTHESIZER]
            
            task_context = {
                "workflow_stage": "synthesis",
                "all_outputs": state["agent_outputs"],
                "synthesis_requirements": "Combine all agent outputs into final result"
            }
            
            result = await synthesizer.process_message(state, task_context)
            state["agent_outputs"]["synthesizer"] = result
            state["execution_path"].append("synthesizer")
            state["final_result"] = result.get("response", "")
            
            self.workflow_stats["agent_utilization"]["synthesizer"] += 1
        
        state["step_count"] += 1
        return state
    
    async def _end_workflow(self, state: WorkflowState) -> WorkflowState:
        """End workflow and finalize results"""
        
        state["execution_path"].append("end")
        
        # Calculate performance metrics
        total_processing_time = sum(
            output.get("processing_time", 0) 
            for output in state["agent_outputs"].values()
            if isinstance(output, dict)
        )
        
        state["performance_metrics"] = {
            "total_steps": state["step_count"],
            "total_processing_time": total_processing_time,
            "agents_involved": len(state["agent_outputs"]),
            "execution_path": state["execution_path"]
        }
        
        logger.info(f"Completed workflow: {state['workflow_id']} in {state['step_count']} steps")
        
        return state
    
    def _coordinator_routing(self, state: WorkflowState) -> str:
        """Route decisions from coordinator"""
        
        coordinator_output = state["agent_outputs"].get("coordinator", {})
        response = coordinator_output.get("response", "").lower()
        
        if "research" in response or "search" in response:
            return "research"
        elif "analyze" in response or "analysis" in response:
            return "analyze"
        elif "write" in response or "generate" in response:
            return "write"
        elif state["step_count"] > 10:  # Prevent infinite loops
            return "end"
        else:
            return "research"  # Default to research
    
    def _critic_routing(self, state: WorkflowState) -> str:
        """Route decisions from critic"""
        
        critic_output = state["agent_outputs"].get("critic", {})
        response = critic_output.get("response", "").lower()
        confidence = critic_output.get("confidence", 0.5)
        
        if "revise" in response or "improve" in response or confidence < 0.6:
            return "revise"
        elif "synthesize" in response or "final" in response:
            return "synthesize"
        elif state["step_count"] > 15:  # Prevent infinite loops
            return "end"
        else:
            return "synthesize"  # Default to synthesis
    
    def _assess_task_complexity(self, task: str) -> Dict[str, Any]:
        """Assess task complexity for coordinator"""
        
        complexity_indicators = {
            "high": ["analyze", "research", "comprehensive", "detailed", "complex"],
            "medium": ["explain", "describe", "summarize", "compare"],
            "low": ["list", "simple", "basic", "quick"]
        }
        
        task_lower = task.lower()
        
        high_count = sum(1 for word in complexity_indicators["high"] if word in task_lower)
        medium_count = sum(1 for word in complexity_indicators["medium"] if word in task_lower)
        low_count = sum(1 for word in complexity_indicators["low"] if word in task_lower)
        
        if high_count > 0:
            complexity = "high"
            score = 0.8
        elif medium_count > 0:
            complexity = "medium"
            score = 0.5
        else:
            complexity = "low"
            score = 0.2
        
        return {
            "complexity": complexity,
            "score": score,
            "indicators": {
                "high": high_count,
                "medium": medium_count,
                "low": low_count
            }
        }
    
    def _extract_research_focus(self, task: str) -> str:
        """Extract research focus from task"""
        
        research_keywords = {
            "technology": ["AI", "machine learning", "software", "technology", "programming"],
            "business": ["market", "business", "strategy", "finance", "economics"],
            "science": ["research", "study", "scientific", "analysis", "data"],
            "general": ["information", "facts", "details", "overview"]
        }
        
        task_lower = task.lower()
        
        for category, keywords in research_keywords.items():
            if any(keyword.lower() in task_lower for keyword in keywords):
                return category
        
        return "general"
    
    def _determine_analysis_requirements(self, task: str) -> List[str]:
        """Determine what type of analysis is needed"""
        
        requirements = []
        task_lower = task.lower()
        
        if any(word in task_lower for word in ["compare", "contrast", "difference"]):
            requirements.append("comparative_analysis")
        
        if any(word in task_lower for word in ["trend", "pattern", "change"]):
            requirements.append("trend_analysis")
        
        if any(word in task_lower for word in ["pros", "cons", "advantage", "disadvantage"]):
            requirements.append("pros_cons_analysis")
        
        if any(word in task_lower for word in ["impact", "effect", "consequence"]):
            requirements.append("impact_analysis")
        
        if not requirements:
            requirements.append("general_analysis")
        
        return requirements
    
    def _determine_writing_requirements(self, task: str) -> Dict[str, Any]:
        """Determine writing style and format requirements"""
        
        task_lower = task.lower()
        
        requirements = {
            "style": "informative",
            "format": "structured",
            "length": "medium",
            "audience": "general"
        }
        
        # Determine style
        if any(word in task_lower for word in ["formal", "academic", "professional"]):
            requirements["style"] = "formal"
        elif any(word in task_lower for word in ["casual", "friendly", "conversational"]):
            requirements["style"] = "casual"
        
        # Determine format
        if any(word in task_lower for word in ["report", "document", "detailed"]):
            requirements["format"] = "report"
        elif any(word in task_lower for word in ["summary", "brief", "overview"]):
            requirements["format"] = "summary"
        elif any(word in task_lower for word in ["list", "bullet", "points"]):
            requirements["format"] = "list"
        
        # Determine length
        if any(word in task_lower for word in ["detailed", "comprehensive", "thorough"]):
            requirements["length"] = "long"
        elif any(word in task_lower for word in ["brief", "short", "quick"]):
            requirements["length"] = "short"
        
        return requirements
    
    def _get_quality_criteria(self, task: str) -> List[str]:
        """Get quality criteria for critic evaluation"""
        
        criteria = ["accuracy", "completeness", "clarity"]
        task_lower = task.lower()
        
        if "research" in task_lower:
            criteria.extend(["source_credibility", "factual_accuracy"])
        
        if "analysis" in task_lower:
            criteria.extend(["logical_reasoning", "evidence_support"])
        
        if "creative" in task_lower:
            criteria.extend(["originality", "engagement"])
        
        if "technical" in task_lower:
            criteria.extend(["technical_accuracy", "detail_level"])
        
        return criteria
    
    async def execute_workflow(self, task: str, workflow_id: Optional[str] = None) -> Dict[str, Any]:
        """Execute multi-agent workflow for given task"""
        
        if not workflow_id:
            workflow_id = str(uuid.uuid4())
        
        start_time = time.time()
        active_workflows.inc()
        self.workflow_stats["total_workflows"] += 1
        
        try:
            # Initialize workflow state
            initial_state = WorkflowState(
                messages=[HumanMessage(content=task)],
                current_task=task,
                workflow_id=workflow_id,
                step_count=0,
                agent_outputs={},
                shared_context={},
                execution_path=[],
                error_log=[],
                performance_metrics={},
                final_result=None
            )
            
            # Execute workflow
            config = {"configurable": {"thread_id": workflow_id}}
            final_state = await self.workflow_graph.ainvoke(initial_state, config)
            
            # Calculate metrics
            execution_time = time.time() - start_time
            
            # Update statistics
            self.workflow_stats["successful_workflows"] += 1
            self._update_average_execution_time(execution_time)
            
            # Log metrics
            workflow_execution_time.observe(execution_time)
            
            # Prepare result
            result = {
                "workflow_id": workflow_id,
                "task": task,
                "final_result": final_state.get("final_result", ""),
                "execution_path": final_state.get("execution_path", []),
                "agent_outputs": final_state.get("agent_outputs", {}),
                "performance_metrics": final_state.get("performance_metrics", {}),
                "execution_time": execution_time,
                "step_count": final_state.get("step_count", 0),
                "success": True
            }
            
            logger.info(f"Workflow {workflow_id} completed successfully in {execution_time:.2f}s")
            
            return result
            
        except Exception as e:
            logger.error(f"Workflow {workflow_id} failed: {e}")
            
            return {
                "workflow_id": workflow_id,
                "task": task,
                "error": str(e),
                "execution_time": time.time() - start_time,
                "success": False
            }
        finally:
            active_workflows.dec()
    
    def _update_average_execution_time(self, execution_time: float):
        """Update average execution time"""
        
        total_workflows = self.workflow_stats["total_workflows"]
        current_avg = self.workflow_stats["average_execution_time"]
        
        new_avg = ((current_avg * (total_workflows - 1)) + execution_time) / total_workflows
        self.workflow_stats["average_execution_time"] = new_avg
    
    def get_orchestrator_statistics(self) -> Dict[str, Any]:
        """Get comprehensive orchestrator statistics"""
        
        agent_stats = {
            name: agent.get_performance_stats() 
            for name, agent in self.agents.items()
        }
        
        return {
            "workflow_statistics": self.workflow_stats,
            "agent_statistics": agent_stats,
            "total_agents": len(self.agents),
            "available_roles": [role.value for role in self.agent_roles.keys()],
            "graph_nodes": len(self.workflow_graph.nodes) if self.workflow_graph else 0
        }

# Demonstration and testing functions
async def comprehensive_langgraph_demonstration():
    """Comprehensive demonstration of LangGraph multi-agent system"""
    
    logger.info("=== Comprehensive LangGraph Multi-Agent Demonstration ===")
    
    # Define agent configurations
    agent_configs = [
        AgentConfig(
            role=AgentRole.COORDINATOR,
            name="Project_Coordinator",
            system_prompt="""You are a project coordinator responsible for analyzing tasks, determining workflow requirements, and directing other agents. Your role is to:
1. Assess task complexity and requirements
2. Determine which agents are needed
3. Coordinate the overall workflow
4. Ensure efficient task completion

Analyze the given task and provide clear direction for the next steps.""",
            tools=["search", "text_analyzer"],
            temperature=0.1
        ),
        
        AgentConfig(
            role=AgentRole.RESEARCHER,
            name="Information_Researcher",
            system_prompt="""You are a research specialist focused on gathering comprehensive information. Your role is to:
1. Conduct thorough research on given topics
2. Find reliable and current information
3. Synthesize findings from multiple sources
4. Provide well-structured research summaries

Use available tools to gather the most relevant and accurate information.""",
            tools=["search", "wikipedia", "web_scraper"],
            temperature=0.2
        ),
        
        AgentConfig(
            role=AgentRole.ANALYST,
            name="Data_Analyst",
            system_prompt="""You are a data analyst specializing in information analysis and insight generation. Your role is to:
1. Analyze research findings and data
2. Identify patterns, trends, and key insights
3. Perform comparative analysis when needed
4. Generate actionable conclusions

Focus on providing clear, evidence-based analysis with supporting data.""",
            tools=["data_analyzer", "financial_data", "python", "text_analyzer"],
            temperature=0.1
        ),
        
        AgentConfig(
            role=AgentRole.WRITER,
            name="Content_Writer",
            system_prompt="""You are a content writer responsible for creating clear, engaging, and well-structured content. Your role is to:
1. Transform analysis and research into readable content
2. Adapt writing style to audience and purpose
3. Ensure logical flow and structure
4. Create compelling and informative text

Write content that is accurate, engaging, and meets the specified requirements.""",
            tools=["text_analyzer"],
            temperature=0.3
        ),
        
        AgentConfig(
            role=AgentRole.CRITIC,
            name="Quality_Critic",
            system_prompt="""You are a quality critic responsible for evaluating and improving work quality. Your role is to:
1. Review content for accuracy, completeness, and clarity
2. Identify areas for improvement
3. Ensure quality standards are met
4. Recommend revisions or approve final output

Provide constructive feedback and clear recommendations for improvement.""",
            tools=["text_analyzer"],
            temperature=0.1
        ),
        
        AgentConfig(
            role=AgentRole.SYNTHESIZER,
            name="Result_Synthesizer",
            system_prompt="""You are a synthesizer responsible for combining all agent outputs into a final comprehensive result. Your role is to:
1. Integrate insights from all previous agents
2. Create a cohesive final response
3. Ensure all requirements are addressed
4. Provide a polished final deliverable

Synthesize all information into a complete, well-organized final result.""",
            tools=["text_analyzer"],
            temperature=0.2
        )
    ]
    
    # Initialize agents
    logger.info("1. Initializing Multi-Agent System")
    agents = []
    
    for config in agent_configs:
        agent = AdvancedLangGraphAgent(config)
        agents.append(agent)
    
    # Initialize orchestrator
    orchestrator = MultiAgentOrchestrator(agents)
    
    # Test tasks of varying complexity
    logger.info("2. Testing Multi-Agent Workflows")
    
    test_tasks = [
        {
            "task": "Analyze the current state of artificial intelligence in healthcare and provide recommendations for implementation",
            "description": "Complex research and analysis task",
            "expected_agents": ["coordinator", "researcher", "analyst", "writer", "critic", "synthesizer"]
        },
        {
            "task": "Compare the performance of different machine learning frameworks for deep learning applications",
            "description": "Comparative analysis task",
            "expected_agents": ["coordinator", "researcher", "analyst", "writer", "critic"]
        },
        {
            "task": "Create a comprehensive guide for implementing RAG systems in enterprise applications",
            "description": "Technical writing task",
            "expected_agents": ["coordinator", "researcher", "writer", "critic", "synthesizer"]
        },
        {
            "task": "Evaluate the impact of large language models on software development practices",
            "description": "Impact analysis task",
            "expected_agents": ["coordinator", "researcher", "analyst", "writer"]
        }
    ]
    
    workflow_results = []
    
    for i, test_case in enumerate(test_tasks):
        logger.info(f"Executing workflow {i+1}: {test_case['description']}")
        
        result = await orchestrator.execute_workflow(
            task=test_case["task"],
            workflow_id=f"demo_workflow_{i+1}"
        )
        
        workflow_results.append({
            "test_case": test_case,
            "result": result
        })
        
        logger.info(f"Workflow {i+1} completed: {result['success']}")
        if result['success']:
            logger.info(f"Execution time: {result['execution_time']:.2f}s")
            logger.info(f"Steps: {result['step_count']}")
            logger.info(f"Agents involved: {len(result['agent_outputs'])}")
    
    # Performance analysis
    logger.info("3. Analyzing Multi-Agent Performance")
    
    orchestrator_stats = orchestrator.get_orchestrator_statistics()
    
    # Create comprehensive analysis report
    analysis_report = {
        "demonstration_timestamp": datetime.now(timezone.utc).isoformat(),
        "system_configuration": {
            "total_agents": len(agents),
            "agent_roles": [config.role.value for config in agent_configs],
            "workflow_graph_nodes": len(orchestrator.workflow_graph.nodes),
            "tools_available": list(set(tool for config in agent_configs for tool in config.tools))
        },
        "workflow_executions": len(workflow_results),
        "orchestrator_statistics": orchestrator_stats,
        "workflow_results": workflow_results,
        "performance_analysis": {},
        "agent_utilization": {},
        "insights_and_recommendations": []
    }
    
    # Analyze workflow performance
    successful_workflows = [r for r in workflow_results if r["result"]["success"]]
    
    if successful_workflows:
        execution_times = [r["result"]["execution_time"] for r in successful_workflows]
        step_counts = [r["result"]["step_count"] for r in successful_workflows]
        
        analysis_report["performance_analysis"] = {
            "success_rate": len(successful_workflows) / len(workflow_results),
            "average_execution_time": np.mean(execution_times),
            "median_execution_time": np.median(execution_times),
            "average_step_count": np.mean(step_counts),
            "min_execution_time": min(execution_times),
            "max_execution_time": max(execution_times)
        }
    
    # Analyze agent utilization
    agent_usage = defaultdict(int)
    for workflow_result in successful_workflows:
        for agent_name in workflow_result["result"]["agent_outputs"].keys():
            agent_usage[agent_name] += 1
    
    analysis_report["agent_utilization"] = dict(agent_usage)
    
    # Generate insights
    insights = [
        f"Successfully executed {len(successful_workflows)} out of {len(workflow_results)} workflows",
        f"Average workflow execution time: {analysis_report['performance_analysis'].get('average_execution_time', 0):.2f} seconds",
        f"Most utilized agent: {max(agent_usage, key=agent_usage.get) if agent_usage else 'None'}",
        f"Average workflow complexity: {analysis_report['performance_analysis'].get('average_step_count', 0):.1f} steps"
    ]
    
    # Add role-specific insights
    if orchestrator_stats["workflow_statistics"]["agent_utilization"]:
        top_role = max(
            orchestrator_stats["workflow_statistics"]["agent_utilization"],
            key=orchestrator_stats["workflow_statistics"]["agent_utilization"].get
        )
        insights.append(f"Most active role: {top_role}")
    
    analysis_report["insights_and_recommendations"] = insights
    
    # Save results
    with open("langgraph_multi_agent_results.json", "w") as f:
        json.dump(analysis_report, f, indent=2, default=str)
    
    # Create visualizations
    try:
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # Workflow execution times
        if successful_workflows:
            execution_times = [r["result"]["execution_time"] for r in successful_workflows]
            workflow_labels = [f"Workflow {i+1}" for i in range(len(execution_times))]
            
            axes[0, 0].bar(workflow_labels, execution_times, color='skyblue')
            axes[0, 0].set_title('Workflow Execution Times')
            axes[0, 0].set_ylabel('Time (seconds)')
            axes[0, 0].tick_params(axis='x', rotation=45)
        
        # Agent utilization
        if agent_usage:
            agents = list(agent_usage.keys())
            usage_counts = list(agent_usage.values())
            
            axes[0, 1].pie(usage_counts, labels=agents, autopct='%1.1f%%')
            axes[0, 1].set_title('Agent Utilization Distribution')
        
        # Step count distribution
        if successful_workflows:
            step_counts = [r["result"]["step_count"] for r in successful_workflows]
            
            axes[1, 0].hist(step_counts, bins=max(1, len(set(step_counts))), color='lightgreen', alpha=0.7)
            axes[1, 0].set_title('Workflow Step Count Distribution')
            axes[1, 0].set_xlabel('Number of Steps')
            axes[1, 0].set_ylabel('Frequency')
        
        # Performance trends
        if len(successful_workflows) > 1:
            workflow_indices = range(1, len(successful_workflows) + 1)
            execution_times = [r["result"]["execution_time"] for r in successful_workflows]
            
            axes[1, 1].plot(workflow_indices, execution_times, marker='o', color='purple')
            axes[1, 1].set_title('Execution Time Trends')
            axes[1, 1].set_xlabel('Workflow Number')
            axes[1, 1].set_ylabel('Execution Time (seconds)')
        
        plt.tight_layout()
        plt.savefig('langgraph_multi_agent_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
    except Exception as e:
        logger.warning(f"Error creating visualizations: {e}")
    
    logger.info("LangGraph Multi-Agent demonstration completed!")
    logger.info("Check 'langgraph_multi_agent_results.json' for detailed results")
    
    return analysis_report

# Main execution
if __name__ == "__main__":
    asyncio.run(comprehensive_langgraph_demonstration())
````

## Conclusion

This comprehensive LangGraph multi-agent orchestration framework demonstrates the paradigm shift from linear AI workflows to sophisticated graph-based systems that enable intelligent collaboration between specialized agents, providing unprecedented flexibility and capability for complex problem-solving tasks requiring diverse expertise and adaptive decision-making.

**Advanced Graph-Based Architecture** through LangGraph's stateful workflow management enables conditional routing, parallel execution, and dynamic agent coordination that surpasses traditional chain-based approaches, allowing for complex decision trees, error handling, and adaptive workflow paths based on intermediate results and changing requirements.

**Specialized Agent Ecosystem** with role-based configurations creates a collaborative environment where each agent contributes unique expertise, from research and analysis to writing and quality assurance, demonstrating how distributed intelligence can solve complex problems more effectively than monolithic AI systems.

**Intelligent Workflow Orchestration** through conditional routing logic and state management ensures optimal agent utilization while maintaining workflow coherence, enabling dynamic adaptation to task complexity and requirements without manual intervention or predefined rigid sequences.

**Sophisticated State Management** with persistent memory, shared context, and inter-agent communication protocols enables agents to build upon each other's work, maintain consistency across multiple steps, and preserve critical information throughout complex multi-stage problem-solving processes.

**Production-Ready Performance Monitoring** with comprehensive metrics collection, agent utilization tracking, and workflow analytics provides essential insights for optimization, debugging, and scaling multi-agent systems in enterprise environments requiring reliability and transparency.

**Adaptive Decision-Making Capabilities** through critic evaluation, confidence scoring, and iterative refinement processes ensure high-quality outputs while enabling continuous improvement and self-correction mechanisms that enhance overall system reliability and accuracy.

This framework empowers developers to build sophisticated AI systems that leverage the collective intelligence of specialized agents, creating applications that can handle complex, multi-faceted tasks with the same level of coordination and expertise distribution found in human collaborative teams, while maintaining the speed and consistency advantages of artificial intelligence.