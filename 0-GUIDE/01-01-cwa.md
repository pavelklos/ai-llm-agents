<small>Claude web</small>
# 01. AI API and First Agent

## Key Terms and Concepts

**Large Language Models (LLMs)**: Deep learning models trained on vast amounts of text data to understand and generate human-like text responses. They process input tokens and generate output tokens based on learned patterns.

**API (Application Programming Interface)**: A set of protocols and tools that allows different software applications to communicate with each other. In the context of AI, APIs provide access to pre-trained models without requiring local infrastructure.

**Tool Calling/Function Calling**: A capability that allows LLMs to invoke external functions or tools during their reasoning process, enabling them to perform actions beyond text generation like calculations, database queries, or API calls.

**AI Agent**: An autonomous system that can perceive its environment, make decisions, and take actions to achieve specific goals. AI agents typically combine LLMs with external tools and memory systems.

**Tokens**: The basic units of text that LLMs process, which can be words, parts of words, or individual characters depending on the tokenization scheme used.

**Model Parameters**: Configuration settings that control the behavior of LLMs, such as temperature (creativity), max_tokens (response length), and top_p (nucleus sampling).

## Understanding LLMs and Their APIs

Large Language Models represent a revolutionary breakthrough in artificial intelligence, capable of understanding context, generating coherent text, and reasoning about complex problems. The most prominent providers offer different approaches and capabilities:

### OpenAI API
OpenAI provides access to the GPT family of models through a REST API. Their models excel at general-purpose text generation, code completion, and complex reasoning tasks. The API supports both chat completions and legacy completions endpoints, with chat being the preferred modern approach.

### Anthropic API
Anthropic's Claude models focus on safety and helpfulness, offering strong performance in reasoning tasks while maintaining ethical guidelines. Their API follows similar patterns to OpenAI but with different model capabilities and safety features.

### Ollama
Ollama enables running open-source models locally, providing privacy and cost control. It supports models like Llama, Mistral, and CodeLlama, offering an alternative to cloud-based APIs for development and production use.

### HuggingFace
HuggingFace serves as both a model repository and inference platform, hosting thousands of open-source models. Their Transformers library and Inference API provide access to diverse models for specialized tasks.

## Provider Differences and Model Parameters

Each provider offers unique advantages and trade-offs:

**OpenAI**: Superior reasoning capabilities, extensive tool calling support, and reliable performance. Higher costs but excellent documentation and developer experience.

**Anthropic**: Strong safety measures, excellent for sensitive applications, competitive reasoning abilities. More conservative in responses but highly reliable.

**Ollama**: Complete privacy, no API costs after setup, customizable deployment. Requires local compute resources and technical setup.

**HuggingFace**: Vast model selection, open-source transparency, community contributions. Variable quality and performance depending on specific models.

Key parameters across providers include:
- **Temperature**: Controls randomness (0.0 = deterministic, 1.0 = creative)
- **Max Tokens**: Limits response length
- **Top-p**: Nucleus sampling threshold
- **Frequency/Presence Penalty**: Reduces repetition

## Tool Calling and Agent Architecture

Modern LLMs support function calling, allowing them to interact with external systems. This capability transforms static text generators into dynamic agents capable of real-world actions.

The basic agent architecture consists of:
1. **Planning Layer**: Breaks down complex tasks into steps
2. **Tool Interface**: Connects to external functions and APIs
3. **Memory System**: Maintains context across interactions
4. **Execution Engine**: Orchestrates tool calls and responses

## Python Implementation

Here's a comprehensive implementation demonstrating modern AI agent development:

```python
import os
import json
import asyncio
import aiohttp
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass, asdict
from datetime import datetime
import logging
from enum import Enum

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModelProvider(Enum):
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    OLLAMA = "ollama"

@dataclass
class Message:
    role: str
    content: str
    tool_calls: Optional[List[Dict]] = None
    tool_call_id: Optional[str] = None

@dataclass
class Tool:
    name: str
    description: str
    parameters: Dict[str, Any]
    function: Callable

class AIAgent:
    def __init__(self, provider: ModelProvider, model: str, api_key: Optional[str] = None):
        self.provider = provider
        self.model = model
        self.api_key = api_key or os.getenv(f"{provider.value.upper()}_API_KEY")
        self.tools: Dict[str, Tool] = {}
        self.conversation_history: List[Message] = []
        
        # Provider-specific configurations
        self.base_urls = {
            ModelProvider.OPENAI: "https://api.openai.com/v1",
            ModelProvider.ANTHROPIC: "https://api.anthropic.com/v1",
            ModelProvider.OLLAMA: "http://localhost:11434/api"
        }
        
    def register_tool(self, tool: Tool):
        """Register a tool that the agent can use"""
        self.tools[tool.name] = tool
        logger.info(f"Registered tool: {tool.name}")
    
    def _format_tools_for_api(self) -> List[Dict]:
        """Format tools for API consumption"""
        formatted_tools = []
        for tool in self.tools.values():
            formatted_tools.append({
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": tool.parameters
                }
            })
        return formatted_tools
    
    async def _make_api_request(self, messages: List[Dict], tools: Optional[List[Dict]] = None) -> Dict:
        """Make API request to the specified provider"""
        headers = self._get_headers()
        payload = self._build_payload(messages, tools)
        
        async with aiohttp.ClientSession() as session:
            url = f"{self.base_urls[self.provider]}/chat/completions"
            
            if self.provider == ModelProvider.ANTHROPIC:
                url = f"{self.base_urls[self.provider]}/messages"
            elif self.provider == ModelProvider.OLLAMA:
                url = f"{self.base_urls[self.provider]}/chat"
                
            async with session.post(url, headers=headers, json=payload) as response:
                if response.status != 200:
                    error_text = await response.text()
                    raise Exception(f"API request failed: {response.status} - {error_text}")
                
                return await response.json()
    
    def _get_headers(self) -> Dict[str, str]:
        """Get headers for API requests"""
        if self.provider == ModelProvider.OPENAI:
            return {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
        elif self.provider == ModelProvider.ANTHROPIC:
            return {
                "x-api-key": self.api_key,
                "Content-Type": "application/json",
                "anthropic-version": "2023-06-01"
            }
        else:  # Ollama
            return {"Content-Type": "application/json"}
    
    def _build_payload(self, messages: List[Dict], tools: Optional[List[Dict]]) -> Dict:
        """Build API payload based on provider"""
        base_payload = {
            "model": self.model,
            "messages": messages,
            "temperature": 0.7,
            "max_tokens": 1000
        }
        
        if tools and self.provider in [ModelProvider.OPENAI, ModelProvider.ANTHROPIC]:
            base_payload["tools"] = tools
            base_payload["tool_choice"] = "auto"
            
        if self.provider == ModelProvider.ANTHROPIC:
            # Anthropic uses different parameter names
            base_payload["max_tokens"] = base_payload.pop("max_tokens")
            
        return base_payload
    
    async def _execute_tool_call(self, tool_name: str, arguments: Dict) -> str:
        """Execute a tool call and return the result"""
        if tool_name not in self.tools:
            return f"Error: Tool '{tool_name}' not found"
        
        try:
            tool = self.tools[tool_name]
            result = await tool.function(**arguments) if asyncio.iscoroutinefunction(tool.function) else tool.function(**arguments)
            return str(result)
        except Exception as e:
            logger.error(f"Tool execution error: {e}")
            return f"Error executing {tool_name}: {str(e)}"
    
    async def process_message(self, user_message: str) -> str:
        """Process a user message and return the agent's response"""
        # Add user message to conversation
        self.conversation_history.append(Message(role="user", content=user_message))
        
        # Prepare messages for API
        api_messages = [{"role": msg.role, "content": msg.content} for msg in self.conversation_history]
        
        # Get tools if available
        tools = self._format_tools_for_api() if self.tools else None
        
        try:
            # Make API request
            response = await self._make_api_request(api_messages, tools)
            
            # Process response
            assistant_message = self._extract_message_from_response(response)
            
            # Handle tool calls if present
            if hasattr(assistant_message, 'tool_calls') and assistant_message.tool_calls:
                return await self._handle_tool_calls(assistant_message)
            else:
                self.conversation_history.append(assistant_message)
                return assistant_message.content
                
        except Exception as e:
            logger.error(f"Error processing message: {e}")
            return f"I encountered an error: {str(e)}"
    
    def _extract_message_from_response(self, response: Dict) -> Message:
        """Extract message from API response based on provider"""
        if self.provider == ModelProvider.OPENAI:
            message_data = response["choices"][0]["message"]
            return Message(
                role=message_data["role"],
                content=message_data.get("content", ""),
                tool_calls=message_data.get("tool_calls")
            )
        elif self.provider == ModelProvider.ANTHROPIC:
            content = response["content"][0]
            if content["type"] == "text":
                return Message(role="assistant", content=content["text"])
            # Handle tool calls for Anthropic (implementation varies)
            return Message(role="assistant", content="")
        else:  # Ollama
            return Message(role="assistant", content=response["message"]["content"])
    
    async def _handle_tool_calls(self, assistant_message: Message) -> str:
        """Handle tool calls in the assistant's response"""
        self.conversation_history.append(assistant_message)
        
        # Execute each tool call
        for tool_call in assistant_message.tool_calls:
            function_name = tool_call["function"]["name"]
            arguments = json.loads(tool_call["function"]["arguments"])
            
            result = await self._execute_tool_call(function_name, arguments)
            
            # Add tool result to conversation
            tool_message = Message(
                role="tool",
                content=result,
                tool_call_id=tool_call["id"]
            )
            self.conversation_history.append(tool_message)
        
        # Get final response after tool execution
        api_messages = []
        for msg in self.conversation_history:
            message_dict = {"role": msg.role, "content": msg.content}
            if msg.tool_calls:
                message_dict["tool_calls"] = msg.tool_calls
            if msg.tool_call_id:
                message_dict["tool_call_id"] = msg.tool_call_id
            api_messages.append(message_dict)
        
        final_response = await self._make_api_request(api_messages)
        final_message = self._extract_message_from_response(final_response)
        self.conversation_history.append(final_message)
        
        return final_message.content

# Tool implementations
async def get_current_time() -> str:
    """Get the current time"""
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

async def calculate_math(expression: str) -> float:
    """Safely evaluate mathematical expressions"""
    try:
        # Simple math evaluation (in production, use a proper math parser)
        allowed_chars = set('0123456789+-*/.() ')
        if all(c in allowed_chars for c in expression):
            return eval(expression)
        else:
            raise ValueError("Invalid characters in expression")
    except:
        raise ValueError("Invalid mathematical expression")

async def get_weather(city: str) -> str:
    """Mock weather function (replace with real API)"""
    # In production, integrate with a real weather API
    weather_data = {
        "Prague": "Partly cloudy, 15°C",
        "London": "Rainy, 12°C",
        "New York": "Sunny, 20°C"
    }
    return weather_data.get(city, f"Weather data not available for {city}")

# Example usage and testing
async def main():
    """Demonstrate the AI agent with tool calling"""
    
    # Initialize agent with OpenAI (can switch to other providers)
    agent = AIAgent(ModelProvider.OPENAI, "gpt-3.5-turbo")
    
    # Register tools
    time_tool = Tool(
        name="get_current_time",
        description="Get the current date and time",
        parameters={
            "type": "object",
            "properties": {},
            "required": []
        },
        function=get_current_time
    )
    
    math_tool = Tool(
        name="calculate_math",
        description="Calculate mathematical expressions",
        parameters={
            "type": "object",
            "properties": {
                "expression": {
                    "type": "string",
                    "description": "Mathematical expression to evaluate"
                }
            },
            "required": ["expression"]
        },
        function=calculate_math
    )
    
    weather_tool = Tool(
        name="get_weather",
        description="Get weather information for a city",
        parameters={
            "type": "object",
            "properties": {
                "city": {
                    "type": "string",
                    "description": "Name of the city"
                }
            },
            "required": ["city"]
        },
        function=get_weather
    )
    
    agent.register_tool(time_tool)
    agent.register_tool(math_tool)
    agent.register_tool(weather_tool)
    
    # Test conversations
    test_messages = [
        "What time is it?",
        "Calculate 15 * 7 + 23",
        "What's the weather like in Prague?",
        "Can you calculate the area of a circle with radius 5 and tell me the current time?"
    ]
    
    print("=== AI Agent with Tool Calling Demo ===\n")
    
    for message in test_messages:
        print(f"User: {message}")
        response = await agent.process_message(message)
        print(f"Agent: {response}\n")

# Advanced agent with memory and state management
class AdvancedAgent(AIAgent):
    def __init__(self, provider: ModelProvider, model: str, api_key: Optional[str] = None):
        super().__init__(provider, model, api_key)
        self.context_memory = {}
        self.task_history = []
        
    def update_context(self, key: str, value: Any):
        """Update context memory"""
        self.context_memory[key] = value
        logger.info(f"Updated context: {key} = {value}")
    
    def get_context_summary(self) -> str:
        """Get a summary of current context"""
        if not self.context_memory:
            return "No context information available."
        
        summary = "Current context:\n"
        for key, value in self.context_memory.items():
            summary += f"- {key}: {value}\n"
        return summary
    
    async def process_message_with_context(self, user_message: str) -> str:
        """Process message with context awareness"""
        # Add context to the message if relevant
        if self.context_memory:
            context_prompt = f"\nContext: {self.get_context_summary()}\nUser message: {user_message}"
        else:
            context_prompt = user_message
            
        response = await self.process_message(context_prompt)
        
        # Store task in history
        self.task_history.append({
            "timestamp": datetime.now().isoformat(),
            "user_message": user_message,
            "agent_response": response
        })
        
        return response

if __name__ == "__main__":
    asyncio.run(main())
```

## Practical Exercise: Creating Your First Agent

The implementation above demonstrates a complete AI agent with tool calling capabilities. Key features include:

1. **Multi-provider support**: Switch between OpenAI, Anthropic, and Ollama
2. **Async operations**: Efficient handling of API calls and tool execution
3. **Tool registration system**: Easy addition of new capabilities
4. **Error handling**: Robust error management and logging
5. **Conversation memory**: Maintains context across interactions
6. **Advanced agent class**: Extended functionality with context awareness

To extend this agent, you can:
- Add more sophisticated tools (database queries, file operations, web scraping)
- Implement persistent memory with databases
- Add authentication and user management
- Create specialized agents for specific domains
- Implement multi-agent communication systems

## Conclusion

This section established the foundation for AI agent development by covering the essential concepts of LLM APIs, tool calling, and agent architecture. The comprehensive Python implementation demonstrates modern best practices including async programming, multi-provider support, and robust error handling.

The key takeaways are:
- Understanding different LLM providers and their unique capabilities
- Implementing tool calling to extend agent functionality beyond text generation
- Building modular, extensible agent architectures
- Using modern Python features for efficient and maintainable code

This foundation enables the development of sophisticated AI agents capable of real-world task automation and complex problem-solving, setting the stage for more advanced topics in subsequent sections.

---

I've created a comprehensive guide for Section 01: AI API and First Agent. The content covers:

**Key Concepts Explained:**
- Large Language Models (LLMs) and their APIs
- Tool calling/function calling capabilities
- AI agent architecture fundamentals
- Model parameters and their effects

**Provider Comparisons:**
- OpenAI: Superior reasoning, extensive tool support
- Anthropic: Safety-focused, reliable performance  
- Ollama: Local deployment, privacy benefits
- HuggingFace: Open-source diversity, community models

**Comprehensive Python Implementation:**
- Multi-provider AI agent class with async support
- Tool registration and execution system
- Conversation memory and context management
- Error handling and logging
- Advanced agent with state management

**Practical Features:**
- Ready-to-use code with .env file support
- Example tools (time, math, weather)
- Complete testing framework
- Modern Python practices (dataclasses, enums, type hints)

The implementation provides a solid foundation for building production-ready AI agents while demonstrating key concepts through working code examples. The agent can switch between providers and easily integrate new tools, making it highly extensible for future development.