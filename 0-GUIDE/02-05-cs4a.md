<small>Claude Sonnet 4</small>
# 05. HuggingFace Introduction

## Key Terms

**HuggingFace Hub**: A comprehensive platform and repository for machine learning models, datasets, and spaces that serves as the central ecosystem for sharing, discovering, and deploying state-of-the-art transformer models and related resources across the AI community.

**Transformers Library**: A unified Python library providing thousands of pre-trained models for natural language processing, computer vision, and audio tasks, with standardized APIs for inference, training, and fine-tuning across different frameworks like PyTorch and TensorFlow.

**Datasets Library**: A lightweight and extensible library for easily accessing, processing, and sharing datasets for machine learning, with built-in support for popular benchmarks, efficient data loading, and seamless integration with preprocessing pipelines.

**Tokenization**: The process of converting raw text into numerical tokens that can be processed by transformer models, involving text segmentation, vocabulary mapping, and special token handling while preserving semantic meaning and linguistic structure.

**Model Hub Integration**: The seamless connection between local development environments and the HuggingFace Hub, enabling automatic model downloading, caching, version control, and deployment through standardized APIs and authentication mechanisms.

**Inference Endpoints**: Managed infrastructure services that provide scalable, production-ready deployment of machine learning models with automatic scaling, load balancing, and monitoring capabilities through REST APIs and SDKs.

**Pre-trained Models**: Large-scale neural networks that have been trained on extensive datasets and can be used directly for inference or adapted for specific tasks through fine-tuning, transfer learning, or prompt engineering techniques.

**Pipeline API**: High-level abstraction that simplifies common NLP, computer vision, and audio tasks by providing ready-to-use interfaces that handle tokenization, model inference, and post-processing automatically.

## Comprehensive HuggingFace Ecosystem Integration Framework

HuggingFace has revolutionized the democratization of machine learning by providing an extensive ecosystem of tools, models, and datasets. This framework demonstrates advanced integration patterns across the entire HuggingFace stack for production-ready AI applications.

### Advanced HuggingFace Integration Implementation

````python
import asyncio
import aiohttp
import aiofiles
import json
import logging
import os
import time
import warnings
from typing import Dict, List, Any, Optional, Union, Tuple, Generator, AsyncGenerator
from dataclasses import dataclass, field
from datetime import datetime, timezone
from pathlib import Path
import hashlib
import pickle
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import multiprocessing as mp
import threading
from contextlib import asynccontextmanager
import tempfile
import shutil

# Core HuggingFace libraries
import transformers
from transformers import (
    AutoTokenizer, AutoModel, AutoModelForCausalLM, AutoModelForSequenceClassification,
    AutoModelForQuestionAnswering, AutoModelForTokenClassification,
    AutoConfig, AutoProcessor, AutoImageProcessor,
    pipeline, Pipeline, Trainer, TrainingArguments,
    DataCollatorWithPadding, DataCollatorForLanguageModeling,
    BitsAndBytesConfig, GenerationConfig
)
import datasets
from datasets import Dataset, DatasetDict, load_dataset, load_metric
import tokenizers
from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors

# HuggingFace Hub integration
from huggingface_hub import (
    HfApi, Repository, snapshot_download, hf_hub_download,
    login, logout, whoami, create_repo, delete_repo,
    upload_file, upload_folder, list_models, list_datasets,
    InferenceClient, InferenceApi, AsyncInferenceClient
)

# ML and data processing
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset as TorchDataset
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns

# Async and utilities
import httpx
from tenacity import retry, stop_after_attempt, wait_exponential
import requests
import psutil
import GPUtil

# Optimization libraries
from accelerate import Accelerator
from peft import LoraConfig, get_peft_model, TaskType
import bitsandbytes as bnb

from dotenv import load_dotenv

load_dotenv()

# Suppress warnings for cleaner output
warnings.filterwarnings("ignore", category=FutureWarning)
transformers.logging.set_verbosity_error()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class HuggingFaceConfig:
    """Configuration for HuggingFace operations"""
    token: Optional[str] = None
    cache_dir: str = "./hf_cache"
    max_workers: int = 4
    timeout: int = 300
    retry_attempts: int = 3
    default_device: str = "auto"
    use_auth_token: bool = True
    trust_remote_code: bool = False

@dataclass
class ModelConfig:
    """Configuration for model loading and inference"""
    model_name: str
    task: Optional[str] = None
    device: str = "auto"
    torch_dtype: Optional[torch.dtype] = None
    load_in_8bit: bool = False
    load_in_4bit: bool = False
    use_flash_attention: bool = False
    max_length: int = 512
    generation_config: Optional[Dict[str, Any]] = None

@dataclass
class DatasetConfig:
    """Configuration for dataset operations"""
    dataset_name: str
    config_name: Optional[str] = None
    split: Optional[str] = None
    streaming: bool = False
    cache_dir: Optional[str] = None
    num_proc: Optional[int] = None

@dataclass
class EndpointConfig:
    """Configuration for HuggingFace Inference Endpoints"""
    endpoint_name: str
    model_repository: str
    instance_type: str = "cpu-medium"
    min_replica: int = 1
    max_replica: int = 10
    revision: str = "main"
    framework: str = "pytorch"
    accelerator: Optional[str] = None
    vendor: str = "aws"
    region: str = "us-east-1"

class HuggingFaceClient:
    """Comprehensive HuggingFace client with advanced features"""
    
    def __init__(self, config: HuggingFaceConfig):
        self.config = config
        self.api = HfApi(token=config.token)
        self.cache_dir = Path(config.cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
        # Device management
        self.device = self._setup_device()
        
        # Model and tokenizer cache
        self.model_cache: Dict[str, Any] = {}
        self.tokenizer_cache: Dict[str, Any] = {}
        self.pipeline_cache: Dict[str, Pipeline] = {}
        
        # Metrics tracking
        self.metrics = {
            "models_loaded": 0,
            "datasets_loaded": 0,
            "inferences_performed": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "total_load_time": 0.0,
            "total_inference_time": 0.0
        }
        
        # Authentication
        if config.token:
            try:
                login(token=config.token)
                user_info = whoami(token=config.token)
                logger.info(f"Authenticated as: {user_info['name']}")
            except Exception as e:
                logger.warning(f"Authentication failed: {e}")
    
    def _setup_device(self) -> str:
        """Setup optimal device configuration"""
        if self.config.default_device == "auto":
            if torch.cuda.is_available():
                device = f"cuda:{torch.cuda.current_device()}"
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
                logger.info(f"Using GPU: {device} with {gpu_memory:.1f}GB memory")
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                device = "mps"
                logger.info("Using Apple Silicon MPS")
            else:
                device = "cpu"
                cpu_count = mp.cpu_count()
                memory_gb = psutil.virtual_memory().total / 1e9
                logger.info(f"Using CPU with {cpu_count} cores and {memory_gb:.1f}GB RAM")
        else:
            device = self.config.default_device
        
        return device
    
    def load_model_and_tokenizer(self, model_config: ModelConfig) -> Tuple[Any, Any]:
        """Load model and tokenizer with advanced configuration"""
        
        cache_key = f"{model_config.model_name}_{model_config.task}_{model_config.device}"
        
        # Check cache
        if cache_key in self.model_cache:
            self.metrics["cache_hits"] += 1
            return self.model_cache[cache_key], self.tokenizer_cache[cache_key]
        
        self.metrics["cache_misses"] += 1
        start_time = time.time()
        
        try:
            # Setup device
            device = model_config.device if model_config.device != "auto" else self.device
            
            # Load tokenizer
            logger.info(f"Loading tokenizer: {model_config.model_name}")
            tokenizer = AutoTokenizer.from_pretrained(
                model_config.model_name,
                cache_dir=self.config.cache_dir,
                use_auth_token=self.config.use_auth_token,
                trust_remote_code=self.config.trust_remote_code
            )
            
            # Ensure pad token exists
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            # Setup model loading arguments
            model_kwargs = {
                "cache_dir": self.config.cache_dir,
                "use_auth_token": self.config.use_auth_token,
                "trust_remote_code": self.config.trust_remote_code
            }
            
            # Configure quantization
            if model_config.load_in_4bit:
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_use_double_quant=True
                )
                model_kwargs["quantization_config"] = quantization_config
                logger.info("Configured 4-bit quantization")
            elif model_config.load_in_8bit:
                model_kwargs["load_in_8bit"] = True
                logger.info("Configured 8-bit quantization")
            
            # Set torch dtype
            if model_config.torch_dtype:
                model_kwargs["torch_dtype"] = model_config.torch_dtype
            
            # Load appropriate model class based on task
            if model_config.task == "text-generation":
                model_class = AutoModelForCausalLM
            elif model_config.task == "text-classification":
                model_class = AutoModelForSequenceClassification
            elif model_config.task == "question-answering":
                model_class = AutoModelForQuestionAnswering
            elif model_config.task == "token-classification":
                model_class = AutoModelForTokenClassification
            else:
                model_class = AutoModel
            
            logger.info(f"Loading model: {model_config.model_name} with {model_class.__name__}")
            model = model_class.from_pretrained(model_config.model_name, **model_kwargs)
            
            # Move to device if not using quantization
            if not (model_config.load_in_4bit or model_config.load_in_8bit):
                model = model.to(device)
            
            # Configure generation parameters
            if hasattr(model, 'generation_config') and model_config.generation_config:
                for key, value in model_config.generation_config.items():
                    setattr(model.generation_config, key, value)
            
            # Cache the models
            self.model_cache[cache_key] = model
            self.tokenizer_cache[cache_key] = tokenizer
            
            load_time = time.time() - start_time
            self.metrics["models_loaded"] += 1
            self.metrics["total_load_time"] += load_time
            
            logger.info(f"Model loaded successfully in {load_time:.2f}s")
            return model, tokenizer
            
        except Exception as e:
            logger.error(f"Error loading model {model_config.model_name}: {e}")
            raise
    
    def create_pipeline(self, task: str, model_config: ModelConfig, **kwargs) -> Pipeline:
        """Create optimized pipeline for specific tasks"""
        
        cache_key = f"pipeline_{task}_{model_config.model_name}_{model_config.device}"
        
        if cache_key in self.pipeline_cache:
            self.metrics["cache_hits"] += 1
            return self.pipeline_cache[cache_key]
        
        self.metrics["cache_misses"] += 1
        
        try:
            # Load model and tokenizer
            model, tokenizer = self.load_model_and_tokenizer(model_config)
            
            # Device configuration
            device = model_config.device if model_config.device != "auto" else self.device
            if device == "cuda" and torch.cuda.is_available():
                device_id = 0  # Use first GPU
            else:
                device_id = -1  # Use CPU
            
            # Create pipeline
            pipe = pipeline(
                task=task,
                model=model,
                tokenizer=tokenizer,
                device=device_id,
                **kwargs
            )
            
            # Configure pipeline parameters
            if hasattr(pipe, 'model') and model_config.generation_config:
                for key, value in model_config.generation_config.items():
                    if hasattr(pipe.model.generation_config, key):
                        setattr(pipe.model.generation_config, key, value)
            
            self.pipeline_cache[cache_key] = pipe
            logger.info(f"Pipeline created for task: {task}")
            
            return pipe
            
        except Exception as e:
            logger.error(f"Error creating pipeline for {task}: {e}")
            raise
    
    async def async_inference(self, pipeline_obj: Pipeline, inputs: Union[str, List[str]], 
                            batch_size: int = 8, **kwargs) -> List[Dict[str, Any]]:
        """Perform asynchronous inference with batching"""
        
        start_time = time.time()
        
        try:
            # Handle single input
            if isinstance(inputs, str):
                inputs = [inputs]
            
            # Batch processing
            results = []
            
            for i in range(0, len(inputs), batch_size):
                batch = inputs[i:i + batch_size]
                
                # Run inference in thread pool to avoid blocking
                loop = asyncio.get_event_loop()
                with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
                    batch_results = await loop.run_in_executor(
                        executor, 
                        lambda: pipeline_obj(batch, **kwargs)
                    )
                
                results.extend(batch_results)
            
            inference_time = time.time() - start_time
            self.metrics["inferences_performed"] += len(inputs)
            self.metrics["total_inference_time"] += inference_time
            
            logger.info(f"Processed {len(inputs)} inputs in {inference_time:.2f}s")
            return results
            
        except Exception as e:
            logger.error(f"Error during async inference: {e}")
            raise

class DatasetManager:
    """Advanced dataset management with HuggingFace datasets"""
    
    def __init__(self, hf_client: HuggingFaceClient):
        self.client = hf_client
        self.dataset_cache: Dict[str, DatasetDict] = {}
        self.processed_cache: Dict[str, DatasetDict] = {}
    
    def load_dataset(self, config: DatasetConfig) -> DatasetDict:
        """Load dataset with advanced configuration"""
        
        cache_key = f"{config.dataset_name}_{config.config_name}_{config.split}"
        
        if cache_key in self.dataset_cache:
            self.client.metrics["cache_hits"] += 1
            return self.dataset_cache[cache_key]
        
        self.client.metrics["cache_misses"] += 1
        start_time = time.time()
        
        try:
            logger.info(f"Loading dataset: {config.dataset_name}")
            
            dataset = load_dataset(
                config.dataset_name,
                config.config_name,
                split=config.split,
                streaming=config.streaming,
                cache_dir=config.cache_dir or self.client.config.cache_dir,
                use_auth_token=self.client.config.use_auth_token,
                num_proc=config.num_proc
            )
            
            # Convert to DatasetDict if single split
            if not isinstance(dataset, DatasetDict):
                dataset = DatasetDict({config.split or "train": dataset})
            
            self.dataset_cache[cache_key] = dataset
            
            load_time = time.time() - start_time
            self.client.metrics["datasets_loaded"] += 1
            
            # Log dataset info
            total_samples = sum(len(split) for split in dataset.values())
            logger.info(f"Dataset loaded: {total_samples} samples in {load_time:.2f}s")
            
            return dataset
            
        except Exception as e:
            logger.error(f"Error loading dataset {config.dataset_name}: {e}")
            raise
    
    def preprocess_dataset(self, dataset: DatasetDict, tokenizer: Any,
                          preprocessing_fn: Optional[callable] = None,
                          tokenization_params: Optional[Dict[str, Any]] = None,
                          num_proc: Optional[int] = None) -> DatasetDict:
        """Advanced dataset preprocessing with tokenization"""
        
        # Generate cache key
        cache_key = hashlib.md5(
            f"{str(dataset)}_{tokenizer.name_or_path}_{str(preprocessing_fn)}_{str(tokenization_params)}".encode()
        ).hexdigest()
        
        if cache_key in self.processed_cache:
            self.client.metrics["cache_hits"] += 1
            return self.processed_cache[cache_key]
        
        self.client.metrics["cache_misses"] += 1
        
        try:
            processed_dataset = DatasetDict()
            
            for split_name, split_dataset in dataset.items():
                logger.info(f"Preprocessing {split_name} split...")
                
                # Apply custom preprocessing if provided
                if preprocessing_fn:
                    split_dataset = split_dataset.map(
                        preprocessing_fn,
                        batched=True,
                        num_proc=num_proc or self.client.config.max_workers,
                        desc=f"Preprocessing {split_name}"
                    )
                
                # Tokenization
                if tokenization_params:
                    def tokenize_function(examples):
                        return tokenizer(
                            examples[tokenization_params.get("text_column", "text")],
                            **{k: v for k, v in tokenization_params.items() if k != "text_column"}
                        )
                    
                    split_dataset = split_dataset.map(
                        tokenize_function,
                        batched=True,
                        num_proc=num_proc or self.client.config.max_workers,
                        desc=f"Tokenizing {split_name}"
                    )
                
                processed_dataset[split_name] = split_dataset
            
            self.processed_cache[cache_key] = processed_dataset
            logger.info("Dataset preprocessing completed")
            
            return processed_dataset
            
        except Exception as e:
            logger.error(f"Error preprocessing dataset: {e}")
            raise
    
    def create_custom_dataset(self, data: List[Dict[str, Any]], 
                            split_ratios: Optional[Dict[str, float]] = None) -> DatasetDict:
        """Create custom dataset from data"""
        
        try:
            # Default split ratios
            if split_ratios is None:
                split_ratios = {"train": 0.8, "validation": 0.1, "test": 0.1}
            
            # Convert to pandas for easier manipulation
            df = pd.DataFrame(data)
            
            # Create splits
            splits = {}
            remaining_data = df.copy()
            
            for split_name, ratio in split_ratios.items():
                if split_name == list(split_ratios.keys())[-1]:
                    # Last split gets all remaining data
                    split_data = remaining_data
                else:
                    split_size = int(len(df) * ratio)
                    split_data = remaining_data.sample(n=split_size, random_state=42)
                    remaining_data = remaining_data.drop(split_data.index)
                
                splits[split_name] = Dataset.from_pandas(split_data)
            
            dataset = DatasetDict(splits)
            
            logger.info(f"Created custom dataset with splits: {list(splits.keys())}")
            return dataset
            
        except Exception as e:
            logger.error(f"Error creating custom dataset: {e}")
            raise
    
    def get_dataset_statistics(self, dataset: DatasetDict) -> Dict[str, Any]:
        """Generate comprehensive dataset statistics"""
        
        stats = {
            "splits": {},
            "total_samples": 0,
            "columns": [],
            "data_types": {},
            "sample_examples": {}
        }
        
        for split_name, split_dataset in dataset.items():
            split_stats = {
                "num_samples": len(split_dataset),
                "columns": split_dataset.column_names,
                "features": split_dataset.features
            }
            
            # Sample statistics for text columns
            for column in split_dataset.column_names:
                if split_dataset[column][0] and isinstance(split_dataset[column][0], str):
                    lengths = [len(text.split()) for text in split_dataset[column][:1000]]  # Sample first 1000
                    split_stats[f"{column}_stats"] = {
                        "avg_length": np.mean(lengths),
                        "max_length": max(lengths),
                        "min_length": min(lengths),
                        "std_length": np.std(lengths)
                    }
            
            stats["splits"][split_name] = split_stats
            stats["total_samples"] += len(split_dataset)
            
            # Store sample examples
            if len(split_dataset) > 0:
                stats["sample_examples"][split_name] = split_dataset[0]
        
        if dataset:
            first_split = next(iter(dataset.values()))
            stats["columns"] = first_split.column_names
            stats["data_types"] = {col: str(feat) for col, feat in first_split.features.items()}
        
        return stats

class HuggingFaceEndpointManager:
    """Manage HuggingFace Inference Endpoints"""
    
    def __init__(self, hf_client: HuggingFaceClient):
        self.client = hf_client
        self.endpoints: Dict[str, Dict[str, Any]] = {}
        self.inference_clients: Dict[str, InferenceClient] = {}
    
    async def create_endpoint(self, config: EndpointConfig) -> Dict[str, Any]:
        """Create a new inference endpoint"""
        
        try:
            logger.info(f"Creating endpoint: {config.endpoint_name}")
            
            # Create endpoint configuration
            endpoint_config = {
                "repository": config.model_repository,
                "framework": config.framework,
                "accelerator": config.accelerator,
                "instance_type": config.instance_type,
                "region": config.region,
                "vendor": config.vendor,
                "min_replica": config.min_replica,
                "max_replica": config.max_replica,
                "revision": config.revision,
                "type": "protected"  # or "public"
            }
            
            # Note: This would use the actual HuggingFace API in production
            # For demonstration, we'll simulate the endpoint creation
            endpoint_info = {
                "name": config.endpoint_name,
                "status": "initializing",
                "url": f"https://api-inference.huggingface.co/models/{config.model_repository}",
                "config": endpoint_config,
                "created_at": datetime.now(timezone.utc),
                "cost_estimate": self._calculate_cost_estimate(config)
            }
            
            self.endpoints[config.endpoint_name] = endpoint_info
            
            # Create inference client
            self.inference_clients[config.endpoint_name] = InferenceClient(
                model=config.model_repository,
                token=self.client.config.token
            )
            
            logger.info(f"Endpoint created: {config.endpoint_name}")
            return endpoint_info
            
        except Exception as e:
            logger.error(f"Error creating endpoint {config.endpoint_name}: {e}")
            raise
    
    async def inference_on_endpoint(self, endpoint_name: str, inputs: Union[str, List[str]],
                                  parameters: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """Perform inference on endpoint"""
        
        if endpoint_name not in self.inference_clients:
            raise ValueError(f"Endpoint {endpoint_name} not found")
        
        try:
            client = self.inference_clients[endpoint_name]
            
            # Handle single input
            if isinstance(inputs, str):
                inputs = [inputs]
            
            results = []
            
            for input_text in inputs:
                try:
                    # Use text generation by default
                    result = await asyncio.to_thread(
                        client.text_generation,
                        input_text,
                        **parameters or {}
                    )
                    results.append({"input": input_text, "output": result, "status": "success"})
                except Exception as e:
                    results.append({"input": input_text, "error": str(e), "status": "error"})
            
            return results
            
        except Exception as e:
            logger.error(f"Error during endpoint inference: {e}")
            raise
    
    def _calculate_cost_estimate(self, config: EndpointConfig) -> Dict[str, float]:
        """Calculate cost estimate for endpoint"""
        
        # Simplified cost calculation (actual costs vary by provider)
        instance_costs = {
            "cpu-small": 0.06,    # per hour
            "cpu-medium": 0.12,
            "cpu-large": 0.24,
            "gpu-small": 0.60,
            "gpu-medium": 1.20,
            "gpu-large": 2.40
        }
        
        hourly_cost = instance_costs.get(config.instance_type, 0.12)
        
        return {
            "hourly_cost": hourly_cost,
            "daily_cost": hourly_cost * 24,
            "monthly_cost": hourly_cost * 24 * 30,
            "currency": "USD"
        }
    
    def get_endpoint_status(self, endpoint_name: str) -> Dict[str, Any]:
        """Get endpoint status and metrics"""
        
        if endpoint_name not in self.endpoints:
            return {"error": "Endpoint not found"}
        
        endpoint = self.endpoints[endpoint_name]
        
        # Simulate status update
        statuses = ["initializing", "building", "running", "failed"]
        current_status = endpoint.get("status", "initializing")
        
        if current_status == "initializing":
            endpoint["status"] = "running"  # Simulate progression
        
        return {
            "name": endpoint_name,
            "status": endpoint["status"],
            "url": endpoint["url"],
            "uptime": str(datetime.now(timezone.utc) - endpoint["created_at"]),
            "cost_estimate": endpoint["cost_estimate"]
        }
    
    def list_endpoints(self) -> List[Dict[str, Any]]:
        """List all managed endpoints"""
        return [
            {
                "name": name,
                "status": info["status"],
                "model": info["config"]["repository"],
                "instance_type": info["config"]["instance_type"],
                "created_at": info["created_at"]
            }
            for name, info in self.endpoints.items()
        ]

class ModelRepository:
    """Manage models in HuggingFace Hub"""
    
    def __init__(self, hf_client: HuggingFaceClient):
        self.client = hf_client
        self.api = hf_client.api
    
    def search_models(self, query: str, task: Optional[str] = None, 
                     library: Optional[str] = None, limit: int = 10) -> List[Dict[str, Any]]:
        """Search for models in the Hub"""
        
        try:
            models = self.api.list_models(
                search=query,
                task=task,
                library=library,
                limit=limit,
                sort="downloads",
                direction=-1
            )
            
            model_info = []
            for model in models:
                model_info.append({
                    "id": model.modelId,
                    "task": getattr(model, 'pipeline_tag', 'unknown'),
                    "downloads": getattr(model, 'downloads', 0),
                    "likes": getattr(model, 'likes', 0),
                    "library": getattr(model, 'library_name', 'unknown'),
                    "created_at": getattr(model, 'created_at', None),
                    "tags": getattr(model, 'tags', [])
                })
            
            logger.info(f"Found {len(model_info)} models for query: {query}")
            return model_info
            
        except Exception as e:
            logger.error(f"Error searching models: {e}")
            raise
    
    def get_model_info(self, model_name: str) -> Dict[str, Any]:
        """Get detailed model information"""
        
        try:
            model_info = self.api.model_info(model_name)
            
            info = {
                "id": model_info.modelId,
                "task": getattr(model_info, 'pipeline_tag', 'unknown'),
                "downloads": getattr(model_info, 'downloads', 0),
                "likes": getattr(model_info, 'likes', 0),
                "library": getattr(model_info, 'library_name', 'unknown'),
                "tags": getattr(model_info, 'tags', []),
                "created_at": getattr(model_info, 'created_at', None),
                "last_modified": getattr(model_info, 'last_modified', None),
                "siblings": [f.rfilename for f in getattr(model_info, 'siblings', [])],
                "config": {}
            }
            
            # Try to get config information
            try:
                config = AutoConfig.from_pretrained(
                    model_name, 
                    use_auth_token=self.client.config.use_auth_token
                )
                info["config"] = {
                    "model_type": getattr(config, 'model_type', 'unknown'),
                    "vocab_size": getattr(config, 'vocab_size', 0),
                    "hidden_size": getattr(config, 'hidden_size', 0),
                    "num_layers": getattr(config, 'num_hidden_layers', getattr(config, 'n_layer', 0)),
                    "num_attention_heads": getattr(config, 'num_attention_heads', getattr(config, 'n_head', 0))
                }
            except:
                pass
            
            return info
            
        except Exception as e:
            logger.error(f"Error getting model info for {model_name}: {e}")
            raise
    
    def upload_model(self, model_path: str, repo_name: str, 
                    commit_message: str = "Upload model") -> str:
        """Upload model to HuggingFace Hub"""
        
        try:
            # Create repository if it doesn't exist
            try:
                self.api.create_repo(repo_name, private=False, exist_ok=True)
            except Exception as e:
                logger.warning(f"Repository might already exist: {e}")
            
            # Upload model files
            self.api.upload_folder(
                folder_path=model_path,
                repo_id=repo_name,
                commit_message=commit_message
            )
            
            logger.info(f"Model uploaded to: {repo_name}")
            return f"https://huggingface.co/{repo_name}"
            
        except Exception as e:
            logger.error(f"Error uploading model: {e}")
            raise

# Comprehensive demonstration and practical exercises
async def demonstrate_huggingface_ecosystem():
    """Comprehensive demonstration of HuggingFace ecosystem"""
    
    logger.info("=== HuggingFace Ecosystem Comprehensive Demonstration ===")
    
    # Initialize configuration
    config = HuggingFaceConfig(
        token=os.getenv('HUGGINGFACE_TOKEN'),
        cache_dir="./hf_demo_cache",
        max_workers=4
    )
    
    # Initialize client
    hf_client = HuggingFaceClient(config)
    
    # 1. Model Loading and Pipeline Creation
    logger.info("\n1. Model Loading and Pipeline Creation")
    
    # Text generation model
    text_gen_config = ModelConfig(
        model_name="microsoft/DialoGPT-medium",
        task="text-generation",
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        generation_config={
            "max_length": 100,
            "temperature": 0.8,
            "do_sample": True,
            "pad_token_id": 50256
        }
    )
    
    try:
        # Load model and tokenizer
        model, tokenizer = hf_client.load_model_and_tokenizer(text_gen_config)
        logger.info(f"Loaded model: {text_gen_config.model_name}")
        
        # Create pipeline
        text_gen_pipeline = hf_client.create_pipeline(
            "text-generation", 
            text_gen_config,
            max_length=100,
            num_return_sequences=1
        )
        
        # Test generation
        test_inputs = [
            "Hello, how are you?",
            "What is artificial intelligence?",
            "Can you help me with programming?"
        ]
        
        results = await hf_client.async_inference(
            text_gen_pipeline, 
            test_inputs,
            batch_size=2
        )
        
        for i, result in enumerate(results):
            logger.info(f"Input: {test_inputs[i]}")
            logger.info(f"Output: {result[0]['generated_text']}")
            
    except Exception as e:
        logger.error(f"Error with text generation: {e}")
    
    # 2. Classification Pipeline
    logger.info("\n2. Text Classification Pipeline")
    
    try:
        # Sentiment analysis
        sentiment_config = ModelConfig(
            model_name="cardiffnlp/twitter-roberta-base-sentiment-latest",
            task="text-classification"
        )
        
        sentiment_pipeline = hf_client.create_pipeline(
            "sentiment-analysis",
            sentiment_config
        )
        
        sentiment_texts = [
            "I love this new technology!",
            "This is terrible and doesn't work.",
            "It's okay, nothing special.",
            "Amazing results, highly recommended!",
            "Waste of time and money."
        ]
        
        sentiment_results = await hf_client.async_inference(
            sentiment_pipeline,
            sentiment_texts
        )
        
        for text, result in zip(sentiment_texts, sentiment_results):
            logger.info(f"Text: {text}")
            logger.info(f"Sentiment: {result['label']} (confidence: {result['score']:.3f})")
            
    except Exception as e:
        logger.error(f"Error with sentiment analysis: {e}")
    
    # 3. Dataset Management
    logger.info("\n3. Dataset Management")
    
    dataset_manager = DatasetManager(hf_client)
    
    try:
        # Load popular dataset
        dataset_config = DatasetConfig(
            dataset_name="squad",
            split="train[:1000]",  # Load first 1000 examples
            num_proc=2
        )
        
        squad_dataset = dataset_manager.load_dataset(dataset_config)
        
        # Get dataset statistics
        stats = dataset_manager.get_dataset_statistics(squad_dataset)
        logger.info(f"Dataset statistics: {stats['total_samples']} samples")
        logger.info(f"Columns: {stats['columns']}")
        
        # Example preprocessing
        def preprocess_qa(examples):
            """Preprocess question-answering data"""
            processed = {
                "question_length": [len(q.split()) for q in examples["question"]],
                "context_length": [len(c.split()) for c in examples["context"]],
                "has_answer": [len(ans["text"]) > 0 for ans in examples["answers"]]
            }
            return processed
        
        # Apply preprocessing
        processed_dataset = dataset_manager.preprocess_dataset(
            squad_dataset,
            tokenizer,
            preprocessing_fn=preprocess_qa,
            tokenization_params={
                "text_column": "question",
                "padding": "max_length",
                "truncation": True,
                "max_length": 512
            }
        )
        
        logger.info("Dataset preprocessing completed")
        
    except Exception as e:
        logger.error(f"Error with dataset management: {e}")
    
    # 4. Custom Dataset Creation
    logger.info("\n4. Custom Dataset Creation")
    
    try:
        # Create sample custom data
        custom_data = [
            {"text": "HuggingFace makes machine learning accessible to everyone.", "label": "positive"},
            {"text": "Transformers have revolutionized natural language processing.", "label": "positive"},
            {"text": "Open source AI models are democratizing technology.", "label": "positive"},
            {"text": "Complex models require significant computational resources.", "label": "neutral"},
            {"text": "Model deployment can be challenging without proper tools.", "label": "negative"},
            {"text": "Pre-trained models save enormous amounts of development time.", "label": "positive"},
            {"text": "Fine-tuning allows customization for specific use cases.", "label": "positive"},
            {"text": "Large language models can have bias issues.", "label": "negative"},
            {"text": "The AI community benefits from shared models and datasets.", "label": "positive"},
            {"text": "Ethical considerations are important in AI development.", "label": "neutral"}
        ]
        
        custom_dataset = dataset_manager.create_custom_dataset(
            custom_data,
            split_ratios={"train": 0.7, "validation": 0.2, "test": 0.1}
        )
        
        custom_stats = dataset_manager.get_dataset_statistics(custom_dataset)
        logger.info(f"Custom dataset created with {custom_stats['total_samples']} samples")
        
    except Exception as e:
        logger.error(f"Error creating custom dataset: {e}")
    
    # 5. Model Repository Operations
    logger.info("\n5. Model Repository Operations")
    
    repo_manager = ModelRepository(hf_client)
    
    try:
        # Search for models
        search_results = repo_manager.search_models(
            query="bert",
            task="text-classification",
            limit=5
        )
        
        logger.info(f"Found {len(search_results)} BERT classification models")
        for model in search_results[:3]:
            logger.info(f"Model: {model['id']} (downloads: {model['downloads']})")
        
        # Get detailed model info
        if search_results:
            model_info = repo_manager.get_model_info(search_results[0]['id'])
            logger.info(f"Model details: {model_info['id']}")
            logger.info(f"Task: {model_info['task']}")
            logger.info(f"Library: {model_info['library']}")
            
    except Exception as e:
        logger.error(f"Error with repository operations: {e}")
    
    # 6. Inference Endpoints (Simulation)
    logger.info("\n6. Inference Endpoints Management")
    
    endpoint_manager = HuggingFaceEndpointManager(hf_client)
    
    try:
        # Create endpoint configuration
        endpoint_config = EndpointConfig(
            endpoint_name="demo-text-generation",
            model_repository="microsoft/DialoGPT-medium",
            instance_type="cpu-medium",
            min_replica=1,
            max_replica=3
        )
        
        # Create endpoint (simulated)
        endpoint_info = await endpoint_manager.create_endpoint(endpoint_config)
        logger.info(f"Endpoint created: {endpoint_info['name']}")
        logger.info(f"Cost estimate: ${endpoint_info['cost_estimate']['hourly_cost']:.2f}/hour")
        
        # Test inference on endpoint
        endpoint_inputs = [
            "Hello, can you help me?",
            "What's the weather like?"
        ]
        
        # Note: This would work with actual endpoints
        # endpoint_results = await endpoint_manager.inference_on_endpoint(
        #     "demo-text-generation",
        #     endpoint_inputs,
        #     parameters={"max_length": 50, "temperature": 0.8}
        # )
        
        # Get endpoint status
        status = endpoint_manager.get_endpoint_status("demo-text-generation")
        logger.info(f"Endpoint status: {status['status']}")
        
    except Exception as e:
        logger.error(f"Error with endpoint management: {e}")
    
    # 7. Advanced Tokenization
    logger.info("\n7. Advanced Tokenization Techniques")
    
    try:
        # Compare different tokenizers
        tokenizer_models = [
            "bert-base-uncased",
            "gpt2",
            "microsoft/DialoGPT-medium"
        ]
        
        test_text = "HuggingFace transformers make natural language processing accessible to everyone!"
        
        tokenization_results = {}
        
        for model_name in tokenizer_models:
            try:
                tokenizer = AutoTokenizer.from_pretrained(
                    model_name,
                    cache_dir=config.cache_dir
                )
                
                tokens = tokenizer.encode(test_text)
                decoded = tokenizer.decode(tokens)
                
                tokenization_results[model_name] = {
                    "num_tokens": len(tokens),
                    "tokens": tokenizer.convert_ids_to_tokens(tokens),
                    "decoded": decoded,
                    "vocab_size": tokenizer.vocab_size
                }
                
            except Exception as e:
                logger.warning(f"Error with tokenizer {model_name}: {e}")
        
        # Compare tokenization results
        for model_name, result in tokenization_results.items():
            logger.info(f"\nTokenizer: {model_name}")
            logger.info(f"Tokens: {result['num_tokens']}")
            logger.info(f"Vocab size: {result['vocab_size']}")
            logger.info(f"Sample tokens: {result['tokens'][:10]}")
            
    except Exception as e:
        logger.error(f"Error with tokenization comparison: {e}")
    
    # 8. Performance Analysis and Metrics
    logger.info("\n8. Performance Analysis")
    
    # Get comprehensive metrics
    metrics = hf_client.metrics
    
    performance_report = {
        "models_loaded": metrics["models_loaded"],
        "datasets_loaded": metrics["datasets_loaded"],
        "total_inferences": metrics["inferences_performed"],
        "cache_efficiency": {
            "hits": metrics["cache_hits"],
            "misses": metrics["cache_misses"],
            "hit_rate": metrics["cache_hits"] / max(metrics["cache_hits"] + metrics["cache_misses"], 1)
        },
        "timing": {
            "avg_load_time": metrics["total_load_time"] / max(metrics["models_loaded"], 1),
            "avg_inference_time": metrics["total_inference_time"] / max(metrics["inferences_performed"], 1)
        },
        "system_resources": {
            "cpu_percent": psutil.cpu_percent(),
            "memory_percent": psutil.virtual_memory().percent,
            "disk_usage": psutil.disk_usage('/').percent
        }
    }
    
    # Add GPU metrics if available
    if torch.cuda.is_available():
        try:
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu = gpus[0]
                performance_report["gpu"] = {
                    "gpu_load": gpu.load * 100,
                    "gpu_memory": gpu.memoryUtil * 100,
                    "gpu_temperature": gpu.temperature
                }
        except:
            pass
    
    logger.info(f"Performance Report:")
    logger.info(f"  Models loaded: {performance_report['models_loaded']}")
    logger.info(f"  Cache hit rate: {performance_report['cache_efficiency']['hit_rate']:.2%}")
    logger.info(f"  Avg load time: {performance_report['timing']['avg_load_time']:.2f}s")
    logger.info(f"  System memory: {performance_report['system_resources']['memory_percent']:.1f}%")
    
    # 9. Create Visualization
    logger.info("\n9. Creating Performance Visualizations")
    
    try:
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # Cache efficiency
        cache_data = [metrics["cache_hits"], metrics["cache_misses"]]
        cache_labels = ["Cache Hits", "Cache Misses"]
        axes[0, 0].pie(cache_data, labels=cache_labels, autopct='%1.1f%%')
        axes[0, 0].set_title('Cache Efficiency')
        
        # Resource usage
        if tokenization_results:
            models = list(tokenization_results.keys())
            token_counts = [tokenization_results[m]["num_tokens"] for m in models]
            axes[0, 1].bar(range(len(models)), token_counts)
            axes[0, 1].set_title('Tokenization Comparison')
            axes[0, 1].set_xticks(range(len(models)))
            axes[0, 1].set_xticklabels([m.split('/')[-1] for m in models], rotation=45)
        
        # Performance metrics over time (simulated)
        time_points = list(range(10))
        load_times = np.random.normal(2.0, 0.5, 10)
        inference_times = np.random.normal(0.1, 0.02, 10)
        
        axes[1, 0].plot(time_points, load_times, 'b-', label='Load Time')
        axes[1, 0].plot(time_points, inference_times, 'r-', label='Inference Time')
        axes[1, 0].set_title('Performance Over Time')
        axes[1, 0].legend()
        
        # System resources
        resources = ['CPU', 'Memory', 'Disk']
        usage = [
            performance_report['system_resources']['cpu_percent'],
            performance_report['system_resources']['memory_percent'],
            performance_report['system_resources']['disk_usage']
        ]
        colors = ['red' if u > 80 else 'orange' if u > 60 else 'green' for u in usage]
        axes[1, 1].bar(resources, usage, color=colors)
        axes[1, 1].set_title('System Resource Usage (%)')
        axes[1, 1].set_ylim(0, 100)
        
        plt.tight_layout()
        plt.savefig('huggingface_performance_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info("Performance visualization saved as 'huggingface_performance_analysis.png'")
        
    except Exception as e:
        logger.error(f"Error creating visualizations: {e}")
    
    # 10. Generate Comprehensive Report
    comprehensive_report = {
        "demonstration_summary": {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "features_tested": [
                "Model loading and caching",
                "Pipeline creation and inference",
                "Dataset management and preprocessing",
                "Custom dataset creation",
                "Model repository operations",
                "Endpoint management simulation",
                "Advanced tokenization",
                "Performance monitoring"
            ],
            "performance_metrics": performance_report
        },
        "models_tested": list(set([
            text_gen_config.model_name,
            sentiment_config.model_name if 'sentiment_config' in locals() else None
        ])),
        "datasets_used": [
            "squad (sample)",
            "custom_sentiment_data"
        ],
        "recommendations": [
            "Use model caching for repeated loads",
            "Implement proper error handling for production",
            "Monitor resource usage during inference",
            "Consider quantization for large models",
            "Use appropriate batch sizes for optimal performance",
            "Implement proper authentication for Hub operations"
        ],
        "next_steps": [
            "Explore fine-tuning with PEFT methods",
            "Implement custom training loops",
            "Set up production endpoints",
            "Create domain-specific datasets",
            "Optimize models for deployment"
        ]
    }
    
    # Save comprehensive report
    with open("huggingface_ecosystem_report.json", "w") as f:
        json.dump(comprehensive_report, f, indent=2, default=str)
    
    logger.info("Comprehensive demonstration completed!")
    logger.info("Check 'huggingface_ecosystem_report.json' for detailed results")
    
    return comprehensive_report

# Main execution
async def main():
    """Main execution for HuggingFace ecosystem demonstration"""
    try:
        report = await demonstrate_huggingface_ecosystem()
        
        # Display key results
        logger.info("\n=== Demonstration Summary ===")
        logger.info(f"Features tested: {len(report['demonstration_summary']['features_tested'])}")
        logger.info(f"Models tested: {len([m for m in report['models_tested'] if m])}")
        logger.info(f"Cache hit rate: {report['demonstration_summary']['performance_metrics']['cache_efficiency']['hit_rate']:.2%}")
        
    except Exception as e:
        logger.error(f"Demonstration failed: {e}")
        import traceback
        logger.error(traceback.format_exc())

if __name__ == "__main__":
    asyncio.run(main())
````

## Conclusion

The comprehensive HuggingFace ecosystem framework demonstrates the powerful capabilities and seamless integration patterns available for modern machine learning development. This implementation provides production-ready solutions for model management, dataset processing, and deployment workflows.

**Unified Model Access** through the transformers library enables consistent interfaces across thousands of pre-trained models, supporting diverse tasks from text generation to classification with standardized APIs and automatic optimization for different hardware configurations.

**Advanced Dataset Management** via the datasets library provides efficient data loading, preprocessing, and manipulation capabilities with built-in support for popular benchmarks, streaming large datasets, and parallel processing for optimal performance.

**Intelligent Caching Systems** optimize performance and reduce computational overhead through sophisticated model and dataset caching, dramatically improving development iteration speed and resource utilization in production environments.

**Flexible Tokenization Framework** supports multiple tokenization strategies and model-specific vocabularies, ensuring optimal text processing for different architectures while maintaining compatibility across diverse language models.

**Production Deployment Solutions** through Inference Endpoints provide scalable, managed infrastructure for model serving with automatic scaling, load balancing, and cost optimization for enterprise applications.

**Comprehensive Performance Monitoring** enables detailed tracking of resource usage, inference timing, cache efficiency, and system metrics, providing essential insights for optimization and scaling decisions.

**Hub Integration Excellence** facilitates seamless model and dataset discovery, sharing, and collaboration through the HuggingFace Hub, supporting both public and private repositories with proper authentication and version control.

**Resource Optimization Strategies** including quantization support, device management, and batch processing ensure efficient utilization of computational resources across different hardware configurations from CPUs to high-end GPUs.

This comprehensive framework establishes the foundation for leveraging HuggingFace's ecosystem effectively in production environments, providing developers with the tools and methodologies necessary for building, deploying, and maintaining sophisticated machine learning applications at scale.