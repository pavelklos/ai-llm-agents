<small>Claude 3.7 Sonnet Thinking</small>
# 02. Prompt Design and LLM Evaluation

## Key Terms

- **Prompt Engineering**: The systematic design of text inputs to elicit desired outputs from language models, involving strategies like formatting, context provision, and instruction clarity.
- **One-shot Learning**: Providing a single example in a prompt to guide the model's output format and reasoning process.
- **Few-shot Learning**: Including multiple examples in a prompt to help the model recognize patterns and generate appropriate responses.
- **Chain of Thought (CoT)**: A prompting technique that encourages step-by-step reasoning by explicitly asking the model to explain its thought process.
- **Benchmarking**: Systematic evaluation of LLM performance using standardized tests, tasks, and datasets.
- **ROUGE/BLEU/METEOR**: Text evaluation metrics that compare model outputs against reference texts based on n-gram overlap and other syntactic features.
- **Perplexity**: A measurement of how well a language model predicts a sample of text, with lower values indicating better prediction capabilities.
- **Human Evaluation**: Assessment of LLM outputs by human raters based on qualitative criteria like coherence, factuality, and relevance.

## Advanced Prompt Engineering Strategies

### One-shot and Few-shot Prompting

One-shot and few-shot prompting leverage in-context learning to guide model behavior without fine-tuning:

```python
import os
from dotenv import load_dotenv
from openai import OpenAI
import anthropic
from typing import Dict, List, Optional, Union, Any

load_dotenv()

class PromptStrategies:
    def __init__(self, model_provider="openai", model_name=None):
        self.provider = model_provider.lower()
        
        if self.provider == "openai":
            self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            self.model = model_name or "gpt-4-turbo"
        elif self.provider == "anthropic":
            self.client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
            self.model = model_name or "claude-3-opus-20240229"
        else:
            raise ValueError(f"Unsupported provider: {model_provider}")
    
    def zero_shot_prompt(self, instruction: str, content: str) -> str:
        """
        Execute a zero-shot prompt with just instructions and content.
        
        Args:
            instruction: The task instruction
            content: The content to process
            
        Returns:
            Model response
        """
        prompt = f"{instruction}\n\n{content}"
        
        if self.provider == "openai":
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2
            )
            return response.choices[0].message.content
            
        elif self.provider == "anthropic":
            response = self.client.messages.create(
                model=self.model,
                max_tokens=1000,
                temperature=0.2,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.content[0].text
    
    def one_shot_prompt(self, instruction: str, example_input: str, example_output: str, 
                        new_input: str) -> str:
        """
        Execute a one-shot prompt with one example and a new input.
        
        Args:
            instruction: The task instruction
            example_input: An example input
            example_output: The expected output for the example
            new_input: The new input to process
            
        Returns:
            Model response
        """
        prompt = (
            f"{instruction}\n\n"
            f"Example Input: {example_input}\n"
            f"Example Output: {example_output}\n\n"
            f"New Input: {new_input}\n"
            f"Output:"
        )
        
        if self.provider == "openai":
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2
            )
            return response.choices[0].message.content
            
        elif self.provider == "anthropic":
            response = self.client.messages.create(
                model=self.model,
                max_tokens=1000,
                temperature=0.2,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.content[0].text
    
    def few_shot_prompt(self, instruction: str, examples: List[Dict[str, str]], 
                        new_input: str) -> str:
        """
        Execute a few-shot prompt with multiple examples and a new input.
        
        Args:
            instruction: The task instruction
            examples: List of dictionaries with 'input' and 'output' keys
            new_input: The new input to process
            
        Returns:
            Model response
        """
        examples_text = ""
        for i, example in enumerate(examples):
            examples_text += f"Example {i+1} Input: {example['input']}\n"
            examples_text += f"Example {i+1} Output: {example['output']}\n\n"
        
        prompt = (
            f"{instruction}\n\n"
            f"{examples_text}"
            f"New Input: {new_input}\n"
            f"Output:"
        )
        
        if self.provider == "openai":
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2
            )
            return response.choices[0].message.content
            
        elif self.provider == "anthropic":
            response = self.client.messages.create(
                model=self.model,
                max_tokens=1000,
                temperature=0.2,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.content[0].text
```

### Chain of Thought Prompting

Chain of Thought prompting improves reasoning by encouraging step-by-step thinking:

```python
class AdvancedPromptTechniques:
    def __init__(self, prompt_strategies: PromptStrategies):
        self.strategies = prompt_strategies
    
    def chain_of_thought_prompt(self, problem: str) -> str:
        """
        Implement Chain of Thought prompting for complex reasoning.
        
        Args:
            problem: The problem statement requiring multi-step reasoning
            
        Returns:
            Model response with step-by-step reasoning
        """
        prompt = (
            f"Problem: {problem}\n\n"
            f"Let's solve this step-by-step:\n"
            f"1. First, I'll analyze what information we have and what we need to find.\n"
            f"2. Then, I'll determine the appropriate approach to solve the problem.\n"
            f"3. Next, I'll work through the solution carefully, showing each step.\n"
            f"4. Finally, I'll verify the answer and provide the final solution."
        )
        
        if self.strategies.provider == "openai":
            response = self.strategies.client.chat.completions.create(
                model=self.strategies.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2
            )
            return response.choices[0].message.content
            
        elif self.strategies.provider == "anthropic":
            response = self.strategies.client.messages.create(
                model=self.strategies.model,
                max_tokens=2000,
                temperature=0.2,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.content[0].text
    
    def zero_shot_cot(self, problem: str) -> str:
        """
        Zero-shot Chain of Thought prompting.
        
        Args:
            problem: The problem statement
            
        Returns:
            Model response with reasoning
        """
        prompt = (
            f"Problem: {problem}\n\n"
            f"Let's think about this step by step to find the solution."
        )
        
        if self.strategies.provider == "openai":
            response = self.strategies.client.chat.completions.create(
                model=self.strategies.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2
            )
            return response.choices[0].message.content
            
        elif self.strategies.provider == "anthropic":
            response = self.strategies.client.messages.create(
                model=self.strategies.model,
                max_tokens=2000,
                temperature=0.2,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.content[0].text
    
    def tree_of_thoughts_prompt(self, problem: str, branching_factor: int = 3, depth: int = 2) -> str:
        """
        Implement Tree of Thoughts prompting with multiple reasoning paths.
        
        Args:
            problem: The problem statement
            branching_factor: Number of different approaches to consider at each step
            depth: Number of steps to think ahead
            
        Returns:
            Model response with tree-structured reasoning
        """
        prompt = (
            f"Problem: {problem}\n\n"
            f"I'll explore {branching_factor} different approaches to this problem, "
            f"and for each approach, I'll think {depth} steps ahead.\n\n"
        )
        
        for i in range(1, branching_factor + 1):
            prompt += f"Approach {i}:\n"
            prompt += f"Initial thought: Let me consider a potential solution method.\n"
            
            for j in range(1, depth + 1):
                prompt += f"Step {j}: Consider implications and next steps for this approach.\n"
            
            prompt += "\n"
        
        prompt += "After exploring these approaches, I'll determine which one leads to the best solution and provide my final answer."
        
        if self.strategies.provider == "openai":
            response = self.strategies.client.chat.completions.create(
                model=self.strategies.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7  # Higher temperature for more diverse thinking paths
            )
            return response.choices[0].message.content
            
        elif self.strategies.provider == "anthropic":
            response = self.strategies.client.messages.create(
                model=self.strategies.model,
                max_tokens=3000,
                temperature=0.7,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.content[0].text
```

## LLM Benchmarking and Evaluation

### Comprehensive Benchmarking Framework

```python
import pandas as pd
import numpy as np
import time
import json
from typing import List, Dict, Any, Callable, Optional, Union, Tuple
import matplotlib.pyplot as plt
from tqdm.auto import tqdm
import datasets
from rouge_score import rouge_scorer
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import nltk
import logging

# Download needed NLTK packages
nltk.download('punkt', quiet=True)

class LLMBenchmark:
    def __init__(self, models: List[Dict[str, Any]], tasks: List[Dict[str, Any]]):
        """
        Initialize benchmarking framework.
        
        Args:
            models: List of model configs with provider, name and optional params
            tasks: List of task configs with name, description, prompt_template and evaluation_fn
        """
        self.models = models
        self.tasks = tasks
        self.results = {}
        self.logger = self._setup_logger()
        
        # Initialize prompt strategies for each model
        self.model_clients = {}
        for model in self.models:
            model_key = f"{model['provider']}_{model['name']}"
            try:
                self.model_clients[model_key] = PromptStrategies(
                    model_provider=model['provider'],
                    model_name=model['name']
                )
                self.logger.info(f"Successfully initialized model: {model_key}")
            except Exception as e:
                self.logger.error(f"Failed to initialize model {model_key}: {str(e)}")
                
    def _setup_logger(self) -> logging.Logger:
        """Configure and return a logger"""
        logger = logging.getLogger("LLMBenchmark")
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            
        return logger
                
    def run_benchmark(self, dataset: Union[List[Dict], datasets.Dataset], 
                      output_path: Optional[str] = None) -> pd.DataFrame:
        """
        Run benchmark on all models and tasks.
        
        Args:
            dataset: Dataset containing inputs for tasks
            output_path: Path to save benchmark results
            
        Returns:
            DataFrame with benchmark results
        """
        results = []
        
        # Convert datasets.Dataset to list if needed
        if isinstance(dataset, datasets.Dataset):
            dataset = dataset.to_dict()
            data_list = []
            for i in range(len(dataset[list(dataset.keys())[0]])):
                item = {k: v[i] for k, v in dataset.items()}
                data_list.append(item)
            dataset = data_list
        
        # Run benchmark
        for model in tqdm(self.models, desc="Models"):
            model_key = f"{model['provider']}_{model['name']}"
            model_client = self.model_clients.get(model_key)
            
            if not model_client:
                self.logger.warning(f"Skipping model {model_key}: Not initialized")
                continue
                
            for task in tqdm(self.tasks, desc=f"Tasks for {model_key}", leave=False):
                task_results = []
                
                for item in tqdm(dataset[:min(len(dataset), 10)], desc=f"Examples for {task['name']}", leave=False):
                    try:
                        # Format prompt using template
                        prompt = task['prompt_template'].format(**item)
                        
                        # Track timing
                        start_time = time.time()
                        response = model_client.zero_shot_prompt(
                            instruction=task['description'],
                            content=prompt
                        )
                        elapsed_time = time.time() - start_time
                        
                        # Evaluate response
                        if 'evaluation_fn' in task and task['evaluation_fn']:
                            evaluation_result = task['evaluation_fn'](
                                response=response,
                                reference=item.get('reference', ''),
                                **item
                            )
                        else:
                            evaluation_result = {'success': None, 'score': None}
                        
                        # Store result
                        result = {
                            'model': model['name'],
                            'provider': model['provider'],
                            'task': task['name'],
                            'input': prompt[:100] + '...' if len(prompt) > 100 else prompt,
                            'response': response[:100] + '...' if len(response) > 100 else response,
                            'metrics': evaluation_result,
                            'latency': elapsed_time
                        }
                        
                        task_results.append(result)
                    except Exception as e:
                        self.logger.error(f"Error with model {model_key} on task {task['name']}: {str(e)}")
                        
                # Aggregate task results
                if task_results:
                    avg_latency = sum(r['latency'] for r in task_results) / len(task_results)
                    
                    # Aggregate metrics if numeric
                    metrics = {}
                    for r in task_results:
                        for k, v in r['metrics'].items():
                            if isinstance(v, (int, float)):
                                metrics[k] = metrics.get(k, 0) + v
                    
                    # Calculate averages
                    for k in metrics:
                        metrics[k] /= len(task_results)
                    
                    aggregated_result = {
                        'model': model['name'],
                        'provider': model['provider'],
                        'task': task['name'],
                        'avg_latency': avg_latency,
                        'metrics': metrics,
                        'sample_size': len(task_results)
                    }
                    
                    results.append(aggregated_result)
        
        # Convert results to DataFrame
        flat_results = []
        for r in results:
            flat_r = {
                'model': r['model'],
                'provider': r['provider'],
                'task': r['task'],
                'avg_latency': r['avg_latency'],
                'sample_size': r['sample_size']
            }
            
            # Flatten metrics
            for k, v in r['metrics'].items():
                flat_r[f'metric_{k}'] = v
                
            flat_results.append(flat_r)
            
        results_df = pd.DataFrame(flat_results)
        
        # Save results if output path provided
        if output_path:
            results_df.to_csv(output_path, index=False)
            
            # Save raw results as JSON
            with open(output_path.replace('.csv', '.json'), 'w') as f:
                json.dump(results, f)
        
        return results_df
    
    def visualize_results(self, results_df: pd.DataFrame, metric_col: str = 'metric_score',
                          output_path: Optional[str] = None):
        """
        Visualize benchmark results.
        
        Args:
            results_df: DataFrame with benchmark results
            metric_col: Column name for the metric to visualize
            output_path: Path to save visualization
        """
        plt.figure(figsize=(12, 8))
        
        # Create pivot table
        pivot = pd.pivot_table(
            results_df,
            values=metric_col,
            index='task',
            columns=['provider', 'model'],
            aggfunc='mean'
        )
        
        # Plot heatmap
        ax = plt.subplot(111)
        im = ax.imshow(pivot.values, cmap='YlGn')
        
        # Set ticks
        ax.set_xticks(np.arange(len(pivot.columns)))
        ax.set_yticks(np.arange(len(pivot.index)))
        
        # Set tick labels
        ax.set_xticklabels([f"{p}_{m}" for p, m in pivot.columns])
        ax.set_yticklabels(pivot.index)
        
        # Rotate x tick labels
        plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")
        
        # Add colorbar
        plt.colorbar(im)
        
        # Add title
        plt.title(f"LLM Benchmark Results - {metric_col}")
        
        plt.tight_layout()
        
        if output_path:
            plt.savefig(output_path)
        
        plt.close()
```

### Evaluation Metrics and Functions

```python
class EvaluationMetrics:
    @staticmethod
    def rouge_evaluation(response: str, reference: str, **kwargs) -> Dict[str, float]:
        """
        Calculate ROUGE scores for a response against a reference.
        
        Args:
            response: Model generated text
            reference: Gold reference text
            
        Returns:
            Dictionary of ROUGE scores
        """
        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
        scores = scorer.score(reference, response)
        
        return {
            'rouge1_f': scores['rouge1'].fmeasure,
            'rouge2_f': scores['rouge2'].fmeasure,
            'rougeL_f': scores['rougeL'].fmeasure,
            'score': scores['rougeL'].fmeasure  # Use rougeL as the primary score
        }
    
    @staticmethod
    def bleu_evaluation(response: str, reference: str, **kwargs) -> Dict[str, float]:
        """
        Calculate BLEU score for a response against a reference.
        
        Args:
            response: Model generated text
            reference: Gold reference text
            
        Returns:
            Dictionary with BLEU score
        """
        # Tokenize
        reference_tokens = nltk.word_tokenize(reference.lower())
        hypothesis_tokens = nltk.word_tokenize(response.lower())
        
        # Calculate BLEU with smoothing
        smoothie = SmoothingFunction().method1
        bleu_score = sentence_bleu([reference_tokens], hypothesis_tokens, 
                                 smoothing_function=smoothie)
        
        return {
            'bleu': bleu_score,
            'score': bleu_score
        }
    
    @staticmethod
    def exact_match_evaluation(response: str, reference: str, **kwargs) -> Dict[str, float]:
        """
        Check for exact match between response and reference.
        
        Args:
            response: Model generated text
            reference: Gold reference text
            
        Returns:
            Dictionary with exact match score (0 or 1)
        """
        # Normalize text: lowercase and strip whitespace
        norm_response = response.lower().strip()
        norm_reference = reference.lower().strip()
        
        match = int(norm_response == norm_reference)
        
        return {
            'exact_match': match,
            'score': match
        }
    
    @staticmethod
    def custom_rule_evaluation(response: str, rules: List[Callable], **kwargs) -> Dict[str, float]:
        """
        Evaluate response using custom rules.
        
        Args:
            response: Model generated text
            rules: List of rule functions that return True/False
            
        Returns:
            Dictionary with rule compliance scores
        """
        results = {}
        
        # Apply each rule
        for i, rule in enumerate(rules):
            rule_name = rule.__name__ if hasattr(rule, '__name__') else f'rule_{i}'
            results[rule_name] = int(rule(response))
        
        # Calculate overall score as percentage of rules passed
        if rules:
            overall_score = sum(results.values()) / len(rules)
        else:
            overall_score = 0
            
        results['score'] = overall_score
        
        return results
```

## Human and Automated Evaluation Systems

### Automated Evaluation System

```python
class AutomatedEvaluation:
    def __init__(self, evaluator_model="gpt-4-turbo"):
        """
        Initialize automated evaluator using an LLM.
        
        Args:
            evaluator_model: Model to use for evaluation
        """
        load_dotenv()
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.model = evaluator_model
    
    def evaluate_factuality(self, response: str, reference_or_context: str) -> Dict[str, Any]:
        """
        Evaluate factual accuracy of a response against reference information.
        
        Args:
            response: Model response to evaluate
            reference_or_context: Reference text or context information
            
        Returns:
            Evaluation results including score and explanation
        """
        prompt = (
            "You are an expert evaluator assessing the factual accuracy of an AI assistant's response. "
            "Please evaluate the factual correctness of the response based on the reference information. "
            "Consider only factual accuracy, not style, tone, or comprehensiveness.\n\n"
            f"Reference information: {reference_or_context}\n\n"
            f"Response to evaluate: {response}\n\n"
            "Evaluation instructions:\n"
            "1. Identify any factual claims in the response\n"
            "2. Determine if each claim is supported by the reference information\n"
            "3. Note any contradictions or unsupported claims\n"
            "4. Provide a factuality score from 1-5, where:\n"
            "   - 1: Contains major factual errors or contradictions\n"
            "   - 2: Contains some factual errors\n"
            "   - 3: Mostly accurate with minor errors\n"
            "   - 4: Highly accurate with very minor imprecisions\n"
            "   - 5: Perfectly factual, all claims supported by reference\n\n"
            "Output format:\n"
            "Score: [1-5]\n"
            "Explanation: [Your reasoning]\n"
            "Factual errors: [List specific errors if any]"
        )
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
            response_format={"type": "text"}
        )
        
        evaluation_text = response.choices[0].message.content
        
        # Parse evaluation
        try:
            lines = evaluation_text.strip().split('\n')
            score_line = next((line for line in lines if line.lower().startswith('score:')), '')
            score = int(score_line.split(':')[1].strip().split()[0]) if score_line else 0
            
            result = {
                'score': score / 5.0,  # Normalize to 0-1 scale
                'raw_score': score,
                'evaluation': evaluation_text
            }
        except Exception:
            result = {
                'score': 0,
                'raw_score': 0,
                'evaluation': "Failed to parse evaluation",
                'error': "Parsing error"
            }
        
        return result
    
    def evaluate_coherence(self, response: str) -> Dict[str, Any]:
        """
        Evaluate coherence, clarity and logical flow of a response.
        
        Args:
            response: Model response to evaluate
            
        Returns:
            Evaluation results including score and explanation
        """
        prompt = (
            "You are an expert evaluator assessing the coherence and clarity of an AI assistant's response. "
            "Please evaluate how well-structured, logical, and clear the response is.\n\n"
            f"Response to evaluate: {response}\n\n"
            "Evaluation instructions:\n"
            "1. Assess the logical flow and organization of ideas\n"
            "2. Check if the reasoning is clear and well-structured\n"
            "3. Determine if the response is easy to follow and understand\n"
            "4. Provide a coherence score from 1-5, where:\n"
            "   - 1: Incoherent, illogical, or contradictory\n"
            "   - 2: Confusing with significant issues in organization\n"
            "   - 3: Somewhat clear but with organizational issues\n"
            "   - 4: Clear and mostly well-organized\n"
            "   - 5: Exceptionally clear, logical, and well-structured\n\n"
            "Output format:\n"
            "Score: [1-5]\n"
            "Explanation: [Your reasoning]\n"
            "Areas for improvement: [Specific suggestions]"
        )
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2
        )
        
        evaluation_text = response.choices[0].message.content
        
        # Parse evaluation
        try:
            lines = evaluation_text.strip().split('\n')
            score_line = next((line for line in lines if line.lower().startswith('score:')), '')
            score = int(score_line.split(':')[1].strip().split()[0]) if score_line else 0
            
            result = {
                'score': score / 5.0,  # Normalize to 0-1 scale
                'raw_score': score,
                'evaluation': evaluation_text
            }
        except Exception:
            result = {
                'score': 0,
                'raw_score': 0,
                'evaluation': "Failed to parse evaluation",
                'error': "Parsing error"
            }
        
        return result
    
    def evaluate_helpfulness(self, question: str, response: str) -> Dict[str, Any]:
        """
        Evaluate how helpful a response is for the given question.
        
        Args:
            question: Original question or instruction
            response: Model response to evaluate
            
        Returns:
            Evaluation results including score and explanation
        """
        prompt = (
            "You are an expert evaluator assessing how helpful an AI assistant's response is. "
            "Please evaluate how well the response addresses the user's question or need.\n\n"
            f"User question: {question}\n\n"
            f"AI response: {response}\n\n"
            "Evaluation instructions:\n"
            "1. Determine if the response directly addresses the question\n"
            "2. Assess if the response provides useful information or guidance\n"
            "3. Consider if the response is complete or leaves important gaps\n"
            "4. Provide a helpfulness score from 1-5, where:\n"
            "   - 1: Not helpful, doesn't address the question\n"
            "   - 2: Minimally helpful with significant gaps\n"
            "   - 3: Moderately helpful but incomplete\n"
            "   - 4: Very helpful with minor omissions\n"
            "   - 5: Exceptionally helpful and comprehensive\n\n"
            "Output format:\n"
            "Score: [1-5]\n"
            "Explanation: [Your reasoning]\n"
            "Missing information: [What else could have been included]"
        )
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2
        )
        
        evaluation_text = response.choices[0].message.content
        
        # Parse evaluation
        try:
            lines = evaluation_text.strip().split('\n')
            score_line = next((line for line in lines if line.lower().startswith('score:')), '')
            score = int(score_line.split(':')[1].strip().split()[0]) if score_line else 0
            
            result = {
                'score': score / 5.0,  # Normalize to 0-1 scale
                'raw_score': score,
                'evaluation': evaluation_text
            }
        except Exception:
            result = {
                'score': 0,
                'raw_score': 0,
                'evaluation': "Failed to parse evaluation",
                'error': "Parsing error"
            }
        
        return result
```

### Human Evaluation System

```python
class HumanEvaluationSystem:
    def __init__(self, db_path="human_evaluations.sqlite"):
        """
        Initialize human evaluation system with database storage.
        
        Args:
            db_path: Path to SQLite database
        """
        self.db_path = db_path
        self._setup_database()
    
    def _setup_database(self):
        """Create database tables if they don't exist"""
        import sqlite3
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Create evaluations table
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS evaluations (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            model_name TEXT,
            prompt_id TEXT,
            prompt_text TEXT,
            response_text TEXT,
            evaluator_id TEXT,
            score_accuracy REAL,
            score_coherence REAL,
            score_helpfulness REAL,
            score_overall REAL,
            comments TEXT,
            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        ''')
        
        # Create evaluators table
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS evaluators (
            id TEXT PRIMARY KEY,
            name TEXT,
            expertise_level TEXT,
            registration_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        ''')
        
        conn.commit()
        conn.close()
    
    def create_evaluation_form(self, model_name: str, prompt_id: str, 
                              prompt_text: str, response_text: str) -> Dict[str, Any]:
        """
        Create evaluation form for human evaluator.
        
        Args:
            model_name: Name of the model being evaluated
            prompt_id: Unique ID for the prompt
            prompt_text: The prompt text
            response_text: The model's response
            
        Returns:
            Dictionary with evaluation form data
        """
        return {
            'model_name': model_name,
            'prompt_id': prompt_id,
            'prompt_text': prompt_text,
            'response_text': response_text,
            'evaluation_criteria': [
                {
                    'name': 'accuracy',
                    'description': 'How factually accurate is the response?',
                    'scale': 'From 1 (contains significant errors) to 5 (completely accurate)'
                },
                {
                    'name': 'coherence',
                    'description': 'How well-organized and logical is the response?',
                    'scale': 'From 1 (incoherent) to 5 (exceptionally clear and logical)'
                },
                {
                    'name': 'helpfulness',
                    'description': 'How helpful is the response in addressing the prompt?',
                    'scale': 'From 1 (not helpful) to 5 (exceptionally helpful)'
                },
                {
                    'name': 'overall',
                    'description': 'Overall quality of the response',
                    'scale': 'From 1 (poor) to 5 (excellent)'
                }
            ]
        }
    
    def submit_evaluation(self, evaluation_data: Dict[str, Any]) -> bool:
        """
        Submit a human evaluation.
        
        Args:
            evaluation_data: Dictionary with evaluation data including scores
            
        Returns:
            True if successfully saved, False otherwise
        """
        import sqlite3
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
            INSERT INTO evaluations (
                model_name, prompt_id, prompt_text, response_text, evaluator_id,
                score_accuracy, score_coherence, score_helpfulness, score_overall, comments
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                evaluation_data['model_name'],
                evaluation_data['prompt_id'],
                evaluation_data['prompt_text'],
                evaluation_data['response_text'],
                evaluation_data['evaluator_id'],
                evaluation_data['scores']['accuracy'],
                evaluation_data['scores']['coherence'],
                evaluation_data['scores']['helpfulness'],
                evaluation_data['scores']['overall'],
                evaluation_data.get('comments', '')
            ))
            
            conn.commit()
            conn.close()
            
            return True
        except Exception as e:
            print(f"Error saving evaluation: {str(e)}")
            return False
            
    def get_evaluation_results(self, model_name: Optional[str] = None, 
                              prompt_id: Optional[str] = None) -> pd.DataFrame:
        """
        Retrieve evaluation results with optional filtering.
        
        Args:
            model_name: Filter by model name
            prompt_id: Filter by prompt ID
            
        Returns:
            DataFrame with evaluation results
        """
        import sqlite3
        
        conn = sqlite3.connect(self.db_path)
        
        query = "SELECT * FROM evaluations"
        params = []
        
        # Apply filters
        if model_name or prompt_id:
            query += " WHERE "
            conditions = []
            
            if model_name:
                conditions.append("model_name = ?")
                params.append(model_name)
                
            if prompt_id:
                conditions.append("prompt_id = ?")
                params.append(prompt_id)
                
            query += " AND ".join(conditions)
        
        # Execute query
        df = pd.read_sql_query(query, conn, params=params)
        conn.close()
        
        return df
    
    def analyze_evaluation_results(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        Analyze human evaluation results.
        
        Args:
            df: DataFrame with evaluation results
            
        Returns:
            Dictionary with analysis results
        """
        if df.empty:
            return {"error": "No evaluation data available"}
            
        # Calculate mean scores
        mean_scores = {
            'accuracy': df['score_accuracy'].mean(),
            'coherence': df['score_coherence'].mean(),
            'helpfulness': df['score_helpfulness'].mean(),
            'overall': df['score_overall'].mean()
        }
        
        # Calculate score distributions
        score_distributions = {
            'accuracy': df['score_accuracy'].value_counts().to_dict(),
            'coherence': df['score_coherence'].value_counts().to_dict(),
            'helpfulness': df['score_helpfulness'].value_counts().to_dict(),
            'overall': df['score_overall'].value_counts().to_dict()
        }
        
        # Group by model if multiple models
        model_comparison = None
        if len(df['model_name'].unique()) > 1:
            model_comparison = df.groupby('model_name')[
                ['score_accuracy', 'score_coherence', 'score_helpfulness', 'score_overall']
            ].mean().to_dict()
        
        return {
            'mean_scores': mean_scores,
            'score_distributions': score_distributions,
            'model_comparison': model_comparison,
            'sample_size': len(df),
            'unique_evaluators': df['evaluator_id'].nunique()
        }
```

## Practical Exercise: LLM Benchmarking with Prompt Engineering

```python
def llm_benchmarking_exercise():
    """
    Practical exercise for benchmarking LLMs with different prompt engineering techniques.
    """
    # Load environment variables
    load_dotenv()
    
    # Define models to benchmark
    models = [
        {'provider': 'openai', 'name': 'gpt-4-turbo'},
        {'provider': 'openai', 'name': 'gpt-3.5-turbo'},
        {'provider': 'anthropic', 'name': 'claude-3-haiku-20240307'},
    ]
    
    # Define evaluation functions
    def math_evaluation(response, reference=None, **kwargs):
        """Evaluate math problem solution"""
        # Extract numerical answer from response
        import re
        answer_match = re.search(r'(?:answer|result|solution)[:\s]+([+-]?\d+(?:\.\d+)?)', 
                                  response.lower())
        
        if not answer_match:
            return {'success': False, 'score': 0, 'error': 'No numerical answer found'}
            
        try:
            answer = float(answer_match.group(1))
            expected = float(kwargs.get('expected_answer', reference))
            
            # Check if answer is within tolerance
            tolerance = 1e-6
            is_correct = abs(answer - expected) < tolerance
            
            return {
                'success': is_correct,
                'score': 1.0 if is_correct else 0.0,
                'answer': answer,
                'expected': expected
            }
        except Exception as e:
            return {'success': False, 'score': 0, 'error': str(e)}
    
    # Define tasks with different prompting strategies
    tasks = [
        {
            'name': 'Math problem solving (zero-shot)',
            'description': 'Solve the following math problem step by step.',
            'prompt_template': '{problem}',
            'evaluation_fn': math_evaluation
        },
        {
            'name': 'Math problem solving (CoT)',
            'description': 'Solve the following math problem by thinking step by step.',
            'prompt_template': '{problem}\n\nLet\'s solve this step-by-step:',
            'evaluation_fn': math_evaluation
        },
        {
            'name': 'Math problem solving (few-shot)',
            'description': 'Solve the following math problem.',
            'prompt_template': 'Example 1: What is 25 * 9?\nLet me solve this step-by-step:\n'
                               '25 * 9 = (20 + 5) * 9\n= 20 * 9 + 5 * 9\n= 180 + 45\n= 225\n'
                               'The answer is 225.\n\n'
                               'Example 2: If a triangle has a base of 8 cm and a height of 5 cm, what is its area?\n'
                               'Let me solve this step-by-step:\n'
                               'Area of a triangle = (1/2) * base * height\n'
                               '= (1/2) * 8 * 5\n= (1/2) * 40\n= 20\n'
                               'The area of the triangle is 20 square cm.\n\n'
                               'Now solve: {problem}',
            'evaluation_fn': math_evaluation
        }
    ]
    
    # Create benchmark dataset
    benchmark_dataset = [
        {
            'problem': 'If a rectangle has a length of 12 meters and a width of 8 meters, what is its area?',
            'expected_answer': 96
        },
        {
            'problem': 'A store sells notebooks for $3 each. If you buy 5 notebooks and pay with a $20 bill, how much change will you receive?',
            'expected_answer': 5
        },
        {
            'problem': 'If a train travels at 80 km/h, how long will it take to travel 240 km?',
            'expected_answer': 3
        },
        {
            'problem': 'Find the value of x in the equation 3x + 7 = 22.',
            'expected_answer': 5
        },
        {
            'problem': 'If the average of three numbers is 15 and two of the numbers are 12 and 18, what is the third number?',
            'expected_answer': 15
        }
    ]
    
    # Initialize benchmark
    print("Initializing benchmark...")
    benchmark = LLMBenchmark(models=models, tasks=tasks)
    
    # Run benchmark
    print("Running benchmark...")
    results_df = benchmark.run_benchmark(
        dataset=benchmark_dataset,
        output_path="benchmark_results.csv"
    )
    
    # Visualize results
    print("Visualizing results...")
    benchmark.visualize_results(
        results_df=results_df,
        metric_col='metric_score',
        output_path="benchmark_visualization.png"
    )
    
    # Print summary
    print("\nBenchmark Summary:")
    for model in models:
        model_name = model['name']
        model_results = results_df[results_df['model'] == model_name]
        
        if not model_results.empty:
            avg_score = model_results['metric_score'].mean()
            avg_latency = model_results['avg_latency'].mean()
            
            print(f"Model: {model_name}")
            print(f"  Average Score: {avg_score:.4f}")
            print(f"  Average Latency: {avg_latency:.4f}s")
            
            # Performance by task type
            print("  Performance by Task:")
            for task in tasks:
                task_name = task['name']
                task_results = model_results[model_results['task'] == task_name]
                
                if not task_results.empty:
                    task_score = task_results['metric_score'].mean()
                    print(f"    {task_name}: {task_score:.4f}")
            
            print()
    
    return results_df

# Run the exercise
if __name__ == "__main__":
    llm_benchmarking_exercise()
```

## Conclusion

Prompt engineering and LLM evaluation form the cornerstone of effective AI system development. The strategies and techniques explored in this section demonstrate how strategic prompt design can dramatically impact model performance across various tasks. One-shot and few-shot learning techniques allow models to adapt to new tasks without fine-tuning, while Chain of Thought prompting significantly enhances reasoning capabilities for complex problems.

Robust evaluation frameworks are essential for measuring and comparing LLM performance across multiple dimensions, from factual accuracy to logical coherence. The combination of automated metrics (ROUGE, BLEU) with LLM-as-judge approaches and human evaluation provides a comprehensive assessment methodology that can guide model selection and improvement efforts.

The practical benchmarking exercise illustrates how different prompt engineering techniques affect model performance on specific tasks, highlighting the importance of prompt design in extracting optimal performance from language models. This systematic approach to prompting and evaluation enables developers to make informed decisions about which models and techniques best suit their specific applications.

As language models continue to evolve, prompt engineering will remain a critical skill for AI developers, allowing them to maximize model capabilities while maintaining control over outputs. Similarly, robust evaluation methodologies will be essential for measuring progress, comparing models, and ensuring AI systems meet performance requirements across diverse applications.