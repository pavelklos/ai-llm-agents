<small>Claude web</small>
# 06. LangChain and LangGraph

## Key Terms and Concepts

**LangChain**: A comprehensive framework for developing applications powered by large language models (LLMs). It provides abstractions for prompts, chains, tools, and memory management, enabling rapid development of AI-powered applications.

**LangGraph**: A library built on top of LangChain for creating stateful, multi-actor applications with LLMs. It allows modeling agent workflows as graphs where nodes represent different actions and edges represent transitions between states.

**Retrieval-Augmented Generation (RAG)**: A technique that combines information retrieval with text generation, allowing models to access and incorporate external knowledge sources during response generation.

**LangServe**: A deployment framework for LangChain applications that enables easy serving of chains and agents as REST APIs with automatic OpenAPI documentation.

**LangSmith**: A monitoring and debugging platform for LangChain applications that provides tracing, evaluation, and performance analytics for production AI systems.

**Prompt Templates**: Structured templates that define how to format inputs for language models, supporting variable substitution and conditional logic.

**Chains**: Sequential operations that combine multiple components (prompts, models, parsers) to accomplish complex tasks through composition.

**Tools**: External functions or APIs that agents can invoke to perform specific actions or retrieve information from external systems.

## LangChain Fundamentals

### Advanced Prompt Engineering with Templates

```python
import os
from langchain.prompts import PromptTemplate, ChatPromptTemplate, FewShotPromptTemplate
from langchain.prompts.example_selector import SemanticSimilarityExampleSelector
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.schema import BaseOutputParser
from langchain.chat_models import ChatOpenAI
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field
from typing import List, Dict, Any
import json

# Load environment variables
from dotenv import load_dotenv
load_dotenv()

class AnalysisResult(BaseModel):
    sentiment: str = Field(description="Overall sentiment: positive, negative, or neutral")
    key_topics: List[str] = Field(description="Main topics discussed")
    confidence: float = Field(description="Confidence score between 0 and 1")
    summary: str = Field(description="Brief summary of the analysis")

class AdvancedPromptManager:
    def __init__(self, model_name: str = "gpt-4"):
        self.llm = ChatOpenAI(
            model=model_name,
            temperature=0.1,
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        self.embeddings = OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY"))
        
    def create_few_shot_prompt(self, examples: List[Dict], task_description: str):
        """Create a few-shot learning prompt with semantic similarity selection"""
        
        # Create example selector based on semantic similarity
        example_selector = SemanticSimilarityExampleSelector.from_examples(
            examples,
            self.embeddings,
            Chroma,
            k=2  # Select 2 most similar examples
        )
        
        # Define the few-shot prompt template
        few_shot_prompt = FewShotPromptTemplate(
            example_selector=example_selector,
            example_prompt=PromptTemplate(
                input_variables=["input", "output"],
                template="Input: {input}\nOutput: {output}"
            ),
            prefix=f"{task_description}\n\nHere are some examples:",
            suffix="Input: {input}\nOutput:",
            input_variables=["input"],
        )
        
        return few_shot_prompt
    
    def create_structured_output_chain(self):
        """Create a chain that produces structured output using Pydantic"""
        
        parser = PydanticOutputParser(pydantic_object=AnalysisResult)
        
        prompt = PromptTemplate(
            template="""Analyze the following text and provide a structured response.

{format_instructions}

Text to analyze: {text}

Your analysis:""",
            input_variables=["text"],
            partial_variables={"format_instructions": parser.get_format_instructions()}
        )
        
        return prompt | self.llm | parser

# Example usage
examples = [
    {
        "input": "The product launch was incredibly successful, exceeding all expectations!",
        "output": json.dumps({
            "sentiment": "positive",
            "key_topics": ["product launch", "success", "expectations"],
            "confidence": 0.95,
            "summary": "Highly positive response to successful product launch"
        })
    },
    {
        "input": "The server crashed again today, causing significant downtime.",
        "output": json.dumps({
            "sentiment": "negative", 
            "key_topics": ["server issues", "downtime", "technical problems"],
            "confidence": 0.88,
            "summary": "Technical difficulties with server reliability"
        })
    }
]

prompt_manager = AdvancedPromptManager()
structured_chain = prompt_manager.create_structured_output_chain()

# Test the structured output chain
sample_text = "Our AI agent implementation has shown remarkable improvements in efficiency, though we've encountered some minor integration challenges."

try:
    result = structured_chain.invoke({"text": sample_text})
    print(f"Analysis Result: {result}")
except Exception as e:
    print(f"Error: {e}")
```

### Custom Tools and Function Calling

```python
from langchain.tools import BaseTool, StructuredTool
from langchain.agents import create_openai_functions_agent, AgentExecutor
from langchain.schema import SystemMessage
import requests
import sqlite3
from datetime import datetime
from typing import Optional

class DatabaseTool(BaseTool):
    name: str = "database_query"
    description: str = "Execute SQL queries on the local database to retrieve or store information"
    
    def __init__(self, db_path: str = "agent_data.db"):
        super().__init__()
        self.db_path = db_path
        self._init_database()
    
    def _init_database(self):
        """Initialize the database with sample tables"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS tasks (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT NOT NULL,
                description TEXT,
                status TEXT DEFAULT 'pending',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                completed_at TIMESTAMP
            )
        """)
        
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS knowledge_base (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                category TEXT NOT NULL,
                question TEXT NOT NULL,
                answer TEXT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        conn.commit()
        conn.close()
    
    def _run(self, query: str) -> str:
        """Execute the SQL query and return results"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Security check - only allow SELECT, INSERT, UPDATE
            query_upper = query.upper().strip()
            if not any(query_upper.startswith(cmd) for cmd in ['SELECT', 'INSERT', 'UPDATE']):
                return "Error: Only SELECT, INSERT, and UPDATE queries are allowed"
            
            cursor.execute(query)
            
            if query_upper.startswith('SELECT'):
                results = cursor.fetchall()
                columns = [description[0] for description in cursor.description]
                
                if results:
                    formatted_results = []
                    for row in results:
                        row_dict = dict(zip(columns, row))
                        formatted_results.append(row_dict)
                    return json.dumps(formatted_results, indent=2)
                else:
                    return "No results found"
            else:
                conn.commit()
                return f"Query executed successfully. Rows affected: {cursor.rowcount}"
                
        except Exception as e:
            return f"Database error: {str(e)}"
        finally:
            conn.close()

class WeatherTool(BaseTool):
    name: str = "get_weather"
    description: str = "Get current weather information for a specified city"
    
    def _run(self, city: str) -> str:
        """Get weather data for the specified city"""
        api_key = os.getenv("WEATHER_API_KEY")
        if not api_key:
            return "Weather API key not configured"
        
        try:
            url = f"http://api.openweathermap.org/data/2.5/weather"
            params = {
                "q": city,
                "appid": api_key,
                "units": "metric"
            }
            
            response = requests.get(url, params=params)
            response.raise_for_status()
            
            data = response.json()
            weather_info = {
                "city": data["name"],
                "temperature": data["main"]["temp"],
                "description": data["weather"][0]["description"],
                "humidity": data["main"]["humidity"],
                "pressure": data["main"]["pressure"]
            }
            
            return json.dumps(weather_info, indent=2)
            
        except Exception as e:
            return f"Weather API error: {str(e)}"

class AdvancedAgentSystem:
    def __init__(self):
        self.llm = ChatOpenAI(
            model="gpt-4",
            temperature=0.1,
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        
        self.tools = [
            DatabaseTool(),
            WeatherTool(),
            StructuredTool.from_function(
                func=self._calculate_metrics,
                name="calculate_metrics",
                description="Calculate various metrics and statistics from numerical data"
            )
        ]
        
        self.system_message = SystemMessage(content="""
You are an advanced AI agent with access to multiple tools. You can:
- Query and update a local database to store and retrieve information
- Get weather information for any city
- Calculate metrics and statistics

Always think step by step and use the appropriate tools to help users accomplish their tasks.
When working with data, be precise and provide clear explanations of your actions.
""")
        
        self.agent = create_openai_functions_agent(
            llm=self.llm,
            tools=self.tools,
            prompt=ChatPromptTemplate.from_messages([
                self.system_message,
                ("human", "{input}"),
                ("placeholder", "{agent_scratchpad}")
            ])
        )
        
        self.agent_executor = AgentExecutor(
            agent=self.agent,
            tools=self.tools,
            verbose=True,
            max_iterations=5
        )
    
    def _calculate_metrics(self, numbers: str) -> str:
        """Calculate mean, median, and standard deviation from a list of numbers"""
        try:
            nums = [float(x.strip()) for x in numbers.split(',')]
            
            mean = sum(nums) / len(nums)
            sorted_nums = sorted(nums)
            n = len(sorted_nums)
            median = sorted_nums[n//2] if n % 2 == 1 else (sorted_nums[n//2-1] + sorted_nums[n//2]) / 2
            
            variance = sum((x - mean) ** 2 for x in nums) / len(nums)
            std_dev = variance ** 0.5
            
            return json.dumps({
                "count": len(nums),
                "mean": round(mean, 2),
                "median": median,
                "std_deviation": round(std_dev, 2),
                "min": min(nums),
                "max": max(nums)
            }, indent=2)
            
        except Exception as e:
            return f"Error calculating metrics: {str(e)}"
    
    def run(self, query: str) -> str:
        """Execute a query using the agent system"""
        try:
            result = self.agent_executor.invoke({"input": query})
            return result["output"]
        except Exception as e:
            return f"Agent execution error: {str(e)}"

# Example usage
agent_system = AdvancedAgentSystem()

# Test the agent with various tasks
test_queries = [
    "Create a new task in the database: 'Implement RAG system' with description 'Build retrieval-augmented generation for customer support'",
    "Show me all pending tasks from the database",
    "What's the weather like in Prague?",
    "Calculate metrics for these numbers: 12, 15, 18, 22, 25, 28, 30, 33"
]

for query in test_queries:
    print(f"\nQuery: {query}")
    print(f"Response: {agent_system.run(query)}")
    print("-" * 80)
```

## Retrieval-Augmented Generation (RAG) Implementation

```python
from langchain.document_loaders import PyPDFLoader, TextLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma, FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.chains import RetrievalQA, ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.schema import Document
import os
from pathlib import Path
from typing import List, Optional

class AdvancedRAGSystem:
    def __init__(self, persist_directory: str = "./chroma_db"):
        self.embeddings = OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY"))
        self.llm = ChatOpenAI(
            model="gpt-4",
            temperature=0.1,
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        self.persist_directory = persist_directory
        self.vectorstore = None
        self.retrieval_chain = None
        
        # Initialize text splitter with optimal parameters
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )
    
    def load_documents(self, directory_path: str) -> List[Document]:
        """Load documents from various file formats"""
        documents = []
        
        # Load different file types
        loaders = {
            '.pdf': PyPDFLoader,
            '.txt': TextLoader,
            '.md': TextLoader
        }
        
        directory = Path(directory_path)
        for file_path in directory.rglob("*"):
            if file_path.suffix.lower() in loaders:
                try:
                    loader_class = loaders[file_path.suffix.lower()]
                    loader = loader_class(str(file_path))
                    docs = loader.load()
                    
                    # Add metadata
                    for doc in docs:
                        doc.metadata.update({
                            'source_file': file_path.name,
                            'file_path': str(file_path),
                            'file_type': file_path.suffix.lower()
                        })
                    
                    documents.extend(docs)
                except Exception as e:
                    print(f"Error loading {file_path}: {e}")
        
        return documents
    
    def create_vectorstore(self, documents: List[Document]) -> None:
        """Create and persist vector store from documents"""
        # Split documents into chunks
        texts = self.text_splitter.split_documents(documents)
        
        print(f"Created {len(texts)} text chunks from {len(documents)} documents")
        
        # Create vector store
        self.vectorstore = Chroma.from_documents(
            documents=texts,
            embedding=self.embeddings,
            persist_directory=self.persist_directory
        )
        
        self.vectorstore.persist()
        print(f"Vector store created and persisted to {self.persist_directory}")
    
    def load_existing_vectorstore(self) -> bool:
        """Load existing vector store if available"""
        try:
            self.vectorstore = Chroma(
                persist_directory=self.persist_directory,
                embedding_function=self.embeddings
            )
            return True
        except Exception as e:
            print(f"Could not load existing vector store: {e}")
            return False
    
    def setup_retrieval_chain(self, search_type: str = "similarity", k: int = 4) -> None:
        """Set up the retrieval chain with conversation memory"""
        if not self.vectorstore:
            raise ValueError("Vector store not initialized. Load or create one first.")
        
        # Configure retriever
        retriever = self.vectorstore.as_retriever(
            search_type=search_type,
            search_kwargs={"k": k}
        )
        
        # Set up conversation memory
        memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )
        
        # Create conversational retrieval chain
        self.retrieval_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=retriever,
            memory=memory,
            return_source_documents=True,
            verbose=True
        )
    
    def query(self, question: str) -> Dict[str, Any]:
        """Query the RAG system and return answer with sources"""
        if not self.retrieval_chain:
            raise ValueError("Retrieval chain not set up. Call setup_retrieval_chain() first.")
        
        try:
            response = self.retrieval_chain({"question": question})
            
            # Format the response
            result = {
                "answer": response["answer"],
                "sources": []
            }
            
            # Extract source information
            for doc in response.get("source_documents", []):
                source_info = {
                    "content": doc.page_content[:200] + "...",
                    "source_file": doc.metadata.get("source_file", "Unknown"),
                    "file_type": doc.metadata.get("file_type", "Unknown")
                }
                result["sources"].append(source_info)
            
            return result
            
        except Exception as e:
            return {"error": f"Query failed: {str(e)}"}
    
    def add_documents(self, new_documents: List[Document]) -> None:
        """Add new documents to existing vector store"""
        if not self.vectorstore:
            raise ValueError("Vector store not initialized")
        
        # Split new documents
        new_texts = self.text_splitter.split_documents(new_documents)
        
        # Add to existing vector store
        self.vectorstore.add_documents(new_texts)
        self.vectorstore.persist()
        
        print(f"Added {len(new_texts)} new text chunks to vector store")

# Advanced RAG with hybrid search
class HybridRAGSystem(AdvancedRAGSystem):
    def __init__(self, persist_directory: str = "./hybrid_db"):
        super().__init__(persist_directory)
        self.bm25_retriever = None
    
    def setup_hybrid_retrieval(self, documents: List[Document]) -> None:
        """Set up hybrid retrieval combining dense and sparse retrieval"""
        from langchain.retrievers import BM25Retriever, EnsembleRetriever
        
        # Set up BM25 (sparse) retriever
        texts = self.text_splitter.split_documents(documents)
        self.bm25_retriever = BM25Retriever.from_documents(texts)
        
        # Set up dense retriever
        if not self.vectorstore:
            self.create_vectorstore(documents)
        
        dense_retriever = self.vectorstore.as_retriever(search_kwargs={"k": 4})
        
        # Combine retrievers
        ensemble_retriever = EnsembleRetriever(
            retrievers=[self.bm25_retriever, dense_retriever],
            weights=[0.3, 0.7]  # Favor dense retrieval slightly
        )
        
        # Set up chain with hybrid retriever
        memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )
        
        self.retrieval_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=ensemble_retriever,
            memory=memory,
            return_source_documents=True
        )

# Example usage
def test_rag_system():
    # Initialize RAG system
    rag = AdvancedRAGSystem()
    
    # Try to load existing vector store
    if not rag.load_existing_vectorstore():
        # Create sample documents if no existing store
        sample_docs = [
            Document(
                page_content="LangChain is a framework for developing applications powered by language models. It provides tools for prompt management, chains, and agents.",
                metadata={"source": "langchain_intro.txt", "type": "documentation"}
            ),
            Document(
                page_content="LangGraph allows you to build stateful applications with LLMs by modeling workflows as graphs with nodes and edges.",
                metadata={"source": "langgraph_guide.txt", "type": "documentation"}
            ),
            Document(
                page_content="Retrieval-Augmented Generation combines information retrieval with text generation to provide accurate, contextual responses.",
                metadata={"source": "rag_explanation.txt", "type": "documentation"}
            )
        ]
        
        rag.create_vectorstore(sample_docs)
    
    # Set up retrieval chain
    rag.setup_retrieval_chain()
    
    # Test queries
    test_questions = [
        "What is LangChain and what does it provide?",
        "How does LangGraph work with LLMs?",
        "Explain the concept of Retrieval-Augmented Generation"
    ]
    
    for question in test_questions:
        print(f"\nQuestion: {question}")
        result = rag.query(question)
        
        if "error" in result:
            print(f"Error: {result['error']}")
        else:
            print(f"Answer: {result['answer']}")
            print("Sources:")
            for i, source in enumerate(result['sources'], 1):
                print(f"  {i}. {source['source_file']} ({source['file_type']})")
                print(f"     {source['content']}")

if __name__ == "__main__":
    test_rag_system()
```

## LangGraph for Complex Agent Workflows

```python
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode, ToolExecutor
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from typing import TypedDict, Annotated, List, Union, Optional
import operator
from enum import Enum

class WorkflowState(TypedDict):
    messages: Annotated[List[Union[AIMessage, HumanMessage, SystemMessage]], operator.add]
    current_task: Optional[str]
    task_status: str
    results: Dict[str, Any]
    iteration_count: int
    max_iterations: int

class TaskStatus(Enum):
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"
    REQUIRES_HUMAN_INPUT = "requires_human_input"

class ComplexAgentWorkflow:
    def __init__(self, tools: List, max_iterations: int = 10):
        self.tools = tools
        self.tool_executor = ToolExecutor(tools)
        self.llm = ChatOpenAI(
            model="gpt-4",
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        self.max_iterations = max_iterations
        
        # Create the workflow graph
        self.workflow = self._create_workflow()
        self.app = self.workflow.compile()
    
    def _create_workflow(self) -> StateGraph:
        """Create the complex agent workflow graph"""
        workflow = StateGraph(WorkflowState)
        
        # Add nodes
        workflow.add_node("planner", self._planner_node)
        workflow.add_node("executor", self._executor_node) 
        workflow.add_node("validator", self._validator_node)
        workflow.add_node("error_handler", self._error_handler_node)
        workflow.add_node("human_input", self._human_input_node)
        workflow.add_node("finalizer", self._finalizer_node)
        
        # Define the workflow edges
        workflow.set_entry_point("planner")
        
        # Planner can go to executor or human input
        workflow.add_conditional_edges(
            "planner",
            self._should_execute_or_ask_human,
            {
                "execute": "executor",
                "ask_human": "human_input",
                "end": END
            }
        )
        
        # Human input goes to executor
        workflow.add_edge("human_input", "executor")
        
        # Executor goes to validator
        workflow.add_edge("executor", "validator")
        
        # Validator can continue, retry, or handle error
        workflow.add_conditional_edges(
            "validator",
            self._validate_execution,
            {
                "continue": "planner",
                "retry": "executor", 
                "error": "error_handler",
                "complete": "finalizer"
            }
        )
        
        # Error handler can retry or ask for human input
        workflow.add_conditional_edges(
            "error_handler",
            self._handle_error,
            {
                "retry": "executor",
                "ask_human": "human_input",
                "abort": END
            }
        )
        
        # Finalizer ends the workflow
        workflow.add_edge("finalizer", END)
        
        return workflow
    
    def _planner_node(self, state: WorkflowState) -> WorkflowState:
        """Plan the next action based on current state"""
        messages = state["messages"]
        current_task = state.get("current_task")
        results = state.get("results", {})
        
        planning_prompt = f"""
You are an intelligent task planner. Based on the conversation history and current state, determine the next action.

Current task: {current_task}
Previous results: {json.dumps(results, indent=2) if results else "None"}

Available tools: {[tool.name for tool in self.tools]}

Determine:
1. What needs to be done next
2. Which tool(s) to use
3. What information is needed
4. If human input is required

Respond with a clear plan for the next step.
"""
        
        planning_message = HumanMessage(content=planning_prompt)
        response = self.llm.invoke([planning_message] + messages[-3:])  # Use recent context
        
        return {
            **state,
            "messages": [response],
            "task_status": TaskStatus.IN_PROGRESS.value,
            "iteration_count": state.get("iteration_count", 0) + 1
        }
    
    def _executor_node(self, state: WorkflowState) -> WorkflowState:
        """Execute the planned action using available tools"""
        messages = state["messages"]
        last_message = messages[-1] if messages else None
        
        if not last_message:
            return {**state, "task_status": TaskStatus.FAILED.value}
        
        # Create agent with tools
        agent = create_openai_functions_agent(
            llm=self.llm,
            tools=self.tools,
            prompt=ChatPromptTemplate.from_messages([
                SystemMessage(content="Execute the planned action using available tools."),
                ("placeholder", "{chat_history}"),
                ("human", "{input}"),
                ("placeholder", "{agent_scratchpad}")
            ])
        )
        
        agent_executor = AgentExecutor(
            agent=agent,
            tools=self.tools,
            max_iterations=3,
            verbose=True
        )
        
        try:
            result = agent_executor.invoke({
                "input": last_message.content,
                "chat_history": messages[:-1]
            })
            
            execution_result = AIMessage(content=result["output"])
            
            return {
                **state,
                "messages": [execution_result],
                "results": {
                    **state.get("results", {}),
                    f"step_{state.get('iteration_count', 0)}": result["output"]
                }
            }
            
        except Exception as e:
            error_message = AIMessage(content=f"Execution failed: {str(e)}")
            return {
                **state,
                "messages": [error_message],
                "task_status": TaskStatus.FAILED.value
            }
    
    def _validator_node(self, state: WorkflowState) -> WorkflowState:
        """Validate the execution results"""
        messages = state["messages"]
        results = state.get("results", {})
        current_task = state.get("current_task")
        
        validation_prompt = f"""
Evaluate the execution results for the current task.

Task: {current_task}
Latest result: {messages[-1].content if messages else "No result"}
All results so far: {json.dumps(results, indent=2)}

Determine:
1. Was the execution successful?
2. Is the task complete or does it need more steps?
3. Are there any errors that need to be handled?
4. Should we continue, retry, or complete?

Respond with: CONTINUE, RETRY, ERROR, or COMPLETE
"""
        
        validation_response = self.llm.invoke([HumanMessage(content=validation_prompt)])
        
        return {
            **state,
            "messages": [validation_response],
            "task_status": self._determine_status_from_validation(validation_response.content)
        }
    
    def _error_handler_node(self, state: WorkflowState) -> WorkflowState:
        """Handle errors and determine recovery strategy"""
        messages = state["messages"]
        
        error_analysis_prompt = """
An error occurred during execution. Analyze the error and determine the best recovery strategy.

Error context: {error_context}

Options:
1. RETRY - Try the same action again
2. ASK_HUMAN - Request human intervention
3. ABORT - Stop the workflow

Choose the best option and explain why.
""".format(error_context=messages[-1].content if messages else "Unknown error")
        
        recovery_response = self.llm.invoke([HumanMessage(content=error_analysis_prompt)])
        
        return {
            **state,
            "messages": [recovery_response],
            "task_status": TaskStatus.FAILED.value
        }
    
    def _human_input_node(self, state: WorkflowState) -> WorkflowState:
        """Handle human input requests"""
        messages = state["messages"]
        
        # In a real implementation, this would interface with a human
        # For demo purposes, we'll simulate human input
        human_input_prompt = "Human input required. Please provide guidance."
        
        # Simulate human response
        simulated_human_response = HumanMessage(
            content="Proceed with the suggested approach and continue execution."
        )
        
        return {
            **state,
            "messages": [simulated_human_response],
            "task_status": TaskStatus.IN_PROGRESS.value
        }
    
    def _finalizer_node(self, state: WorkflowState) -> WorkflowState:
        """Finalize the workflow and prepare results"""
        results = state.get("results", {})
        
        final_summary = f"""
Workflow completed successfully!

Total iterations: {state.get('iteration_count', 0)}
Final status: {state.get('task_status')}
Results summary: {len(results)} steps completed

Key outputs:
{json.dumps(results, indent=2)}
"""
        
        return {
            **state,
            "messages": [AIMessage(content=final_summary)],
            "task_status": TaskStatus.COMPLETED.value
        }
    
    def _should_execute_or_ask_human(self, state: WorkflowState) -> str:
        """Determine if we should execute, ask human, or end"""
        iteration_count = state.get("iteration_count", 0)
        max_iterations = state.get("max_iterations", self.max_iterations)
        
        if iteration_count >= max_iterations:
            return "end"
        
        messages = state.get("messages", [])
        if not messages:
            return "end"
        
        last_message = messages[-1].content.lower()
        
        if "human" in last_message or "clarification" in last_message:
            return "ask_human"
        
        return "execute"
    
    def _validate_execution(self, state: WorkflowState) -> str:
        """Determine next step based on validation"""
        messages = state.get("messages", [])
        if not messages:
            return "error"
        
        validation_result = messages[-1].content.upper()
        
        if "COMPLETE" in validation_result:
            return "complete"
        elif "RETRY" in validation_result:
            return "retry"
        elif "ERROR" in validation_result:
            return "error"
        else:
            return "continue"
    
    def _handle_error(self, state: WorkflowState) -> str:
        """Determine error handling strategy"""
        messages = state.get("messages", [])
        if not messages:
            return "abort"
        
        recovery_plan = messages[-1].content.upper()
        
        if "RETRY" in recovery_plan:
            return "retry"
        elif "ASK_HUMAN" in recovery_plan or "HUMAN" in recovery_plan:
            return "ask_human"
        else:
            return "abort"
    
    def _determine_status_from_validation(self, validation_content: str) -> str:
        """Convert validation response to task status"""
        content_upper = validation_content.upper()
        
        if "COMPLETE" in content_upper:
            return TaskStatus.COMPLETED.value
        elif "ERROR" in content_upper:
            return TaskStatus.FAILED.value
        else:
            return TaskStatus.IN_PROGRESS.value
    
    def run_workflow(self, initial_task: str) -> Dict[str, Any]:
        """Run the complete workflow"""
        initial_state = {
            "messages": [HumanMessage(content=initial_task)],
            "current_task": initial_task,
            "task_status": TaskStatus.PENDING.value,
            "results": {},
            "iteration_count": 0,
            "max_iterations": self.max_iterations
        }
        
        try:
            final_state = self.app.invoke(initial_state)
            return {
                "success": True,
                "final_state": final_state,
                "task_status": final_state.get("task_status"),
                "results": final_state.get("results", {}),
                "iterations": final_state.get("iteration_count", 0)
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "task_status": TaskStatus.FAILED.value
            }

# Example usage with the previously defined tools
def demonstrate_langgraph_workflow():
    """Demonstrate the LangGraph workflow with complex agent orchestration"""
    
    # Use tools from previous examples
    tools = [
        DatabaseTool(),
        WeatherTool(),
        StructuredTool.from_function(
            func=lambda x: f"Calculated result: {eval(x) if x.replace('.','').replace('-','').replace('+','').replace('*','').replace('/','').replace(' ','').isdigit() else 'Invalid expression'}",
            name="calculator",
            description="Perform mathematical calculations"
        )
    ]
    
    # Create workflow
    workflow = ComplexAgentWorkflow(tools, max_iterations=8)
    
    # Test complex task
    complex_task = """
    I need you to:
    1. Check the weather in London
    2. Calculate the average temperature if it's provided
    3. Store this information in the database as a weather record
    4. Retrieve all weather records to show the complete dataset
    """
    
    print("Starting complex workflow...")
    print(f"Task: {complex_task}")
    print("=" * 80)
    
    result = workflow.run_workflow(complex_task)
    
    if result["success"]:
        print(f"Workflow completed successfully!")
        print(f"Status: {result['task_status']}")
        print(f"Iterations: {result['iterations']}")
        print(f"Results: {json.dumps(result['results'], indent=2)}")
    else:
        print(f"Workflow failed: {result['error']}")
        print(f"Status: {result['task_status']}")
```

## LangServe and LangSmith Integration

```python
from langserve import add_routes
from langsmith import Client
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from langchain.callbacks import LangChainTracer
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
import uvicorn
from datetime import datetime
import asyncio

class ProductionRAGService:
    def __init__(self):
        self.app = FastAPI(
            title="Advanced RAG API",
            description="Production-ready RAG system with monitoring",
            version="1.0.0"
        )
        
        # Configure CORS
        self.app.add_middleware(
            CORSMiddleware,
            allow_origins=["*"],
            allow_credentials=True,
            allow_methods=["*"],
            allow_headers=["*"],
        )
        
        # Initialize LangSmith client for monitoring
        self.langsmith_client = Client(
            api_key=os.getenv("LANGSMITH_API_KEY"),
            api_url="https://api.smith.langchain.com"
        )
        
        # Initialize tracer for monitoring
        self.tracer = LangChainTracer(
            project_name="production-rag-system",
            client=self.langsmith_client
        )
        
        # Initialize RAG system
        self.rag_system = AdvancedRAGSystem()
        self.setup_rag_system()
        
        # Setup API routes
        self.setup_routes()
    
    def setup_rag_system(self):
        """Initialize the RAG system with monitoring"""
        
        # Load or create vector store
        if not self.rag_system.load_existing_vectorstore():
            # Create sample knowledge base
            sample_docs = [
                Document(
                    page_content="""
                    LangChain is a framework for developing applications powered by language models.
                    It provides several key components:
                    - Prompt templates for structured input formatting
                    - Chains for combining multiple operations
                    - Agents for dynamic tool usage
                    - Memory for conversation state management
                    - Callbacks for monitoring and logging
                    """,
                    metadata={"source": "langchain_overview", "type": "documentation"}
                ),
                Document(
                    page_content="""
                    LangGraph enables building stateful, multi-actor applications with LLMs.
                    Key features include:
                    - Graph-based workflow modeling
                    - State management across nodes
                    - Conditional branching logic
                    - Human-in-the-loop capabilities
                    - Error handling and recovery
                    """,
                    metadata={"source": "langgraph_features", "type": "documentation"}
                ),
                Document(
                    page_content="""
                    LangServe provides production deployment capabilities for LangChain applications.
                    Features include:
                    - FastAPI integration
                    - Automatic OpenAPI documentation
                    - Async request handling
                    - Request/response validation
                    - Health checks and monitoring
                    """,
                    metadata={"source": "langserve_deployment", "type": "documentation"}
                )
            ]
            
            self.rag_system.create_vectorstore(sample_docs)
        
        self.rag_system.setup_retrieval_chain()
    
    def create_monitored_chain(self):
        """Create a chain with monitoring and error handling"""
        
        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)
        
        # Create retrieval chain with monitoring
        retrieval_chain = (
            {
                "context": self.rag_system.vectorstore.as_retriever() | format_docs,
                "question": RunnablePassthrough()
            }
            | ChatPromptTemplate.from_messages([
                ("system", """You are a helpful AI assistant. Use the following context to answer questions accurately and comprehensively.

Context: {context}

Instructions:
- Provide detailed, accurate answers based on the context
- If information is not in the context, say so clearly
- Include relevant details and examples when possible
- Maintain a professional, helpful tone"""),
                ("human", "{question}")
            ])
            | self.rag_system.llm
            | StrOutputParser()
        ).with_config(
            callbacks=[self.tracer],
            tags=["rag-query", "production"],
            metadata={"service": "rag-api", "version": "1.0.0"}
        )
        
        return retrieval_chain
    
    def setup_routes(self):
        """Setup FastAPI routes with LangServe integration"""
        
        # Create monitored chain
        rag_chain = self.create_monitored_chain()
        
        # Add LangServe routes
        add_routes(
            self.app,
            rag_chain,
            path="/rag",
            enabled_endpoints=["invoke", "batch", "stream"],
        )
        
        # Custom health check endpoint
        @self.app.get("/health")
        async def health_check():
            try:
                # Test vector store connection
                test_docs = self.rag_system.vectorstore.similarity_search("test", k=1)
                
                return {
                    "status": "healthy",
                    "timestamp": datetime.now().isoformat(),
                    "components": {
                        "vectorstore": "operational",
                        "llm": "operational",
                        "documents_count": len(test_docs)
                    }
                }
            except Exception as e:
                raise HTTPException(status_code=503, detail=f"Service unhealthy: {str(e)}")
        
        # Custom query endpoint with enhanced monitoring
        @self.app.post("/query")
        async def enhanced_query(request: dict):
            try:
                question = request.get("question", "")
                if not question:
                    raise HTTPException(status_code=400, detail="Question is required")
                
                # Log query
                start_time = datetime.now()
                
                # Process query
                result = self.rag_system.query(question)
                
                # Calculate processing time
                processing_time = (datetime.now() - start_time).total_seconds()
                
                # Enhanced response with metadata
                response = {
                    "answer": result.get("answer", ""),
                    "sources": result.get("sources", []),
                    "metadata": {
                        "processing_time_seconds": processing_time,
                        "timestamp": start_time.isoformat(),
                        "model_used": "gpt-4",
                        "retrieval_method": "similarity_search"
                    }
                }
                
                return response
                
            except Exception as e:
                raise HTTPException(status_code=500, detail=f"Query processing failed: {str(e)}")
        
        # Batch processing endpoint
        @self.app.post("/batch_query")
        async def batch_query(request: dict):
            try:
                questions = request.get("questions", [])
                if not questions or not isinstance(questions, list):
                    raise HTTPException(status_code=400, detail="Questions array is required")
                
                if len(questions) > 10:
                    raise HTTPException(status_code=400, detail="Maximum 10 questions per batch")
                
                # Process questions concurrently
                async def process_question(question):
                    return self.rag_system.query(question)
                
                # Create tasks for concurrent processing
                tasks = [process_question(q) for q in questions]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # Format results
                batch_response = {
                    "results": [],
                    "metadata": {
                        "total_questions": len(questions),
                        "successful_queries": 0,
                        "failed_queries": 0,
                        "timestamp": datetime.now().isoformat()
                    }
                }
                
                for i, result in enumerate(results):
                    if isinstance(result, Exception):
                        batch_response["results"].append({
                            "question": questions[i],
                            "error": str(result)
                        })
                        batch_response["metadata"]["failed_queries"] += 1
                    else:
                        batch_response["results"].append({
                            "question": questions[i],
                            "answer": result.get("answer", ""),
                            "sources": result.get("sources", [])
                        })
                        batch_response["metadata"]["successful_queries"] += 1
                
                return batch_response
                
            except Exception as e:
                raise HTTPException(status_code=500, detail=f"Batch processing failed: {str(e)}")
        
        # Analytics endpoint
        @self.app.get("/analytics")
        async def get_analytics():
            try:
                # In a real implementation, this would query LangSmith for metrics
                # For demo purposes, we'll return mock data
                return {
                    "total_queries": 1247,
                    "average_response_time": 2.3,
                    "success_rate": 0.987,
                    "top_queries": [
                        "What is LangChain?",
                        "How does LangGraph work?",
                        "Explain RAG implementation"
                    ],
                    "performance_metrics": {
                        "p50_response_time": 1.8,
                        "p95_response_time": 4.2,
                        "p99_response_time": 8.1
                    },
                    "last_updated": datetime.now().isoformat()
                }
                
            except Exception as e:
                raise HTTPException(status_code=500, detail=f"Analytics unavailable: {str(e)}")
    
    def run(self, host: str = "0.0.0.0", port: int = 8000):
        """Run the production service"""
        uvicorn.run(
            self.app,
            host=host,
            port=port,
            log_level="info"
        )

# Advanced monitoring and evaluation
class RAGEvaluationFramework:
    def __init__(self, rag_system: AdvancedRAGSystem):
        self.rag_system = rag_system
        self.langsmith_client = Client(api_key=os.getenv("LANGSMITH_API_KEY"))
    
    def evaluate_retrieval_quality(self, questions: List[str], expected_docs: List[str]) -> Dict[str, float]:
        """Evaluate retrieval quality using various metrics"""
        
        precision_scores = []
        recall_scores = []
        
        for question, expected in zip(questions, expected_docs):
            # Get retrieved documents
            retrieved_docs = self.rag_system.vectorstore.similarity_search(question, k=5)
            retrieved_content = [doc.page_content for doc in retrieved_docs]
            
            # Calculate precision and recall (simplified)
            relevant_retrieved = sum(1 for doc in retrieved_content if expected.lower() in doc.lower())
            precision = relevant_retrieved / len(retrieved_content) if retrieved_content else 0
            recall = 1.0 if relevant_retrieved > 0 else 0.0
            
            precision_scores.append(precision)
            recall_scores.append(recall)
        
        return {
            "average_precision": sum(precision_scores) / len(precision_scores),
            "average_recall": sum(recall_scores) / len(recall_scores),
            "f1_score": 2 * (sum(precision_scores) / len(precision_scores)) * (sum(recall_scores) / len(recall_scores)) / 
                      ((sum(precision_scores) / len(precision_scores)) + (sum(recall_scores) / len(recall_scores)))
        }
    
    def evaluate_answer_quality(self, test_cases: List[Dict[str, str]]) -> Dict[str, Any]:
        """Evaluate answer quality using LLM-based evaluation"""
        
        evaluation_results = []
        
        for case in test_cases:
            question = case["question"]
            expected_answer = case.get("expected_answer", "")
            
            # Get actual answer
            result = self.rag_system.query(question)
            actual_answer = result.get("answer", "")
            
            # LLM-based evaluation
            evaluation_prompt = f"""
            Evaluate the quality of this answer on a scale of 1-10:
            
            Question: {question}
            Expected Answer: {expected_answer}
            Actual Answer: {actual_answer}
            
            Rate the answer based on:
            1. Accuracy (is it factually correct?)
            2. Completeness (does it address all parts of the question?)
            3. Relevance (is it relevant to the question?)
            4. Clarity (is it well-written and clear?)
            
            Provide only a numeric score from 1-10.
            """
            
            try:
                score_response = self.rag_system.llm.invoke([HumanMessage(content=evaluation_prompt)])
                score = float(score_response.content.strip())
                
                evaluation_results.append({
                    "question": question,
                    "score": score,
                    "actual_answer": actual_answer
                })
                
            except Exception as e:
                evaluation_results.append({
                    "question": question,
                    "score": 0.0,
                    "error": str(e)
                })
        
        average_score = sum(r["score"] for r in evaluation_results) / len(evaluation_results)
        
        return {
            "average_score": average_score,
            "individual_results": evaluation_results,
            "total_evaluations": len(evaluation_results)
        }

# Example deployment and testing
def deploy_production_rag():
    """Deploy the production RAG service"""
    
    print("Initializing Production RAG Service...")
    service = ProductionRAGService()
    
    print("Service initialized successfully!")
    print("Available endpoints:")
    print("- POST /rag/invoke - Single query processing")
    print("- POST /query - Enhanced query with metadata")
    print("- POST /batch_query - Batch processing")
    print("- GET /health - Health check")
    print("- GET /analytics - Usage analytics")
    print("- GET /docs - API documentation")
    
    # Don't actually run the server in this example
    # service.run(host="0.0.0.0", port=8000)
    
    return service

def test_evaluation_framework():
    """Test the RAG evaluation framework"""
    
    rag_system = AdvancedRAGSystem()
    if not rag_system.load_existing_vectorstore():
        # Create test documents
        test_docs = [
            Document(page_content="LangChain is a framework for LLM applications", metadata={"source": "test"}),
            Document(page_content="LangGraph enables stateful LLM workflows", metadata={"source": "test"})
        ]
        rag_system.create_vectorstore(test_docs)
    
    rag_system.setup_retrieval_chain()
    
    # Initialize evaluation framework
    evaluator = RAGEvaluationFramework(rag_system)
    
    # Test cases
    test_cases = [
        {
            "question": "What is LangChain?",
            "expected_answer": "LangChain is a framework for developing applications powered by language models."
        },
        {
            "question": "How does LangGraph work?",
            "expected_answer": "LangGraph enables building stateful, multi-actor applications with LLMs using graph-based workflows."
        }
    ]
    
    # Evaluate answer quality
    quality_results = evaluator.evaluate_answer_quality(test_cases)
    
    print("RAG Evaluation Results:")
    print(f"Average Score: {quality_results['average_score']:.2f}/10")
    print(f"Total Evaluations: {quality_results['total_evaluations']}")
    
    for result in quality_results['individual_results']:
        print(f"\nQuestion: {result['question']}")
        print(f"Score: {result['score']}/10")

if __name__ == "__main__":
    demonstrate_langgraph_workflow()

    # Example usage
    print("LangChain & LangGraph Advanced Implementation")
    print("=" * 50)
    
    # Deploy service (commented out to avoid actually running server)
    # deploy_production_rag()
    
    # Test evaluation
    test_evaluation_framework()
```

## Conclusion

This comprehensive section on LangChain and LangGraph demonstrates the advanced capabilities of modern AI agent orchestration frameworks. We've covered several critical aspects:

**LangChain Fundamentals** provide the foundation for building sophisticated AI applications through structured prompt templates, tool integration, and chain composition. The framework's abstraction layers enable rapid development while maintaining flexibility for complex use cases.

**Retrieval-Augmented Generation (RAG)** represents a paradigm shift in how AI systems access and utilize external knowledge. Our implementation showcases both basic and hybrid approaches, combining dense and sparse retrieval methods for optimal information retrieval accuracy.

**LangGraph's workflow orchestration** enables the creation of complex, stateful agent systems that can handle multi-step tasks, error recovery, and human-in-the-loop scenarios. The graph-based approach provides clear visualization and management of agent decision flows.

**Production deployment considerations** through LangServe and LangSmith integration ensure that AI systems can be monitored, evaluated, and scaled effectively in enterprise environments. The monitoring and evaluation frameworks provide essential feedback loops for continuous improvement.

The combination of these technologies creates a powerful toolkit for building production-ready AI agents that can handle complex, real-world tasks while maintaining reliability, observability, and scalability. Modern applications require this level of sophistication to deliver meaningful business value and user experiences.

Key takeaways include the importance of structured prompt engineering, the value of hybrid retrieval approaches, the necessity of comprehensive monitoring, and the critical role of evaluation frameworks in maintaining system quality over time.

---

I'll create a comprehensive markdown document for Section 06 covering LangChain and LangGraph with technical depth and modern Python implementations.

I've created a comprehensive technical document for Section 06 covering LangChain and LangGraph. The document includes:

**Key Features:**

**1. Advanced LangChain Implementation:**
- Complex prompt templates with few-shot learning
- Structured output parsing with Pydantic
- Custom tool development with database integration
- Multi-tool agent systems with error handling

**2. Production-Ready RAG System:**
- Document loading from multiple formats
- Vector store management with Chroma
- Hybrid retrieval combining dense and sparse methods
- Conversation memory and context management

**3. LangGraph Workflow Orchestration:**
- State-based agent workflows
- Complex decision trees with conditional logic
- Error handling and recovery mechanisms
- Human-in-the-loop integration

**4. Production Deployment:**
- FastAPI integration with LangServe
- Comprehensive monitoring with LangSmith
- Health checks and analytics endpoints
- Batch processing capabilities

**5. Evaluation Framework:**
- Retrieval quality metrics
- LLM-based answer evaluation
- Performance monitoring and analytics

The implementation uses modern Python practices, comprehensive error handling, and production-ready patterns. All API keys and credentials are managed through environment variables as requested.

The code demonstrates real-world applications of these frameworks for building sophisticated AI agents that can handle complex tasks, maintain conversation state, and operate reliably in production environments.