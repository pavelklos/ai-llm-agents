<small>Claude 3.7 Sonnet Thinking</small>
# 09. Testing and Optimization of Customer Assistants

## Key Terms

- **A/B Testing**: Comparing two versions of a system to determine which performs better
- **Conversation Analytics**: Quantitative and qualitative analysis of assistant-user interactions
- **Confusion Matrix**: A table used to evaluate the performance of classification models showing true/false positives/negatives
- **User Satisfaction Score (CSAT)**: Metric measuring user satisfaction with assistant interactions
- **Intent Recognition Accuracy**: Measure of how correctly an assistant identifies user intentions
- **Response Latency**: Time taken by an assistant to generate and deliver a response
- **Regression Testing**: Testing to ensure new changes don't adversely affect existing functionality
- **Prompt Injection**: Attempts to manipulate an AI assistant by providing crafted inputs
- **Conversation Flow Analysis**: Studying the sequence and structure of conversation turns
- **Training/Test Set**: Data divisions used to evaluate assistant performance

## Testing Assistant Responses and Identifying Weaknesses

Systematic testing is essential for creating effective AI assistants. A well-structured testing framework helps identify issues before they impact real users:

```python
import os
import json
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional, Union
from datetime import datetime
from dotenv import load_dotenv
from openai import OpenAI
import logging
from tqdm import tqdm
import uuid

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("AssistantTesting")

# Load environment variables
load_dotenv()

class TestCase:
    """Represents a single test case for assistant evaluation."""
    
    def __init__(
        self,
        id: str,
        input_message: str,
        expected_response_type: str,
        expected_entities: Optional[Dict[str, str]] = None,
        expected_intent: Optional[str] = None,
        tags: Optional[List[str]] = None,
        context: Optional[List[Dict[str, str]]] = None
    ):
        """
        Initialize a test case.
        
        Args:
            id: Unique identifier for the test case
            input_message: User message to test
            expected_response_type: Type of response expected (e.g., "information", "action")
            expected_entities: Expected entities to be extracted
            expected_intent: Expected intent to be identified
            tags: Tags for categorizing the test case
            context: Previous messages for context
        """
        self.id = id
        self.input_message = input_message
        self.expected_response_type = expected_response_type
        self.expected_entities = expected_entities or {}
        self.expected_intent = expected_intent
        self.tags = tags or []
        self.context = context or []
        
    def to_dict(self) -> Dict[str, Any]:
        """Convert test case to dictionary."""
        return {
            "id": self.id,
            "input_message": self.input_message,
            "expected_response_type": self.expected_response_type,
            "expected_entities": self.expected_entities,
            "expected_intent": self.expected_intent,
            "tags": self.tags,
            "context": self.context
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'TestCase':
        """Create test case from dictionary."""
        return cls(
            id=data.get("id", str(uuid.uuid4())),
            input_message=data["input_message"],
            expected_response_type=data["expected_response_type"],
            expected_entities=data.get("expected_entities", {}),
            expected_intent=data.get("expected_intent"),
            tags=data.get("tags", []),
            context=data.get("context", [])
        )

class TestResult:
    """Represents the result of a test case execution."""
    
    def __init__(
        self,
        test_case_id: str,
        response: str,
        execution_time: float,
        detected_intent: Optional[str] = None,
        detected_entities: Optional[Dict[str, str]] = None,
        is_successful: Optional[bool] = None,
        notes: Optional[str] = None
    ):
        """
        Initialize a test result.
        
        Args:
            test_case_id: ID of the test case
            response: Actual response from the assistant
            execution_time: Time taken to generate the response
            detected_intent: Intent detected by the assistant
            detected_entities: Entities detected by the assistant
            is_successful: Whether the test was successful
            notes: Additional notes about the test result
        """
        self.test_case_id = test_case_id
        self.response = response
        self.execution_time = execution_time
        self.detected_intent = detected_intent
        self.detected_entities = detected_entities or {}
        self.is_successful = is_successful
        self.notes = notes
        self.timestamp = datetime.now().isoformat()
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert test result to dictionary."""
        return {
            "test_case_id": self.test_case_id,
            "response": self.response,
            "execution_time": self.execution_time,
            "detected_intent": self.detected_intent,
            "detected_entities": self.detected_entities,
            "is_successful": self.is_successful,
            "notes": self.notes,
            "timestamp": self.timestamp
        }

class AssistantTester:
    """Framework for testing AI assistants."""
    
    def __init__(self, assistant_api_handler, results_dir: str = "test_results"):
        """
        Initialize the tester.
        
        Args:
            assistant_api_handler: Handler for interacting with the assistant API
            results_dir: Directory for storing test results
        """
        self.assistant = assistant_api_handler
        self.results_dir = results_dir
        os.makedirs(results_dir, exist_ok=True)
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    def load_test_cases(self, file_path: str) -> List[TestCase]:
        """
        Load test cases from a file.
        
        Args:
            file_path: Path to the test cases file
            
        Returns:
            List of test cases
        """
        try:
            with open(file_path, 'r') as f:
                data = json.load(f)
            
            return [TestCase.from_dict(tc) for tc in data]
        except Exception as e:
            logger.error(f"Error loading test cases: {str(e)}")
            return []
    
    def run_test(self, test_case: TestCase) -> TestResult:
        """
        Run a single test case.
        
        Args:
            test_case: Test case to run
            
        Returns:
            Test result
        """
        try:
            # Start time measurement
            start_time = datetime.now()
            
            # Set up context if any
            if test_case.context:
                for message in test_case.context:
                    self.assistant.process_message(message["content"], is_test=True)
            
            # Process the test message
            response = self.assistant.process_message(test_case.input_message, is_test=True)
            
            # Calculate execution time
            execution_time = (datetime.now() - start_time).total_seconds()
            
            # Analyze the response to detect intent and entities
            detected_intent, detected_entities = self._analyze_response(test_case.input_message, response)
            
            # Evaluate success
            is_successful = self._evaluate_success(
                test_case, response, detected_intent, detected_entities
            )
            
            # Create test result
            result = TestResult(
                test_case_id=test_case.id,
                response=response,
                execution_time=execution_time,
                detected_intent=detected_intent,
                detected_entities=detected_entities,
                is_successful=is_successful,
                notes="Automated test"
            )
            
            return result
        
        except Exception as e:
            logger.error(f"Error running test case {test_case.id}: {str(e)}")
            return TestResult(
                test_case_id=test_case.id,
                response="",
                execution_time=0.0,
                is_successful=False,
                notes=f"Test error: {str(e)}"
            )
    
    def run_test_suite(self, test_cases: List[TestCase]) -> List[TestResult]:
        """
        Run a suite of test cases.
        
        Args:
            test_cases: List of test cases to run
            
        Returns:
            List of test results
        """
        results = []
        
        for test_case in tqdm(test_cases, desc="Running tests"):
            result = self.run_test(test_case)
            results.append(result)
        
        return results
    
    def save_results(self, results: List[TestResult], file_name: Optional[str] = None) -> str:
        """
        Save test results to a file.
        
        Args:
            results: List of test results
            file_name: Custom file name
            
        Returns:
            Path to the saved file
        """
        if file_name is None:
            file_name = f"results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        file_path = os.path.join(self.results_dir, file_name)
        
        try:
            with open(file_path, 'w') as f:
                json.dump([r.to_dict() for r in results], f, indent=2)
            
            logger.info(f"Saved test results to {file_path}")
            return file_path
        
        except Exception as e:
            logger.error(f"Error saving test results: {str(e)}")
            return ""
    
    def _analyze_response(self, user_message: str, assistant_response: str) -> tuple:
        """
        Analyze assistant response to detect intent and entities.
        
        Args:
            user_message: User message
            assistant_response: Assistant response
            
        Returns:
            Tuple of (detected_intent, detected_entities)
        """
        # Use GPT to analyze the response
        analysis_prompt = f"""
        Analyze this user message and assistant response:
        
        User: {user_message}
        
        Assistant: {assistant_response}
        
        1. What was the primary intent of the user? (one word or short phrase)
        2. What key entities were mentioned by the user? (return as JSON)
        
        Format your response as a JSON object with 'intent' and 'entities' fields.
        """
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": analysis_prompt}],
                response_format={"type": "json_object"},
                temperature=0.1
            )
            
            # Parse the response
            analysis = json.loads(response.choices[0].message.content)
            
            return analysis.get("intent"), analysis.get("entities", {})
        
        except Exception as e:
            logger.error(f"Error analyzing response: {str(e)}")
            return None, {}
    
    def _evaluate_success(
        self, 
        test_case: TestCase, 
        response: str, 
        detected_intent: Optional[str], 
        detected_entities: Dict[str, Any]
    ) -> bool:
        """
        Evaluate if the test was successful.
        
        Args:
            test_case: Original test case
            response: Assistant response
            detected_intent: Detected intent
            detected_entities: Detected entities
            
        Returns:
            Whether the test was successful
        """
        # Check intent if expected
        intent_match = True
        if test_case.expected_intent:
            intent_match = (detected_intent and 
                           test_case.expected_intent.lower() in detected_intent.lower())
        
        # Check entities if expected
        entities_match = True
        for key, value in test_case.expected_entities.items():
            if key not in detected_entities:
                entities_match = False
                break
        
        # Use GPT to evaluate response type
        type_match = self._evaluate_response_type(response, test_case.expected_response_type)
        
        # All criteria must match for success
        return intent_match and entities_match and type_match
    
    def _evaluate_response_type(self, response: str, expected_type: str) -> bool:
        """
        Evaluate if the response matches the expected type.
        
        Args:
            response: Assistant response
            expected_type: Expected response type
            
        Returns:
            Whether the response type matches expectations
        """
        evaluation_prompt = f"""
        Is this assistant response a "{expected_type}" type response?
        
        Response: {response}
        
        Answer with just "Yes" or "No".
        """
        
        try:
            eval_response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": evaluation_prompt}],
                temperature=0.1,
                max_tokens=10
            )
            
            result = eval_response.choices[0].message.content.strip().lower()
            return "yes" in result
        
        except Exception as e:
            logger.error(f"Error evaluating response type: {str(e)}")
            return False

    def generate_report(self, results: List[TestResult]) -> Dict[str, Any]:
        """
        Generate a summary report of test results.
        
        Args:
            results: List of test results
            
        Returns:
            Report dictionary
        """
        total_tests = len(results)
        successful_tests = sum(1 for r in results if r.is_successful)
        success_rate = successful_tests / total_tests if total_tests > 0 else 0
        
        avg_execution_time = sum(r.execution_time for r in results) / total_tests if total_tests > 0 else 0
        
        # Categorize failures
        failures = [r for r in results if not r.is_successful]
        
        return {
            "total_tests": total_tests,
            "successful_tests": successful_tests,
            "success_rate": success_rate,
            "avg_execution_time": avg_execution_time,
            "failure_count": len(failures),
            "timestamp": datetime.now().isoformat()
        }
```

## Collecting Feedback and Analyzing Outputs

Systematic feedback collection is essential for improving AI assistants. Here's a framework to gather, structure, and analyze user feedback:

```python
import os
import json
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional, Union
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import logging
from wordcloud import WordCloud
import re
from textblob import TextBlob

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("FeedbackAnalyzer")

class UserFeedback:
    """Represents user feedback on an assistant interaction."""
    
    def __init__(
        self,
        conversation_id: str,
        rating: int,  # 1-5 scale
        feedback_text: Optional[str] = None,
        user_id: Optional[str] = None,
        timestamp: Optional[str] = None,
        conversation_summary: Optional[str] = None,
        tags: Optional[List[str]] = None
    ):
        """
        Initialize user feedback.
        
        Args:
            conversation_id: ID of the conversation
            rating: User rating (1-5)
            feedback_text: Text feedback
            user_id: ID of the user
            timestamp: Time of feedback
            conversation_summary: Summary of the conversation
            tags: Tags for categorizing feedback
        """
        self.conversation_id = conversation_id
        self.rating = rating
        self.feedback_text = feedback_text
        self.user_id = user_id
        self.timestamp = timestamp or datetime.now().isoformat()
        self.conversation_summary = conversation_summary
        self.tags = tags or []
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert feedback to dictionary."""
        return {
            "conversation_id": self.conversation_id,
            "rating": self.rating,
            "feedback_text": self.feedback_text,
            "user_id": self.user_id,
            "timestamp": self.timestamp,
            "conversation_summary": self.conversation_summary,
            "tags": self.tags
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'UserFeedback':
        """Create feedback from dictionary."""
        return cls(
            conversation_id=data["conversation_id"],
            rating=data["rating"],
            feedback_text=data.get("feedback_text"),
            user_id=data.get("user_id"),
            timestamp=data.get("timestamp"),
            conversation_summary=data.get("conversation_summary"),
            tags=data.get("tags", [])
        )

class FeedbackAnalyzer:
    """Analyzes user feedback to derive insights for assistant improvement."""
    
    def __init__(self, feedback_dir: str = "feedback_data"):
        """
        Initialize the feedback analyzer.
        
        Args:
            feedback_dir: Directory for storing feedback data
        """
        self.feedback_dir = feedback_dir
        os.makedirs(feedback_dir, exist_ok=True)
        self.feedback_data = []
    
    def load_feedback(self, file_path: Optional[str] = None) -> List[UserFeedback]:
        """
        Load feedback data from a file.
        
        Args:
            file_path: Path to the feedback file
            
        Returns:
            List of feedback objects
        """
        if file_path is None:
            # Find the most recent feedback file
            files = [f for f in os.listdir(self.feedback_dir) if f.endswith('.json')]
            if not files:
                return []
            
            file_path = os.path.join(self.feedback_dir, max(files))
        
        try:
            with open(file_path, 'r') as f:
                data = json.load(f)
            
            self.feedback_data = [UserFeedback.from_dict(fb) for fb in data]
            return self.feedback_data
        
        except Exception as e:
            logger.error(f"Error loading feedback: {str(e)}")
            return []
    
    def save_feedback(self, feedback: List[UserFeedback], file_name: Optional[str] = None) -> str:
        """
        Save feedback data to a file.
        
        Args:
            feedback: List of feedback objects
            file_name: Custom file name
            
        Returns:
            Path to the saved file
        """
        if file_name is None:
            file_name = f"feedback_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        file_path = os.path.join(self.feedback_dir, file_name)
        
        try:
            with open(file_path, 'w') as f:
                json.dump([fb.to_dict() for fb in feedback], f, indent=2)
            
            logger.info(f"Saved feedback to {file_path}")
            return file_path
        
        except Exception as e:
            logger.error(f"Error saving feedback: {str(e)}")
            return ""
    
    def add_feedback(self, feedback: UserFeedback) -> None:
        """
        Add a new feedback item.
        
        Args:
            feedback: Feedback to add
        """
        self.feedback_data.append(feedback)
    
    def get_satisfaction_metrics(self) -> Dict[str, Any]:
        """
        Calculate satisfaction metrics from feedback.
        
        Returns:
            Dictionary of metrics
        """
        if not self.feedback_data:
            return {"error": "No feedback data available"}
        
        total = len(self.feedback_data)
        ratings = [fb.rating for fb in self.feedback_data]
        
        average_rating = sum(ratings) / total
        rating_distribution = Counter(ratings)
        
        # Calculate CSAT (% of 4-5 ratings)
        csat = sum(1 for r in ratings if r >= 4) / total * 100
        
        # Calculate NPS categories
        promoters = sum(1 for r in ratings if r >= 5) / total
        passives = sum(1 for r in ratings if r == 3 or r == 4) / total
        detractors = sum(1 for r in ratings if r <= 2) / total
        
        # NPS score (-100 to +100)
        nps = (promoters - detractors) * 100
        
        return {
            "total_feedback": total,
            "average_rating": average_rating,
            "rating_distribution": dict(rating_distribution),
            "csat": csat,
            "nps": nps,
            "promoters_percentage": promoters * 100,
            "passives_percentage": passives * 100,
            "detractors_percentage": detractors * 100
        }
    
    def analyze_feedback_text(self) -> Dict[str, Any]:
        """
        Analyze text feedback to find common themes.
        
        Returns:
            Dictionary of text analysis results
        """
        if not self.feedback_data:
            return {"error": "No feedback data available"}
        
        # Extract text feedback (exclude empty)
        text_feedback = [fb.feedback_text for fb in self.feedback_data 
                        if fb.feedback_text and fb.feedback_text.strip()]
        
        if not text_feedback:
            return {"error": "No text feedback available"}
        
        # Perform sentiment analysis
        sentiments = [TextBlob(text).sentiment.polarity for text in text_feedback]
        avg_sentiment = sum(sentiments) / len(sentiments)
        
        # Extract common phrases (n-grams)
        common_phrases = self._extract_common_phrases(text_feedback)
        
        # Identify themes through clustering
        themes = self._identify_themes(text_feedback)
        
        return {
            "total_text_feedback": len(text_feedback),
            "average_sentiment": avg_sentiment,
            "sentiment_distribution": {
                "positive": sum(1 for s in sentiments if s > 0.2) / len(sentiments),
                "neutral": sum(1 for s in sentiments if -0.2 <= s <= 0.2) / len(sentiments),
                "negative": sum(1 for s in sentiments if s < -0.2) / len(sentiments)
            },
            "common_phrases": common_phrases,
            "themes": themes
        }
    
    def _extract_common_phrases(self, texts: List[str], top_n: int = 10) -> List[Dict[str, Any]]:
        """
        Extract common phrases from text feedback.
        
        Args:
            texts: List of text feedback
            top_n: Number of top phrases to extract
            
        Returns:
            List of common phrases with counts
        """
        # Extract 2-3 word phrases
        all_text = " ".join(texts).lower()
        
        # Simple regex for phrases
        two_word = re.findall(r'\b(\w+\s+\w+)\b', all_text)
        three_word = re.findall(r'\b(\w+\s+\w+\s+\w+)\b', all_text)
        
        # Count phrases
        two_word_counter = Counter(two_word)
        three_word_counter = Counter(three_word)
        
        # Combine and get top phrases
        all_phrases = list(two_word_counter.items()) + list(three_word_counter.items())
        all_phrases.sort(key=lambda x: x[1], reverse=True)
        
        return [{"phrase": phrase, "count": count} for phrase, count in all_phrases[:top_n]]
    
    def _identify_themes(self, texts: List[str], num_clusters: int = 5) -> List[Dict[str, Any]]:
        """
        Identify themes in feedback using clustering.
        
        Args:
            texts: List of text feedback
            num_clusters: Number of themes to identify
            
        Returns:
            List of themes with representative words
        """
        try:
            # Create TF-IDF vectorizer
            vectorizer = TfidfVectorizer(max_features=100, stop_words='english')
            X = vectorizer.fit_transform(texts)
            
            # Apply KMeans clustering
            km = KMeans(n_clusters=min(num_clusters, len(texts)), random_state=42)
            km.fit(X)
            
            # Get top terms per cluster
            order_centroids = km.cluster_centers_.argsort()[:, ::-1]
            terms = vectorizer.get_feature_names_out()
            
            # Construct themes
            themes = []
            for i in range(km.n_clusters):
                theme_terms = [terms[ind] for ind in order_centroids[i, :10]]
                
                # Count documents in this cluster
                cluster_docs = [texts[j] for j in range(len(texts)) if km.labels_[j] == i]
                
                themes.append({
                    "id": i,
                    "terms": theme_terms,
                    "document_count": len(cluster_docs),
                    "sample_text": cluster_docs[0] if cluster_docs else ""
                })
            
            return themes
        
        except Exception as e:
            logger.error(f"Error identifying themes: {str(e)}")
            return []
    
    def generate_report(self, output_file: Optional[str] = None) -> Dict[str, Any]:
        """
        Generate a comprehensive feedback analysis report.
        
        Args:
            output_file: File to save the report
            
        Returns:
            Report dictionary
        """
        if not self.feedback_data:
            return {"error": "No feedback data available"}
        
        # Calculate metrics
        satisfaction = self.get_satisfaction_metrics()
        text_analysis = self.analyze_feedback_text()
        
        # Combine into a report
        report = {
            "timestamp": datetime.now().isoformat(),
            "total_feedback": len(self.feedback_data),
            "date_range": {
                "start": min(fb.timestamp for fb in self.feedback_data),
                "end": max(fb.timestamp for fb in self.feedback_data)
            },
            "satisfaction_metrics": satisfaction,
            "text_analysis": text_analysis,
        }
        
        # Save to file if requested
        if output_file:
            try:
                with open(output_file, 'w') as f:
                    json.dump(report, f, indent=2)
                logger.info(f"Saved report to {output_file}")
            except Exception as e:
                logger.error(f"Error saving report: {str(e)}")
        
        return report
```

## Making Adjustments Based on Real Conversations

After collecting and analyzing feedback, the next step is implementing improvements. Here's a framework for making data-driven adjustments:

```python
import os
import json
import pandas as pd
from typing import Dict, List, Any, Optional, Union, Callable
from datetime import datetime
import logging
from collections import defaultdict

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("OptimizationManager")

class OptimizationAction:
    """Represents an optimization action to improve the assistant."""
    
    def __init__(
        self,
        id: str,
        type: str,  # "prompt", "flow", "training_data", "response_template"
        description: str,
        changes: Dict[str, Any],
        target_metrics: List[str],
        expected_improvement: Dict[str, float],
        priority: int,  # 1-5, higher is more important
        feedback_ids: Optional[List[str]] = None
    ):
        """
        Initialize an optimization action.
        
        Args:
            id: Unique identifier
            type: Type of optimization
            description: Description of the action
            changes: Specific changes to make
            target_metrics: Metrics expected to improve
            expected_improvement: Expected improvement for each metric
            priority: Priority level (1-5)
            feedback_ids: IDs of feedback items that led to this action
        """
        self.id = id
        self.type = type
        self.description = description
        self.changes = changes
        self.target_metrics = target_metrics
        self.expected_improvement = expected_improvement
        self.priority = priority
        self.feedback_ids = feedback_ids or []
        self.created_at = datetime.now().isoformat()
        self.implemented = False
        self.implemented_at = None
        self.actual_improvement = {}
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        return {
            "id": self.id,
            "type": self.type,
            "description": self.description,
            "changes": self.changes,
            "target_metrics": self.target_metrics,
            "expected_improvement": self.expected_improvement,
            "priority": self.priority,
            "feedback_ids": self.feedback_ids,
            "created_at": self.created_at,
            "implemented": self.implemented,
            "implemented_at": self.implemented_at,
            "actual_improvement": self.actual_improvement
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'OptimizationAction':
        """Create from dictionary."""
        action = cls(
            id=data["id"],
            type=data["type"],
            description=data["description"],
            changes=data["changes"],
            target_metrics=data["target_metrics"],
            expected_improvement=data["expected_improvement"],
            priority=data["priority"],
            feedback_ids=data.get("feedback_ids", [])
        )
        
        action.created_at = data.get("created_at", action.created_at)
        action.implemented = data.get("implemented", False)
        action.implemented_at = data.get("implemented_at")
        action.actual_improvement = data.get("actual_improvement", {})
        
        return action

class OptimizationManager:
    """Manages the process of optimizing an assistant based on feedback."""
    
    def __init__(
        self, 
        assistant_config_handler, 
        actions_dir: str = "optimization_actions"
    ):
        """
        Initialize the optimization manager.
        
        Args:
            assistant_config_handler: Handler for modifying assistant configuration
            actions_dir: Directory for storing optimization actions
        """
        self.config_handler = assistant_config_handler
        self.actions_dir = actions_dir
        os.makedirs(actions_dir, exist_ok=True)
        self.actions = []
    
    def generate_optimization_actions(
        self, 
        feedback_data: List[Dict[str, Any]], 
        test_results: Optional[List[Dict[str, Any]]] = None
    ) -> List[OptimizationAction]:
        """
        Generate optimization actions based on feedback and test results.
        
        Args:
            feedback_data: User feedback data
            test_results: Test results data
            
        Returns:
            List of optimization actions
        """
        actions = []
        
        # Analyze low ratings
        low_ratings = [fb for fb in feedback_data if fb.get("rating", 5) <= 2]
        
        # Group by feedback themes
        theme_groups = defaultdict(list)
        for fb in low_ratings:
            for tag in fb.get("tags", []):
                theme_groups[tag].append(fb)
        
        # Generate actions for each theme
        for theme, feedback_group in theme_groups.items():
            if len(feedback_group) >= 3:  # Only act on themes with multiple occurrences
                action = self._create_action_for_theme(theme, feedback_group)
                if action:
                    actions.append(action)
        
        # Analyze test failures if available
        if test_results:
            failed_tests = [tr for tr in test_results if not tr.get("is_successful", False)]
            
            # Group by test tags
            test_groups = defaultdict(list)
            for test in failed_tests:
                test_case_id = test.get("test_case_id")
                # Find test case to get tags (would need test case data here)
                # For now, using a simple approach
                test_groups["failed_tests"].append(test)
            
            # Generate actions for test failures
            for tag, tests in test_groups.items():
                action = self._create_action_for_failed_tests(tag, tests)
                if action:
                    actions.append(action)
        
        # Save the new actions
        self.actions.extend(actions)
        self.save_actions()
        
        return actions
    
    def _create_action_for_theme(self, theme: str, feedback_items: List[Dict[str, Any]]) -> Optional[OptimizationAction]:
        """
        Create an optimization action for a feedback theme.
        
        Args:
            theme: Theme of the feedback
            feedback_items: Related feedback items
            
        Returns:
            Optimization action or None
        """
        # Extract common words from feedback
        feedback_texts = [fb.get("feedback_text", "") for fb in feedback_items if fb.get("feedback_text")]
        common_text = " ".join(feedback_texts)
        
        # Determine action type based on theme
        action_type = "prompt"  # Default
        changes = {}
        
        if "unclear" in theme or "confusing" in theme:
            action_type = "response_template"
            changes = {
                "action": "improve_clarity",
                "template_modifications": [
                    "Add more step-by-step explanations",
                    "Break down complex information",
                    "Use simpler language"
                ]
            }
        elif "slow" in theme:
            action_type = "flow"
            changes = {
                "action": "optimize_response_time",
                "modifications": [
                    "Simplify decision trees",
                    "Cache common responses",
                    "Reduce external API calls"
                ]
            }
        elif "incorrect" in theme or "wrong" in theme:
            action_type = "training_data"
            changes = {
                "action": "improve_accuracy",
                "data_modifications": [
                    "Add examples from failed conversations",
                    "Clarify ambiguous training examples",
                    "Add edge cases"
                ]
            }
        
        # Create the action
        action_id = f"auto_{datetime.now().strftime('%Y%m%d%H%M%S')}"
        return OptimizationAction(
            id=action_id,
            type=action_type,
            description=f"Improve {theme} based on user feedback",
            changes=changes,
            target_metrics=["user_satisfaction", "completion_rate"],
            expected_improvement={"user_satisfaction": 0.5, "completion_rate": 0.3},
            priority=3,
            feedback_ids=[fb.get("conversation_id", "") for fb in feedback_items]
        )
    
    def _create_action_for_failed_tests(self, tag: str, tests: List[Dict[str, Any]]) -> Optional[OptimizationAction]:
        """
        Create an optimization action for failed tests.
        
        Args:
            tag: Tag for the tests
            tests: Failed test results
            
        Returns:
            Optimization action or None
        """
        # Simple implementation - would be more sophisticated in real system
        action_id = f"test_{datetime.now().strftime('%Y%m%d%H%M%S')}"
        return OptimizationAction(
            id=action_id,
            type="prompt",
            description=f"Fix issues identified in {len(tests)} failed tests",
            changes={
                "action": "improve_test_cases",
                "modifications": [
                    "Update prompt to handle edge cases",
                    "Add explicit handling for ambiguous inputs",
                    "Improve intent recognition"
                ]
            },
            target_metrics=["test_success_rate", "intent_accuracy"],
            expected_improvement={"test_success_rate": 0.7, "intent_accuracy": 0.5},
            priority=4,
            feedback_ids=[test.get("test_case_id", "") for test in tests]
        )
    
    def load_actions(self, file_path: Optional[str] = None) -> List[OptimizationAction]:
        """
        Load optimization actions from a file.
        
        Args:
            file_path: Path to the actions file
            
        Returns:
            List of optimization actions
        """
        if file_path is None:
            # Find the most recent actions file
            files = [f for f in os.listdir(self.actions_dir) if f.endswith('.json')]
            if not files:
                return []
            
            file_path = os.path.join(self.actions_dir, max(files))
        
        try:
            with open(file_path, 'r') as f:
                data = json.load(f)
            
            self.actions = [OptimizationAction.from_dict(action) for action in data]
            return self.actions
        
        except Exception as e:
            logger.error(f"Error loading actions: {str(e)}")
            return []
    
    def save_actions(self, file_name: Optional[str] = None) -> str:
        """
        Save optimization actions to a file.
        
        Args:
            file_name: Custom file name
            
        Returns:
            Path to the saved file
        """
        if file_name is None:
            file_name = f"actions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        file_path = os.path.join(self.actions_dir, file_name)
        
        try:
            with open(file_path, 'w') as f:
                json.dump([action.to_dict() for action in self.actions], f, indent=2)
            
            logger.info(f"Saved optimization actions to {file_path}")
            return file_path
        
        except Exception as e:
            logger.error(f"Error saving actions: {str(e)}")
            return ""
    
    def implement_action(self, action_id: str) -> bool:
        """
        Implement an optimization action.
        
        Args:
            action_id: ID of the action to implement
            
        Returns:
            Whether the implementation was successful
        """
        action = next((a for a in self.actions if a.id == action_id), None)
        if not action:
            logger.error(f"Action {action_id} not found")
            return False
        
        try:
            # Implement changes based on action type
            if action.type == "prompt":
                self._implement_prompt_changes(action.changes)
            elif action.type == "flow":
                self._implement_flow_changes(action.changes)
            elif action.type == "training_data":
                self._implement_training_data_changes(action.changes)
            elif action.type == "response_template":
                self._implement_template_changes(action.changes)
            
            # Mark as implemented
            action.implemented = True
            action.implemented_at = datetime.now().isoformat()
            
            # Save updated actions
            self.save_actions()
            
            logger.info(f"Implemented action {action_id}")
            return True
        
        except Exception as e:
            logger.error(f"Error implementing action {action_id}: {str(e)}")
            return False
    
    def _implement_prompt_changes(self, changes: Dict[str, Any]) -> None:
        """
        Implement changes to the assistant's prompt.
        
        Args:
            changes: Changes to make
        """
        # Simplified implementation - would call config handler to update prompt
        logger.info(f"Implementing prompt changes: {changes}")
        
        # In real system, would update the assistant's system prompt
        if self.config_handler and hasattr(self.config_handler, 'update_system_prompt'):
            self.config_handler.update_system_prompt(changes)
    
    def _implement_flow_changes(self, changes: Dict[str, Any]) -> None:
        """
        Implement changes to conversation flows.
        
        Args:
            changes: Changes to make
        """
        logger.info(f"Implementing flow changes: {changes}")
        
        # In real system, would update the conversation flow
        if self.config_handler and hasattr(self.config_handler, 'update_conversation_flow'):
            self.config_handler.update_conversation_flow(changes)
    
    def _implement_training_data_changes(self, changes: Dict[str, Any]) -> None:
        """
        Implement changes to training data.
        
        Args:
            changes: Changes to make
        """
        logger.info(f"Implementing training data changes: {changes}")
        
        # In real system, would update training data
        if self.config_handler and hasattr(self.config_handler, 'update_training_data'):
            self.config_handler.update_training_data(changes)
    
    def _implement_template_changes(self, changes: Dict[str, Any]) -> None:
        """
        Implement changes to response templates.
        
        Args:
            changes: Changes to make
        """
        logger.info(f"Implementing template changes: {changes}")
        
        # In real system, would update response templates
        if self.config_handler and hasattr(self.config_handler, 'update_response_templates'):
            self.config_handler.update_response_templates(changes)
```

## Practical Exercise: Testing and Optimizing a Customer Assistant

Let's create a complete example for testing and optimizing a customer support assistant:

```python
import os
import json
import pandas as pd
from typing import Dict, List, Any
from datetime import datetime
from dotenv import load_dotenv
from openai import OpenAI
import logging
from assistant_testing.test_framework import AssistantTester, TestCase, TestResult
from assistant_testing.feedback_analyzer import FeedbackAnalyzer, UserFeedback
from assistant_testing.optimization_manager import OptimizationManager, OptimizationAction
import uuid

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("TestingWorkflow")

# Mock assistant handler (replace with your actual assistant)
class MockAssistantHandler:
    def __init__(self):
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.system_prompt = """
        You are a helpful customer support assistant for an e-commerce store.
        You can help with:
        - Order tracking and status
        - Returns and refunds
        - Product information
        - Account issues
        
        Be friendly, concise, and helpful.
        """
    
    def process_message(self, message, is_test=False):
        """Process a user message and return a response."""
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": self.system_prompt},
                    {"role": "user", "content": message}
                ],
                temperature=0.7
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"Error processing message: {str(e)}")
            return "I'm sorry, I'm having trouble processing your request."
    
    def update_system_prompt(self, changes):
        """Update the system prompt based on changes."""
        # In a real system, this would apply specific modifications
        logger.info(f"Updating system prompt with changes: {changes}")
        
        if changes.get("action") == "improve_clarity":
            self.system_prompt += "\n\nMake sure to provide clear, step-by-step explanations."
        elif changes.get("action") == "improve_accuracy":
            self.system_prompt += "\n\nVerify information carefully before responding."

def main():
    """Run the complete testing and optimization workflow."""
    # Initialize components
    assistant = MockAssistantHandler()
    tester = AssistantTester(assistant)
    feedback_analyzer = FeedbackAnalyzer()
    optimization_manager = OptimizationManager(assistant)
    
    # 1. Create test cases
    test_cases = [
        TestCase(
            id=str(uuid.uuid4()),
            input_message="When will my order #12345 arrive?",
            expected_response_type="information",
            expected_entities={"order_number": "12345"},
            expected_intent="order_status",
            tags=["order", "tracking"]
        ),
        TestCase(
            id=str(uuid.uuid4()),
            input_message="I need to return a damaged product I received yesterday",
            expected_response_type="process",
            expected_entities={"issue": "damaged"},
            expected_intent="return_request",
            tags=["returns", "damaged"]
        ),
        TestCase(
            id=str(uuid.uuid4()),
            input_message="Do you have this laptop in blue?",
            expected_response_type="information",
            expected_entities={"product": "laptop", "color": "blue"},
            expected_intent="product_inquiry",
            tags=["product", "availability"]
        ),
        TestCase(
            id=str(uuid.uuid4()),
            input_message="I forgot my password",
            expected_response_type="process",
            expected_entities={},
            expected_intent="account_issue",
            tags=["account", "password"]
        ),
        TestCase(
            id=str(uuid.uuid4()),
            input_message="Tell me about your return policy for electronics",
            expected_response_type="information",
            expected_entities={"category": "electronics"},
            expected_intent="policy_inquiry",
            tags=["policy", "returns"]
        )
    ]
    
    # Save test cases
    with open("test_cases.json", "w") as f:
        json.dump([tc.to_dict() for tc in test_cases], f, indent=2)
    
    # 2. Run tests
    logger.info("Running tests...")
    test_results = tester.run_test_suite(test_cases)
    results_file = tester.save_results(test_results)
    
    # Generate test report
    test_report = tester.generate_report(test_results)
    logger.info(f"Test success rate: {test_report['success_rate'] * 100:.2f}%")
    
    # 3. Simulate user feedback
    logger.info("Collecting user feedback...")
    user_feedback = [
        UserFeedback(
            conversation_id=str(uuid.uuid4()),
            rating=2,
            feedback_text="The assistant didn't understand my question about shipping options.",
            tags=["unclear", "shipping"]
        ),
        UserFeedback(
            conversation_id=str(uuid.uuid4()),
            rating=3,
            feedback_text="Response was okay but could be more detailed.",
            tags=["incomplete"]
        ),
        UserFeedback(
            conversation_id=str(uuid.uuid4()),
            rating=1,
            feedback_text="Completely wrong information about return policy.",
            tags=["incorrect", "policy"]
        ),
        UserFeedback(
            conversation_id=str(uuid.uuid4()),
            rating=4,
            feedback_text="Very helpful with my order tracking!",
            tags=["helpful", "tracking"]
        ),
        UserFeedback(
            conversation_id=str(uuid.uuid4()),
            rating=2,
            feedback_text="Too slow to respond and didn't answer my question directly.",
            tags=["slow", "unclear"]
        )
    ]
    
    # Save feedback
    feedback_file = feedback_analyzer.save_feedback(user_feedback)
    
    # 4. Analyze feedback
    logger.info("Analyzing feedback...")
    feedback_analyzer.feedback_data = user_feedback
    satisfaction_metrics = feedback_analyzer.get_satisfaction_metrics()
    logger.info(f"Average rating: {satisfaction_metrics['average_rating']:.2f}")
    logger.info(f"CSAT: {satisfaction_metrics['csat']:.2f}%")
    
    text_analysis = feedback_analyzer.analyze_feedback_text()
    if "error" not in text_analysis:
        logger.info(f"Average sentiment: {text_analysis['average_sentiment']:.2f}")
        if "themes" in text_analysis:
            for theme in text_analysis["themes"]:
                logger.info(f"Theme {theme['id']}: {', '.join(theme['terms'][:3])}")
    
    # 5. Generate optimization actions
    logger.info("Generating optimization actions...")
    optimization_actions = optimization_manager.generate_optimization_actions(
        feedback_data=user_feedback,
        test_results=test_results
    )
    
    # 6. Implement highest priority action
    if optimization_actions:
        # Sort by priority (higher is more important)
        priority_action = max(optimization_actions, key=lambda x: x.priority)
        logger.info(f"Implementing highest priority action: {priority_action.description}")
        optimization_manager.implement_action(priority_action.id)
    
    # 7. Run tests again to verify improvements
    logger.info("Running verification tests...")
    verification_results = tester.run_test_suite(test_cases)
    verification_report = tester.generate_report(verification_results)
    logger.info(f"New test success rate: {verification_report['success_rate'] * 100:.2f}%")
    
    # Compare before and after
    improvement = verification_report['success_rate'] - test_report['success_rate']
    logger.info(f"Success rate improvement: {improvement * 100:.2f}%")
    
    # 8. Final report
    logger.info("\nTesting and Optimization Summary:")
    logger.info(f"Initial test success rate: {test_report['success_rate'] * 100:.2f}%")
    logger.info(f"Customer satisfaction: {satisfaction_metrics['average_rating']:.2f}/5.0")
    logger.info(f"Optimization actions created: {len(optimization_actions)}")
    logger.info(f"Final test success rate: {verification_report['success_rate'] * 100:.2f}%")
    logger.info(f"Improvement: {improvement * 100:.2f}%")

if __name__ == "__main__":
    main()
```

## Conclusion

Testing and optimizing customer assistants is an iterative process that requires systematic approaches to identify issues, gather feedback, and implement improvements. The frameworks presented here provide a robust foundation for this continuous improvement cycle.

Key takeaways from this section include:

1. **Structured Testing Approach**: A comprehensive testing framework helps identify weaknesses in assistant responses before they affect real users.

2. **Data-Driven Optimization**: By gathering and analyzing user feedback systematically, you can prioritize improvements based on actual user needs rather than assumptions.

3. **Continuous Improvement Loop**: The most effective assistants evolve through cycles of testing, feedback collection, analysis, and targeted optimization.

4. **Measuring Impact**: Quantifying improvements through metrics like success rate, satisfaction scores, and response accuracy helps validate optimization efforts.

5. **Balancing Metrics**: Different stakeholders may prioritize different aspects of performance—from accuracy to speed to tone—requiring a balanced optimization approach.

By implementing these testing and optimization frameworks, you can create customer assistants that continually improve in response to real-world usage patterns. This approach not only enhances the technical performance of assistants but also ensures they deliver meaningful value to end users and business stakeholders.