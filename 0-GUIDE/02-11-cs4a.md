<small>Claude Sonnet 4</small>
# 11. AI Agent Development Workshop

## Key Terms

**LangGraph Framework**: An advanced orchestration framework built on LangChain that enables the creation of stateful, multi-actor applications through directed acyclic graphs (DAGs), providing sophisticated workflow management for AI agents with branching logic, conditional execution, and complex state management.

**Agent State Management**: The systematic handling of persistent information across agent interactions, including conversation history, intermediate results, decision context, and workflow progress, enabling complex multi-step reasoning and task execution with memory continuity.

**Graph-Based Workflow**: A computational model where AI agents and their interactions are represented as nodes and edges in a directed graph, allowing for sophisticated orchestration patterns including parallel execution, conditional branching, and dynamic routing based on agent outputs.

**Stateful Agent Execution**: The capability for AI agents to maintain and modify persistent state throughout their execution lifecycle, enabling complex reasoning chains, multi-turn conversations, and collaborative problem-solving with shared context and memory.

**Agent Orchestration Patterns**: Predefined architectural patterns for coordinating multiple AI agents including sequential chains, parallel execution, hierarchical coordination, event-driven workflows, and dynamic routing based on agent outputs and system state.

**Conditional Routing**: Dynamic workflow control mechanisms that route execution flow between different agents or processes based on runtime conditions, agent outputs, user inputs, or system state, enabling adaptive and responsive agent behavior.

**Tool Integration Architecture**: Systematic approaches for connecting AI agents with external systems, APIs, databases, and services through standardized interfaces, enabling agents to perform real-world actions and access external information sources.

**Agent Testing Frameworks**: Comprehensive testing methodologies specifically designed for AI agents including unit testing for individual agent functions, integration testing for agent interactions, end-to-end workflow validation, and performance benchmarking.

## Comprehensive LangGraph Agent Development Workshop

LangGraph represents the evolution of AI agent orchestration, providing sophisticated graph-based workflows that enable complex, stateful interactions between multiple AI agents. This workshop demonstrates production-ready patterns for designing, testing, and deploying intelligent agent systems.

### Advanced LangGraph Implementation

````python
import asyncio
import json
import logging
import os
import time
import warnings
from typing import Dict, List, Any, Optional, Union, Tuple, Callable, TypedDict, Annotated
from dataclasses import dataclass, field
from datetime import datetime, timezone
from pathlib import Path
import uuid
import inspect
from enum import Enum
import threading
import operator
from concurrent.futures import ThreadPoolExecutor

# Core LangGraph imports
from langgraph.graph import StateGraph, END, START
from langgraph.graph.message import MessageGraph
from langgraph.prebuilt import ToolExecutor, ToolInvocation
from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.checkpoint.memory import MemorySaver

# LangChain imports for agent components
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_community.tools import WikipediaQueryRun
from langchain_community.utilities import WikipediaAPIWrapper
from langchain.tools import BaseTool, StructuredTool, tool
from langchain.schema import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema.runnable import RunnablePassthrough
from langchain_core.messages import ToolMessage
from langchain_core.output_parsers import JsonOutputParser, PydanticOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field

# Additional LangChain components
from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory
from langchain.callbacks.base import BaseCallbackHandler
from langchain.callbacks.manager import CallbackManager
from langchain_community.vectorstores import FAISS, Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader, PyPDFLoader

# External libraries
import sqlite3
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import requests
import aiohttp
import aiofiles
from bs4 import BeautifulSoup
import yfinance as yf

# Testing and validation
import pytest
import unittest
from unittest.mock import Mock, patch, AsyncMock
import tempfile
import shutil

# Monitoring and observability
import structlog
import wandb
from datadog import initialize, api
import psutil

# Data handling
import pickle
import yaml
import xml.etree.ElementTree as ET
from pydantic import BaseModel as PydanticBaseModel, Field as PydanticField, validator

from dotenv import load_dotenv

load_dotenv()

warnings.filterwarnings("ignore", category=DeprecationWarning)

# Setup structured logging
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    wrapper_class=structlog.stdlib.BoundLogger,
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()

class AgentRole(Enum):
    """Agent roles for different responsibilities"""
    PLANNER = "planner"
    RESEARCHER = "researcher"
    ANALYST = "analyst"
    SYNTHESIZER = "synthesizer"
    REVIEWER = "reviewer"
    EXECUTOR = "executor"
    COORDINATOR = "coordinator"

class WorkflowState(TypedDict):
    """State definition for LangGraph workflows"""
    messages: Annotated[List[BaseMessage], operator.add]
    current_task: str
    research_data: Dict[str, Any]
    analysis_results: Dict[str, Any]
    final_output: str
    agent_history: List[Dict[str, Any]]
    workflow_metadata: Dict[str, Any]
    error_log: List[str]
    iteration_count: int

class AgentConfig(PydanticBaseModel):
    """Configuration for individual agents"""
    name: str
    role: AgentRole
    system_prompt: str
    model_name: str = "gpt-4"
    temperature: float = 0.7
    max_tokens: int = 1000
    tools: List[str] = []
    memory_enabled: bool = True
    timeout: int = 300

class WorkflowConfig(PydanticBaseModel):
    """Configuration for entire workflow"""
    workflow_id: str
    name: str
    description: str
    agents: List[AgentConfig]
    checkpointer_type: str = "memory"  # memory, sqlite
    max_iterations: int = 20
    enable_human_feedback: bool = False
    parallel_execution: bool = False

class CustomTool(BaseTool):
    """Custom tool for specific agent operations"""
    
    name: str = "custom_operation"
    description: str = "Performs custom operations for agents"
    
    def _run(self, operation: str, data: str = "") -> str:
        """Execute custom operation"""
        try:
            if operation == "calculate":
                # Safe mathematical calculations
                result = eval(data.replace("^", "**"))
                return f"Calculation result: {result}"
            elif operation == "format_data":
                # Format data for better presentation
                try:
                    parsed_data = json.loads(data)
                    return json.dumps(parsed_data, indent=2)
                except:
                    return f"Formatted text: {data}"
            elif operation == "validate":
                # Validate data or results
                if len(data) > 10:
                    return "Validation: PASSED - Sufficient content"
                else:
                    return "Validation: FAILED - Insufficient content"
            else:
                return f"Operation '{operation}' completed with data: {data[:100]}"
        except Exception as e:
            return f"Error in operation '{operation}': {str(e)}"
    
    async def _arun(self, operation: str, data: str = "") -> str:
        """Async version of the tool"""
        return self._run(operation, data)

class WebSearchTool(BaseTool):
    """Web search tool for research agents"""
    
    name: str = "web_search"
    description: str = "Search the web for information"
    
    def _run(self, query: str, max_results: int = 5) -> str:
        """Search web for information"""
        try:
            # Simulate web search (in production, use actual search API)
            search_results = [
                {
                    "title": f"Result {i+1} for '{query}'",
                    "url": f"https://example.com/result{i+1}",
                    "snippet": f"This is a summary of result {i+1} related to {query}. It provides relevant information about the topic."
                }
                for i in range(min(max_results, 3))
            ]
            
            formatted_results = []
            for result in search_results:
                formatted_results.append(f"Title: {result['title']}\nURL: {result['url']}\nSummary: {result['snippet']}\n")
            
            return "\n".join(formatted_results)
            
        except Exception as e:
            return f"Search error: {str(e)}"
    
    async def _arun(self, query: str, max_results: int = 5) -> str:
        """Async version of web search"""
        return self._run(query, max_results)

class DataAnalysisTool(BaseTool):
    """Data analysis tool for analyst agents"""
    
    name: str = "data_analysis"
    description: str = "Perform statistical analysis on data"
    
    def _run(self, data: str, analysis_type: str = "summary") -> str:
        """Analyze data"""
        try:
            # Parse data (expecting JSON or CSV format)
            if data.startswith('[') or data.startswith('{'):
                # JSON data
                parsed_data = json.loads(data)
                if isinstance(parsed_data, list) and all(isinstance(x, (int, float)) for x in parsed_data):
                    numbers = parsed_data
                else:
                    return "Error: JSON data should be a list of numbers for analysis"
            else:
                # Assume CSV-like data
                numbers = [float(x.strip()) for x in data.split(',') if x.strip().replace('.', '').replace('-', '').isdigit()]
            
            if not numbers:
                return "Error: No numeric data found for analysis"
            
            # Perform analysis
            results = {
                "count": len(numbers),
                "sum": sum(numbers),
                "mean": np.mean(numbers),
                "median": np.median(numbers),
                "std": np.std(numbers),
                "min": min(numbers),
                "max": max(numbers),
                "range": max(numbers) - min(numbers)
            }
            
            if analysis_type == "detailed":
                results.update({
                    "q1": np.percentile(numbers, 25),
                    "q3": np.percentile(numbers, 75),
                    "iqr": np.percentile(numbers, 75) - np.percentile(numbers, 25),
                    "variance": np.var(numbers)
                })
            
            return json.dumps(results, indent=2)
            
        except Exception as e:
            return f"Analysis error: {str(e)}"
    
    async def _arun(self, data: str, analysis_type: str = "summary") -> str:
        """Async version of data analysis"""
        return self._run(data, analysis_type)

class AdvancedAgent:
    """Advanced agent with LangGraph integration"""
    
    def __init__(self, config: AgentConfig):
        self.config = config
        self.llm = None
        self.tools = []
        self.memory = None
        self._setup_agent()
    
    def _setup_agent(self):
        """Setup agent components"""
        try:
            # Initialize LLM
            self.llm = ChatOpenAI(
                model=self.config.model_name,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
                openai_api_key=os.getenv('OPENAI_API_KEY')
            )
            
            # Setup tools based on configuration
            available_tools = {
                "web_search": WebSearchTool(),
                "data_analysis": DataAnalysisTool(),
                "custom_operation": CustomTool()
            }
            
            self.tools = [available_tools[tool_name] for tool_name in self.config.tools if tool_name in available_tools]
            
            # Setup memory if enabled
            if self.config.memory_enabled:
                self.memory = ConversationBufferMemory(
                    memory_key="chat_history",
                    return_messages=True
                )
            
            logger.info(f"Agent '{self.config.name}' initialized with {len(self.tools)} tools")
            
        except Exception as e:
            logger.error(f"Error setting up agent '{self.config.name}': {e}")
            raise
    
    async def execute(self, state: WorkflowState) -> WorkflowState:
        """Execute agent logic"""
        start_time = time.time()
        
        try:
            # Prepare system message
            system_message = SystemMessage(content=self.config.system_prompt)
            
            # Get recent messages for context
            recent_messages = state["messages"][-5:] if state["messages"] else []
            
            # Prepare prompt with context
            prompt_messages = [system_message] + recent_messages
            
            # Add current task context
            if state["current_task"]:
                task_message = HumanMessage(content=f"Current task: {state['current_task']}")
                prompt_messages.append(task_message)
            
            # Execute LLM call
            response = await self.llm.ainvoke(prompt_messages)
            
            # Process response and potentially use tools
            processed_response = await self._process_response(response, state)
            
            # Update state
            state["messages"].append(AIMessage(content=processed_response))
            state["agent_history"].append({
                "agent": self.config.name,
                "role": self.config.role.value,
                "response": processed_response,
                "execution_time": time.time() - start_time,
                "timestamp": datetime.now(timezone.utc).isoformat()
            })
            
            logger.info(f"Agent '{self.config.name}' executed successfully")
            return state
            
        except Exception as e:
            error_msg = f"Agent '{self.config.name}' execution error: {str(e)}"
            logger.error(error_msg)
            
            state["error_log"].append(error_msg)
            state["messages"].append(AIMessage(content=f"Error: {error_msg}"))
            
            return state
    
    async def _process_response(self, response: AIMessage, state: WorkflowState) -> str:
        """Process agent response and execute tools if needed"""
        
        response_content = response.content
        
        # Check if response indicates tool usage
        if "TOOL:" in response_content and self.tools:
            try:
                # Extract tool usage (simplified parsing)
                lines = response_content.split('\n')
                for line in lines:
                    if line.startswith("TOOL:"):
                        tool_instruction = line.replace("TOOL:", "").strip()
                        
                        # Parse tool instruction (format: tool_name:parameter)
                        if ':' in tool_instruction:
                            tool_name, tool_params = tool_instruction.split(':', 1)
                            
                            # Find and execute tool
                            for tool in self.tools:
                                if tool.name == tool_name.strip():
                                    tool_result = await tool._arun(tool_params.strip())
                                    response_content += f"\n\nTool Result:\n{tool_result}"
                                    break
            except Exception as e:
                response_content += f"\n\nTool execution error: {str(e)}"
        
        return response_content

class LangGraphWorkflow:
    """Advanced LangGraph workflow manager"""
    
    def __init__(self, config: WorkflowConfig):
        self.config = config
        self.agents = {}
        self.graph = None
        self.checkpointer = None
        self._setup_workflow()
    
    def _setup_workflow(self):
        """Setup workflow components"""
        try:
            # Create agents
            for agent_config in self.config.agents:
                agent = AdvancedAgent(agent_config)
                self.agents[agent_config.name] = agent
            
            # Setup checkpointer
            if self.config.checkpointer_type == "sqlite":
                self.checkpointer = SqliteSaver.from_conn_string(":memory:")
            else:
                self.checkpointer = MemorySaver()
            
            # Build graph
            self._build_graph()
            
            logger.info(f"Workflow '{self.config.workflow_id}' initialized with {len(self.agents)} agents")
            
        except Exception as e:
            logger.error(f"Error setting up workflow: {e}")
            raise
    
    def _build_graph(self):
        """Build the LangGraph workflow"""
        
        # Create state graph
        graph = StateGraph(WorkflowState)
        
        # Add agent nodes
        for agent_name, agent in self.agents.items():
            graph.add_node(agent_name, agent.execute)
        
        # Add special nodes
        graph.add_node("initialize", self._initialize_workflow)
        graph.add_node("coordinator", self._coordinate_agents)
        graph.add_node("finalizer", self._finalize_workflow)
        
        # Define workflow edges based on agent roles
        self._define_workflow_edges(graph)
        
        # Set entry point
        graph.set_entry_point("initialize")
        
        # Compile graph
        self.graph = graph.compile(checkpointer=self.checkpointer)
    
    def _define_workflow_edges(self, graph):
        """Define edges between workflow nodes"""
        
        # Get agents by role
        planners = [name for name, agent in self.agents.items() if agent.config.role == AgentRole.PLANNER]
        researchers = [name for name, agent in self.agents.items() if agent.config.role == AgentRole.RESEARCHER]
        analysts = [name for name, agent in self.agents.items() if agent.config.role == AgentRole.ANALYST]
        synthesizers = [name for name, agent in self.agents.items() if agent.config.role == AgentRole.SYNTHESIZER]
        reviewers = [name for name, agent in self.agents.items() if agent.config.role == AgentRole.REVIEWER]
        
        # Initialize to coordinator
        graph.add_edge("initialize", "coordinator")
        
        # Coordinator routes to appropriate agents
        graph.add_conditional_edges(
            "coordinator",
            self._route_to_agent,
            {
                "planner": planners[0] if planners else "finalizer",
                "researcher": researchers[0] if researchers else "finalizer",
                "analyst": analysts[0] if analysts else "finalizer",
                "synthesizer": synthesizers[0] if synthesizers else "finalizer",
                "reviewer": reviewers[0] if reviewers else "finalizer",
                "end": "finalizer"
            }
        )
        
        # Agents back to coordinator
        for agent_name in self.agents.keys():
            graph.add_edge(agent_name, "coordinator")
        
        # Finalizer to end
        graph.add_edge("finalizer", END)
    
    async def _initialize_workflow(self, state: WorkflowState) -> WorkflowState:
        """Initialize workflow state"""
        
        if not state.get("workflow_metadata"):
            state["workflow_metadata"] = {
                "workflow_id": self.config.workflow_id,
                "started_at": datetime.now(timezone.utc).isoformat(),
                "max_iterations": self.config.max_iterations
            }
        
        if not state.get("iteration_count"):
            state["iteration_count"] = 0
        
        if not state.get("agent_history"):
            state["agent_history"] = []
        
        if not state.get("error_log"):
            state["error_log"] = []
        
        logger.info(f"Workflow '{self.config.workflow_id}' initialized")
        return state
    
    async def _coordinate_agents(self, state: WorkflowState) -> WorkflowState:
        """Coordinate agent execution"""
        
        state["iteration_count"] += 1
        
        # Check termination conditions
        if state["iteration_count"] >= self.config.max_iterations:
            state["current_task"] = "end"
            return state
        
        # Determine next agent based on workflow progress
        if not state.get("research_data"):
            state["current_task"] = "research"
            return state
        elif not state.get("analysis_results"):
            state["current_task"] = "analysis"
            return state
        elif not state.get("final_output"):
            state["current_task"] = "synthesis"
            return state
        else:
            state["current_task"] = "end"
            return state
    
    def _route_to_agent(self, state: WorkflowState) -> str:
        """Route to appropriate agent based on current task"""
        
        task = state.get("current_task", "")
        
        if task == "research":
            return "researcher"
        elif task == "analysis":
            return "analyst"
        elif task == "synthesis":
            return "synthesizer"
        elif task == "review":
            return "reviewer"
        elif task == "planning":
            return "planner"
        else:
            return "end"
    
    async def _finalize_workflow(self, state: WorkflowState) -> WorkflowState:
        """Finalize workflow execution"""
        
        state["workflow_metadata"]["completed_at"] = datetime.now(timezone.utc).isoformat()
        state["workflow_metadata"]["total_iterations"] = state["iteration_count"]
        state["workflow_metadata"]["agents_used"] = len(set(
            entry["agent"] for entry in state["agent_history"]
        ))
        
        logger.info(f"Workflow '{self.config.workflow_id}' finalized")
        return state
    
    async def execute(self, initial_input: str, config_override: Dict[str, Any] = None) -> Dict[str, Any]:
        """Execute the workflow"""
        
        try:
            # Prepare initial state
            initial_state = WorkflowState(
                messages=[HumanMessage(content=initial_input)],
                current_task="planning",
                research_data={},
                analysis_results={},
                final_output="",
                agent_history=[],
                workflow_metadata={},
                error_log=[],
                iteration_count=0
            )
            
            # Execute workflow
            config = {"configurable": {"thread_id": str(uuid.uuid4())}}
            if config_override:
                config.update(config_override)
            
            result = await self.graph.ainvoke(initial_state, config=config)
            
            # Process results
            execution_summary = {
                "workflow_id": self.config.workflow_id,
                "status": "completed" if not result["error_log"] else "completed_with_errors",
                "execution_time": self._calculate_execution_time(result),
                "iterations": result["iteration_count"],
                "agents_used": result["workflow_metadata"].get("agents_used", 0),
                "messages_exchanged": len(result["messages"]),
                "final_output": result.get("final_output", ""),
                "errors": result["error_log"],
                "agent_history": result["agent_history"]
            }
            
            return execution_summary
            
        except Exception as e:
            logger.error(f"Workflow execution error: {e}")
            return {
                "workflow_id": self.config.workflow_id,
                "status": "failed",
                "error": str(e),
                "execution_time": 0
            }
    
    def _calculate_execution_time(self, result: WorkflowState) -> float:
        """Calculate total execution time from agent history"""
        
        if not result["agent_history"]:
            return 0.0
        
        return sum(entry.get("execution_time", 0) for entry in result["agent_history"])

class AgentTestFramework:
    """Comprehensive testing framework for AI agents"""
    
    def __init__(self):
        self.test_results = []
        self.performance_metrics = {}
    
    async def test_individual_agent(self, agent: AdvancedAgent, test_cases: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Test individual agent with various scenarios"""
        
        results = {
            "agent_name": agent.config.name,
            "test_cases": [],
            "success_rate": 0.0,
            "average_response_time": 0.0
        }
        
        successful_tests = 0
        total_response_time = 0.0
        
        for i, test_case in enumerate(test_cases):
            start_time = time.time()
            
            try:
                # Prepare test state
                test_state = WorkflowState(
                    messages=[HumanMessage(content=test_case["input"])],
                    current_task=test_case.get("task", "test"),
                    research_data={},
                    analysis_results={},
                    final_output="",
                    agent_history=[],
                    workflow_metadata={},
                    error_log=[],
                    iteration_count=0
                )
                
                # Execute agent
                result_state = await agent.execute(test_state)
                
                response_time = time.time() - start_time
                total_response_time += response_time
                
                # Evaluate result
                test_result = {
                    "test_id": f"test_{i+1}",
                    "input": test_case["input"],
                    "expected": test_case.get("expected", ""),
                    "actual": result_state["messages"][-1].content if result_state["messages"] else "",
                    "response_time": response_time,
                    "passed": self._evaluate_test_case(test_case, result_state),
                    "errors": result_state["error_log"]
                }
                
                if test_result["passed"]:
                    successful_tests += 1
                
                results["test_cases"].append(test_result)
                
            except Exception as e:
                results["test_cases"].append({
                    "test_id": f"test_{i+1}",
                    "input": test_case["input"],
                    "error": str(e),
                    "passed": False,
                    "response_time": time.time() - start_time
                })
        
        # Calculate metrics
        results["success_rate"] = successful_tests / len(test_cases) if test_cases else 0.0
        results["average_response_time"] = total_response_time / len(test_cases) if test_cases else 0.0
        
        return results
    
    def _evaluate_test_case(self, test_case: Dict[str, Any], result_state: WorkflowState) -> bool:
        """Evaluate if test case passed"""
        
        # Basic evaluation criteria
        if result_state["error_log"]:
            return False
        
        if not result_state["messages"]:
            return False
        
        response = result_state["messages"][-1].content
        
        # Check for expected keywords or patterns
        if "expected_keywords" in test_case:
            for keyword in test_case["expected_keywords"]:
                if keyword.lower() not in response.lower():
                    return False
        
        # Check minimum response length
        if "min_length" in test_case:
            if len(response) < test_case["min_length"]:
                return False
        
        return True
    
    async def test_workflow_integration(self, workflow: LangGraphWorkflow, test_scenarios: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Test complete workflow integration"""
        
        results = {
            "workflow_id": workflow.config.workflow_id,
            "scenarios": [],
            "success_rate": 0.0,
            "average_execution_time": 0.0
        }
        
        successful_scenarios = 0
        total_execution_time = 0.0
        
        for i, scenario in enumerate(test_scenarios):
            start_time = time.time()
            
            try:
                # Execute workflow
                execution_result = await workflow.execute(scenario["input"])
                
                execution_time = time.time() - start_time
                total_execution_time += execution_time
                
                # Evaluate scenario
                scenario_result = {
                    "scenario_id": f"scenario_{i+1}",
                    "input": scenario["input"],
                    "status": execution_result["status"],
                    "execution_time": execution_time,
                    "iterations": execution_result.get("iterations", 0),
                    "agents_used": execution_result.get("agents_used", 0),
                    "passed": execution_result["status"] == "completed",
                    "errors": execution_result.get("errors", [])
                }
                
                if scenario_result["passed"]:
                    successful_scenarios += 1
                
                results["scenarios"].append(scenario_result)
                
            except Exception as e:
                results["scenarios"].append({
                    "scenario_id": f"scenario_{i+1}",
                    "input": scenario["input"],
                    "error": str(e),
                    "passed": False,
                    "execution_time": time.time() - start_time
                })
        
        # Calculate metrics
        results["success_rate"] = successful_scenarios / len(test_scenarios) if test_scenarios else 0.0
        results["average_execution_time"] = total_execution_time / len(test_scenarios) if test_scenarios else 0.0
        
        return results
    
    def generate_test_report(self, test_results: List[Dict[str, Any]]) -> str:
        """Generate comprehensive test report"""
        
        report_lines = []
        report_lines.append("=== AI Agent Testing Report ===")
        report_lines.append(f"Generated: {datetime.now(timezone.utc).isoformat()}")
        report_lines.append("")
        
        for result in test_results:
            if "agent_name" in result:
                # Individual agent test result
                report_lines.append(f"Agent: {result['agent_name']}")
                report_lines.append(f"Success Rate: {result['success_rate']:.2%}")
                report_lines.append(f"Average Response Time: {result['average_response_time']:.2f}s")
                report_lines.append(f"Test Cases: {len(result['test_cases'])}")
                report_lines.append("")
                
            elif "workflow_id" in result:
                # Workflow integration test result
                report_lines.append(f"Workflow: {result['workflow_id']}")
                report_lines.append(f"Success Rate: {result['success_rate']:.2%}")
                report_lines.append(f"Average Execution Time: {result['average_execution_time']:.2f}s")
                report_lines.append(f"Scenarios: {len(result['scenarios'])}")
                report_lines.append("")
        
        return "\n".join(report_lines)

# Practical workshop demonstration
async def conduct_agent_development_workshop():
    """Comprehensive agent development workshop"""
    
    logger.info("=== AI Agent Development Workshop ===")
    
    # Check for OpenAI API key
    openai_api_key = os.getenv('OPENAI_API_KEY')
    if not openai_api_key:
        logger.error("OpenAI API key not found. Please set OPENAI_API_KEY environment variable.")
        return
    
    # 1. Design Phase - Define Agent Configurations
    logger.info("\n1. Agent Design Phase")
    
    # Research Agent Configuration
    researcher_config = AgentConfig(
        name="research_agent",
        role=AgentRole.RESEARCHER,
        system_prompt="""
        You are a Research Agent responsible for gathering and analyzing information.
        Your capabilities include:
        - Conducting thorough research on given topics
        - Gathering data from multiple sources
        - Synthesizing information from various formats
        - Providing comprehensive research summaries
        
        When you need to search for information, use: TOOL:web_search:your_search_query
        Always provide detailed, well-structured research findings.
        """,
        model_name="gpt-4",
        temperature=0.3,
        tools=["web_search", "custom_operation"]
    )
    
    # Analyst Agent Configuration
    analyst_config = AgentConfig(
        name="analysis_agent",
        role=AgentRole.ANALYST,
        system_prompt="""
        You are a Data Analysis Agent specialized in processing and analyzing data.
        Your capabilities include:
        - Performing statistical analysis on datasets
        - Identifying patterns and trends
        - Creating data visualizations
        - Generating actionable insights
        
        When you need to analyze data, use: TOOL:data_analysis:your_data
        Provide clear, statistically sound analysis with actionable recommendations.
        """,
        model_name="gpt-4",
        temperature=0.2,
        tools=["data_analysis", "custom_operation"]
    )
    
    # Synthesizer Agent Configuration
    synthesizer_config = AgentConfig(
        name="synthesis_agent",
        role=AgentRole.SYNTHESIZER,
        system_prompt="""
        You are a Synthesis Agent responsible for combining information from multiple sources.
        Your capabilities include:
        - Synthesizing research findings and analysis results
        - Creating comprehensive reports
        - Identifying connections between different data points
        - Generating final recommendations
        
        Focus on creating coherent, well-structured outputs that combine all available information.
        """,
        model_name="gpt-4",
        temperature=0.4,
        tools=["custom_operation"]
    )
    
    # 2. Workflow Configuration
    logger.info("\n2. Workflow Configuration Phase")
    
    workflow_config = WorkflowConfig(
        workflow_id="research_analysis_workflow",
        name="Research and Analysis Pipeline",
        description="Multi-agent workflow for research, analysis, and synthesis",
        agents=[researcher_config, analyst_config, synthesizer_config],
        checkpointer_type="memory",
        max_iterations=15,
        enable_human_feedback=False
    )
    
    # 3. Agent Testing Phase
    logger.info("\n3. Agent Testing Phase")
    
    test_framework = AgentTestFramework()
    
    # Test individual agents
    individual_test_results = []
    
    # Test Research Agent
    research_agent = AdvancedAgent(researcher_config)
    research_test_cases = [
        {
            "input": "Research the latest trends in artificial intelligence for healthcare",
            "expected_keywords": ["AI", "healthcare", "trends"],
            "min_length": 100
        },
        {
            "input": "Find information about machine learning in finance",
            "expected_keywords": ["machine learning", "finance"],
            "min_length": 100
        }
    ]
    
    research_results = await test_framework.test_individual_agent(research_agent, research_test_cases)
    individual_test_results.append(research_results)
    logger.info(f"Research Agent Tests: {research_results['success_rate']:.2%} success rate")
    
    # Test Analysis Agent
    analysis_agent = AdvancedAgent(analyst_config)
    analysis_test_cases = [
        {
            "input": "Analyze this sales data: [100, 150, 200, 175, 225, 300, 250]",
            "expected_keywords": ["mean", "analysis", "data"],
            "min_length": 50
        },
        {
            "input": "Perform statistical analysis on: 45, 67, 23, 89, 56, 78, 34",
            "expected_keywords": ["statistics", "analysis"],
            "min_length": 50
        }
    ]
    
    analysis_results = await test_framework.test_individual_agent(analysis_agent, analysis_test_cases)
    individual_test_results.append(analysis_results)
    logger.info(f"Analysis Agent Tests: {analysis_results['success_rate']:.2%} success rate")
    
    # 4. Workflow Integration Testing
    logger.info("\n4. Workflow Integration Testing")
    
    workflow = LangGraphWorkflow(workflow_config)
    
    integration_test_scenarios = [
        {
            "input": "Conduct a comprehensive analysis of the impact of AI on the job market. Include research findings, statistical analysis, and final recommendations."
        },
        {
            "input": "Research and analyze the adoption rates of electric vehicles globally. Provide data-driven insights and market projections."
        }
    ]
    
    integration_results = await test_framework.test_workflow_integration(workflow, integration_test_scenarios)
    logger.info(f"Workflow Integration Tests: {integration_results['success_rate']:.2%} success rate")
    
    # 5. Production Deployment Simulation
    logger.info("\n5. Production Deployment Simulation")
    
    production_scenarios = [
        "Analyze the financial performance of tech companies in Q3 2023",
        "Research emerging technologies in renewable energy",
        "Evaluate market opportunities for AI-powered healthcare solutions"
    ]
    
    deployment_results = []
    
    for i, scenario in enumerate(production_scenarios):
        try:
            logger.info(f"Executing production scenario {i+1}: {scenario[:50]}...")
            
            start_time = time.time()
            result = await workflow.execute(scenario)
            execution_time = time.time() - start_time
            
            deployment_results.append({
                "scenario": scenario,
                "status": result["status"],
                "execution_time": execution_time,
                "iterations": result.get("iterations", 0),
                "agents_used": result.get("agents_used", 0),
                "errors": result.get("errors", [])
            })
            
            logger.info(f"Scenario {i+1} completed: {result['status']} in {execution_time:.2f}s")
            
        except Exception as e:
            logger.error(f"Scenario {i+1} failed: {e}")
            deployment_results.append({
                "scenario": scenario,
                "status": "failed",
                "error": str(e)
            })
    
    # 6. Performance Analysis
    logger.info("\n6. Performance Analysis")
    
    # Create performance visualization
    try:
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # Individual agent performance
        agent_names = [result["agent_name"] for result in individual_test_results]
        success_rates = [result["success_rate"] for result in individual_test_results]
        
        axes[0, 0].bar(agent_names, success_rates)
        axes[0, 0].set_title('Individual Agent Success Rates')
        axes[0, 0].set_ylabel('Success Rate')
        axes[0, 0].set_ylim(0, 1)
        
        # Response times
        response_times = [result["average_response_time"] for result in individual_test_results]
        
        axes[0, 1].bar(agent_names, response_times)
        axes[0, 1].set_title('Average Response Times')
        axes[0, 1].set_ylabel('Time (seconds)')
        
        # Workflow performance
        scenario_names = [f"Scenario {i+1}" for i in range(len(deployment_results))]
        execution_times = [result.get("execution_time", 0) for result in deployment_results]
        
        axes[1, 0].bar(scenario_names, execution_times)
        axes[1, 0].set_title('Workflow Execution Times')
        axes[1, 0].set_ylabel('Time (seconds)')
        
        # Success distribution
        successful_deployments = len([r for r in deployment_results if r.get("status") == "completed"])
        failed_deployments = len(deployment_results) - successful_deployments
        
        axes[1, 1].pie([successful_deployments, failed_deployments], 
                       labels=['Successful', 'Failed'], 
                       autopct='%1.1f%%')
        axes[1, 1].set_title('Deployment Success Rate')
        
        plt.tight_layout()
        plt.savefig('agent_workshop_performance.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info("Performance visualization saved")
        
    except Exception as e:
        logger.warning(f"Error creating visualizations: {e}")
    
    # 7. Generate Test Report
    logger.info("\n7. Generating Test Report")
    
    all_test_results = individual_test_results + [integration_results]
    test_report = test_framework.generate_test_report(all_test_results)
    
    with open("agent_testing_report.txt", "w") as f:
        f.write(test_report)
    
    # 8. Generate Comprehensive Workshop Report
    workshop_report = {
        "workshop_summary": {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "agents_created": len(workflow_config.agents),
            "individual_tests_conducted": sum(len(result["test_cases"]) for result in individual_test_results),
            "integration_scenarios_tested": len(integration_test_scenarios),
            "production_scenarios_executed": len(production_scenarios),
            "overall_success_rate": sum(result["success_rate"] for result in individual_test_results) / len(individual_test_results) if individual_test_results else 0
        },
        
        "agent_configurations": [
            {
                "name": config.name,
                "role": config.role.value,
                "tools": config.tools,
                "temperature": config.temperature,
                "memory_enabled": config.memory_enabled
            }
            for config in [researcher_config, analyst_config, synthesizer_config]
        ],
        
        "testing_results": {
            "individual_agent_tests": individual_test_results,
            "workflow_integration_test": integration_results,
            "production_deployment_results": deployment_results
        },
        
        "performance_metrics": {
            "average_agent_success_rate": sum(result["success_rate"] for result in individual_test_results) / len(individual_test_results) if individual_test_results else 0,
            "average_response_time": sum(result["average_response_time"] for result in individual_test_results) / len(individual_test_results) if individual_test_results else 0,
            "workflow_success_rate": integration_results["success_rate"],
            "production_success_rate": len([r for r in deployment_results if r.get("status") == "completed"]) / len(deployment_results) if deployment_results else 0
        },
        
        "key_insights": [
            "LangGraph provides robust state management for complex workflows",
            "Individual agent testing reveals specific performance characteristics",
            "Integration testing identifies coordination challenges",
            "Production simulation validates real-world deployment readiness",
            "Tool integration enhances agent capabilities significantly",
            "Proper error handling is crucial for production systems"
        ],
        
        "best_practices_identified": [
            "Design agents with clear, specific roles and responsibilities",
            "Implement comprehensive testing at multiple levels",
            "Use appropriate tools for each agent's domain expertise",
            "Configure proper state management and checkpointing",
            "Monitor performance metrics continuously",
            "Implement proper error handling and recovery mechanisms",
            "Use conditional routing for dynamic workflow control",
            "Test with realistic production scenarios",
            "Document agent capabilities and limitations",
            "Implement proper logging and observability"
        ],
        
        "deployment_recommendations": [
            "Start with simple workflows before complex orchestrations",
            "Implement proper authentication and security measures",
            "Use persistent checkpointing for long-running workflows",
            "Monitor resource usage and costs in production",
            "Implement human-in-the-loop for critical decisions",
            "Use A/B testing for workflow optimization",
            "Implement proper backup and recovery procedures",
            "Scale horizontally for high-throughput scenarios",
            "Use appropriate database backends for state management",
            "Implement comprehensive monitoring and alerting"
        ]
    }
    
    # Save comprehensive report
    with open("agent_development_workshop_report.json", "w") as f:
        json.dump(workshop_report, f, indent=2, default=str)
    
    logger.info("Agent Development Workshop completed!")
    logger.info("Check 'agent_development_workshop_report.json' for detailed results")
    
    return workshop_report

# Main execution
async def main():
    """Main execution for agent development workshop"""
    try:
        report = await conduct_agent_development_workshop()
        
        # Display key results
        logger.info("\n=== Workshop Summary ===")
        logger.info(f"Agents created: {report['workshop_summary']['agents_created']}")
        logger.info(f"Individual tests: {report['workshop_summary']['individual_tests_conducted']}")
        logger.info(f"Integration scenarios: {report['workshop_summary']['integration_scenarios_tested']}")
        logger.info(f"Production scenarios: {report['workshop_summary']['production_scenarios_executed']}")
        logger.info(f"Overall success rate: {report['workshop_summary']['overall_success_rate']:.2%}")
        
        # Display performance metrics
        metrics = report["performance_metrics"]
        logger.info(f"\nPerformance Metrics:")
        logger.info(f"Agent success rate: {metrics['average_agent_success_rate']:.2%}")
        logger.info(f"Average response time: {metrics['average_response_time']:.2f}s")
        logger.info(f"Workflow success rate: {metrics['workflow_success_rate']:.2%}")
        logger.info(f"Production success rate: {metrics['production_success_rate']:.2%}")
        
    except Exception as e:
        logger.error(f"Workshop execution failed: {e}")
        import traceback
        logger.error(traceback.format_exc())

if __name__ == "__main__":
    asyncio.run(main())
````

## Conclusion

This comprehensive AI Agent Development Workshop demonstrates the complete lifecycle of building production-ready intelligent agents using LangGraph and OpenAI, establishing sophisticated patterns for stateful multi-agent orchestration that bridges the gap between experimental AI research and practical enterprise deployment.

**LangGraph Excellence** through graph-based workflow orchestration enables the creation of complex, stateful agent systems that maintain context across multiple interactions while providing sophisticated routing, conditional execution, and parallel processing capabilities that exceed traditional sequential processing limitations.

**Comprehensive Testing Methodology** via multi-level validation frameworks ensures agent reliability through individual component testing, integration validation, and production scenario simulation, establishing confidence in system behavior before deployment to critical enterprise environments.

**Advanced State Management** through persistent checkpointing and memory systems enables agents to maintain complex conversation states, intermediate results, and workflow progress across extended interactions, providing the foundation for sophisticated reasoning chains and collaborative problem-solving.

**Production-Ready Architecture** including error handling, performance monitoring, resource management, and scalability considerations ensures that LangGraph-based agent systems can be deployed reliably in enterprise environments with appropriate governance, observability, and operational excellence.

**Tool Integration Mastery** through standardized interfaces for external systems, APIs, and services enables agents to perform real-world actions beyond conversational interactions, transforming theoretical AI capabilities into practical automation solutions that integrate seamlessly with existing infrastructure.

**Automated Inter-Model Coordination** via sophisticated orchestration patterns enables multiple AI models to collaborate effectively on complex tasks, with intelligent routing, dynamic task allocation, and emergent problem-solving capabilities that leverage the strengths of different model architectures.

**Enterprise Deployment Patterns** including proper authentication, security measures, monitoring, and maintenance procedures provide the operational foundation necessary for running AI agent systems in production environments with appropriate risk management and compliance requirements.

This advanced workshop framework establishes the complete foundation for building, testing, and deploying intelligent agent systems that can handle complex, multi-step tasks through sophisticated coordination patterns, providing organizations with the knowledge and tools necessary to implement AI automation solutions that deliver measurable business value while maintaining operational excellence and system reliability.