<small>Claude 3.7 Sonnet Thinking</small>
# 03. Training Data Preparation

## Key Terms

- **Data Collection**: The systematic acquisition of information from various sources for model training purposes.
- **Data Cleaning**: The process of fixing or removing incorrect, corrupted, incorrectly formatted, duplicate, or incomplete data within a dataset.
- **Data Augmentation**: Techniques to increase the diversity of training data without collecting new data, through transformations, paraphrasing, or synthetic generation.
- **Tokenization**: The process of converting text into smaller units (tokens) that can be processed by language models.
- **Domain Adaptation**: Tailoring a model to perform well on a specific domain or subject area by exposing it to domain-relevant data.
- **Data Schema**: The structural framework that defines how training data is organized and formatted.
- **Data Drift**: The phenomenon where statistical properties of target variables change over time, affecting model performance.
- **Data Lineage**: Documentation of data's origins, transformations, and movements through systems.
- **Preprocessing Pipeline**: The sequence of operations applied to raw data to transform it into model-ready format.

## Advanced Data Collection Techniques

Modern training data collection requires robust infrastructure to gather diverse, representative datasets. Here's an implementation of a versatile data collector:

```python
import os
import logging
import asyncio
import aiohttp
import pandas as pd
from typing import List, Dict, Any, Union, Optional
from datetime import datetime
from dotenv import load_dotenv
from bs4 import BeautifulSoup
from tqdm.asyncio import tqdm
import json
import re
import hashlib

# Load environment variables
load_dotenv()

class AdvancedDataCollector:
    """Advanced data collection system supporting multiple sources and formats."""
    
    def __init__(
        self,
        output_dir: str = "collected_data",
        rate_limit: int = 5,
        timeout: int = 30,
        user_agent: Optional[str] = None
    ):
        """
        Initialize the data collector with configuration parameters.
        
        Args:
            output_dir: Directory to store collected data
            rate_limit: Maximum requests per second
            timeout: Request timeout in seconds
            user_agent: Custom user agent string
        """
        self.output_dir = output_dir
        self.rate_limit = rate_limit
        self.timeout = timeout
        self.user_agent = user_agent or "DataCollector/1.0"
        self.semaphore = asyncio.Semaphore(rate_limit)
        self.logger = self._setup_logger()
        
        # Ensure output directory exists
        os.makedirs(output_dir, exist_ok=True)
        
    def _setup_logger(self) -> logging.Logger:
        """Configure and return logger instance."""
        logger = logging.getLogger("DataCollector")
        logger.setLevel(logging.INFO)
        
        # Create handler if none exists
        if not logger.handlers:
            handler = logging.FileHandler(f"data_collection_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            
            # Add console handler
            console = logging.StreamHandler()
            console.setFormatter(formatter)
            logger.addHandler(console)
            
        return logger
    
    async def _fetch_url(self, session: aiohttp.ClientSession, url: str) -> Optional[str]:
        """
        Fetch content from URL with rate limiting and error handling.
        
        Args:
            session: aiohttp client session
            url: URL to fetch
            
        Returns:
            Page content as string or None if failed
        """
        async with self.semaphore:
            try:
                headers = {"User-Agent": self.user_agent}
                async with session.get(url, headers=headers, timeout=self.timeout) as response:
                    if response.status == 200:
                        return await response.text()
                    else:
                        self.logger.error(f"Failed to fetch {url}: HTTP {response.status}")
                        return None
            except Exception as e:
                self.logger.error(f"Error fetching {url}: {str(e)}")
                return None
            
            # Rate limiting pause
            await asyncio.sleep(1.0 / self.rate_limit)
    
    async def scrape_web_content(
        self,
        urls: List[str],
        content_selector: str,
        title_selector: Optional[str] = None,
        metadata_selectors: Optional[Dict[str, str]] = None
    ) -> List[Dict[str, Any]]:
        """
        Scrape content from multiple web pages concurrently.
        
        Args:
            urls: List of URLs to scrape
            content_selector: CSS selector for main content
            title_selector: CSS selector for title (optional)
            metadata_selectors: Dict of name -> CSS selector for metadata (optional)
            
        Returns:
            List of dictionaries with scraped data
        """
        results = []
        
        async with aiohttp.ClientSession() as session:
            tasks = [self._process_url(session, url, content_selector, title_selector, metadata_selectors) 
                    for url in urls]
            
            for result in await tqdm.gather(*tasks, desc="Scraping URLs"):
                if result:
                    results.append(result)
        
        # Save results
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = os.path.join(self.output_dir, f"web_content_{timestamp}.jsonl")
        
        with open(output_path, 'w', encoding='utf-8') as f:
            for item in results:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        
        self.logger.info(f"Saved {len(results)} scraped items to {output_path}")
        return results
    
    async def _process_url(
        self, 
        session: aiohttp.ClientSession, 
        url: str,
        content_selector: str,
        title_selector: Optional[str],
        metadata_selectors: Optional[Dict[str, str]]
    ) -> Optional[Dict[str, Any]]:
        """Process a single URL and extract content."""
        content = await self._fetch_url(session, url)
        if not content:
            return None
        
        try:
            soup = BeautifulSoup(content, 'html.parser')
            
            # Extract main content
            main_content_elem = soup.select_one(content_selector)
            if not main_content_elem:
                self.logger.warning(f"Content selector '{content_selector}' not found on {url}")
                return None
            
            main_content = main_content_elem.get_text(strip=True)
            
            # Extract title if selector provided
            title = None
            if title_selector:
                title_elem = soup.select_one(title_selector)
                if title_elem:
                    title = title_elem.get_text(strip=True)
            
            # Extract metadata if selectors provided
            metadata = {}
            if metadata_selectors:
                for name, selector in metadata_selectors.items():
                    elem = soup.select_one(selector)
                    if elem:
                        metadata[name] = elem.get_text(strip=True)
            
            # Create document ID based on content
            doc_id = hashlib.md5(main_content.encode()).hexdigest()
            
            return {
                "id": doc_id,
                "url": url,
                "title": title,
                "content": main_content,
                "metadata": metadata,
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            self.logger.error(f"Error processing {url}: {str(e)}")
            return None
    
    async def collect_from_api(
        self,
        api_url: str,
        api_key: Optional[str] = None,
        params: Optional[Dict[str, Any]] = None,
        paginate: bool = False,
        max_pages: int = 10,
        page_param: str = "page",
        results_path: str = "results"
    ) -> List[Dict[str, Any]]:
        """
        Collect data from REST API with pagination support.
        
        Args:
            api_url: Base API URL
            api_key: API key (if needed)
            params: Additional query parameters
            paginate: Whether to use pagination
            max_pages: Maximum number of pages to fetch
            page_param: Query parameter name for page number
            results_path: JSON path to results array in response
            
        Returns:
            List of collected items
        """
        all_results = []
        page = 1
        
        # Setup headers
        headers = {"Accept": "application/json"}
        if api_key:
            if os.getenv("API_KEY"):
                api_key = os.getenv("API_KEY")
            headers["Authorization"] = f"Bearer {api_key}"
        
        # Setup params
        request_params = params.copy() if params else {}
        
        async with aiohttp.ClientSession() as session:
            while True:
                if paginate:
                    request_params[page_param] = page
                
                try:
                    async with session.get(api_url, params=request_params, headers=headers) as response:
                        if response.status != 200:
                            self.logger.error(f"API request failed: {response.status}")
                            break
                        
                        data = await response.json()
                        
                        # Extract results using the results path
                        current_results = data
                        if results_path:
                            for key in results_path.split('.'):
                                if key in current_results:
                                    current_results = current_results[key]
                                else:
                                    self.logger.error(f"Results path '{results_path}' not found in response")
                                    current_results = []
                        
                        # Add results to collection
                        if isinstance(current_results, list):
                            all_results.extend(current_results)
                        else:
                            self.logger.warning("API did not return a list of results")
                            break
                        
                        # Check if we should continue pagination
                        if not paginate or page >= max_pages or len(current_results) == 0:
                            break
                        
                        page += 1
                        
                except Exception as e:
                    self.logger.error(f"Error in API request: {str(e)}")
                    break
        
        # Save results
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = os.path.join(self.output_dir, f"api_data_{timestamp}.json")
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"Saved {len(all_results)} API items to {output_path}")
        return all_results
```

## Advanced Data Cleaning and Preprocessing

Effective data cleaning transforms raw data into high-quality training material for LLMs:

```python
import pandas as pd
import numpy as np
import re
import spacy
import langdetect
from langdetect.lang_detect_exception import LangDetectException
from bs4 import BeautifulSoup
import hashlib
from typing import List, Dict, Union, Optional, Tuple, Set, Any
from multiprocessing import Pool, cpu_count
from tqdm import tqdm
import json
import os
from pathlib import Path

class DataCleaner:
    """Advanced data cleaning and preprocessing system for LLM training data."""
    
    def __init__(
        self,
        language: str = "en",
        min_text_length: int = 50,
        max_text_length: int = 100000,
        remove_duplicates: bool = True,
        detect_language: bool = True,
        normalize_whitespace: bool = True
    ):
        """
        Initialize the data cleaner with configuration parameters.
        
        Args:
            language: Target language code
            min_text_length: Minimum text length to keep
            max_text_length: Maximum text length to keep
            remove_duplicates: Whether to deduplicate content
            detect_language: Whether to filter by language
            normalize_whitespace: Whether to normalize whitespace
        """
        self.language = language
        self.min_text_length = min_text_length
        self.max_text_length = max_text_length
        self.remove_duplicates = remove_duplicates
        self.detect_language = detect_language
        self.normalize_whitespace = normalize_whitespace
        
        # Initialize NLP pipeline for advanced text processing
        try:
            self.nlp = spacy.load(f"{language}_core_web_sm", disable=["parser", "ner"])
        except:
            # Fallback to small model or download
            print(f"SpaCy model for {language} not found. Using small model.")
            try:
                self.nlp = spacy.load(f"{language}_core_web_sm", disable=["parser", "ner"])
            except:
                import subprocess
                subprocess.call(f"python -m spacy download {language}_core_web_sm", shell=True)
                self.nlp = spacy.load(f"{language}_core_web_sm", disable=["parser", "ner"])
    
    def clean_text(self, text: str) -> str:
        """
        Clean a single text string.
        
        Args:
            text: Input text string
            
        Returns:
            Cleaned text string
        """
        if not text:
            return ""
        
        # Remove HTML tags
        text = BeautifulSoup(text, "html.parser").get_text()
        
        # Normalize whitespace
        if self.normalize_whitespace:
            text = re.sub(r'\s+', ' ', text).strip()
        
        # Remove excessive punctuation
        text = re.sub(r'([.,!?;:])\1+', r'\1', text)
        
        # Normalize Unicode
        text = text.replace('\u2019', "'").replace('\u2018', "'")
        text = text.replace('\u201c', '"').replace('\u201d', '"')
        text = text.replace('\u2014', '-').replace('\u2013', '-')
        
        # Remove control characters
        text = ''.join(ch for ch in text if ch.isprintable() or ch in ['\n', '\t'])
        
        return text
    
    def detect_text_language(self, text: str) -> Optional[str]:
        """
        Detect the language of a text string.
        
        Args:
            text: Input text string
            
        Returns:
            Language code or None if detection failed
        """
        try:
            return langdetect.detect(text[:1000])
        except LangDetectException:
            return None
    
    def compute_text_hash(self, text: str) -> str:
        """
        Compute a hash for text deduplication.
        
        Args:
            text: Input text
            
        Returns:
            Hash string
        """
        # Normalize text before hashing to catch near-duplicates
        normalized = re.sub(r'\s+', ' ', text.lower().strip())
        normalized = re.sub(r'[^\w\s]', '', normalized)
        
        return hashlib.md5(normalized.encode('utf-8')).hexdigest()
    
    def clean_dataframe(self, df: pd.DataFrame, text_column: str) -> pd.DataFrame:
        """
        Clean text data in a DataFrame.
        
        Args:
            df: Input DataFrame
            text_column: Column containing text data
            
        Returns:
            Cleaned DataFrame
        """
        # Make a copy to avoid modifying original
        df_clean = df.copy()
        
        # Apply text cleaning
        df_clean[text_column] = df_clean[text_column].astype(str).apply(self.clean_text)
        
        # Filter by text length
        df_clean = df_clean[df_clean[text_column].str.len() >= self.min_text_length]
        df_clean = df_clean[df_clean[text_column].str.len() <= self.max_text_length]
        
        # Filter by language if enabled
        if self.detect_language:
            df_clean['detected_language'] = df_clean[text_column].apply(self.detect_text_language)
            df_clean = df_clean[df_clean['detected_language'] == self.language]
            df_clean = df_clean.drop(columns=['detected_language'])
        
        # Remove duplicates if enabled
        if self.remove_duplicates:
            df_clean['content_hash'] = df_clean[text_column].apply(self.compute_text_hash)
            df_clean = df_clean.drop_duplicates(subset=['content_hash'])
            df_clean = df_clean.drop(columns=['content_hash'])
        
        return df_clean
    
    def process_large_dataset(
        self,
        input_path: str,
        output_path: str,
        text_column: str,
        chunk_size: int = 10000,
        n_workers: int = None
    ) -> None:
        """
        Process a large dataset in chunks with parallel processing.
        
        Args:
            input_path: Path to input file (CSV or JSONL)
            output_path: Path to save cleaned data
            text_column: Column containing text data
            chunk_size: Number of rows per chunk
            n_workers: Number of worker processes
        """
        n_workers = n_workers or max(1, cpu_count() - 1)
        
        # Determine file format
        file_ext = Path(input_path).suffix.lower()
        
        # Process in chunks
        if file_ext == '.csv':
            # Count total rows for progress tracking
            total_rows = sum(1 for _ in open(input_path, 'r', encoding='utf-8')) - 1
            
            # Process CSV in chunks
            chunks = pd.read_csv(input_path, chunksize=chunk_size, encoding='utf-8')
            with tqdm(total=total_rows) as pbar:
                first_chunk = True
                for i, chunk in enumerate(chunks):
                    # Clean the chunk
                    cleaned_chunk = self.clean_dataframe(chunk, text_column)
                    
                    # Write to output file
                    mode = 'w' if first_chunk else 'a'
                    header = first_chunk
                    cleaned_chunk.to_csv(output_path, mode=mode, header=header, index=False, encoding='utf-8')
                    
                    first_chunk = False
                    pbar.update(len(chunk))
        
        elif file_ext == '.jsonl':
            # Process JSONL with parallel workers
            with open(input_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            # Prepare data for parallel processing
            chunks = [lines[i:i + chunk_size] for i in range(0, len(lines), chunk_size)]
            
            # Define worker function
            def process_chunk(chunk_lines):
                processed = []
                for line in chunk_lines:
                    try:
                        data = json.loads(line)
                        if text_column in data:
                            # Clean text
                            cleaned_text = self.clean_text(data[text_column])
                            
                            # Check length
                            if len(cleaned_text) < self.min_text_length or len(cleaned_text) > self.max_text_length:
                                continue
                                
                            # Check language if enabled
                            if self.detect_language:
                                lang = self.detect_text_language(cleaned_text)
                                if lang != self.language:
                                    continue
                            
                            # Update data with cleaned text
                            data[text_column] = cleaned_text
                            processed.append(data)
                    except:
                        continue
                return processed
            
            # Process chunks in parallel
            with Pool(processes=n_workers) as pool:
                results = list(tqdm(pool.imap(process_chunk, chunks), total=len(chunks)))
            
            # Flatten results
            processed_data = [item for sublist in results for item in sublist]
            
            # Deduplicate if enabled
            if self.remove_duplicates:
                hashes = set()
                unique_data = []
                for data in processed_data:
                    content_hash = self.compute_text_hash(data[text_column])
                    if content_hash not in hashes:
                        hashes.add(content_hash)
                        unique_data.append(data)
                processed_data = unique_data
            
            # Save processed data
            with open(output_path, 'w', encoding='utf-8') as f:
                for data in processed_data:
                    f.write(json.dumps(data, ensure_ascii=False) + '\n')
        
        else:
            raise ValueError(f"Unsupported file format: {file_ext}")
```

## Domain-Specific Data Formatting

Creating specialized training data for domain adaptation requires structured formatting:

```python
import json
import os
import numpy as np
import pandas as pd
from typing import List, Dict, Any, Union, Optional, Tuple
from sklearn.model_selection import train_test_split
import random
from transformers import AutoTokenizer
from datasets import Dataset, DatasetDict
import torch
from tqdm.auto import tqdm

class DomainSpecificFormatter:
    """Format training data for domain-specific LLM training."""
    
    def __init__(
        self, 
        tokenizer_name: str = "gpt2",
        max_length: int = 1024,
        instruction_template: str = "### Instruction:\n{instruction}\n\n### Response:\n{response}"
    ):
        """
        Initialize formatter with configuration.
        
        Args:
            tokenizer_name: Base tokenizer to use
            max_length: Maximum sequence length
            instruction_template: Template for instruction-tuning format
        """
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        self.max_length = max_length
        self.instruction_template = instruction_template
        
        # Set padding token if not defined
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
    
    def format_for_causal_lm(self, texts: List[str], output_path: str) -> Dataset:
        """
        Format texts for causal language modeling (continued pretraining).
        
        Args:
            texts: List of text documents
            output_path: Path to save formatted dataset
            
        Returns:
            HuggingFace Dataset
        """
        # Concatenate texts with separator
        combined_text = self.tokenizer.eos_token.join(texts)
        
        # Tokenize
        encodings = self.tokenizer(combined_text, truncation=False, add_special_tokens=False)
        input_ids = encodings['input_ids']
        
        # Create examples of max_length with overlapping windows
        stride = self.max_length // 4  # 75% overlap
        examples = []
        
        for i in range(0, len(input_ids), stride):
            chunk = input_ids[i:i + self.max_length]
            if len(chunk) >= self.max_length // 2:  # Only keep chunks with at least 50% of max_length
                # Pad to max_length if necessary
                if len(chunk) < self.max_length:
                    chunk = chunk + [self.tokenizer.pad_token_id] * (self.max_length - len(chunk))
                examples.append({
                    'input_ids': chunk,
                    'labels': chunk.copy()  # For causal LM, labels are the same as inputs
                })
        
        # Create Dataset
        dataset = Dataset.from_dict({
            'input_ids': [example['input_ids'] for example in examples],
            'labels': [example['labels'] for example in examples],
        })
        
        # Save dataset
        dataset.save_to_disk(output_path)
        print(f"Saved {len(dataset)} examples to {output_path}")
        
        return dataset
    
    def format_instruction_dataset(
        self, 
        data: List[Dict[str, str]],
        output_dir: str,
        train_ratio: float = 0.9,
        seed: int = 42
    ) -> DatasetDict:
        """
        Format data for instruction fine-tuning.
        
        Args:
            data: List of dicts with 'instruction' and 'response' keys
            output_dir: Directory to save formatted dataset
            train_ratio: Ratio of data for training vs. validation
            seed: Random seed for reproducibility
            
        Returns:
            DatasetDict with train and validation splits
        """
        # Format data
        formatted_data = []
        for item in data:
            if 'instruction' in item and 'response' in item:
                # Apply template
                combined_text = self.instruction_template.format(
                    instruction=item['instruction'],
                    response=item['response']
                )
                
                # Tokenize
                encodings = self.tokenizer(
                    combined_text, 
                    truncation=True,
                    max_length=self.max_length, 
                    padding='max_length'
                )
                
                # For instruction tuning, we want to train on predicting the entire sequence
                example = {
                    'input_ids': encodings['input_ids'],
                    'attention_mask': encodings['attention_mask'],
                    'labels': encodings['input_ids'].copy(),
                    'raw_text': combined_text
                }
                
                # Create attention mask for response-only loss
                # Find the position where the response starts
                response_marker = "### Response:"
                response_start_idx = combined_text.find(response_marker) + len(response_marker)
                
                # Convert character position to token position (approximate)
                prefix = combined_text[:response_start_idx]
                prefix_tokens = len(self.tokenizer(prefix, add_special_tokens=False)['input_ids'])
                
                # Set labels to -100 (ignored) for non-response tokens
                example['labels'][:prefix_tokens] = [-100] * prefix_tokens
                
                formatted_data.append(example)
        
        # Split data
        random.seed(seed)
        random.shuffle(formatted_data)
        split_idx = int(len(formatted_data) * train_ratio)
        
        train_data = formatted_data[:split_idx]
        val_data = formatted_data[split_idx:]
        
        # Create datasets
        train_dataset = Dataset.from_dict({
            'input_ids': [item['input_ids'] for item in train_data],
            'attention_mask': [item['attention_mask'] for item in train_data],
            'labels': [item['labels'] for item in train_data],
            'raw_text': [item['raw_text'] for item in train_data]
        })
        
        val_dataset = Dataset.from_dict({
            'input_ids': [item['input_ids'] for item in val_data],
            'attention_mask': [item['attention_mask'] for item in val_data],
            'labels': [item['labels'] for item in val_data],
            'raw_text': [item['raw_text'] for item in val_data]
        })
        
        # Combine into DatasetDict
        dataset_dict = DatasetDict({
            'train': train_dataset,
            'validation': val_dataset
        })
        
        # Save dataset
        os.makedirs(output_dir, exist_ok=True)
        dataset_dict.save_to_disk(output_dir)
        
        print(f"Saved {len(train_dataset)} training examples and {len(val_dataset)} validation examples to {output_dir}")
        return dataset_dict
    
    def format_pairwise_preference_data(
        self,
        preference_data: List[Dict[str, Any]],
        output_dir: str
    ) -> DatasetDict:
        """
        Format data for preference-based RLHF training.
        
        Args:
            preference_data: List of dicts with 'prompt', 'chosen', and 'rejected' keys
            output_dir: Directory to save formatted dataset
            
        Returns:
            DatasetDict with train and validation splits
        """
        # Format data for pairwise preference model
        formatted_data = []
        
        for item in preference_data:
            if 'prompt' in item and 'chosen' in item and 'rejected' in item:
                # Format chosen completion
                chosen_text = f"{item['prompt']}{item['chosen']}"
                chosen_encodings = self.tokenizer(
                    chosen_text,
                    truncation=True,
                    max_length=self.max_length,
                    padding='max_length'
                )
                
                # Format rejected completion
                rejected_text = f"{item['prompt']}{item['rejected']}"
                rejected_encodings = self.tokenizer(
                    rejected_text,
                    truncation=True,
                    max_length=self.max_length,
                    padding='max_length'
                )
                
                # Find prompt end position
                prompt_encodings = self.tokenizer(
                    item['prompt'],
                    truncation=True,
                    max_length=self.max_length
                )
                prompt_length = len(prompt_encodings['input_ids'])
                
                example = {
                    'prompt': item['prompt'],
                    'chosen': item['chosen'],
                    'rejected': item['rejected'],
                    'chosen_input_ids': chosen_encodings['input_ids'],
                    'chosen_attention_mask': chosen_encodings['attention_mask'],
                    'rejected_input_ids': rejected_encodings['input_ids'],
                    'rejected_attention_mask': rejected_encodings['attention_mask'],
                    'prompt_length': prompt_length
                }
                
                formatted_data.append(example)
        
        # Split data
        train_data, val_data = train_test_split(formatted_data, test_size=0.1, random_state=42)
        
        # Create datasets
        train_dataset = Dataset.from_dict({
            'prompt': [item['prompt'] for item in train_data],
            'chosen': [item['chosen'] for item in train_data],
            'rejected': [item['rejected'] for item in train_data],
            'chosen_input_ids': [item['chosen_input_ids'] for item in train_data],
            'chosen_attention_mask': [item['chosen_attention_mask'] for item in train_data],
            'rejected_input_ids': [item['rejected_input_ids'] for item in train_data],
            'rejected_attention_mask': [item['rejected_attention_mask'] for item in train_data],
            'prompt_length': [item['prompt_length'] for item in train_data]
        })
        
        val_dataset = Dataset.from_dict({
            'prompt': [item['prompt'] for item in val_data],
            'chosen': [item['chosen'] for item in val_data],
            'rejected': [item['rejected'] for item in val_data],
            'chosen_input_ids': [item['chosen_input_ids'] for item in val_data],
            'chosen_attention_mask': [item['chosen_attention_mask'] for item in val_data],
            'rejected_input_ids': [item['rejected_input_ids'] for item in val_data],
            'rejected_attention_mask': [item['rejected_attention_mask'] for item in val_data],
            'prompt_length': [item['prompt_length'] for item in val_data]
        })
        
        # Combine into DatasetDict
        dataset_dict = DatasetDict({
            'train': train_dataset,
            'validation': val_dataset
        })
        
        # Save dataset
        os.makedirs(output_dir, exist_ok=True)
        dataset_dict.save_to_disk(output_dir)
        
        print(f"Saved {len(train_dataset)} training pairs and {len(val_dataset)} validation pairs to {output_dir}")
        return dataset_dict
```

## Data Optimization and Analysis

Analyzing and optimizing data for LLM performance:

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from typing import List, Dict, Any, Tuple, Optional
import json
from tqdm.auto import tqdm
import torch
from transformers import AutoTokenizer
from datasets import Dataset, load_from_disk
import random
import os

class DataOptimizer:
    """Analyze and optimize training data for LLM performance."""
    
    def __init__(
        self,
        tokenizer_name: str = "gpt2",
        vocab_size: int = None
    ):
        """
        Initialize data optimizer.
        
        Args:
            tokenizer_name: Base tokenizer to use
            vocab_size: Custom vocabulary size
        """
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        
        # Set vocab size
        if vocab_size:
            self.vocab_size = vocab_size
        else:
            self.vocab_size = len(self.tokenizer)
            
    def analyze_token_distribution(self, texts: List[str]) -> Dict[str, Any]:
        """
        Analyze token distribution in a corpus.
        
        Args:
            texts: List of text samples
            
        Returns:
            Dictionary with analysis results
        """
        all_tokens = []
        token_lengths = []
        
        # Tokenize all texts
        for text in tqdm(texts, desc="Tokenizing texts"):
            tokens = self.tokenizer.encode(text, add_special_tokens=False)
            all_tokens.extend(tokens)
            token_lengths.append(len(tokens))
        
        # Count tokens
        token_counts = Counter(all_tokens)
        total_tokens = len(all_tokens)
        unique_tokens = len(token_counts)
        
        # Get top and bottom tokens
        most_common = token_counts.most_common(50)
        most_common_decoded = [(self.tokenizer.decode([token]), count, count/total_tokens) 
                              for token, count in most_common]
        
        least_common = token_counts.most_common()[:-51:-1]
        least_common_decoded = [(self.tokenizer.decode([token]), count, count/total_tokens) 
                                for token, count in least_common]
        
        # Calculate statistics
        token_length_stats = {
            'mean': np.mean(token_lengths),
            'median': np.median(token_lengths),
            'min': np.min(token_lengths),
            'max': np.max(token_lengths),
            'std': np.std(token_lengths),
            'percentile_90': np.percentile(token_lengths, 90),
            'percentile_99': np.percentile(token_lengths, 99)
        }
        
        # Calculate coverage
        sorted_counts = sorted(token_counts.values(), reverse=True)
        cumulative_counts = np.cumsum(sorted_counts)
        coverage_50 = np.searchsorted(cumulative_counts, total_tokens * 0.5) + 1
        coverage_90 = np.searchsorted(cumulative_counts, total_tokens * 0.9) + 1
        coverage_99 = np.searchsorted(cumulative_counts, total_tokens * 0.99) + 1
        
        return {
            'total_tokens': total_tokens,
            'unique_tokens': unique_tokens,
            'vocabulary_coverage': unique_tokens / self.vocab_size,
            'top_tokens': most_common_decoded,
            'rare_tokens': least_common_decoded,
            'token_length_stats': token_length_stats,
            'tokens_for_50_percent_coverage': coverage_50,
            'tokens_for_90_percent_coverage': coverage_90,
            'tokens_for_99_percent_coverage': coverage_99
        }
    
    def visualize_token_distribution(self, analysis_results: Dict[str, Any], output_path: str = None) -> None:
        """
        Visualize token distribution analysis.
        
        Args:
            analysis_results: Results from analyze_token_distribution
            output_path: Path to save visualizations
        """
        plt.figure(figsize=(14, 10))
        
        # Plot top tokens frequency
        plt.subplot(2, 2, 1)
        top_tokens = analysis_results['top_tokens']
        labels = [token for token, _, _ in top_tokens[:20]]
        frequencies = [freq for _, _, freq in top_tokens[:20]]
        
        bars = plt.bar(range(len(labels)), frequencies)
        plt.xticks(range(len(labels)), labels, rotation=90)
        plt.title('Top 20 Token Frequencies')
        plt.ylabel('Frequency')
        
        # Add percentage labels
        for i, bar in enumerate(bars):
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height + 0.001,
                    f'{frequencies[i]*100:.1f}%',
                    ha='center', va='bottom', rotation=90)
        
        # Plot token length distribution
        plt.subplot(2, 2, 2)
        token_stats = analysis_results['token_length_stats']
        plt.axvline(x=token_stats['mean'], color='r', linestyle='-', label=f"Mean: {token_stats['mean']:.1f}")
        plt.axvline(x=token_stats['median'], color='g', linestyle='--', label=f"Median: {token_stats['median']:.1f}")
        plt.axvline(x=token_stats['percentile_90'], color='b', linestyle=':', label=f"90%: {token_stats['percentile_90']:.1f}")
        plt.title('Token Length Distribution Statistics')
        plt.xlabel('Token Count')
        plt.legend()
        
        # Plot vocabulary coverage
        plt.subplot(2, 2, 3)
        plt.bar(['50%', '90%', '99%'], 
                [analysis_results['tokens_for_50_percent_coverage'],
                 analysis_results['tokens_for_90_percent_coverage'],
                 analysis_results['tokens_for_99_percent_coverage']])
        plt.title('Tokens Needed for Coverage')
        plt.ylabel('Number of Unique Tokens')
        
        # Plot overall stats
        plt.subplot(2, 2, 4)
        plt.axis('off')
        stats_text = f"Total Tokens: {analysis_results['total_tokens']:,}\n"
        stats_text += f"Unique Tokens: {analysis_results['unique_tokens']:,}\n"
        stats_text += f"Vocabulary Coverage: {analysis_results['vocabulary_coverage']*100:.2f}%\n"
        stats_text += f"Mean Length: {analysis_results['token_length_stats']['mean']:.2f} tokens\n"
        stats_text += f"Max Length: {analysis_results['token_length_stats']['max']} tokens\n"
        stats_text += f"50% Coverage: {analysis_results['tokens_for_50_percent_coverage']:,} tokens\n"
        stats_text += f"90% Coverage: {analysis_results['tokens_for_90_percent_coverage']:,} tokens"
        
        plt.text(0.5, 0.5, stats_text, ha='center', va='center', fontsize=12)
        
        plt.tight_layout()
        
        if output_path:
            plt.savefig(output_path)
            print(f"Visualization saved to {output_path}")
        
        plt.show()
    
    def optimize_sequence_length(
        self,
        dataset_path: str,
        target_lengths: List[int] = [128, 256, 512, 1024, 2048]
    ) -> Dict[str, Any]:
        """
        Analyze optimal sequence length for training.
        
        Args:
            dataset_path: Path to HuggingFace dataset
            target_lengths: List of sequence lengths to analyze
            
        Returns:
            Dictionary with analysis results
        """
        # Load dataset
        dataset = load_from_disk(dataset_path)
        
        # Analyze different sequence lengths
        results = {}
        
        for length in target_lengths:
            # Get sample of data
            sample_size = min(1000, len(dataset['train']))
            sample_indices = random.sample(range(len(dataset['train'])), sample_size)
            
            # Calculate truncation percentage
            truncated = 0
            total_tokens = 0
            wasted_padding = 0
            
            for idx in sample_indices:
                example = dataset['train'][idx]
                if 'input_ids' in example:
                    # Remove padding tokens
                    input_ids = [token for token in example['input_ids'] if token != self.tokenizer.pad_token_id]
                    original_length = len(input_ids)
                    
                    # Calculate metrics
                    total_tokens += original_length
                    
                    if original_length > length:
                        truncated += original_length - length
                    else:
                        wasted_padding += length - original_length
            
            # Calculate statistics
            truncation_percentage = truncated / total_tokens if total_tokens > 0 else 0
            padding_percentage = wasted_padding / (length * sample_size) if length * sample_size > 0 else 0
            
            # Estimate efficiency score (higher is better)
            efficiency = 1 - (truncation_percentage + padding_percentage)
            
            results[length] = {
                'truncation_percentage': truncation_percentage,
                'padding_percentage': padding_percentage,
                'efficiency_score': efficiency
            }
        
        # Find optimal length
        optimal_length = max(results.items(), key=lambda x: x[1]['efficiency_score'])[0]
        
        return {
            'length_analysis': results,
            'optimal_length': optimal_length,
            'recommendation': f"Recommended sequence length: {optimal_length}"
        }
    
    def balance_instruction_dataset(
        self,
        input_path: str,
        output_path: str,
        balance_method: str = 'undersample',
        target_count: Optional[int] = None
    ) -> None:
        """
        Balance an instruction dataset by category or task type.
        
        Args:
            input_path: Path to input JSONL file
            output_path: Path to save balanced dataset
            balance_method: 'undersample', 'oversample', or 'weighted'
            target_count: Target count per category (None for auto)
        """
        # Load data
        data = []
        with open(input_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    item = json.loads(line)
                    data.append(item)
                except:
                    continue
        
        # Extract categories
        categories = {}
        for item in data:
            category = item.get('category', 'unknown')
            if category not in categories:
                categories[category] = []
            categories[category].append(item)
        
        # Determine target count
        if not target_count:
            if balance_method == 'undersample':
                # Use size of smallest category
                target_count = min(len(items) for items in categories.values())
            elif balance_method == 'oversample':
                # Use size of largest category
                target_count = max(len(items) for items in categories.values())
            else:  # weighted
                # Use median size
                target_count = int(np.median([len(items) for items in categories.values()]))
        
        # Balance dataset
        balanced_data = []
        
        for category, items in categories.items():
            if balance_method == 'undersample':
                # Randomly select target_count items
                if len(items) > target_count:
                    sampled_items = random.sample(items, target_count)
                else:
                    sampled_items = items
            
            elif balance_method == 'oversample':
                # Oversample to target_count
                if len(items) < target_count:
                    # Original items plus random duplicates
                    duplicates_needed = target_count - len(items)
                    duplicates = random.choices(items, k=duplicates_needed)
                    sampled_items = items + duplicates
                else:
                    sampled_items = items
            
            else:  # weighted
                # Weight items to target_count
                if len(items) > target_count:
                    sampled_items = random.sample(items, target_count)
                else:
                    sampled_items = items
            
            balanced_data.extend(sampled_items)
        
        # Shuffle balanced dataset
        random.shuffle(balanced_data)
        
        # Save balanced dataset
        with open(output_path, 'w', encoding='utf-8') as f:
            for item in balanced_data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        
        print(f"Saved balanced dataset with {len(balanced_data)} examples to {output_path}")
        
        # Print category distribution
        print("\nCategory distribution:")
        total = len(balanced_data)
        category_counts = Counter(item.get('category', 'unknown') for item in balanced_data)
        for category, count in category_counts.most_common():
            print(f"  {category}: {count} ({count/total*100:.1f}%)")
```

## Practical Exercise: Comprehensive Data Preparation Pipeline

```python
def llm_data_preparation_pipeline():
    """
    End-to-end pipeline for preparing data for LLM training.
    """
    import os
    from dotenv import load_dotenv
    import json
    from datasets import load_dataset
    
    # Load environment variables
    load_dotenv()
    
    # Create output directories
    os.makedirs("data_pipeline", exist_ok=True)
    os.makedirs("data_pipeline/raw", exist_ok=True)
    os.makedirs("data_pipeline/cleaned", exist_ok=True)
    os.makedirs("data_pipeline/processed", exist_ok=True)
    os.makedirs("data_pipeline/formatted", exist_ok=True)
    
    print("Step 1: Data Collection - Downloading example dataset")
    
    # Use HuggingFace dataset as our data source
    dataset = load_dataset("databricks/databricks-dolly-15k", split="train")
    
    # Save raw data
    raw_path = "data_pipeline/raw/dolly_data.jsonl"
    with open(raw_path, 'w', encoding='utf-8') as f:
        for item in dataset:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    print(f"Saved {len(dataset)} raw examples to {raw_path}")
    
    print("\nStep 2: Data Analysis - Examining the dataset")
    
    # Analyze categories
    categories = {}
    for item in dataset:
        category = item.get('category', 'unknown')
        categories[category] = categories.get(category, 0) + 1
    
    print("Category distribution:")
    total = len(dataset)
    for category, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
        print(f"  {category}: {count} ({count/total*100:.1f}%)")
    
    print("\nStep 3: Data Cleaning - Cleaning and preprocessing")
    
    # Initialize data cleaner
    cleaner = DataCleaner(
        language="en",
        min_text_length=10,  # Allow shorter examples for instruction data
        max_text_length=4096,
        remove_duplicates=True,
        detect_language=True,
        normalize_whitespace=True
    )
    
    # Clean the data
    cleaned_data = []
    for item in tqdm(dataset, desc="Cleaning data"):
        # Clean instruction and response
        cleaned_instruction = cleaner.clean_text(item['instruction'])
        cleaned_response = cleaner.clean_text(item['response'])
        
        # Skip if either instruction or response is too short after cleaning
        if len(cleaned_instruction) < 10 or len(cleaned_response) < 10:
            continue
            
        # Create cleaned item
        cleaned_item = {
            'instruction': cleaned_instruction,
            'response': cleaned_response,
            'category': item.get('category', 'unknown'),
            'context': cleaner.clean_text(item.get('context', '')) if item.get('context') else None
        }
        
        cleaned_data.append(cleaned_item)
    
    # Save cleaned data
    cleaned_path = "data_pipeline/cleaned/dolly_cleaned.jsonl"
    with open(cleaned_path, 'w', encoding='utf-8') as f:
        for item in cleaned_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    print(f"Saved {len(cleaned_data)} cleaned examples to {cleaned_path}")
    
    print("\nStep 4: Dataset Balancing - Ensuring category balance")
    
    # Initialize data optimizer
    optimizer = DataOptimizer(tokenizer_name="gpt2")
    
    # Balance the dataset
    optimizer.balance_instruction_dataset(
        input_path=cleaned_path,
        output_path="data_pipeline/processed/dolly_balanced.jsonl",
        balance_method="undersample",
        target_count=None  # Auto-determine based on smallest category
    )
    
    # Load balanced data for formatting
    balanced_data = []
    with open("data_pipeline/processed/dolly_balanced.jsonl", 'r', encoding='utf-8') as f:
        for line in f:
            balanced_data.append(json.loads(line))
    
    print("\nStep 5: Data Formatting - Preparing for model training")
    
    # Initialize formatter
    formatter = DomainSpecificFormatter(
        tokenizer_name="gpt2",
        max_length=1024,
        instruction_template="### Instruction:\n{instruction}\n\n### Response:\n{response}"
    )
    
    # Format for instruction tuning
    instruction_dataset = formatter.format_instruction_dataset(
        data=balanced_data,
        output_dir="data_pipeline/formatted/instruction_dataset",
        train_ratio=0.9,
        seed=42
    )
    
    print("\nStep 6: Final Analysis - Analyzing tokenized dataset")
    
    # Get token length distribution from formatted dataset
    token_lengths = []
    for split in ['train', 'validation']:
        for example in instruction_dataset[split]:
            # Count non-padding tokens
            length = sum(1 for token in example['input_ids'] if token != formatter.tokenizer.pad_token_id)
            token_lengths.append(length)
    
    # Calculate statistics
    stats = {
        'mean': np.mean(token_lengths),
        'median': np.median(token_lengths),
        'min': np.min(token_lengths),
        'max': np.max(token_lengths),
        'std': np.std(token_lengths),
        'p90': np.percentile(token_lengths, 90),
        'p99': np.percentile(token_lengths, 99)
    }
    
    print("Token length statistics for formatted dataset:")
    for key, value in stats.items():
        print(f"  {key}: {value:.1f}")
    
    print("\nData preparation pipeline complete!")
    print(f"Final dataset saved to data_pipeline/formatted/instruction_dataset")
    print(f"  - Train set: {len(instruction_dataset['train'])} examples")
    print(f"  - Validation set: {len(instruction_dataset['validation'])} examples")
    
    return {
        'raw_data_path': raw_path,
        'cleaned_data_path': cleaned_path,
        'processed_data_path': "data_pipeline/processed/dolly_balanced.jsonl",
        'formatted_data_path': "data_pipeline/formatted/instruction_dataset",
        'train_size': len(instruction_dataset['train']),
        'validation_size': len(instruction_dataset['validation']),
        'token_stats': stats
    }

# Run the pipeline
if __name__ == "__main__":
    results = llm_data_preparation_pipeline()
```

## Conclusion

Effective data preparation forms the foundation of successful LLM training and adaptation. The techniques explored in this section demonstrate the critical importance of systematic data collection, thorough cleaning processes, and strategic formatting for optimal model performance. By implementing advanced pipelines for data management, developers can significantly improve model quality and domain-specific capabilities.

The comprehensive data preparation workflow we've examined—from collection through cleaning to formatting—enables practitioners to create high-quality training sets suitable for modern language models. Particular attention to domain adaptation ensures that models can be effectively specialized for specific applications without compromising their general capabilities.

Proper data structuring and optimization techniques are essential for extracting maximum value from training data, especially when working with limited computational resources. The balance between dataset size, sequence length, and distribution across categories can dramatically affect training efficiency and model performance.

As language models continue to advance, the importance of meticulous data preparation only increases. Building robust, reproducible data pipelines with appropriate cleaning, validation, and optimization steps is essential for developing reliable AI systems that can perform consistently across a range of domains and applications. Investment in data quality at the preparation stage pays dividends in model performance, reducing the need for extensive retraining and fine-tuning later in the development cycle.