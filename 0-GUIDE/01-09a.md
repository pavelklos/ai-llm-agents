<small>Claude 3.7 Sonnet Thinking</small>
# 09. Introduction to Reinforcement Learning

## Key Terms

- **Reinforcement Learning (RL)**: A machine learning paradigm where an agent learns to make decisions by taking actions in an environment to maximize cumulative reward.
- **State**: The representation of the environment as perceived by the agent at a specific time.
- **Action**: A decision made by the agent that affects the environment.
- **Reward**: A scalar feedback signal indicating the quality of the agent's action.
- **Policy**: A strategy that maps states to actions, determining the agent's behavior.
- **Value Function**: An estimate of expected future reward, guiding the agent's decision-making.
- **Q-Function**: A specific value function that estimates the expected reward for taking a particular action in a given state.
- **Markov Decision Process (MDP)**: A mathematical framework for modeling decision-making with states, actions, and rewards.
- **Environment**: The external system with which the agent interacts.
- **Exploration vs. Exploitation**: The tradeoff between discovering new actions (exploration) and maximizing reward using known effective actions (exploitation).

## RL Algorithms: Q-learning, Actor-Critic, and Policy-Based Approaches

### Q-Learning Implementation

Q-learning is a value-based, model-free RL algorithm that learns the quality of actions in particular states:

```python
import numpy as np
import gymnasium as gym
import matplotlib.pyplot as plt
from collections import defaultdict
from typing import Dict, Tuple, List
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

class QLearningAgent:
    """
    Q-Learning implementation with experience replay and adaptive learning rate
    """
    
    def __init__(
        self, 
        state_space_size: int, 
        action_space_size: int,
        learning_rate: float = 0.1,
        discount_factor: float = 0.99,
        exploration_rate: float = 1.0,
        exploration_decay: float = 0.995,
        min_exploration_rate: float = 0.01,
        replay_buffer_size: int = 10000
    ):
        # Initialize Q-table as a dictionary with default values
        self.q_table = defaultdict(lambda: np.zeros(action_space_size))
        
        # Agent hyperparameters
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor  # Gamma
        self.exploration_rate = exploration_rate  # Epsilon
        self.exploration_decay = exploration_decay
        self.min_exploration_rate = min_exploration_rate
        
        # Experience replay buffer
        self.replay_buffer = []
        self.replay_buffer_size = replay_buffer_size
        
        # For tracking learning progress
        self.training_errors = []
    
    def discretize_state(self, state) -> Tuple:
        """
        Convert continuous state to discrete for Q-table lookup
        Override this method for specific environments
        """
        return tuple(state)
    
    def select_action(self, state) -> int:
        """
        Select action using epsilon-greedy strategy
        """
        state_key = self.discretize_state(state)
        
        # Exploration: random action
        if np.random.random() < self.exploration_rate:
            return np.random.randint(0, len(self.q_table[state_key]))
        
        # Exploitation: best known action
        return np.argmax(self.q_table[state_key])
    
    def update(self, state, action, reward, next_state, done) -> float:
        """
        Update Q-value for state-action pair using Q-learning update rule
        """
        state_key = self.discretize_state(state)
        next_state_key = self.discretize_state(next_state)
        
        # Store experience in replay buffer
        self.store_experience(state_key, action, reward, next_state_key, done)
        
        # Standard Q-learning update
        current_q = self.q_table[state_key][action]
        
        # Calculate TD target
        if done:
            td_target = reward
        else:
            td_target = reward + self.discount_factor * np.max(self.q_table[next_state_key])
        
        # Calculate TD error
        td_error = td_target - current_q
        
        # Update Q-value
        self.q_table[state_key][action] += self.learning_rate * td_error
        
        # Decay exploration rate
        if self.exploration_rate > self.min_exploration_rate:
            self.exploration_rate *= self.exploration_decay
        
        # Track errors for convergence monitoring
        self.training_errors.append(abs(td_error))
        
        # Perform experience replay (mini-batch updates)
        if len(self.replay_buffer) >= 64:  # Mini-batch size
            self._replay_experiences()
            
        return td_error
    
    def store_experience(self, state, action, reward, next_state, done):
        """
        Store experience in replay buffer
        """
        self.replay_buffer.append((state, action, reward, next_state, done))
        
        # Limit buffer size
        if len(self.replay_buffer) > self.replay_buffer_size:
            self.replay_buffer.pop(0)
    
    def _replay_experiences(self):
        """
        Perform experience replay to break correlations in observation sequences
        """
        # Sample mini-batch from replay buffer
        mini_batch = np.random.choice(len(self.replay_buffer), 64, replace=False)
        
        for idx in mini_batch:
            state, action, reward, next_state, done = self.replay_buffer[idx]
            
            # Update Q-values using the same rule
            current_q = self.q_table[state][action]
            
            if done:
                td_target = reward
            else:
                td_target = reward + self.discount_factor * np.max(self.q_table[next_state])
                
            self.q_table[state][action] += self.learning_rate * (td_target - current_q)
    
    def save_model(self, filepath: str):
        """
        Save the Q-table to a file
        """
        # Convert defaultdict to regular dict for saving
        q_dict = {str(k): v.tolist() for k, v in self.q_table.items() if not all(v == 0)}
        
        with open(filepath, 'w') as f:
            import json
            json.dump(q_dict, f)
    
    def load_model(self, filepath: str):
        """
        Load the Q-table from a file
        """
        with open(filepath, 'r') as f:
            import json
            q_dict = json.load(f)
            
        # Convert back to defaultdict
        for k, v in q_dict.items():
            # Convert string representation of tuple back to actual tuple
            tuple_key = eval(k)
            self.q_table[tuple_key] = np.array(v)

# Example usage
def train_cartpole():
    env = gym.make("CartPole-v1")
    
    # Create agent
    agent = QLearningAgent(
        state_space_size=env.observation_space.shape[0],
        action_space_size=env.action_space.n,
        learning_rate=0.05,
        discount_factor=0.99,
        exploration_rate=1.0,
        exploration_decay=0.995
    )
    
    # Training parameters
    episodes = 1000
    max_steps = 500
    
    # Track progress
    scores = []
    
    # Training loop
    for episode in range(episodes):
        state, _ = env.reset()
        episode_reward = 0
        
        for step in range(max_steps):
            # Select action
            action = agent.select_action(state)
            
            # Take action
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            # Update Q-values
            agent.update(state, action, reward, next_state, done)
            
            # Update state and reward
            state = next_state
            episode_reward += reward
            
            if done:
                break
        
        # Track scores
        scores.append(episode_reward)
        
        # Print progress periodically
        if (episode + 1) % 100 == 0:
            avg_score = np.mean(scores[-100:])
            print(f"Episode: {episode+1}, Avg Score (last 100): {avg_score:.2f}, Exploration: {agent.exploration_rate:.4f}")
    
    # Save the trained model
    agent.save_model("cartpole_q_model.json")
    
    # Plot learning curve
    plt.figure(figsize=(10, 6))
    plt.plot(scores)
    plt.xlabel('Episodes')
    plt.ylabel('Score')
    plt.title('Q-Learning Training Progress')
    plt.savefig('q_learning_scores.png')
    
    return agent, scores

if __name__ == "__main__":
    agent, scores = train_cartpole()
```

### Actor-Critic Implementation

Actor-Critic methods combine value-based and policy-based approaches by using two networks: one for policy (actor) and one for value estimation (critic):

```python
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical
import gymnasium as gym
from typing import Tuple, List
import matplotlib.pyplot as plt
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

class ActorCriticNetwork(nn.Module):
    """
    Combined Actor-Critic Network with shared feature extraction
    """
    
    def __init__(self, input_dim: int, n_actions: int, hidden_dim: int = 128):
        super(ActorCriticNetwork, self).__init__()
        
        # Shared feature extraction layers
        self.shared_layers = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        # Actor head (policy network)
        self.actor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, n_actions)
        )
        
        # Critic head (value network)
        self.critic = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1)
        )
    
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Forward pass through the network
        
        Args:
            x: Input state tensor
            
        Returns:
            action_probs: Action probability distribution
            state_value: Value estimate for input state
        """
        # Convert numpy array to tensor if necessary
        if isinstance(x, np.ndarray):
            x = torch.from_numpy(x).float()
            
        # Forward pass through shared layers
        features = self.shared_layers(x)
        
        # Get action probabilities (actor output)
        action_logits = self.actor(features)
        action_probs = F.softmax(action_logits, dim=-1)
        
        # Get state value (critic output)
        state_value = self.critic(features)
        
        return action_probs, state_value

class ActorCriticAgent:
    """
    Actor-Critic Agent implementation
    """
    
    def __init__(
        self, 
        state_dim: int,
        n_actions: int,
        hidden_dim: int = 128,
        learning_rate: float = 0.001,
        gamma: float = 0.99
    ):
        self.gamma = gamma  # Discount factor
        
        # Initialize the Actor-Critic network
        self.network = ActorCriticNetwork(state_dim, n_actions, hidden_dim)
        self.optimizer = optim.Adam(self.network.parameters(), lr=learning_rate)
        
        # For storing episode data
        self.log_probs = []
        self.values = []
        self.rewards = []
        self.entropies = []
        
        # For tracking training progress
        self.episode_rewards = []
        self.running_reward = 0
        self.best_running_reward = 0
    
    def select_action(self, state) -> Tuple[int, float]:
        """
        Select an action according to the policy
        """
        # Get action probabilities and state value
        action_probs, state_value = self.network(state)
        
        # Create a categorical distribution over action probabilities
        dist = Categorical(action_probs)
        
        # Sample an action
        action = dist.sample()
        
        # Calculate log probability of the action
        log_prob = dist.log_prob(action)
        
        # Calculate entropy for encouraging exploration
        entropy = dist.entropy()
        
        # Store for training
        self.log_probs.append(log_prob)
        self.values.append(state_value)
        self.entropies.append(entropy)
        
        return action.item(), state_value.item()
    
    def update(self, next_state=None, done=False):
        """
        Update the network parameters
        """
        # If episode is not done, estimate next state value with critic
        if not done and next_state is not None:
            _, next_value = self.network(next_state)
            next_value = next_value.detach()
        else:
            next_value = 0
            
        # Calculate returns and advantages
        returns = []
        gae = 0  # Generalized Advantage Estimation
        
        # Process rewards in reverse order (from last step to first)
        for i in reversed(range(len(self.rewards))):
            # Calculate return using discounted rewards
            if i == len(self.rewards) - 1:
                # For the last step
                gae = self.rewards[i] + self.gamma * next_value - self.values[i].item()
                returns.insert(0, self.rewards[i] + self.gamma * next_value)
            else:
                # For all other steps
                delta = self.rewards[i] + self.gamma * self.values[i+1].item() - self.values[i].item()
                gae = delta + self.gamma * 0.95 * gae  # 0.95 is lambda in GAE
                returns.insert(0, gae + self.values[i].item())
                
        # Convert lists to tensors
        returns = torch.tensor(returns)
        log_probs = torch.stack(self.log_probs)
        values = torch.cat(self.values)
        entropies = torch.stack(self.entropies)
        
        # Calculate advantages
        advantages = returns - values.detach()
        
        # Normalize advantages to reduce variance
        if len(advantages) > 1:
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # Calculate actor (policy) loss
        actor_loss = -(log_probs * advantages.detach()).mean()
        
        # Calculate critic (value) loss
        critic_loss = F.mse_loss(values.squeeze(), returns)
        
        # Calculate entropy loss (to encourage exploration)
        entropy_loss = -0.01 * entropies.mean()  # 0.01 is entropy coefficient
        
        # Total loss
        loss = actor_loss + 0.5 * critic_loss + entropy_loss  # 0.5 is value coefficient
        
        # Update network
        self.optimizer.zero_grad()
        loss.backward()
        
        # Gradient clipping to prevent exploding gradients
        nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)
        
        self.optimizer.step()
        
        # Clear episode data
        self.log_probs = []
        self.values = []
        self.rewards = []
        self.entropies = []
        
        return loss.item()
    
    def save_model(self, filepath: str):
        """Save the model parameters"""
        torch.save(self.network.state_dict(), filepath)
    
    def load_model(self, filepath: str):
        """Load the model parameters"""
        self.network.load_state_dict(torch.load(filepath))

def train_actor_critic(env_name: str = "CartPole-v1", max_episodes: int = 1000):
    """Train an Actor-Critic agent on the specified environment"""
    env = gym.make(env_name)
    
    # Get environment dimensions
    state_dim = env.observation_space.shape[0]
    n_actions = env.action_space.n
    
    # Create agent
    agent = ActorCriticAgent(
        state_dim=state_dim,
        n_actions=n_actions,
        learning_rate=0.002
    )
    
    # Track scores for plotting
    all_scores = []
    avg_scores = []
    
    # Training loop
    for episode in range(max_episodes):
        state, _ = env.reset()
        episode_reward = 0
        done = False
        
        while not done:
            # Select action
            action, _ = agent.select_action(state)
            
            # Take action in environment
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            # Store reward
            agent.rewards.append(reward)
            episode_reward += reward
            
            # Update state
            state = next_state
            
            # Update if episode is done
            if done:
                loss = agent.update(done=True)
                break
                
            # Otherwise, prepare for next step
            loss = agent.update(next_state=next_state, done=False)
        
        # Track scores
        all_scores.append(episode_reward)
        
        # Update running reward
        agent.running_reward = 0.05 * episode_reward + 0.95 * agent.running_reward
        
        # Calculate average score over last 100 episodes
        if episode >= 100:
            avg_score = np.mean(all_scores[-100:])
            avg_scores.append(avg_score)
        else:
            avg_scores.append(np.mean(all_scores))
        
        # Print training progress
        if episode % 20 == 0:
            print(f"Episode {episode}, Score: {episode_reward}, Average Score: {avg_scores[-1]:.2f}")
        
        # Save best model
        if agent.running_reward > agent.best_running_reward:
            agent.best_running_reward = agent.running_reward
            agent.save_model(f"{env_name}_actor_critic_best.pth")
            
        # Early stopping if solved
        if avg_scores[-1] >= env.spec.reward_threshold:
            print(f"Environment solved in {episode} episodes!")
            agent.save_model(f"{env_name}_actor_critic_solved.pth")
            break
    
    # Save final model
    agent.save_model(f"{env_name}_actor_critic_final.pth")
    
    # Plot results
    plt.figure(figsize=(10, 6))
    plt.plot(all_scores, label='Episode Scores')
    plt.plot(avg_scores, label='Average Scores')
    plt.xlabel('Episode')
    plt.ylabel('Score')
    plt.title('Actor-Critic Training Performance')
    plt.legend()
    plt.savefig(f"{env_name}_actor_critic_performance.png")
    
    return agent, all_scores, avg_scores

if __name__ == "__main__":
    agent, scores, avg_scores = train_actor_critic(env_name="CartPole-v1")
```

### Policy-Based Implementation (PPO)

Proximal Policy Optimization (PPO) is a policy gradient method that has become one of the most popular RL algorithms due to its stability and performance:

```python
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import gymnasium as gym
from typing import List, Tuple, Dict
import matplotlib.pyplot as plt
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class Memory:
    """Memory buffer for storing trajectory data"""
    
    def __init__(self):
        self.states = []
        self.actions = []
        self.logprobs = []
        self.rewards = []
        self.is_terminals = []
    
    def clear_memory(self):
        """Clear all stored memory"""
        self.states = []
        self.actions = []
        self.logprobs = []
        self.rewards = []
        self.is_terminals = []

class ActorNetwork(nn.Module):
    """Actor network for PPO"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 64):
        super(ActorNetwork, self).__init__()
        
        self.actor = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, action_dim),
            nn.Softmax(dim=-1)
        )
    
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        """Forward pass through actor network"""
        return self.actor(state)
    
    def act(self, state: np.ndarray, memory: Memory) -> int:
        """Select action according to actor network"""
        state = torch.FloatTensor(state).to(device)
        action_probs = self.forward(state)
        dist = Categorical(action_probs)
        action = dist.sample()
        
        # Store the trajectory data for later update
        memory.states.append(state)
        memory.actions.append(action)
        memory.logprobs.append(dist.log_prob(action))
        
        return action.item()

class CriticNetwork(nn.Module):
    """Critic network for PPO"""
    
    def __init__(self, state_dim: int, hidden_dim: int = 64):
        super(CriticNetwork, self).__init__()
        
        self.critic = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 1)
        )
    
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        """Forward pass through critic network"""
        return self.critic(state)

class PPO:
    """
    Proximal Policy Optimization implementation
    """
    
    def __init__(
        self,
        state_dim: int,
        action_dim: int,
        lr_actor: float = 0.0003,
        lr_critic: float = 0.001,
        gamma: float = 0.99,
        K_epochs: int = 4,
        eps_clip: float = 0.2,
        hidden_dim: int = 64
    ):
        self.gamma = gamma
        self.eps_clip = eps_clip
        self.K_epochs = K_epochs
        
        # Initialize actor and critic networks
        self.actor = ActorNetwork(state_dim, action_dim, hidden_dim).to(device)
        self.critic = CriticNetwork(state_dim, hidden_dim).to(device)
        
        # Initialize optimizers
        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=lr_actor)
        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=lr_critic)
        
        # Initialize memory buffer
        self.memory = Memory()
        
        # Old actor network for calculating ratio
        self.actor_old = ActorNetwork(state_dim, action_dim, hidden_dim).to(device)
        self.actor_old.load_state_dict(self.actor.state_dict())
        
        # For tracking metrics
        self.episode_rewards = []
        self.avg_rewards = []
    
    def select_action(self, state: np.ndarray) -> int:
        """Select action using current actor network"""
        return self.actor_old.act(state, self.memory)
    
    def update(self):
        """Update actor and critic networks using PPO algorithm"""
        # Monte Carlo estimate of rewards
        rewards = []
        discounted_reward = 0
        for reward, is_terminal in zip(reversed(self.memory.rewards), reversed(self.memory.is_terminals)):
            if is_terminal:
                discounted_reward = 0
            discounted_reward = reward + self.gamma * discounted_reward
            rewards.insert(0, discounted_reward)
        
        # Normalize rewards to reduce variance
        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)
        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)
        
        # Convert list to tensors
        old_states = torch.stack(self.memory.states).to(device).detach()
        old_actions = torch.stack(self.memory.actions).to(device).detach()
        old_logprobs = torch.stack(self.memory.logprobs).to(device).detach()
        
        # Training loop
        for _ in range(self.K_epochs):
            # Evaluate old actions and values using current networks
            action_probs = self.actor(old_states)
            dist = Categorical(action_probs)
            logprobs = dist.log_prob(old_actions)
            values = self.critic(old_states)
            
            # Calculate ratios and surrogate loss
            ratios = torch.exp(logprobs - old_logprobs.detach())
            
            # Calculate advantages
            advantages = rewards - values.detach()
            
            # PPO surrogate loss terms
            surr1 = ratios * advantages
            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages
            
            # Actor loss
            actor_loss = -torch.min(surr1, surr2).mean()
            
            # Value loss
            value_loss = 0.5 * ((values - rewards) ** 2).mean()
            
            # Update actor
            self.optimizer_actor.zero_grad()
            actor_loss.backward(retain_graph=True)
            self.optimizer_actor.step()
            
            # Update critic
            self.optimizer_critic.zero_grad()
            value_loss.backward()
            self.optimizer_critic.step()
        
        # Copy new weights to old actor
        self.actor_old.load_state_dict(self.actor.state_dict())
        
        # Clear memory
        self.memory.clear_memory()
    
    def save_models(self, filepath: str):
        """Save actor and critic models"""
        torch.save({
            'actor': self.actor.state_dict(),
            'critic': self.critic.state_dict(),
            'actor_old': self.actor_old.state_dict()
        }, filepath)
    
    def load_models(self, filepath: str):
        """Load actor and critic models"""
        checkpoint = torch.load(filepath)
        self.actor.load_state_dict(checkpoint['actor'])
        self.critic.load_state_dict(checkpoint['critic'])
        self.actor_old.load_state_dict(checkpoint['actor_old'])

def train_ppo(env_name: str = "LunarLander-v2", max_episodes: int = 1000, max_timesteps: int = 1000):
    """Train a PPO agent on the specified environment"""
    env = gym.make(env_name)
    
    # Get environment dimensions
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n
    
    # Create PPO agent
    agent = PPO(state_dim, action_dim)
    
    # Track metrics
    all_rewards = []
    avg_rewards = []
    
    # Training loop
    for episode in range(max_episodes):
        state, _ = env.reset()
        episode_reward = 0
        
        for t in range(max_timesteps):
            # Select action
            action = agent.select_action(state)
            
            # Take action in environment
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            # Store in memory
            agent.memory.rewards.append(reward)
            agent.memory.is_terminals.append(done)
            
            # Update state and reward
            state = next_state
            episode_reward += reward
            
            # Update if we've reached batch size or episode is done
            if done or (t == max_timesteps - 1):
                agent.update()
                break
        
        # Track rewards
        all_rewards.append(episode_reward)
        
        # Calculate average reward over last 100 episodes
        if episode >= 100:
            avg_reward = np.mean(all_rewards[-100:])
            avg_rewards.append(avg_reward)
        else:
            avg_rewards.append(np.mean(all_rewards))
        
        # Print progress
        if (episode + 1) % 20 == 0:
            print(f"Episode: {episode+1}, Reward: {episode_reward}, Avg Reward: {avg_rewards[-1]:.2f}")
        
        # Save model periodically
        if (episode + 1) % 100 == 0:
            agent.save_models(f"{env_name}_ppo_episode_{episode+1}.pth")
            
        # Early stopping if solved
        if hasattr(env.spec, 'reward_threshold') and avg_rewards[-1] >= env.spec.reward_threshold:
            print(f"Environment solved in {episode+1} episodes!")
            agent.save_models(f"{env_name}_ppo_solved.pth")
            break
    
    # Save final model
    agent.save_models(f"{env_name}_ppo_final.pth")
    
    # Plot results
    plt.figure(figsize=(10, 6))
    plt.plot(all_rewards, label='Episode Rewards')
    plt.plot(avg_rewards, label='Average Rewards (100 episodes)')
    plt.xlabel('Episode')
    plt.ylabel('Reward')
    plt.title('PPO Training Performance')
    plt.legend()
    plt.savefig(f"{env_name}_ppo_performance.png")
    
    return agent, all_rewards, avg_rewards

if __name__ == "__main__":
    agent, rewards, avg_rewards = train_ppo()
```

## Working with Gymnasium and PettingZoo Environments

Gymnasium is the modern successor to OpenAI Gym, providing standardized environments for RL research, while PettingZoo extends this to multi-agent domains:

```python
import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
from pettingzoo.mpe import simple_spread_v3
from typing import List, Dict, Tuple
import time
from gymnasium.wrappers import RecordVideo

def explore_gymnasium_environment(env_name: str = "CartPole-v1"):
    """
    Explore and visualize a Gymnasium environment
    """
    # Create the environment
    env = gym.make(env_name, render_mode="human")
    
    # Print environment information
    print(f"Environment: {env_name}")
    print(f"Observation Space: {env.observation_space}")
    print(f"Action Space: {env.action_space}")
    
    if hasattr(env.spec, 'reward_threshold'):
        print(f"Reward Threshold: {env.spec.reward_threshold}")
    
    # Reset environment and get initial observation
    obs, info = env.reset()
    print(f"Initial Observation: {obs}")
    print(f"Info: {info}")
    
    # Run a few random actions
    total_steps = 0
    total_reward = 0
    done = False
    
    while not done and total_steps < 200:
        action = env.action_space.sample()  # Random action
        obs, reward, terminated, truncated, info = env.step(action)
        
        total_steps += 1
        total_reward += reward
        done = terminated or truncated
        
        print(f"Step {total_steps}: Action={action}, Reward={reward}")
        time.sleep(0.01)  # Slow down rendering
    
    env.close()
    
    print(f"Episode ended after {total_steps} steps with total reward {total_reward}")
    return total_steps, total_reward

def record_agent_video(env_name: str, agent, video_path: str = "./videos", 
                      episode_trigger=lambda x: x % 20 == 0):
    """
    Record video of an agent performing in an environment
    
    Args:
        env_name: Name of the environment
        agent: Agent with select_action method
        video_path: Path to save videos
        episode_trigger: Function to determine which episodes to record
    """
    # Create environment with recording wrapper
    env = gym.make(env_name, render_mode="rgb_array")
    env = RecordVideo(env, video_path, episode_trigger=episode_trigger)
    
    # Run episodes
    for episode in range(5):  # Record 5 episodes
        state, _ = env.reset()
        total_reward = 0
        done = False
        
        while not done:
            # Select action using agent's policy
            action = agent.select_action(state)  # This should be implemented in your agent class
            
            # Take action
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            # Update reward and state
            total_reward += reward
            state = next_state
        
        print(f"Episode {episode} finished with reward {total_reward}")
    
    env.close()

def explore_pettingzoo_environment():
    """
    Explore and visualize a PettingZoo multi-agent environment
    """
    # Create the environment
    env = simple_spread_v3.parallel_env(N=3, local_ratio=0.5, max_cycles=25, render_mode="human")
    
    # Print environment information
    print("Multi-Agent Environment: simple_spread_v3")
    print(f"Agents: {env.agents}")
    print(f"Observation Spaces: {env.observation_spaces}")
    print(f"Action Spaces: {env.action_spaces}")
    
    # Reset environment and get initial observations
    observations = env.reset()
    print("Initial Observations:")
    for agent, obs in observations.items():
        print(f"{agent}: {obs.shape}")
    
    # Run a few random actions
    total_steps = 0
    done = {agent: False for agent in env.agents}
    
    while not all(done.values()) and total_steps < 25:
        # Random actions for each agent
        actions = {agent: env.action_spaces[agent].sample() for agent in env.agents}
        
        # Step environment
        observations, rewards, terminations, truncations, infos = env.step(actions)
        
        # Update done flags
        done = {agent: terminations[agent] or truncations[agent] for agent in env.agents}
        
        total_steps += 1
        
        print(f"\nStep {total_steps}:")
        print(f"Actions: {actions}")
        print(f"Rewards: {rewards}")
        
        time.sleep(0.1)  # Slow down rendering
    
    env.close()
    print(f"Multi-agent episode ended after {total_steps} steps")

# Example usage
if __name__ == "__main__":
    # Explore Gymnasium environment
    explore_gymnasium_environment("CartPole-v1")
    
    # Explore PettingZoo environment
    # explore_pettingzoo_environment()  # Uncomment to run
    
    # For recording videos, you would pass in a trained agent
    # from q_learning_implementation import QLearningAgent
    # agent = QLearningAgent(state_space_size=4, action_space_size=2)
    # agent.load_model("cartpole_q_model.json")
    # record_agent_video("CartPole-v1", agent)
```

## Practical Exercise: Reinforcement Learning Agent for Flappy Bird

Let's implement an RL agent to play Flappy Bird using a Deep Q-Network (DQN) approach:

```python
import os
import numpy as np
import random
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import deque
import gymnasium as gym
import matplotlib.pyplot as plt
from dotenv import load_dotenv
import time
from typing import List, Tuple, Dict, Any
import cv2

# Load environment variables
load_dotenv()

# Try to load flappy bird environment
# Note: You need to install the flappy-bird-gymnasium package
# pip install flappy-bird-gymnasium
try:
    import flappy_bird_gymnasium
except ImportError:
    print("flappy-bird-gymnasium package not found. Installing...")
    import subprocess
    subprocess.check_call(["pip", "install", "flappy-bird-gymnasium"])
    import flappy_bird_gymnasium

# Set device to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class FlappyBirdPreprocessor:
    """Preprocess Flappy Bird observations for the DQN"""
    
    def __init__(self, height: int = 84, width: int = 84):
        self.height = height
        self.width = width
    
    def process_observation(self, observation: np.ndarray) -> np.ndarray:
        """
        Convert RGB observation to grayscale and resize
        """
        # Convert RGB to grayscale
        gray = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)
        
        # Resize to target dimensions
        resized = cv2.resize(gray, (self.width, self.height), interpolation=cv2.INTER_AREA)
        
        # Normalize pixel values
        normalized = resized / 255.0
        
        # Add channel dimension for CNN input
        processed = np.expand_dims(normalized, axis=0)
        
        return processed
    
    def stack_frames(self, frames: List[np.ndarray]) -> np.ndarray:
        """Stack multiple frames to capture motion"""
        stacked = np.concatenate(frames, axis=0)
        return stacked

class DQN(nn.Module):
    """Deep Q-Network for Flappy Bird"""
    
    def __init__(self, input_shape: Tuple[int, int, int], n_actions: int):
        super(DQN, self).__init__()
        
        # CNN layers
        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)
        
        # Calculate size after CNN layers
        def conv2d_size_out(size, kernel_size, stride):
            return (size - (kernel_size - 1) - 1) // stride + 1
        
        h = conv2d_size_out(conv2d_size_out(conv2d_size_out(input_shape[1], 8, 4), 4, 2), 3, 1)
        w = conv2d_size_out(conv2d_size_out(conv2d_size_out(input_shape[2], 8, 4), 4, 2), 3, 1)
        
        # Fully connected layers
        self.fc1 = nn.Linear(64 * h * w, 512)
        self.fc2 = nn.Linear(512, n_actions)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass through the network"""
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        
        # Flatten the tensor
        x = x.view(x.size(0), -1)
        
        # Fully connected layers
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        
        return x

class ReplayMemory:
    """Experience replay buffer for DQN"""
    
    def __init__(self, capacity: int = 100000):
        self.memory = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        """Add experience to memory"""
        self.memory.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size: int) -> List:
        """Sample random batch of experiences"""
        return random.sample(self.memory, batch_size)
    
    def __len__(self) -> int:
        return len(self.memory)

class FlappyBirdAgent:
    """DQN Agent for playing Flappy Bird"""
    
    def __init__(
        self,
        state_shape: Tuple[int, int, int],
        n_actions: int,
        learning_rate: float = 0.0001,
        gamma: float = 0.99,
        epsilon_start: float = 1.0,
        epsilon_final: float = 0.01,
        epsilon_decay: float = 100000,
        memory_size: int = 100000,
        batch_size: int = 32,
        target_update: int = 1000
    ):
        self.state_shape = state_shape
        self.n_actions = n_actions
        self.gamma = gamma  # Discount factor
        
        # Epsilon-greedy exploration parameters
        self.epsilon_start = epsilon_start
        self.epsilon_final = epsilon_final
        self.epsilon_decay = epsilon_decay
        self.epsilon = epsilon_start
        self.steps_done = 0
        
        # Replay memory
        self.memory = ReplayMemory(memory_size)
        self.batch_size = batch_size
        
        # Networks
        self.policy_net = DQN(state_shape, n_actions).to(device)
        self.target_net = DQN(state_shape, n_actions).to(device)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()  # Set target network to evaluation mode
        
        # Optimizer
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)
        
        # Target network update frequency
        self.target_update = target_update
        self.update_count = 0
        
        # For tracking performance
        self.scores = []
        self.avg_scores = []
    
    def select_action(self, state: np.ndarray) -> int:
        """Select action using epsilon-greedy policy"""
        # Decay epsilon
        self.epsilon = self.epsilon_final + (self.epsilon_start - self.epsilon_final) * \
                      np.exp(-1. * self.steps_done / self.epsilon_decay)
        self.steps_done += 1
        
        # Exploration: random action
        if random.random() < self.epsilon:
            return random.randint(0, self.n_actions - 1)
        
        # Exploitation: best action according to policy
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
            q_values = self.policy_net(state_tensor)
            return q_values.max(1)[1].item()
    
    def optimize_model(self):
        """Train the model using a batch from replay memory"""
        if len(self.memory) < self.batch_size:
            return
        
        # Sample batch
        transitions = self.memory.sample(self.batch_size)
        
        # Transpose batch (see https://stackoverflow.com/a/19343/3343043)
        batch = list(zip(*transitions))
        
        # Extract elements
        states = torch.FloatTensor(np.array(batch[0])).to(device)
        actions = torch.LongTensor(np.array(batch[1])).unsqueeze(1).to(device)
        rewards = torch.FloatTensor(np.array(batch[2])).unsqueeze(1).to(device)
        next_states = torch.FloatTensor(np.array(batch[3])).to(device)
        dones = torch.FloatTensor(np.array(batch[4], dtype=np.float32)).unsqueeze(1).to(device)
        
        # Compute Q values
        q_values = self.policy_net(states).gather(1, actions)
        
        # Compute expected Q values using target network
        with torch.no_grad():
            next_q_values = self.target_net(next_states).max(1)[0].unsqueeze(1)
            expected_q_values = rewards + (self.gamma * next_q_values * (1 - dones))
        
        # Compute loss (Huber loss for stability)
        loss = F.smooth_l1_loss(q_values, expected_q_values)
        
        # Optimize model
        self.optimizer.zero_grad()
        loss.backward()
        # Clip gradients for stability
        for param in self.policy_net.parameters():
            param.grad.data.clamp_(-1, 1)
        self.optimizer.step()
        
        # Update target network
        self.update_count += 1
        if self.update_count % self.target_update == 0:
            self.target_net.load_state_dict(self.policy_net.state_dict())
    
    def save_model(self, filepath: str):
        """Save model parameters"""
        torch.save({
            'policy_net': self.policy_net.state_dict(),
            'target_net': self.target_net.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'steps_done': self.steps_done,
            'epsilon': self.epsilon
        }, filepath)
    
    def load_model(self, filepath: str):
        """Load model parameters"""
        if os.path.isfile(filepath):
            checkpoint = torch.load(filepath)
            self.policy_net.load_state_dict(checkpoint['policy_net'])
            self.target_net.load_state_dict(checkpoint['target_net'])
            self.optimizer.load_state_dict(checkpoint['optimizer'])
            self.steps_done = checkpoint['steps_done']
            self.epsilon = checkpoint['epsilon']
            print(f"Loaded model from {filepath}")
        else:
            print(f"No model found at {filepath}")

def train_flappy_bird(episodes: int = 5000, frame_stack: int = 4, render: bool = False):
    """Train a DQN agent to play Flappy Bird"""
    # Create environment
    env = gym.make("FlappyBird-v0", render_mode="rgb_array" if not render else "human")
    
    # Preprocessor for observations
    preprocessor = FlappyBirdPreprocessor()
    
    # Create agent
    state_shape = (frame_stack, 84, 84)  # (stack_size, height, width)
    n_actions = env.action_space.n
    
    agent = FlappyBirdAgent(
        state_shape=state_shape,
        n_actions=n_actions,
        learning_rate=0.0001,
        gamma=0.99,
        epsilon_start=1.0,
        epsilon_final=0.01,
        epsilon_decay=100000,
        memory_size=100000,
        batch_size=32,
        target_update=1000
    )
    
    # Check if model exists and load it
    if os.path.exists("flappy_bird_dqn.pth"):
        agent.load_model("flappy_bird_dqn.pth")
    
    # For early stopping
    best_score = 0
    consecutive_solves = 0
    solved_criterion = 20  # Consider environment solved if score > 20 for consecutive episodes
    
    # For logging
    log_interval = 10
    
    # Training loop
    for episode in range(episodes):
        # Reset environment
        observation, info = env.reset()
        
        # Process first observation
        state = preprocessor.process_observation(observation)
        
        # Initialize frame stack
        frames = [state] * frame_stack
        stacked_state = preprocessor.stack_frames(frames)
        
        # Initialize episode variables
        done = False
        score = 0
        
        while not done:
            # Select action
            action = agent.select_action(stacked_state)
            
            # Take action
            observation, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            
            # Process new observation
            next_state = preprocessor.process_observation(observation)
            
            # Update frame stack
            frames.pop(0)
            frames.append(next_state)
            stacked_next_state = preprocessor.stack_frames(frames)
            
            # Store transition in memory
            agent.memory.push(stacked_state, action, reward, stacked_next_state, done)
            
            # Move to next state
            stacked_state = stacked_next_state
            
            # Update score
            score += reward
            
            # Train model
            agent.optimize_model()
        
        # Track scores
        agent.scores.append(score)
        avg_score = np.mean(agent.scores[-100:]) if len(agent.scores) >= 100 else np.mean(agent.scores)
        agent.avg_scores.append(avg_score)
        
        # Log progress
        if episode % log_interval == 0:
            print(f"Episode {episode}, Score: {score}, Avg Score: {avg_score:.2f}, Epsilon: {agent.epsilon:.4f}")
        
        # Early stopping check
        if score > best_score:
            best_score = score
        
        if score > 20:  # Arbitrary threshold for good performance
            consecutive_solves += 1
            if consecutive_solves >= 5:  # Require multiple consecutive good episodes
                print(f"Environment solved after {episode+1} episodes!")
                agent.save_model("flappy_bird_dqn_solved.pth")
                break
        else:
            consecutive_solves = 0
        
        # Save model periodically
        if episode % 100 == 0:
            agent.save_model(f"flappy_bird_dqn_ep{episode}.pth")
    
    # Save final model
    agent.save_model("flappy_bird_dqn.pth")
    
    # Plot learning curve
    plt.figure(figsize=(10, 6))
    plt.plot(agent.scores, alpha=0.4, label='Episode Score')
    plt.plot(agent.avg_scores, label='Average Score')
    plt.xlabel('Episode')
    plt.ylabel('Score')
    plt.title('DQN Training on Flappy Bird')
    plt.legend()
    plt.savefig('flappy_bird_learning_curve.png')
    
    return agent

def play_flappy_bird(agent=None, episodes: int = 10, render: bool = True, frame_stack: int = 4):
    """Play Flappy Bird using a trained agent"""
    # Create environment
    env = gym.make("FlappyBird-v0", render_mode="human" if render else "rgb_array")
    
    # Preprocessor for observations
    preprocessor = FlappyBirdPreprocessor()
    
    # Create agent if one wasn't provided
    if agent is None:
        state_shape = (frame_stack, 84, 84)
        n_actions = env.action_space.n
        
        agent = FlappyBirdAgent(
            state_shape=state_shape,
            n_actions=n_actions
        )
        
        # Load trained model
        agent.load_model("flappy_bird_dqn.pth")
        
        # Set epsilon to exploration minimum for best performance
        agent.epsilon = agent.epsilon_final
    
    # Play episodes
    scores = []
    
    for episode in range(episodes):
        # Reset environment
        observation, info = env.reset()
        
        # Process first observation
        state = preprocessor.process_observation(observation)
        
        # Initialize frame stack
        frames = [state] * frame_stack
        stacked_state = preprocessor.stack_frames(frames)
        
        # Initialize episode variables
        done = False
        score = 0
        
        while not done:
            # Select action (no exploration)
            with torch.no_grad():
                state_tensor = torch.FloatTensor(stacked_state).unsqueeze(0).to(device)
                q_values = agent.policy_net(state_tensor)
                action = q_values.max(1)[1].item()
            
            # Take action
            observation, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            
            # Process new observation
            next_state = preprocessor.process_observation(observation)
            
            # Update frame stack
            frames.pop(0)
            frames.append(next_state)
            stacked_state = preprocessor.stack_frames(frames)
            
            # Update score
            score += reward
            
            if render:
                time.sleep(0.01)  # Slow down rendering
        
        scores.append(score)
        print(f"Episode {episode+1}, Score: {score}")
    
    print(f"Average Score: {np.mean(scores)}")
    return scores

if __name__ == "__main__":
    # Uncomment to train the agent
    # agent = train_flappy_bird(episodes=2000, render=False)
    
    # Play using a trained agent
    play_flappy_bird(episodes=5, render=True)
```

## Conclusion

Reinforcement learning represents a powerful paradigm for training agents through interaction with environments. Our exploration of RL algorithms, environments, and practical implementations has demonstrated several key insights:

1. **Algorithm Selection Matters**: Q-learning offers simplicity and works well for discrete state/action spaces, Actor-Critic methods provide stability by combining value and policy approaches, while PPO delivers robust performance through policy optimization with bounded updates.

2. **Environment Standardization**: Gymnasium provides a consistent interface for RL experimentation across various domains, while PettingZoo extends this to multi-agent scenarios, enabling research into collaborative and competitive behaviors.

3. **Deep RL is Powerful but Complex**: As seen in our Flappy Bird implementation, Deep Q-Networks can learn complex visual patterns and policies, but require careful hyperparameter tuning, preprocessing steps, and training stability techniques.

4. **Experience Replay and Target Networks**: These techniques are crucial for stabilizing training, breaking correlations between consecutive samples, and preventing catastrophic forgetting.

5. **Exploration-Exploitation Balance**: Managing this tradeoff through techniques like epsilon-greedy policies is essential for discovering optimal behaviors while capitalizing on learned knowledge.

The field of reinforcement learning continues to evolve rapidly, with recent advancements focusing on sample efficiency, transfer learning, and multi-agent coordination. Despite its challengeslike sample inefficiency, hyperparameter sensitivity, and reward specificationRL remains one of the most promising approaches for creating agents that can learn to interact with complex, dynamic environments.

For practitioners, starting with well-understood benchmark environments like those in Gymnasium provides a solid foundation before tackling domain-specific challenges. The combination of modern deep learning techniques with classical RL algorithms offers a powerful toolkit for developing agents capable of solving increasingly complex sequential decision-making problems.