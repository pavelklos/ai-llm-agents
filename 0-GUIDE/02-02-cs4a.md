<small>Claude Sonnet 4</small>
# 02. Prompt Design and LLM Evaluation

## Key Terms

**Prompt Engineering**: The systematic practice of designing, optimizing, and refining input prompts to elicit desired responses from large language models, involving techniques for structuring instructions, context, and examples to maximize model performance and reliability.

**One-shot Learning**: A prompting technique where a single example is provided to demonstrate the desired task format or output style, enabling the model to understand and replicate the pattern with minimal training data.

**Few-shot Learning**: An approach that provides multiple examples (typically 2-8) within the prompt to establish a clear pattern and context, allowing the model to generalize from these examples to perform similar tasks on new inputs.

**Chain of Thought (CoT)**: A prompting methodology that encourages models to break down complex problems into step-by-step reasoning processes, explicitly showing intermediate steps to improve accuracy and interpretability of responses.

**LLM Benchmarking**: The systematic evaluation of large language model performance across standardized datasets and tasks, measuring capabilities such as reasoning, knowledge retention, factual accuracy, and task-specific competencies.

**Performance Metrics**: Quantitative measures used to assess model quality including BLEU scores, ROUGE metrics, perplexity, accuracy, F1 scores, and task-specific evaluation criteria that provide objective assessment of model capabilities.

**Automated Evaluation**: The use of algorithmic approaches to assess model outputs, including semantic similarity measures, factual consistency checks, and rule-based validation systems that can scale evaluation processes.

**Human Evaluation**: Manual assessment of model outputs by human annotators, focusing on qualities like coherence, relevance, creativity, and appropriateness that are difficult to measure algorithmically.

**Prompt Templates**: Reusable structures and formats for organizing prompts that can be systematically applied across different tasks while maintaining consistency and effectiveness.

## Comprehensive Prompt Engineering and LLM Evaluation Framework

Modern prompt engineering represents both an art and a science, requiring systematic approaches to design, testing, and optimization. This comprehensive framework demonstrates advanced techniques for creating effective prompts and establishing robust evaluation methodologies for large language models.

### Advanced Prompt Engineering Implementation

````python
import asyncio
import json
import re
import statistics
import time
from typing import Dict, List, Any, Optional, Union, Callable, Tuple
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
from enum import Enum
import logging
from datetime import datetime, timezone
import hashlib
import numpy as np
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
import tiktoken
import openai
import anthropic
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from rouge_score import rouge_scorer
import bert_score
from sentence_transformers import SentenceTransformer
import wandb

from dotenv import load_dotenv
import os

load_dotenv()

# Download required NLTK data
try:
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
except:
    pass

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PromptType(Enum):
    """Types of prompting strategies"""
    ZERO_SHOT = "zero_shot"
    ONE_SHOT = "one_shot"
    FEW_SHOT = "few_shot"
    CHAIN_OF_THOUGHT = "chain_of_thought"
    TREE_OF_THOUGHT = "tree_of_thought"
    SELF_CONSISTENCY = "self_consistency"
    INSTRUCTION_FOLLOWING = "instruction_following"

class EvaluationMetric(Enum):
    """Evaluation metrics for LLM outputs"""
    BLEU = "bleu"
    ROUGE = "rouge"
    BERTSCORE = "bertscore"
    SEMANTIC_SIMILARITY = "semantic_similarity"
    FACTUAL_ACCURACY = "factual_accuracy"
    COHERENCE = "coherence"
    RELEVANCE = "relevance"
    CREATIVITY = "creativity"
    SAFETY = "safety"

@dataclass
class PromptExample:
    """Structure for prompt examples"""
    input_text: str
    expected_output: str
    task_description: str = ""
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class PromptTemplate:
    """Template structure for prompts"""
    system_message: str
    instruction_template: str
    example_template: str = ""
    output_format: str = ""
    constraints: List[str] = field(default_factory=list)
    variables: List[str] = field(default_factory=list)

@dataclass
class EvaluationResult:
    """Results from model evaluation"""
    prompt_id: str
    model_name: str
    task_type: str
    input_prompt: str
    model_output: str
    expected_output: str
    metrics: Dict[str, float]
    execution_time: float
    token_count: int
    cost_estimate: float
    timestamp: datetime

class PromptBuilder:
    """Advanced prompt builder with multiple strategies"""
    
    def __init__(self):
        self.templates: Dict[str, PromptTemplate] = {}
        self.examples: Dict[str, List[PromptExample]] = {}
        self.tokenizer = tiktoken.get_encoding("cl100k_base")  # GPT-4 tokenizer
        
    def register_template(self, name: str, template: PromptTemplate):
        """Register a prompt template"""
        self.templates[name] = template
        logger.info(f"Registered prompt template: {name}")
    
    def add_examples(self, task_name: str, examples: List[PromptExample]):
        """Add examples for a specific task"""
        self.examples[task_name] = examples
        logger.info(f"Added {len(examples)} examples for task: {task_name}")
    
    def build_zero_shot_prompt(self, template_name: str, task_input: str, 
                              **variables) -> str:
        """Build zero-shot prompt"""
        template = self.templates[template_name]
        
        prompt_parts = []
        
        if template.system_message:
            prompt_parts.append(f"System: {template.system_message}")
        
        # Build instruction with variables
        instruction = template.instruction_template.format(
            task_input=task_input, **variables
        )
        prompt_parts.append(f"Instruction: {instruction}")
        
        if template.output_format:
            prompt_parts.append(f"Output Format: {template.output_format}")
        
        if template.constraints:
            constraints_text = "\n".join([f"- {c}" for c in template.constraints])
            prompt_parts.append(f"Constraints:\n{constraints_text}")
        
        return "\n\n".join(prompt_parts)
    
    def build_few_shot_prompt(self, template_name: str, task_name: str, 
                             task_input: str, num_examples: int = 3, 
                             **variables) -> str:
        """Build few-shot prompt with examples"""
        template = self.templates[template_name]
        examples = self.examples.get(task_name, [])
        
        if len(examples) < num_examples:
            logger.warning(f"Requested {num_examples} examples but only {len(examples)} available")
            num_examples = len(examples)
        
        prompt_parts = []
        
        if template.system_message:
            prompt_parts.append(f"System: {template.system_message}")
        
        # Add examples
        if examples and template.example_template:
            examples_section = "Examples:\n"
            for i, example in enumerate(examples[:num_examples]):
                example_text = template.example_template.format(
                    input=example.input_text,
                    output=example.expected_output,
                    number=i+1
                )
                examples_section += f"{example_text}\n"
            prompt_parts.append(examples_section)
        
        # Add main instruction
        instruction = template.instruction_template.format(
            task_input=task_input, **variables
        )
        prompt_parts.append(f"Task: {instruction}")
        
        if template.output_format:
            prompt_parts.append(f"Output Format: {template.output_format}")
        
        return "\n\n".join(prompt_parts)
    
    def build_chain_of_thought_prompt(self, template_name: str, task_input: str,
                                    reasoning_steps: List[str] = None, 
                                    **variables) -> str:
        """Build Chain of Thought prompt"""
        template = self.templates[template_name]
        
        prompt_parts = []
        
        if template.system_message:
            prompt_parts.append(f"System: {template.system_message}")
        
        # Add CoT instruction
        cot_instruction = (
            "Please think through this step by step. "
            "Show your reasoning process clearly before providing the final answer."
        )
        prompt_parts.append(cot_instruction)
        
        if reasoning_steps:
            steps_text = "Consider these steps:\n" + "\n".join([f"{i+1}. {step}" 
                                                               for i, step in enumerate(reasoning_steps)])
            prompt_parts.append(steps_text)
        
        # Main task
        instruction = template.instruction_template.format(
            task_input=task_input, **variables
        )
        prompt_parts.append(f"Task: {instruction}")
        
        if template.output_format:
            format_instruction = template.output_format + "\n\nFormat your response as:\nReasoning: [your step-by-step thinking]\nAnswer: [final answer]"
            prompt_parts.append(f"Output Format: {format_instruction}")
        
        return "\n\n".join(prompt_parts)
    
    def build_self_consistency_prompts(self, template_name: str, task_input: str,
                                     num_variations: int = 5, **variables) -> List[str]:
        """Build multiple prompt variations for self-consistency"""
        base_prompt = self.build_chain_of_thought_prompt(template_name, task_input, **variables)
        
        variations = [base_prompt]
        
        # Create variations by modifying phrasing
        variation_phrases = [
            "Let's approach this systematically:",
            "Think carefully about this problem:",
            "Break this down into logical steps:",
            "Analyze this step by step:",
            "Work through this methodically:"
        ]
        
        for i in range(1, num_variations):
            if i-1 < len(variation_phrases):
                varied_prompt = base_prompt.replace(
                    "Please think through this step by step.",
                    variation_phrases[i-1]
                )
                variations.append(varied_prompt)
            else:
                # Add slight variations in instruction phrasing
                variations.append(base_prompt)
        
        return variations
    
    def estimate_tokens(self, prompt: str) -> int:
        """Estimate token count for prompt"""
        return len(self.tokenizer.encode(prompt))
    
    def optimize_prompt_length(self, prompt: str, max_tokens: int = 4000) -> str:
        """Optimize prompt to fit within token limits"""
        current_tokens = self.estimate_tokens(prompt)
        
        if current_tokens <= max_tokens:
            return prompt
        
        # Simple truncation strategy (in practice, you'd want more sophisticated approaches)
        target_chars = len(prompt) * (max_tokens / current_tokens)
        truncated = prompt[:int(target_chars)]
        
        # Try to end at a natural break
        last_newline = truncated.rfind('\n')
        if last_newline > target_chars * 0.8:
            truncated = truncated[:last_newline]
        
        logger.warning(f"Prompt truncated from {current_tokens} to ~{max_tokens} tokens")
        return truncated

class ModelInterface(ABC):
    """Abstract interface for different LLM providers"""
    
    @abstractmethod
    async def generate(self, prompt: str, **kwargs) -> Tuple[str, Dict[str, Any]]:
        """Generate response from prompt"""
        pass
    
    @abstractmethod
    def estimate_cost(self, input_tokens: int, output_tokens: int) -> float:
        """Estimate cost for generation"""
        pass

class OpenAIInterface(ModelInterface):
    """OpenAI API interface"""
    
    def __init__(self, model_name: str = "gpt-4", api_key: str = None):
        self.model_name = model_name
        self.client = openai.AsyncOpenAI(api_key=api_key or os.getenv('OPENAI_API_KEY'))
        self.tokenizer = tiktoken.encoding_for_model(model_name if "gpt" in model_name else "gpt-4")
        
        # Pricing per 1K tokens (as of 2024)
        self.pricing = {
            "gpt-4": {"input": 0.03, "output": 0.06},
            "gpt-4-turbo": {"input": 0.01, "output": 0.03},
            "gpt-3.5-turbo": {"input": 0.0015, "output": 0.002}
        }
    
    async def generate(self, prompt: str, **kwargs) -> Tuple[str, Dict[str, Any]]:
        """Generate response using OpenAI API"""
        try:
            start_time = time.time()
            
            # Prepare messages
            messages = [{"role": "user", "content": prompt}]
            
            # API call
            response = await self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                temperature=kwargs.get('temperature', 0.7),
                max_tokens=kwargs.get('max_tokens', 1000),
                top_p=kwargs.get('top_p', 1.0),
                frequency_penalty=kwargs.get('frequency_penalty', 0.0),
                presence_penalty=kwargs.get('presence_penalty', 0.0)
            )
            
            execution_time = time.time() - start_time
            
            # Extract response
            output_text = response.choices[0].message.content
            
            # Metadata
            metadata = {
                "execution_time": execution_time,
                "input_tokens": response.usage.prompt_tokens,
                "output_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens,
                "model": self.model_name,
                "finish_reason": response.choices[0].finish_reason
            }
            
            return output_text, metadata
            
        except Exception as e:
            logger.error(f"OpenAI API error: {e}")
            raise
    
    def estimate_cost(self, input_tokens: int, output_tokens: int) -> float:
        """Estimate cost for OpenAI API call"""
        model_key = self.model_name
        if model_key not in self.pricing:
            model_key = "gpt-4"  # Default fallback
        
        input_cost = (input_tokens / 1000) * self.pricing[model_key]["input"]
        output_cost = (output_tokens / 1000) * self.pricing[model_key]["output"]
        
        return input_cost + output_cost

class AnthropicInterface(ModelInterface):
    """Anthropic Claude API interface"""
    
    def __init__(self, model_name: str = "claude-3-sonnet-20240229", api_key: str = None):
        self.model_name = model_name
        self.client = anthropic.AsyncAnthropic(api_key=api_key or os.getenv('ANTHROPIC_API_KEY'))
        
        # Pricing per 1K tokens
        self.pricing = {
            "claude-3-opus-20240229": {"input": 0.015, "output": 0.075},
            "claude-3-sonnet-20240229": {"input": 0.003, "output": 0.015},
            "claude-3-haiku-20240307": {"input": 0.00025, "output": 0.00125}
        }
    
    async def generate(self, prompt: str, **kwargs) -> Tuple[str, Dict[str, Any]]:
        """Generate response using Anthropic API"""
        try:
            start_time = time.time()
            
            response = await self.client.messages.create(
                model=self.model_name,
                max_tokens=kwargs.get('max_tokens', 1000),
                temperature=kwargs.get('temperature', 0.7),
                messages=[{"role": "user", "content": prompt}]
            )
            
            execution_time = time.time() - start_time
            
            output_text = response.content[0].text
            
            metadata = {
                "execution_time": execution_time,
                "input_tokens": response.usage.input_tokens,
                "output_tokens": response.usage.output_tokens,
                "total_tokens": response.usage.input_tokens + response.usage.output_tokens,
                "model": self.model_name,
                "stop_reason": response.stop_reason
            }
            
            return output_text, metadata
            
        except Exception as e:
            logger.error(f"Anthropic API error: {e}")
            raise
    
    def estimate_cost(self, input_tokens: int, output_tokens: int) -> float:
        """Estimate cost for Anthropic API call"""
        model_key = self.model_name
        if model_key not in self.pricing:
            model_key = "claude-3-sonnet-20240229"  # Default fallback
        
        input_cost = (input_tokens / 1000) * self.pricing[model_key]["input"]
        output_cost = (output_tokens / 1000) * self.pricing[model_key]["output"]
        
        return input_cost + output_cost

class LocalModelInterface(ModelInterface):
    """Interface for local/open-source models"""
    
    def __init__(self, model_name: str = "microsoft/DialoGPT-medium"):
        self.model_name = model_name
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Load model and tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        self.model.to(self.device)
        
        # Add pad token if missing
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
    
    async def generate(self, prompt: str, **kwargs) -> Tuple[str, Dict[str, Any]]:
        """Generate response using local model"""
        try:
            start_time = time.time()
            
            # Tokenize input
            inputs = self.tokenizer.encode(prompt, return_tensors='pt', truncation=True, max_length=2048)
            inputs = inputs.to(self.device)
            
            # Generate
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_length=inputs.shape[1] + kwargs.get('max_tokens', 200),
                    temperature=kwargs.get('temperature', 0.7),
                    do_sample=True,
                    top_p=kwargs.get('top_p', 0.9),
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # Decode output
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            output_text = generated_text[len(prompt):].strip()
            
            execution_time = time.time() - start_time
            
            metadata = {
                "execution_time": execution_time,
                "input_tokens": inputs.shape[1],
                "output_tokens": outputs.shape[1] - inputs.shape[1],
                "total_tokens": outputs.shape[1],
                "model": self.model_name
            }
            
            return output_text, metadata
            
        except Exception as e:
            logger.error(f"Local model error: {e}")
            raise
    
    def estimate_cost(self, input_tokens: int, output_tokens: int) -> float:
        """Local models have no API cost"""
        return 0.0

class EvaluationFramework:
    """Comprehensive evaluation framework for LLM outputs"""
    
    def __init__(self):
        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.smoothing_function = SmoothingFunction().method1
        
        # Initialize evaluation metrics
        self.evaluators = {
            EvaluationMetric.BLEU: self._evaluate_bleu,
            EvaluationMetric.ROUGE: self._evaluate_rouge,
            EvaluationMetric.BERTSCORE: self._evaluate_bertscore,
            EvaluationMetric.SEMANTIC_SIMILARITY: self._evaluate_semantic_similarity,
            EvaluationMetric.COHERENCE: self._evaluate_coherence,
            EvaluationMetric.RELEVANCE: self._evaluate_relevance
        }
    
    def _evaluate_bleu(self, output: str, reference: str) -> Dict[str, float]:
        """Calculate BLEU score"""
        try:
            output_tokens = nltk.word_tokenize(output.lower())
            reference_tokens = [nltk.word_tokenize(reference.lower())]
            
            bleu_score = sentence_bleu(
                reference_tokens, 
                output_tokens, 
                smoothing_function=self.smoothing_function
            )
            
            return {"bleu_score": bleu_score}
        except:
            return {"bleu_score": 0.0}
    
    def _evaluate_rouge(self, output: str, reference: str) -> Dict[str, float]:
        """Calculate ROUGE scores"""
        try:
            scores = self.rouge_scorer.score(reference, output)
            return {
                "rouge1_f": scores['rouge1'].fmeasure,
                "rouge2_f": scores['rouge2'].fmeasure,
                "rougeL_f": scores['rougeL'].fmeasure
            }
        except:
            return {"rouge1_f": 0.0, "rouge2_f": 0.0, "rougeL_f": 0.0}
    
    def _evaluate_bertscore(self, output: str, reference: str) -> Dict[str, float]:
        """Calculate BERTScore"""
        try:
            P, R, F1 = bert_score.score([output], [reference], lang="en", verbose=False)
            return {
                "bertscore_precision": P.item(),
                "bertscore_recall": R.item(),
                "bertscore_f1": F1.item()
            }
        except:
            return {"bertscore_precision": 0.0, "bertscore_recall": 0.0, "bertscore_f1": 0.0}
    
    def _evaluate_semantic_similarity(self, output: str, reference: str) -> Dict[str, float]:
        """Calculate semantic similarity using sentence transformers"""
        try:
            embeddings = self.sentence_model.encode([output, reference])
            similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
            return {"semantic_similarity": float(similarity)}
        except:
            return {"semantic_similarity": 0.0}
    
    def _evaluate_coherence(self, output: str, reference: str = None) -> Dict[str, float]:
        """Evaluate text coherence using heuristics"""
        try:
            sentences = nltk.sent_tokenize(output)
            if len(sentences) < 2:
                return {"coherence_score": 1.0}
            
            # Calculate sentence-to-sentence similarity
            embeddings = self.sentence_model.encode(sentences)
            similarities = []
            
            for i in range(len(embeddings) - 1):
                sim = cosine_similarity([embeddings[i]], [embeddings[i+1]])[0][0]
                similarities.append(sim)
            
            coherence_score = np.mean(similarities) if similarities else 0.0
            return {"coherence_score": float(coherence_score)}
        except:
            return {"coherence_score": 0.0}
    
    def _evaluate_relevance(self, output: str, reference: str) -> Dict[str, float]:
        """Evaluate relevance to reference using TF-IDF"""
        try:
            vectorizer = TfidfVectorizer(stop_words='english')
            tfidf_matrix = vectorizer.fit_transform([output, reference])
            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
            return {"relevance_score": float(similarity)}
        except:
            return {"relevance_score": 0.0}
    
    def evaluate_output(self, output: str, reference: str, 
                       metrics: List[EvaluationMetric] = None) -> Dict[str, float]:
        """Evaluate output using specified metrics"""
        if metrics is None:
            metrics = list(self.evaluators.keys())
        
        results = {}
        
        for metric in metrics:
            if metric in self.evaluators:
                try:
                    metric_results = self.evaluators[metric](output, reference)
                    results.update(metric_results)
                except Exception as e:
                    logger.warning(f"Failed to evaluate {metric.value}: {e}")
        
        return results

class BenchmarkSuite:
    """Comprehensive benchmarking suite for LLM evaluation"""
    
    def __init__(self, models: Dict[str, ModelInterface]):
        self.models = models
        self.evaluation_framework = EvaluationFramework()
        self.prompt_builder = PromptBuilder()
        self.results: List[EvaluationResult] = []
        
        # Initialize benchmark templates
        self._initialize_templates()
        self._initialize_benchmark_tasks()
    
    def _initialize_templates(self):
        """Initialize standard prompt templates"""
        
        # Classification template
        classification_template = PromptTemplate(
            system_message="You are an expert classifier. Analyze the given text and provide accurate classifications.",
            instruction_template="Classify the following text: '{task_input}'",
            example_template="Input: {input}\nOutput: {output}",
            output_format="Provide only the classification label.",
            constraints=["Use only the provided categories", "Be concise and accurate"]
        )
        self.prompt_builder.register_template("classification", classification_template)
        
        # Question Answering template
        qa_template = PromptTemplate(
            system_message="You are a knowledgeable assistant. Answer questions accurately and concisely.",
            instruction_template="Question: {task_input}",
            example_template="Q: {input}\nA: {output}",
            output_format="Provide a clear, factual answer.",
            constraints=["Base answers on factual knowledge", "Admit uncertainty when appropriate"]
        )
        self.prompt_builder.register_template("qa", qa_template)
        
        # Reasoning template
        reasoning_template = PromptTemplate(
            system_message="You are a logical reasoning expert. Think step by step.",
            instruction_template="Solve this problem: {task_input}",
            example_template="Problem: {input}\nSolution: {output}",
            output_format="Show your reasoning process and final answer.",
            constraints=["Use logical step-by-step reasoning", "Verify your answer"]
        )
        self.prompt_builder.register_template("reasoning", reasoning_template)
    
    def _initialize_benchmark_tasks(self):
        """Initialize benchmark tasks with examples"""
        
        # Sentiment classification examples
        sentiment_examples = [
            PromptExample("I love this product!", "positive"),
            PromptExample("This is the worst thing ever.", "negative"),
            PromptExample("It's okay, nothing special.", "neutral")
        ]
        self.prompt_builder.add_examples("sentiment", sentiment_examples)
        
        # Math reasoning examples
        math_examples = [
            PromptExample(
                "If a train travels 60 miles in 1 hour, how far will it travel in 3.5 hours?",
                "60 miles/hour × 3.5 hours = 210 miles"
            ),
            PromptExample(
                "What is 15% of 240?",
                "15% = 0.15, so 0.15 × 240 = 36"
            )
        ]
        self.prompt_builder.add_examples("math", math_examples)
        
        # Question answering examples
        qa_examples = [
            PromptExample(
                "What is the capital of France?",
                "The capital of France is Paris."
            ),
            PromptExample(
                "Who wrote Romeo and Juliet?",
                "Romeo and Juliet was written by William Shakespeare."
            )
        ]
        self.prompt_builder.add_examples("qa", qa_examples)
    
    def create_benchmark_dataset(self, task_type: str, size: int = 100) -> List[Tuple[str, str]]:
        """Create synthetic benchmark dataset"""
        
        datasets = {
            "sentiment": [
                ("I absolutely love this new smartphone!", "positive"),
                ("The movie was terrible and boring.", "negative"),
                ("The weather is fine today.", "neutral"),
                ("This restaurant exceeded my expectations!", "positive"),
                ("I'm disappointed with the service.", "negative"),
                ("The book was decent but not memorable.", "neutral"),
                ("Outstanding performance by the team!", "positive"),
                ("This software is full of bugs.", "negative"),
                ("The presentation was informative.", "neutral"),
                ("Worst purchase I've ever made.", "negative")
            ],
            "math": [
                ("What is 25 × 8?", "200"),
                ("If 3x + 7 = 22, what is x?", "5"),
                ("What is the area of a circle with radius 5?", "25π or approximately 78.54"),
                ("Solve: 2^3 + 3^2", "8 + 9 = 17"),
                ("What is 15% of 80?", "12"),
                ("If a rectangle has length 12 and width 8, what is its perimeter?", "40"),
                ("What is the square root of 144?", "12"),
                ("Convert 0.75 to a fraction", "3/4"),
                ("What is 7! (7 factorial)?", "5040"),
                ("If y = 2x + 3 and x = 5, what is y?", "13")
            ],
            "qa": [
                ("What is the largest planet in our solar system?", "Jupiter"),
                ("Who painted the Mona Lisa?", "Leonardo da Vinci"),
                ("What is the chemical symbol for gold?", "Au"),
                ("In which year did World War II end?", "1945"),
                ("What is the speed of light in vacuum?", "299,792,458 meters per second"),
                ("Who wrote '1984'?", "George Orwell"),
                ("What is the capital of Australia?", "Canberra"),
                ("What is the hardest natural substance?", "Diamond"),
                ("How many chambers does a human heart have?", "Four"),
                ("What is the smallest unit of matter?", "Atom")
            ]
        }
        
        if task_type not in datasets:
            raise ValueError(f"Unknown task type: {task_type}")
        
        # Repeat and shuffle to create larger dataset
        base_data = datasets[task_type]
        expanded_data = (base_data * (size // len(base_data) + 1))[:size]
        
        return expanded_data
    
    async def run_benchmark(self, task_type: str, prompt_types: List[PromptType],
                           dataset_size: int = 50, max_concurrent: int = 5) -> pd.DataFrame:
        """Run comprehensive benchmark across models and prompt types"""
        
        logger.info(f"Starting benchmark for {task_type} with {len(self.models)} models")
        
        # Create benchmark dataset
        benchmark_data = self.create_benchmark_dataset(task_type, dataset_size)
        
        # Prepare evaluation tasks
        tasks = []
        
        for model_name, model_interface in self.models.items():
            for prompt_type in prompt_types:
                for i, (input_text, expected_output) in enumerate(benchmark_data):
                    
                    # Build appropriate prompt
                    if prompt_type == PromptType.ZERO_SHOT:
                        template_name = {"sentiment": "classification", 
                                       "math": "reasoning", 
                                       "qa": "qa"}[task_type]
                        prompt = self.prompt_builder.build_zero_shot_prompt(
                            template_name, input_text
                        )
                    elif prompt_type == PromptType.FEW_SHOT:
                        template_name = {"sentiment": "classification", 
                                       "math": "reasoning", 
                                       "qa": "qa"}[task_type]
                        prompt = self.prompt_builder.build_few_shot_prompt(
                            template_name, task_type, input_text, num_examples=3
                        )
                    elif prompt_type == PromptType.CHAIN_OF_THOUGHT:
                        template_name = "reasoning"
                        prompt = self.prompt_builder.build_chain_of_thought_prompt(
                            template_name, input_text
                        )
                    else:
                        continue  # Skip unsupported prompt types for now
                    
                    task_id = f"{model_name}_{prompt_type.value}_{i}"
                    
                    tasks.append({
                        'task_id': task_id,
                        'model_name': model_name,
                        'model_interface': model_interface,
                        'prompt_type': prompt_type,
                        'prompt': prompt,
                        'input_text': input_text,
                        'expected_output': expected_output,
                        'task_type': task_type
                    })
        
        logger.info(f"Created {len(tasks)} evaluation tasks")
        
        # Execute tasks with concurrency control
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def execute_task(task):
            async with semaphore:
                return await self._execute_single_evaluation(task)
        
        # Run all tasks
        results = []
        completed_tasks = 0
        
        for coro in asyncio.as_completed([execute_task(task) for task in tasks]):
            try:
                result = await coro
                if result:
                    results.append(result)
                    self.results.append(result)
                
                completed_tasks += 1
                if completed_tasks % 10 == 0:
                    logger.info(f"Completed {completed_tasks}/{len(tasks)} tasks")
                    
            except Exception as e:
                logger.error(f"Task failed: {e}")
        
        logger.info(f"Benchmark completed. {len(results)} successful evaluations.")
        
        # Convert to DataFrame for analysis
        df_data = []
        for result in results:
            row = {
                'model_name': result.model_name,
                'prompt_type': result.prompt_id.split('_')[1],  # Extract prompt type
                'task_type': result.task_type,
                'execution_time': result.execution_time,
                'token_count': result.token_count,
                'cost_estimate': result.cost_estimate,
                **result.metrics
            }
            df_data.append(row)
        
        return pd.DataFrame(df_data)
    
    async def _execute_single_evaluation(self, task: Dict[str, Any]) -> Optional[EvaluationResult]:
        """Execute a single evaluation task"""
        try:
            model_interface = task['model_interface']
            
            # Generate response
            output, metadata = await model_interface.generate(
                task['prompt'],
                temperature=0.1,  # Lower temperature for consistency
                max_tokens=500
            )
            
            # Evaluate output
            metrics = self.evaluation_framework.evaluate_output(
                output, 
                task['expected_output'],
                metrics=[
                    EvaluationMetric.BLEU,
                    EvaluationMetric.ROUGE,
                    EvaluationMetric.SEMANTIC_SIMILARITY,
                    EvaluationMetric.COHERENCE
                ]
            )
            
            # Calculate cost
            cost = model_interface.estimate_cost(
                metadata.get('input_tokens', 0),
                metadata.get('output_tokens', 0)
            )
            
            # Create result
            result = EvaluationResult(
                prompt_id=task['task_id'],
                model_name=task['model_name'],
                task_type=task['task_type'],
                input_prompt=task['prompt'],
                model_output=output,
                expected_output=task['expected_output'],
                metrics=metrics,
                execution_time=metadata.get('execution_time', 0),
                token_count=metadata.get('total_tokens', 0),
                cost_estimate=cost,
                timestamp=datetime.now(timezone.utc)
            )
            
            return result
            
        except Exception as e:
            logger.error(f"Evaluation failed for task {task['task_id']}: {e}")
            return None
    
    def analyze_results(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Analyze benchmark results"""
        
        analysis = {
            'summary_stats': {},
            'model_rankings': {},
            'prompt_type_analysis': {},
            'cost_analysis': {}
        }
        
        # Summary statistics
        for metric in ['bleu_score', 'rouge1_f', 'semantic_similarity', 'coherence_score']:
            if metric in df.columns:
                analysis['summary_stats'][metric] = {
                    'mean': df[metric].mean(),
                    'std': df[metric].std(),
                    'min': df[metric].min(),
                    'max': df[metric].max()
                }
        
        # Model rankings
        for metric in ['bleu_score', 'rouge1_f', 'semantic_similarity']:
            if metric in df.columns:
                rankings = df.groupby('model_name')[metric].mean().sort_values(ascending=False)
                analysis['model_rankings'][metric] = rankings.to_dict()
        
        # Prompt type analysis
        for metric in ['bleu_score', 'rouge1_f', 'semantic_similarity']:
            if metric in df.columns:
                prompt_analysis = df.groupby('prompt_type')[metric].mean().sort_values(ascending=False)
                analysis['prompt_type_analysis'][metric] = prompt_analysis.to_dict()
        
        # Cost analysis
        if 'cost_estimate' in df.columns:
            analysis['cost_analysis'] = {
                'total_cost': df['cost_estimate'].sum(),
                'cost_by_model': df.groupby('model_name')['cost_estimate'].sum().to_dict(),
                'avg_cost_per_request': df['cost_estimate'].mean()
            }
        
        return analysis
    
    def generate_report(self, df: pd.DataFrame, analysis: Dict[str, Any], 
                       output_path: str = "benchmark_report.html") -> str:
        """Generate comprehensive HTML report"""
        
        # Create visualizations
        plt.style.use('seaborn-v0_8')
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # Model performance comparison
        if 'bleu_score' in df.columns:
            model_scores = df.groupby('model_name')['bleu_score'].mean()
            axes[0, 0].bar(model_scores.index, model_scores.values)
            axes[0, 0].set_title('Average BLEU Score by Model')
            axes[0, 0].set_ylabel('BLEU Score')
            axes[0, 0].tick_params(axis='x', rotation=45)
        
        # Prompt type comparison
        if 'semantic_similarity' in df.columns:
            prompt_scores = df.groupby('prompt_type')['semantic_similarity'].mean()
            axes[0, 1].bar(prompt_scores.index, prompt_scores.values)
            axes[0, 1].set_title('Average Semantic Similarity by Prompt Type')
            axes[0, 1].set_ylabel('Semantic Similarity')
            axes[0, 1].tick_params(axis='x', rotation=45)
        
        # Cost analysis
        if 'cost_estimate' in df.columns:
            cost_by_model = df.groupby('model_name')['cost_estimate'].sum()
            axes[1, 0].bar(cost_by_model.index, cost_by_model.values)
            axes[1, 0].set_title('Total Cost by Model')
            axes[1, 0].set_ylabel('Cost ($)')
            axes[1, 0].tick_params(axis='x', rotation=45)
        
        # Performance distribution
        if 'rouge1_f' in df.columns:
            axes[1, 1].hist(df['rouge1_f'], bins=20, alpha=0.7, edgecolor='black')
            axes[1, 1].set_title('Distribution of ROUGE-1 F-scores')
            axes[1, 1].set_xlabel('ROUGE-1 F-score')
            axes[1, 1].set_ylabel('Frequency')
        
        plt.tight_layout()
        plt.savefig('benchmark_results.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # Generate HTML report
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>LLM Benchmark Report</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 40px; }}
                .header {{ background-color: #f0f0f0; padding: 20px; text-align: center; }}
                .section {{ margin: 30px 0; }}
                .metric-table {{ border-collapse: collapse; width: 100%; }}
                .metric-table th, .metric-table td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                .metric-table th {{ background-color: #f2f2f2; }}
                .highlight {{ background-color: #ffffcc; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>LLM Benchmark Report</h1>
                <p>Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            </div>
            
            <div class="section">
                <h2>Executive Summary</h2>
                <p>This report presents comprehensive evaluation results for {len(df['model_name'].unique())} models 
                across {len(df['prompt_type'].unique())} prompt types with {len(df)} total evaluations.</p>
            </div>
            
            <div class="section">
                <h2>Model Performance Rankings</h2>
                <table class="metric-table">
                    <tr><th>Model</th><th>Avg BLEU</th><th>Avg ROUGE-1</th><th>Avg Semantic Similarity</th></tr>
        """
        
        # Add model rankings
        for model in df['model_name'].unique():
            model_data = df[df['model_name'] == model]
            bleu = model_data['bleu_score'].mean() if 'bleu_score' in df.columns else 0
            rouge = model_data['rouge1_f'].mean() if 'rouge1_f' in df.columns else 0
            semantic = model_data['semantic_similarity'].mean() if 'semantic_similarity' in df.columns else 0
            
            html_content += f"""
                    <tr>
                        <td>{model}</td>
                        <td>{bleu:.3f}</td>
                        <td>{rouge:.3f}</td>
                        <td>{semantic:.3f}</td>
                    </tr>
            """
        
        html_content += """
                </table>
            </div>
            
            <div class="section">
                <h2>Visualization</h2>
                <img src="benchmark_results.png" alt="Benchmark Results" style="max-width: 100%;">
            </div>
            
            <div class="section">
                <h2>Statistical Analysis</h2>
                <pre>
        """
        
        html_content += json.dumps(analysis, indent=2)
        html_content += """
                </pre>
            </div>
        </body>
        </html>
        """
        
        # Save report
        with open(output_path, 'w') as f:
            f.write(html_content)
        
        logger.info(f"Report generated: {output_path}")
        return output_path

# Practical demonstration
async def main():
    """Main demonstration of prompt engineering and LLM evaluation"""
    
    logger.info("=== Prompt Engineering and LLM Evaluation Demonstration ===")
    
    # Initialize model interfaces
    models = {}
    
    # Add OpenAI if API key available
    if os.getenv('OPENAI_API_KEY'):
        models['gpt-4'] = OpenAIInterface('gpt-4')
        models['gpt-3.5-turbo'] = OpenAIInterface('gpt-3.5-turbo')
        logger.info("Added OpenAI models")
    
    # Add Anthropic if API key available
    if os.getenv('ANTHROPIC_API_KEY'):
        models['claude-3-sonnet'] = AnthropicInterface('claude-3-sonnet-20240229')
        logger.info("Added Anthropic models")
    
    # Add local model (always available)
    try:
        models['local-gpt2'] = LocalModelInterface('gpt2')
        logger.info("Added local GPT-2 model")
    except Exception as e:
        logger.warning(f"Could not load local model: {e}")
    
    if not models:
        logger.error("No models available. Please set API keys or ensure local models are available.")
        return
    
    # Initialize benchmark suite
    benchmark_suite = BenchmarkSuite(models)
    
    # Run benchmark on sentiment analysis
    logger.info("Running sentiment analysis benchmark...")
    
    prompt_types = [PromptType.ZERO_SHOT, PromptType.FEW_SHOT, PromptType.CHAIN_OF_THOUGHT]
    
    try:
        results_df = await benchmark_suite.run_benchmark(
            task_type="sentiment",
            prompt_types=prompt_types,
            dataset_size=20,  # Small dataset for demo
            max_concurrent=3
        )
        
        logger.info(f"Benchmark completed with {len(results_df)} results")
        
        # Analyze results
        analysis = benchmark_suite.analyze_results(results_df)
        
        # Generate report
        report_path = benchmark_suite.generate_report(results_df, analysis)
        
        # Display key findings
        logger.info("=== Key Findings ===")
        
        if 'model_rankings' in analysis and 'bleu_score' in analysis['model_rankings']:
            best_model = max(analysis['model_rankings']['bleu_score'].items(), 
                           key=lambda x: x[1])
            logger.info(f"Best performing model (BLEU): {best_model[0]} ({best_model[1]:.3f})")
        
        if 'prompt_type_analysis' in analysis and 'semantic_similarity' in analysis['prompt_type_analysis']:
            best_prompt = max(analysis['prompt_type_analysis']['semantic_similarity'].items(),
                            key=lambda x: x[1])
            logger.info(f"Best prompt type (Semantic Similarity): {best_prompt[0]} ({best_prompt[1]:.3f})")
        
        if 'cost_analysis' in analysis:
            total_cost = analysis['cost_analysis']['total_cost']
            logger.info(f"Total evaluation cost: ${total_cost:.4f}")
        
        # Save detailed results
        results_df.to_csv('benchmark_results.csv', index=False)
        logger.info("Detailed results saved to benchmark_results.csv")
        
        # Demonstrate specific prompt techniques
        logger.info("\n=== Prompt Technique Demonstrations ===")
        
        await demonstrate_prompt_techniques(models)
        
    except Exception as e:
        logger.error(f"Benchmark failed: {e}")
        import traceback
        logger.error(traceback.format_exc())

async def demonstrate_prompt_techniques(models: Dict[str, ModelInterface]):
    """Demonstrate various prompt engineering techniques"""
    
    if not models:
        return
    
    # Use first available model
    model_name, model = next(iter(models.items()))
    logger.info(f"Demonstrating techniques with {model_name}")
    
    # Example problem
    problem = "A company has 100 employees. 60% work in engineering, 25% in sales, and the rest in marketing. If the company hires 20 new employees, all for engineering, what percentage will work in engineering?"
    
    # Zero-shot prompt
    zero_shot_prompt = f"Solve this problem: {problem}"
    
    # Few-shot prompt
    few_shot_prompt = f"""Here are some examples of solving percentage problems:

Example 1:
Problem: A class has 30 students. 40% are boys. How many girls are there?
Solution: 40% are boys, so 60% are girls. 60% of 30 = 0.6 × 30 = 18 girls.

Example 2:
Problem: A store had 200 items. They sold 25%. How many items are left?
Solution: 25% sold means 75% remain. 75% of 200 = 0.75 × 200 = 150 items left.

Now solve this problem: {problem}"""
    
    # Chain of thought prompt
    cot_prompt = f"""Solve this step by step, showing your reasoning:

Problem: {problem}

Let me think through this step by step:
1. First, I'll identify the current distribution
2. Then, I'll calculate the new totals after hiring
3. Finally, I'll calculate the new percentage

Step 1: Current distribution
Step 2: After hiring 20 new engineers
Step 3: New engineering percentage

Please solve following this approach."""
    
    prompts = {
        "Zero-shot": zero_shot_prompt,
        "Few-shot": few_shot_prompt,
        "Chain of Thought": cot_prompt
    }
    
    for technique, prompt in prompts.items():
        try:
            logger.info(f"\n--- {technique} Technique ---")
            output, metadata = await model.generate(prompt, temperature=0.1)
            
            logger.info(f"Response length: {len(output)} characters")
            logger.info(f"Tokens used: {metadata.get('total_tokens', 'N/A')}")
            logger.info(f"Response preview: {output[:200]}...")
            
        except Exception as e:
            logger.error(f"Error with {technique}: {e}")

if __name__ == "__main__":
    asyncio.run(main())
````

## Conclusion

The comprehensive exploration of prompt engineering and LLM evaluation demonstrates the critical importance of systematic approaches to designing, testing, and optimizing interactions with large language models. This framework establishes the foundation for creating reliable, high-performing AI applications.

**Prompt Engineering Mastery** across zero-shot, few-shot, and chain-of-thought techniques provides developers with a systematic toolkit for eliciting desired behaviors from LLMs. Understanding when and how to apply each technique enables optimization of model performance for specific tasks and domains.

**Evaluation Framework Sophistication** through automated metrics (BLEU, ROUGE, BERTScore) combined with semantic similarity measures provides objective assessment capabilities that scale across different tasks and models. This multi-metric approach captures various aspects of output quality.

**Benchmarking Methodology** enables systematic comparison of different models and prompt strategies, providing data-driven insights for model selection and optimization. The comprehensive benchmark suite supports informed decision-making in AI system design.

**Cost-Performance Analysis** integrates performance metrics with cost estimation, enabling practical optimization decisions that balance quality requirements with budget constraints. This economic perspective is essential for production deployments.

**Automated Testing Infrastructure** supports continuous evaluation and improvement of prompt designs, enabling iterative refinement and quality assurance throughout the development lifecycle. This automation capability is crucial for maintaining system reliability.

**Human-AI Collaboration** through combined automated and human evaluation approaches leverages the strengths of both algorithmic efficiency and human judgment, providing comprehensive assessment of model outputs across multiple dimensions.

**Practical Implementation** considerations including concurrency control, error handling, and result analysis demonstrate production-ready approaches to LLM evaluation that can scale to meet enterprise requirements.

**Future Adaptability** through modular design and extensible evaluation frameworks enables adaptation to new models, metrics, and evaluation paradigms as the field continues to evolve rapidly.

This comprehensive framework provides developers with the tools and methodologies necessary to create robust, well-evaluated AI applications that meet both technical and business requirements while maintaining high standards of quality and reliability.