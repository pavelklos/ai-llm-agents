<small>Claude web</small>
# 09. Testing and Optimization of Customer Assistants

## Key Terms and Concepts

**A/B Testing**: Statistical method comparing two versions of an assistant to determine which performs better through controlled experiments with real user interactions.

**Conversation Analytics**: Systematic analysis of chat logs, user interactions, and dialogue patterns to identify improvement opportunities and measure assistant performance.

**Feedback Loop**: Continuous cycle of collecting user feedback, analyzing performance metrics, implementing changes, and measuring results to iteratively improve assistant quality.

**Intent Recognition Accuracy**: Metric measuring how well the assistant understands and correctly identifies user intentions from their messages.

**Response Quality Scoring**: Quantitative evaluation of assistant responses based on relevance, accuracy, helpfulness, and user satisfaction ratings.

**Edge Case Testing**: Process of testing unusual, unexpected, or boundary conditions that might cause the assistant to fail or respond inappropriately.

**Regression Testing**: Verification that new changes or improvements don't negatively impact existing functionality or previously working features.

## Comprehensive Testing Framework Implementation

Testing AI assistants requires a systematic approach combining automated testing, human evaluation, and real-world user feedback collection. Modern testing frameworks integrate multiple evaluation dimensions to ensure robust performance across diverse scenarios.

### Automated Testing Infrastructure

```python
import asyncio
import json
import logging
import pandas as pd
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import openai
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferWindowMemory
from langchain.schema import BaseMessage, HumanMessage, AIMessage
from langsmith import Client
import pytest
from unittest.mock import AsyncMock
import aiohttp
import sqlite3
from contextlib import asynccontextmanager
import hashlib
import uuid

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TestType(Enum):
    UNIT = "unit"
    INTEGRATION = "integration"
    PERFORMANCE = "performance"
    USER_ACCEPTANCE = "user_acceptance"
    REGRESSION = "regression"

class ResponseQuality(Enum):
    EXCELLENT = 5
    GOOD = 4
    AVERAGE = 3
    POOR = 2
    UNACCEPTABLE = 1

@dataclass
class TestCase:
    id: str
    input_message: str
    expected_intent: str
    expected_entities: Dict
    context: Dict
    test_type: TestType
    priority: int
    created_at: datetime
    tags: List[str]

@dataclass
class TestResult:
    test_case_id: str
    actual_response: str
    intent_accuracy: float
    entity_extraction_accuracy: float
    response_quality: ResponseQuality
    response_time: float
    execution_timestamp: datetime
    passed: bool
    error_message: Optional[str] = None

class AssistantTester:
    def __init__(self, assistant_chain, langsmith_client: Optional[Client] = None):
        self.assistant = assistant_chain
        self.langsmith_client = langsmith_client
        self.test_database = "test_results.db"
        self.conversation_logs = []
        self.performance_metrics = {}
        self._init_database()
    
    def _init_database(self):
        """Initialize SQLite database for test results storage"""
        conn = sqlite3.connect(self.test_database)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS test_results (
                id TEXT PRIMARY KEY,
                test_case_id TEXT,
                actual_response TEXT,
                intent_accuracy REAL,
                entity_extraction_accuracy REAL,
                response_quality INTEGER,
                response_time REAL,
                execution_timestamp TIMESTAMP,
                passed BOOLEAN,
                error_message TEXT
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS conversation_logs (
                id TEXT PRIMARY KEY,
                session_id TEXT,
                user_message TEXT,
                assistant_response TEXT,
                timestamp TIMESTAMP,
                user_feedback INTEGER,
                context_data TEXT
            )
        ''')
        
        conn.commit()
        conn.close()
    
    async def run_test_suite(self, test_cases: List[TestCase]) -> Dict[str, any]:
        """Execute comprehensive test suite with parallel processing"""
        results = []
        
        # Group tests by type for optimized execution
        test_groups = {}
        for test_case in test_cases:
            test_type = test_case.test_type.value
            if test_type not in test_groups:
                test_groups[test_type] = []
            test_groups[test_type].append(test_case)
        
        # Execute tests with appropriate concurrency
        for test_type, cases in test_groups.items():
            if test_type == "performance":
                # Performance tests run sequentially for accurate measurements
                for case in cases:
                    result = await self._execute_single_test(case)
                    results.append(result)
            else:
                # Other tests can run concurrently
                batch_size = 5  # Limit concurrent requests
                for i in range(0, len(cases), batch_size):
                    batch = cases[i:i + batch_size]
                    batch_results = await asyncio.gather(
                        *[self._execute_single_test(case) for case in batch]
                    )
                    results.extend(batch_results)
        
        # Store results and generate report
        await self._store_test_results(results)
        return self._generate_test_report(results)
    
    async def _execute_single_test(self, test_case: TestCase) -> TestResult:
        """Execute individual test case with comprehensive evaluation"""
        start_time = datetime.now()
        
        try:
            # Prepare context for the assistant
            if test_case.context:
                for key, value in test_case.context.items():
                    self.assistant.memory.chat_memory.add_user_message(f"Context: {key} = {value}")
            
            # Execute the test
            response = await self._get_assistant_response(test_case.input_message)
            
            # Calculate response time
            response_time = (datetime.now() - start_time).total_seconds()
            
            # Evaluate response quality
            intent_accuracy = await self._evaluate_intent_recognition(
                test_case.input_message, 
                response, 
                test_case.expected_intent
            )
            
            entity_accuracy = await self._evaluate_entity_extraction(
                test_case.input_message,
                response,
                test_case.expected_entities
            )
            
            response_quality = await self._evaluate_response_quality(
                test_case.input_message,
                response
            )
            
            # Determine if test passed
            passed = (
                intent_accuracy >= 0.8 and
                entity_accuracy >= 0.7 and
                response_quality.value >= 3 and
                response_time <= 5.0
            )
            
            return TestResult(
                test_case_id=test_case.id,
                actual_response=response,
                intent_accuracy=intent_accuracy,
                entity_extraction_accuracy=entity_accuracy,
                response_quality=response_quality,
                response_time=response_time,
                execution_timestamp=datetime.now(),
                passed=passed
            )
            
        except Exception as e:
            logger.error(f"Test case {test_case.id} failed with error: {str(e)}")
            return TestResult(
                test_case_id=test_case.id,
                actual_response="",
                intent_accuracy=0.0,
                entity_extraction_accuracy=0.0,
                response_quality=ResponseQuality.UNACCEPTABLE,
                response_time=0.0,
                execution_timestamp=datetime.now(),
                passed=False,
                error_message=str(e)
            )
    
    async def _get_assistant_response(self, message: str) -> str:
        """Get response from assistant with error handling"""
        try:
            if hasattr(self.assistant, 'arun'):
                response = await self.assistant.arun(message)
            else:
                # Synchronous fallback
                response = self.assistant.run(message)
            return response
        except Exception as e:
            logger.error(f"Assistant response error: {str(e)}")
            raise
    
    async def _evaluate_intent_recognition(self, input_msg: str, response: str, expected_intent: str) -> float:
        """Evaluate intent recognition accuracy using LLM-based scoring"""
        evaluation_prompt = f"""
        Evaluate if the assistant correctly understood the user's intent.
        
        User message: "{input_msg}"
        Assistant response: "{response}"
        Expected intent: "{expected_intent}"
        
        Rate the intent recognition accuracy from 0.0 to 1.0:
        - 1.0: Perfect understanding
        - 0.8: Good understanding with minor gaps
        - 0.6: Partial understanding
        - 0.4: Poor understanding
        - 0.0: Completely missed the intent
        
        Respond with only the numerical score.
        """
        
        try:
            # Use a separate LLM call for evaluation
            evaluation_response = await openai.ChatCompletion.acreate(
                model="gpt-4",
                messages=[{"role": "user", "content": evaluation_prompt}],
                max_tokens=10,
                temperature=0
            )
            
            score_text = evaluation_response.choices[0].message.content.strip()
            return float(score_text)
        except:
            # Fallback to keyword-based evaluation
            intent_keywords = expected_intent.lower().split()
            response_lower = response.lower()
            matches = sum(1 for keyword in intent_keywords if keyword in response_lower)
            return matches / len(intent_keywords) if intent_keywords else 0.0
    
    async def _evaluate_entity_extraction(self, input_msg: str, response: str, expected_entities: Dict) -> float:
        """Evaluate entity extraction accuracy"""
        if not expected_entities:
            return 1.0
        
        # Simple entity matching - in production, use NER models
        extracted_count = 0
        for entity_type, entity_values in expected_entities.items():
            if isinstance(entity_values, list):
                for value in entity_values:
                    if str(value).lower() in response.lower():
                        extracted_count += 1
            else:
                if str(entity_values).lower() in response.lower():
                    extracted_count += 1
        
        total_entities = sum(len(v) if isinstance(v, list) else 1 for v in expected_entities.values())
        return extracted_count / total_entities if total_entities > 0 else 1.0
    
    async def _evaluate_response_quality(self, input_msg: str, response: str) -> ResponseQuality:
        """Evaluate overall response quality using multiple criteria"""
        quality_prompt = f"""
        Evaluate the quality of this assistant response on a scale of 1-5:
        
        User: "{input_msg}"
        Assistant: "{response}"
        
        Consider:
        - Relevance to the question
        - Accuracy of information
        - Helpfulness
        - Clarity and coherence
        - Appropriate tone
        
        5: Excellent - Perfect response
        4: Good - Minor issues
        3: Average - Acceptable but could improve
        2: Poor - Significant issues
        1: Unacceptable - Wrong or harmful
        
        Respond with only the number (1-5).
        """
        
        try:
            evaluation_response = await openai.ChatCompletion.acreate(
                model="gpt-4",
                messages=[{"role": "user", "content": quality_prompt}],
                max_tokens=5,
                temperature=0
            )
            
            score = int(evaluation_response.choices[0].message.content.strip())
            return ResponseQuality(score)
        except:
            # Fallback evaluation based on response length and basic checks
            if len(response) < 10:
                return ResponseQuality.POOR
            elif "I don't know" in response or "I'm not sure" in response:
                return ResponseQuality.AVERAGE
            else:
                return ResponseQuality.GOOD

class ConversationAnalyzer:
    def __init__(self, database_path: str = "test_results.db"):
        self.database_path = database_path
        self.analytics_cache = {}
    
    async def analyze_conversation_patterns(self, session_ids: Optional[List[str]] = None) -> Dict:
        """Analyze conversation patterns and identify improvement areas"""
        conn = sqlite3.connect(self.database_path)
        
        # Build query based on session filtering
        query = """
            SELECT session_id, user_message, assistant_response, timestamp, user_feedback, context_data
            FROM conversation_logs
        """
        params = []
        
        if session_ids:
            placeholders = ",".join(["?" for _ in session_ids])
            query += f" WHERE session_id IN ({placeholders})"
            params.extend(session_ids)
        
        query += " ORDER BY timestamp"
        
        df = pd.read_sql_query(query, conn, params=params)
        conn.close()
        
        if df.empty:
            return {"error": "No conversation data found"}
        
        analysis_results = {
            "conversation_metrics": await self._calculate_conversation_metrics(df),
            "common_failure_patterns": await self._identify_failure_patterns(df),
            "user_satisfaction_trends": await self._analyze_satisfaction_trends(df),
            "response_time_analysis": await self._analyze_response_times(df),
            "topic_distribution": await self._analyze_topic_distribution(df),
            "improvement_recommendations": []
        }
        
        # Generate recommendations based on analysis
        analysis_results["improvement_recommendations"] = await self._generate_recommendations(analysis_results)
        
        return analysis_results
    
    async def _calculate_conversation_metrics(self, df: pd.DataFrame) -> Dict:
        """Calculate key conversation performance metrics"""
        total_conversations = len(df)
        
        # User satisfaction metrics
        feedback_scores = df['user_feedback'].dropna()
        avg_satisfaction = feedback_scores.mean() if not feedback_scores.empty else 0
        satisfaction_distribution = feedback_scores.value_counts().to_dict() if not feedback_scores.empty else {}
        
        # Response characteristics
        response_lengths = df['assistant_response'].str.len()
        avg_response_length = response_lengths.mean()
        
        # Conversation flow metrics
        sessions = df.groupby('session_id')
        avg_messages_per_session = sessions.size().mean()
        
        return {
            "total_conversations": total_conversations,
            "average_satisfaction": round(avg_satisfaction, 2),
            "satisfaction_distribution": satisfaction_distribution,
            "average_response_length": round(avg_response_length, 2),
            "average_messages_per_session": round(avg_messages_per_session, 2),
            "unique_sessions": df['session_id'].nunique()
        }
    
    async def _identify_failure_patterns(self, df: pd.DataFrame) -> List[Dict]:
        """Identify common patterns in low-rated interactions"""
        low_rated = df[df['user_feedback'] <= 2]
        
        if low_rated.empty:
            return []
        
        patterns = []
        
        # Analyze common phrases in failed interactions
        failed_responses = low_rated['assistant_response'].str.lower()
        common_failures = [
            ("unclear_responses", failed_responses.str.contains("i don't understand").sum()),
            ("generic_responses", failed_responses.str.contains("i'm sorry").sum()),
            ("incomplete_responses", failed_responses.str.len().mean()),
            ("repetitive_responses", len(failed_responses) - failed_responses.nunique())
        ]
        
        for pattern_type, count in common_failures:
            if count > 0:
                patterns.append({
                    "pattern": pattern_type,
                    "frequency": count,
                    "percentage": round((count / len(low_rated)) * 100, 2)
                })
        
        return patterns

class FeedbackCollector:
    def __init__(self, database_path: str = "test_results.db"):
        self.database_path = database_path
        self.feedback_queue = asyncio.Queue()
        self.processing_task = None
    
    async def start_collection(self):
        """Start async feedback collection process"""
        self.processing_task = asyncio.create_task(self._process_feedback_queue())
    
    async def collect_user_feedback(self, session_id: str, message_id: str, rating: int, 
                                   feedback_text: Optional[str] = None, 
                                   feedback_type: str = "general") -> bool:
        """Collect user feedback asynchronously"""
        feedback_data = {
            "session_id": session_id,
            "message_id": message_id,
            "rating": rating,
            "feedback_text": feedback_text,
            "feedback_type": feedback_type,
            "timestamp": datetime.now(),
            "id": str(uuid.uuid4())
        }
        
        await self.feedback_queue.put(feedback_data)
        return True
    
    async def _process_feedback_queue(self):
        """Process feedback queue and store in database"""
        while True:
            try:
                feedback = await self.feedback_queue.get()
                await self._store_feedback(feedback)
                self.feedback_queue.task_done()
            except Exception as e:
                logger.error(f"Error processing feedback: {str(e)}")
                await asyncio.sleep(1)
    
    async def _store_feedback(self, feedback: Dict):
        """Store feedback in database"""
        conn = sqlite3.connect(self.database_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT OR REPLACE INTO user_feedback 
            (id, session_id, message_id, rating, feedback_text, feedback_type, timestamp)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (
            feedback["id"],
            feedback["session_id"],
            feedback["message_id"],
            feedback["rating"],
            feedback["feedback_text"],
            feedback["feedback_type"],
            feedback["timestamp"]
        ))
        
        conn.commit()
        conn.close()

# Example usage and testing framework
async def main():
    """Demonstration of comprehensive assistant testing"""
    
    # Initialize testing components
    assistant = ConversationChain(
        memory=ConversationBufferWindowMemory(k=5),
        verbose=True
    )
    
    tester = AssistantTester(assistant)
    analyzer = ConversationAnalyzer()
    feedback_collector = FeedbackCollector()
    
    # Define comprehensive test cases
    test_cases = [
        TestCase(
            id="test_001",
            input_message="What's the weather like today?",
            expected_intent="weather_query",
            expected_entities={"time": ["today"]},
            context={"location": "New York"},
            test_type=TestType.UNIT,
            priority=1,
            created_at=datetime.now(),
            tags=["weather", "basic_query"]
        ),
        TestCase(
            id="test_002",
            input_message="I need help with my order #12345",
            expected_intent="order_support",
            expected_entities={"order_id": ["12345"]},
            context={"user_id": "user_123"},
            test_type=TestType.INTEGRATION,
            priority=1,
            created_at=datetime.now(),
            tags=["support", "order_management"]
        ),
        TestCase(
            id="test_003",
            input_message="Can you explain quantum computing in simple terms?",
            expected_intent="explanation_request",
            expected_entities={"topic": ["quantum computing"], "complexity": ["simple"]},
            context={},
            test_type=TestType.USER_ACCEPTANCE,
            priority=2,
            created_at=datetime.now(),
            tags=["explanation", "technical"]
        )
    ]
    
    # Execute test suite
    print("Running comprehensive test suite...")
    test_results = await tester.run_test_suite(test_cases)
    
    print("\nTest Results Summary:")
    print(f"Total tests: {test_results['total_tests']}")
    print(f"Passed: {test_results['passed_tests']}")
    print(f"Failed: {test_results['failed_tests']}")
    print(f"Success rate: {test_results['success_rate']:.2%}")
    
    # Analyze conversation patterns
    print("\nAnalyzing conversation patterns...")
    analysis = await analyzer.analyze_conversation_patterns()
    
    if "conversation_metrics" in analysis:
        metrics = analysis["conversation_metrics"]
        print(f"Average satisfaction: {metrics['average_satisfaction']}")
        print(f"Total conversations analyzed: {metrics['total_conversations']}")
    
    # Start feedback collection
    await feedback_collector.start_collection()
    
    # Simulate some feedback
    await feedback_collector.collect_user_feedback(
        session_id="session_123",
        message_id="msg_456",
        rating=4,
        feedback_text="Good response but could be more detailed"
    )

if __name__ == "__main__":
    asyncio.run(main())
```

### Advanced A/B Testing Framework

```python
import random
from scipy import stats
from typing import Dict, List, Optional
import numpy as np
from dataclasses import dataclass
from datetime import datetime, timedelta
import json

@dataclass
class ExperimentConfig:
    name: str
    description: str
    variants: Dict[str, str]  # variant_id -> prompt/config
    traffic_split: Dict[str, float]  # variant_id -> percentage
    success_metrics: List[str]
    duration_days: int
    minimum_sample_size: int

class ABTestManager:
    def __init__(self):
        self.active_experiments = {}
        self.experiment_results = {}
        self.user_assignments = {}  # user_id -> variant_id
    
    def create_experiment(self, config: ExperimentConfig) -> str:
        """Create and start A/B test experiment"""
        experiment_id = f"exp_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # Validate traffic split
        total_traffic = sum(config.traffic_split.values())
        if abs(total_traffic - 1.0) > 0.01:
            raise ValueError("Traffic split must sum to 1.0")
        
        self.active_experiments[experiment_id] = {
            "config": config,
            "start_time": datetime.now(),
            "end_time": datetime.now() + timedelta(days=config.duration_days),
            "results": {variant: {"interactions": 0, "successes": 0, "metrics": {}} 
                       for variant in config.variants.keys()}
        }
        
        return experiment_id
    
    def assign_user_to_variant(self, experiment_id: str, user_id: str) -> str:
        """Assign user to experiment variant using consistent hashing"""
        if experiment_id not in self.active_experiments:
            return "control"
        
        # Check if user already assigned
        assignment_key = f"{experiment_id}_{user_id}"
        if assignment_key in self.user_assignments:
            return self.user_assignments[assignment_key]
        
        # Assign based on consistent hash and traffic split
        hash_value = hash(f"{experiment_id}_{user_id}") % 10000 / 10000
        
        cumulative_probability = 0
        config = self.active_experiments[experiment_id]["config"]
        
        for variant_id, probability in config.traffic_split.items():
            cumulative_probability += probability
            if hash_value <= cumulative_probability:
                self.user_assignments[assignment_key] = variant_id
                return variant_id
        
        # Default to first variant
        first_variant = list(config.variants.keys())[0]
        self.user_assignments[assignment_key] = first_variant
        return first_variant
    
    def record_interaction(self, experiment_id: str, user_id: str, 
                          success: bool, metrics: Dict[str, float]):
        """Record interaction result for experiment analysis"""
        if experiment_id not in self.active_experiments:
            return
        
        variant = self.assign_user_to_variant(experiment_id, user_id)
        experiment = self.active_experiments[experiment_id]
        
        # Update interaction count
        experiment["results"][variant]["interactions"] += 1
        
        # Update success count
        if success:
            experiment["results"][variant]["successes"] += 1
        
        # Update metrics
        for metric_name, value in metrics.items():
            if metric_name not in experiment["results"][variant]["metrics"]:
                experiment["results"][variant]["metrics"][metric_name] = []
            experiment["results"][variant]["metrics"][metric_name].append(value)
    
    def analyze_experiment(self, experiment_id: str) -> Dict:
        """Perform statistical analysis of experiment results"""
        if experiment_id not in self.active_experiments:
            return {"error": "Experiment not found"}
        
        experiment = self.active_experiments[experiment_id]
        results = experiment["results"]
        
        # Check if minimum sample size reached
        min_sample = experiment["config"].minimum_sample_size
        variants_ready = all(
            results[variant]["interactions"] >= min_sample 
            for variant in results.keys()
        )
        
        if not variants_ready:
            return {
                "status": "insufficient_data",
                "current_samples": {
                    variant: results[variant]["interactions"] 
                    for variant in results.keys()
                },
                "required_samples": min_sample
            }
        
        # Perform statistical tests
        analysis_results = {
            "experiment_id": experiment_id,
            "status": "complete",
            "variants": {},
            "statistical_significance": {},
            "recommendations": []
        }
        
        # Calculate conversion rates and confidence intervals
        for variant in results.keys():
            interactions = results[variant]["interactions"]
            successes = results[variant]["successes"]
            conversion_rate = successes / interactions if interactions > 0 else 0
            
            # Calculate confidence interval for conversion rate
            if interactions > 0:
                z_score = 1.96  # 95% confidence
                se = np.sqrt(conversion_rate * (1 - conversion_rate) / interactions)
                ci_lower = max(0, conversion_rate - z_score * se)
                ci_upper = min(1, conversion_rate + z_score * se)
            else:
                ci_lower = ci_upper = 0
            
            analysis_results["variants"][variant] = {
                "interactions": interactions,
                "successes": successes,
                "conversion_rate": round(conversion_rate, 4),
                "confidence_interval": [round(ci_lower, 4), round(ci_upper, 4)]
            }
        
        # Perform pairwise statistical tests
        variants = list(results.keys())
        for i, variant_a in enumerate(variants):
            for variant_b in variants[i+1:]:
                success_a = results[variant_a]["successes"]
                total_a = results[variant_a]["interactions"]
                success_b = results[variant_b]["successes"]
                total_b = results[variant_b]["interactions"]
                
                # Chi-square test for significance
                if total_a > 0 and total_b > 0:
                    contingency_table = np.array([
                        [success_a, total_a - success_a],
                        [success_b, total_b - success_b]
                    ])
                    
                    chi2, p_value = stats.chi2_contingency(contingency_table)[:2]
                    
                    analysis_results["statistical_significance"][f"{variant_a}_vs_{variant_b}"] = {
                        "p_value": round(p_value, 6),
                        "significant": p_value < 0.05,
                        "chi_square": round(chi2, 4)
                    }
        
        # Generate recommendations
        best_variant = max(
            analysis_results["variants"].items(),
            key=lambda x: x[1]["conversion_rate"]
        )
        
        analysis_results["recommendations"].append(
            f"Variant '{best_variant[0]}' shows highest conversion rate: {best_variant[1]['conversion_rate']:.2%}"
        )
        
        return analysis_results

# Usage example
def create_response_style_test():
    """Example: Testing different response styles"""
    config = ExperimentConfig(
        name="response_style_test",
        description="Test formal vs casual response styles",
        variants={
            "formal": "Respond in a professional, formal tone",
            "casual": "Respond in a friendly, casual tone", 
            "balanced": "Respond in a balanced, approachable tone"
        },
        traffic_split={
            "formal": 0.33,
            "casual": 0.33,
            "balanced": 0.34
        },
        success_metrics=["user_satisfaction", "response_time"],
        duration_days=14,
        minimum_sample_size=100
    )
    
    return config
```

## Real-World Optimization Strategies

Modern assistant optimization requires continuous monitoring and iterative improvement based on actual user interactions. The optimization process involves multiple feedback loops operating at different timescales.

### Performance Monitoring and Alerting

```python
import asyncio
import aioredis
from datetime import datetime, timedelta
import json
import smtplib
from email.mime.text import MIMEText
from typing import Dict, List, Optional, Callable
import logging

class PerformanceMonitor:
    def __init__(self, redis_url: str = "redis://localhost", alert_thresholds: Dict = None):
        self.redis_url = redis_url
        self.redis_client = None
        self.alert_thresholds = alert_thresholds or {
            "response_time_p95": 3.0,  # seconds
            "error_rate": 0.05,  # 5%
            "user_satisfaction": 3.5,  # out of 5
            "intent_accuracy": 0.8  # 80%
        }
        self.alert_callbacks = []
        self.monitoring_active = False
    
    async def start_monitoring(self):
        """Start real-time performance monitoring"""
        self.redis_client = await aioredis.from_url(self.redis_url)
        self.monitoring_active = True
        
        # Start monitoring tasks
        await asyncio.gather(
            self._monitor_response_times(),
            self._monitor_error_rates(),
            self._monitor_user_satisfaction(),
            self._check_alert_conditions()
        )
    
    async def record_interaction(self, interaction_data: Dict):
        """Record interaction metrics for monitoring"""
        timestamp = datetime.now().isoformat()
        
        # Store in Redis with TTL
        await self.redis_client.zadd(
            "interactions", 
            {json.dumps(interaction_data): timestamp}
        )
        await self.redis_client.expire("interactions", 86400)  # 24 hours
        
        # Update real-time counters
        await self._update_counters(interaction_data)
    
    async def _update_counters(self, data: Dict):
        """Update real-time performance counters"""
        current_hour = datetime.now().strftime("%Y%m%d_%H")
        
        # Response time tracking
        if "response_time" in data:
            await self.redis_client.lpush(f"response_times:{current_hour}", data["response_time"])
            await self.redis_client.expire(f"response_times:{current_hour}", 7200)  # 2 hours
        
        # Error tracking
        if data.get("error", False):
            await self.redis_client.incr(f"errors:{current_hour}")
            await self.redis_client.expire(f"errors:{current_hour}", 7200)
        
        # Success tracking
        await self.redis_client.incr(f"total:{current_hour}")
        await self.redis_client.expire(f"total:{current_hour}", 7200)
        
        # User satisfaction
        if "satisfaction_score" in data:
            await self.redis_client.lpush(f"satisfaction:{current_hour}", data["satisfaction_score"])
            await self.redis_client.expire(f"satisfaction:{current_hour}", 7200)
    
    async def _monitor_response_times(self):
        """Monitor response time percentiles"""
        while self.monitoring_active:
            try:
                current_hour = datetime.now().strftime("%Y%m%d_%H")
                response_times = await self.redis_client.lrange(f"response_times:{current_hour}", 0, -1)
                
                if response_times:
                    times = [float(t) for t in response_times]
                    p95 = np.percentile(times, 95)
                    
                    if p95 > self.alert_thresholds["response_time_p95"]:
                        await self._trigger_alert(
                            "HIGH_RESPONSE_TIME",
                            f"95th percentile response time: {p95:.2f}s (threshold: {self.alert_thresholds['response_time_p95']}s)"
                        )
                
                await asyncio.sleep(60)  # Check every minute
            except Exception as e:
                logging.error(f"Response time monitoring error: {e}")
                await asyncio.sleep(60)
    
    async def _monitor_error_rates(self):
        """Monitor error rates and alert on spikes"""
        while self.monitoring_active:
            try:
                current_hour = datetime.now().strftime("%Y%m%d_%H")
                
                errors = await self.redis_client.get(f"errors:{current_hour}") or 0
                total = await self.redis_client.get(f"total:{current_hour}") or 0
                
                if int(total) > 10:  # Minimum sample size
                    error_rate = int(errors) / int(total)
                    
                    if error_rate > self.alert_thresholds["error_rate"]:
                        await self._trigger_alert(
                            "HIGH_ERROR_RATE",
                            f"Error rate: {error_rate:.2%} (threshold: {self.alert_thresholds['error_rate']:.2%})"
                        )
                
                await asyncio.sleep(300)  # Check every 5 minutes
            except Exception as e:
                logging.error(f"Error rate monitoring error: {e}")
                await asyncio.sleep(300)
    
    async def _trigger_alert(self, alert_type: str, message: str):
        """Trigger alert through configured channels"""
        alert_data = {
            "type": alert_type,
            "message": message,
            "timestamp": datetime.now().isoformat(),
            "severity": "HIGH"
        }
        
        # Execute alert callbacks
        for callback in self.alert_callbacks:
            try:
                await callback(alert_data)
            except Exception as e:
                logging.error(f"Alert callback error: {e}")
    
    def add_alert_callback(self, callback: Callable):
        """Add custom alert callback function"""
        self.alert_callbacks.append(callback)

class ConversationOptimizer:
    def __init__(self, assistant_chain, performance_monitor: PerformanceMonitor):
        self.assistant = assistant_chain
        self.monitor = performance_monitor
        self.optimization_history = []
        self.current_optimizations = {}
    
    async def optimize_based_on_feedback(self, feedback_data: List[Dict]) -> Dict:
        """Optimize assistant based on collected feedback"""
        optimization_results = {
            "optimizations_applied": [],
            "performance_improvements": {},
            "failed_optimizations": []
        }
        
        # Analyze feedback patterns
        feedback_analysis = await self._analyze_feedback_patterns(feedback_data)
        
        # Generate optimization strategies
        strategies = await self._generate_optimization_strategies(feedback_analysis)
        
        # Apply optimizations
        for strategy in strategies:
            try:
                result = await self._apply_optimization(strategy)
                if result["success"]:
                    optimization_results["optimizations_applied"].append(result)
                else:
                    optimization_results["failed_optimizations"].append(result)
            except Exception as e:
                logging.error(f"Optimization error: {e}")
                optimization_results["failed_optimizations"].append({
                    "strategy": strategy,
                    "error": str(e)
                })
        
        return optimization_results
    
    async def _analyze_feedback_patterns(self, feedback_data: List[Dict]) -> Dict:
        """Analyze feedback to identify optimization opportunities"""
        patterns = {
            "common_complaints": {},
            "satisfaction_by_topic": {},
            "response_quality_issues": {},
            "timing_issues": {}
        }
        
        for feedback in feedback_data:
            # Extract complaint patterns
            if feedback.get("rating", 5) <= 2:
                complaint_text = feedback.get("feedback_text", "").lower()
                
                # Common complaint keywords
                complaint_keywords = {
                    "too_slow": ["slow", "taking too long", "wait"],
                    "unhelpful": ["not helpful", "useless", "doesn't answer"],
                    "confusing": ["confusing", "unclear", "don't understand"],
                    "repetitive": ["same answer", "repetitive", "keeps saying"],
                    "incorrect": ["wrong", "incorrect", "mistake", "error"]
                }
                
                for complaint_type, keywords in complaint_keywords.items():
                    if any(keyword in complaint_text for keyword in keywords):
                        patterns["common_complaints"][complaint_type] = patterns["common_complaints"].get(complaint_type, 0) + 1
            
            # Analyze satisfaction by topic/intent
            if "intent" in feedback:
                intent = feedback["intent"]
                rating = feedback.get("rating", 3)
                
                if intent not in patterns["satisfaction_by_topic"]:
                    patterns["satisfaction_by_topic"][intent] = []
                patterns["satisfaction_by_topic"][intent].append(rating)
        
        # Calculate averages
        for intent, ratings in patterns["satisfaction_by_topic"].items():
            patterns["satisfaction_by_topic"][intent] = {
                "average_rating": sum(ratings) / len(ratings),
                "sample_size": len(ratings)
            }
        
        return patterns
    
    async def _generate_optimization_strategies(self, analysis: Dict) -> List[Dict]:
        """Generate specific optimization strategies based on analysis"""
        strategies = []
        
        # Address common complaints
        complaints = analysis.get("common_complaints", {})
        
        if complaints.get("too_slow", 0) > 5:
            strategies.append({
                "type": "response_optimization",
                "target": "speed",
                "action": "reduce_max_tokens",
                "parameters": {"max_tokens": 150}
            })
        
        if complaints.get("confusing", 0) > 3:
            strategies.append({
                "type": "prompt_optimization",
                "target": "clarity",
                "action": "add_clarity_instruction",
                "parameters": {"instruction": "Always provide clear, step-by-step explanations"}
            })
        
        if complaints.get("repetitive", 0) > 3:
            strategies.append({
                "type": "memory_optimization",
                "target": "variety",
                "action": "increase_context_window",
                "parameters": {"window_size": 10}
            })
        
        # Address low-performing topics
        satisfaction_data = analysis.get("satisfaction_by_topic", {})
        for intent, data in satisfaction_data.items():
            if data["average_rating"] < 3.0 and data["sample_size"] >= 5:
                strategies.append({
                    "type": "intent_specific_optimization",
                    "target": intent,
                    "action": "enhance_prompt_for_intent",
                    "parameters": {"intent": intent, "enhancement": "specialized_knowledge"}
                })
        
        return strategies
    
    async def _apply_optimization(self, strategy: Dict) -> Dict:
        """Apply specific optimization strategy"""
        strategy_type = strategy["type"]
        
        try:
            if strategy_type == "response_optimization":
                return await self._optimize_response_parameters(strategy)
            elif strategy_type == "prompt_optimization":
                return await self._optimize_prompt(strategy)
            elif strategy_type == "memory_optimization":
                return await self._optimize_memory(strategy)
            elif strategy_type == "intent_specific_optimization":
                return await self._optimize_intent_handling(strategy)
            else:
                return {"success": False, "error": f"Unknown strategy type: {strategy_type}"}
        
        except Exception as e:
            return {"success": False, "error": str(e), "strategy": strategy}
    
    async def _optimize_response_parameters(self, strategy: Dict) -> Dict:
        """Optimize response generation parameters"""
        action = strategy["action"]
        params = strategy["parameters"]
        
        if action == "reduce_max_tokens":
            # Adjust max tokens for faster responses
            if hasattr(self.assistant.llm, 'max_tokens'):
                old_value = self.assistant.llm.max_tokens
                self.assistant.llm.max_tokens = params["max_tokens"]
                
                return {
                    "success": True,
                    "optimization": "max_tokens_reduced",
                    "old_value": old_value,
                    "new_value": params["max_tokens"]
                }
        
        return {"success": False, "error": "Unsupported response optimization"}
    
    async def _optimize_prompt(self, strategy: Dict) -> Dict:
        """Optimize system prompt based on feedback"""
        action = strategy["action"]
        params = strategy["parameters"]
        
        if action == "add_clarity_instruction":
            # Add clarity instruction to system prompt
            current_prompt = getattr(self.assistant, 'system_message', '')
            clarity_instruction = params["instruction"]
            
            enhanced_prompt = f"{current_prompt}\n\nIMPORTANT: {clarity_instruction}"
            
            # Update system message
            if hasattr(self.assistant, 'system_message'):
                self.assistant.system_message = enhanced_prompt
            
            return {
                "success": True,
                "optimization": "clarity_instruction_added",
                "instruction": clarity_instruction
            }
        
        return {"success": False, "error": "Unsupported prompt optimization"}

class RealTimeOptimizer:
    def __init__(self, assistant_chain):
        self.assistant = assistant_chain
        self.performance_buffer = []
        self.optimization_triggers = {
            "response_time_spike": 3.0,  # seconds
            "error_rate_spike": 0.1,     # 10%
            "satisfaction_drop": 0.5     # rating points
        }
        self.auto_optimize = True
    
    async def process_interaction(self, interaction_data: Dict) -> Dict:
        """Process interaction and trigger optimizations if needed"""
        # Add to performance buffer
        self.performance_buffer.append({
            **interaction_data,
            "timestamp": datetime.now()
        })
        
        # Keep only recent interactions (last hour)
        cutoff_time = datetime.now() - timedelta(hours=1)
        self.performance_buffer = [
            item for item in self.performance_buffer 
            if item["timestamp"] > cutoff_time
        ]
        
        # Check for optimization triggers
        if len(self.performance_buffer) >= 10:  # Minimum sample size
            optimization_needed = await self._check_optimization_triggers()
            
            if optimization_needed and self.auto_optimize:
                return await self._apply_real_time_optimization(optimization_needed)
        
        return {"optimization_applied": False}
    
    async def _check_optimization_triggers(self) -> Optional[str]:
        """Check if any optimization triggers are activated"""
        recent_data = self.performance_buffer[-10:]  # Last 10 interactions
        
        # Check response time spike
        response_times = [item.get("response_time", 0) for item in recent_data]
        avg_response_time = sum(response_times) / len(response_times)
        
        if avg_response_time > self.optimization_triggers["response_time_spike"]:
            return "response_time_spike"
        
        # Check error rate spike
        errors = sum(1 for item in recent_data if item.get("error", False))
        error_rate = errors / len(recent_data)
        
        if error_rate > self.optimization_triggers["error_rate_spike"]:
            return "error_rate_spike"
        
        # Check satisfaction drop
        satisfaction_scores = [
            item.get("satisfaction_score", 5) 
            for item in recent_data 
            if "satisfaction_score" in item
        ]
        
        if satisfaction_scores:
            avg_satisfaction = sum(satisfaction_scores) / len(satisfaction_scores)
            if avg_satisfaction < (5.0 - self.optimization_triggers["satisfaction_drop"]):
                return "satisfaction_drop"
        
        return None
    
    async def _apply_real_time_optimization(self, trigger_type: str) -> Dict:
        """Apply real-time optimization based on trigger"""
        if trigger_type == "response_time_spike":
            # Reduce response complexity temporarily
            if hasattr(self.assistant.llm, 'temperature'):
                self.assistant.llm.temperature = max(0.1, self.assistant.llm.temperature - 0.2)
            
            return {
                "optimization_applied": True,
                "type": "response_speed_boost",
                "trigger": trigger_type,
                "action": "reduced_temperature"
            }
        
        elif trigger_type == "error_rate_spike":
            # Add error handling instructions
            current_prompt = getattr(self.assistant, 'system_message', '')
            error_handling = "\n\nIf unsure about something, clearly state your uncertainty rather than guessing."
            
            self.assistant.system_message = current_prompt + error_handling
            
            return {
                "optimization_applied": True,
                "type": "error_reduction",
                "trigger": trigger_type,
                "action": "enhanced_error_handling"
            }
        
        return {"optimization_applied": False, "error": "Unknown trigger type"}

# Integration example
async def run_comprehensive_testing_and_optimization():
    """Complete testing and optimization workflow"""
    
    # Initialize components
    assistant = ConversationChain(memory=ConversationBufferWindowMemory(k=5))
    monitor = PerformanceMonitor()
    optimizer = ConversationOptimizer(assistant, monitor)
    real_time_optimizer = RealTimeOptimizer(assistant)
    
    # Start monitoring
    await monitor.start_monitoring()
    
    # Simulate production interactions with testing
    test_interactions = [
        {
            "user_message": "Help me with my account",
            "assistant_response": "I'd be happy to help with your account. What specific issue are you experiencing?",
            "response_time": 1.2,
            "satisfaction_score": 4,
            "intent": "account_support",
            "error": False
        },
        {
            "user_message": "What's the weather today?",
            "assistant_response": "I don't have access to current weather data. Please check a weather service.",
            "response_time": 0.8,
            "satisfaction_score": 2,
            "intent": "weather_query",
            "error": False
        }
    ]
    
    # Process interactions with real-time optimization
    for interaction in test_interactions:
        await monitor.record_interaction(interaction)
        optimization_result = await real_time_optimizer.process_interaction(interaction)
        
        if optimization_result.get("optimization_applied"):
            print(f"Real-time optimization applied: {optimization_result['type']}")
    
    # Periodic optimization based on collected feedback
    feedback_data = [
        {
            "rating": 2,
            "feedback_text": "The response was confusing and didn't help",
            "intent": "weather_query"
        },
        {
            "rating": 4,
            "feedback_text": "Good response but a bit slow",
            "intent": "account_support"
        }
    ]
    
    optimization_results = await optimizer.optimize_based_on_feedback(feedback_data)
    print(f"Applied {len(optimization_results['optimizations_applied'])} optimizations")
    
    return {
        "monitoring_active": True,
        "optimizations_applied": optimization_results,
        "real_time_optimization_enabled": True
    }

if __name__ == "__main__":
    asyncio.run(run_comprehensive_testing_and_optimization())
```

## Advanced Regression Testing and Quality Assurance

The final component of comprehensive testing involves automated regression testing to ensure that optimizations and updates don't break existing functionality.

```python
import hashlib
import pickle
from typing import Set, Dict, List
import asyncio
from dataclasses import dataclass
from datetime import datetime

@dataclass
class RegressionTestSuite:
    name: str
    test_cases: List[TestCase]
    baseline_results: Dict[str, TestResult]
    acceptance_criteria: Dict[str, float]
    last_run: Optional[datetime] = None

class RegressionTester:
    def __init__(self, assistant_chain):
        self.assistant = assistant_chain
        self.test_suites = {}
        self.baseline_snapshots = {}
        self.regression_threshold = 0.05  # 5% performance degradation tolerance
    
    def create_baseline(self, suite_name: str, test_cases: List[TestCase]) -> str:
        """Create performance baseline from current assistant state"""
        baseline_id = f"baseline_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # Run tests to establish baseline
        baseline_results = asyncio.run(self._run_test_cases(test_cases))
        
        # Store baseline
        suite = RegressionTestSuite(
            name=suite_name,
            test_cases=test_cases,
            baseline_results=baseline_results,
            acceptance_criteria={
                "intent_accuracy_min": 0.8,
                "response_quality_min": 3.0,
                "response_time_max": 3.0,
                "success_rate_min": 0.9
            }
        )
        
        self.test_suites[baseline_id] = suite
        return baseline_id
    
    async def run_regression_test(self, baseline_id: str) -> Dict:
        """Run regression test against established baseline"""
        if baseline_id not in self.test_suites:
            raise ValueError(f"Baseline {baseline_id} not found")
        
        suite = self.test_suites[baseline_id]
        
        # Run current tests
        current_results = await self._run_test_cases(suite.test_cases)
        
        # Compare with baseline
        comparison = await self._compare_results(suite.baseline_results, current_results)
        
        # Generate regression report
        report = self._generate_regression_report(suite, current_results, comparison)
        
        # Update suite with latest run
        suite.last_run = datetime.now()
        
        return report
    
    async def _compare_results(self, baseline: Dict, current: Dict) -> Dict:
        """Compare current results with baseline"""
        comparison = {
            "degraded_tests": [],
            "improved_tests": [],
            "stable_tests": [],
            "new_failures": [],
            "overall_regression": False
        }
        
        for test_id in baseline.keys():
            baseline_result = baseline[test_id]
            current_result = current.get(test_id)
            
            if not current_result:
                comparison["new_failures"].append(test_id)
                continue
            
            # Compare key metrics
            intent_diff = current_result.intent_accuracy - baseline_result.intent_accuracy
            quality_diff = current_result.response_quality.value - baseline_result.response_quality.value
            time_diff = current_result.response_time - baseline_result.response_time
            
            # Determine if performance degraded
            degraded = (
                intent_diff < -self.regression_threshold or
                quality_diff < -1 or  # Quality scale difference
                time_diff > 1.0  # 1 second slower
            )
            
            improved = (
                intent_diff > self.regression_threshold and
                quality_diff >= 0 and
                time_diff <= 0
            )
            
            if degraded:
                comparison["degraded_tests"].append({
                    "test_id": test_id,
                    "intent_diff": intent_diff,
                    "quality_diff": quality_diff,
                    "time_diff": time_diff
                })
            elif improved:
                comparison["improved_tests"].append(test_id)
            else:
                comparison["stable_tests"].append(test_id)
        
        # Determine overall regression
        comparison["overall_regression"] = len(comparison["degraded_tests"]) > 0 or len(comparison["new_failures"]) > 0
        
        return comparison

# Example usage and final integration
def main_testing_workflow():
    """Complete testing workflow demonstration"""
    print("=== AI Assistant Testing and Optimization Framework ===")
    print()
    
    # This would integrate with your actual assistant
    assistant = ConversationChain(memory=ConversationBufferWindowMemory(k=5))
    
    # Initialize testing components
    tester = AssistantTester(assistant)
    analyzer = ConversationAnalyzer()
    ab_test_manager = ABTestManager()
    regression_tester = RegressionTester(assistant)
    
    print("1. Creating comprehensive test suite...")
    test_cases = [
        TestCase(
            id="regression_001",
            input_message="What's my account balance?",
            expected_intent="account_inquiry",
            expected_entities={"account_type": ["balance"]},
            context={"user_authenticated": True},
            test_type=TestType.REGRESSION,
            priority=1,
            created_at=datetime.now(),
            tags=["account", "financial"]
        )
    ]
    
    print("2. Establishing performance baseline...")
    baseline_id = regression_tester.create_baseline("production_v1", test_cases)
    
    print("3. Setting up A/B test for response optimization...")
    experiment_config = ExperimentConfig(
        name="response_optimization_v2",
        description="Test optimized vs standard responses",
        variants={
            "standard": "Respond normally",
            "optimized": "Respond with enhanced clarity and structure"
        },
        traffic_split={"standard": 0.5, "optimized": 0.5},
        success_metrics=["user_satisfaction", "task_completion"],
        duration_days=7,
        minimum_sample_size=50
    )
    
    experiment_id = ab_test_manager.create_experiment(experiment_config)
    
    print("4. Framework ready for production monitoring and optimization")
    print(f"   - Baseline established: {baseline_id}")
    print(f"   - A/B test running: {experiment_id}")
    print("   - Real-time monitoring active")
    
    return {
        "baseline_id": baseline_id,
        "experiment_id": experiment_id,
        "components_initialized": True
    }

if __name__ == "__main__":
    main_testing_workflow()
```

## Conclusion

This comprehensive testing and optimization framework provides enterprise-grade capabilities for AI assistant quality assurance. The system implements multiple layers of testing including unit tests, integration tests, A/B experiments, and regression testing with real-time monitoring and automatic optimization.

Key benefits of this approach include automated quality assurance through continuous testing pipelines, data-driven optimization based on real user feedback and interaction analytics, scalable testing infrastructure that can handle production workloads, and comprehensive monitoring with alerting for performance degradation.

The framework addresses critical production concerns such as maintaining consistent performance under varying loads, ensuring new features don't break existing functionality, optimizing user experience through systematic experimentation, and providing actionable insights for iterative improvement.

Implementation of this testing framework enables teams to deploy AI assistants with confidence, knowing that quality issues will be detected early and optimizations will be applied systematically based on real-world usage patterns rather than assumptions.

The modular design allows teams to adopt components incrementally, starting with basic automated testing and gradually adding more sophisticated monitoring, A/B testing, and optimization capabilities as their assistant systems mature and scale.

---

The comprehensive testing and optimization framework for customer AI assistants is now complete. This section covers all essential aspects of testing AI chatbots in production environments, from automated testing infrastructure to real-time optimization based on user feedback.

The framework provides enterprise-grade testing capabilities including:

- **Automated Testing Infrastructure** with parallel test execution, comprehensive result storage, and multiple evaluation dimensions
- **A/B Testing Framework** for systematic experimentation with statistical significance testing
- **Real-time Performance Monitoring** with automatic alerting and optimization triggers  
- **Conversation Analytics** for identifying patterns and improvement opportunities
- **Regression Testing** to ensure new changes don't break existing functionality
- **Feedback Collection and Processing** with async processing and database storage

The code demonstrates modern Python practices using asyncio for concurrent operations, proper database integration, statistical analysis for A/B testing, and modular design for scalability. All components are designed to work together while remaining independently testable and maintainable.

This testing approach enables teams to deploy AI assistants with confidence, systematically improve performance based on real user data, and maintain high quality standards as the system evolves.