<small>Claude Sonnet 4</small>
# 03. Training Data Preparation

## Key Terms

**Data Collection**: The systematic process of gathering, acquiring, and sourcing raw textual data from various sources including web scraping, APIs, databases, and document repositories to create comprehensive training datasets for large language models.

**Data Cleaning**: The methodical process of identifying, correcting, and removing errors, inconsistencies, duplicates, and irrelevant information from raw datasets to ensure high-quality training data that improves model performance and reduces noise.

**Data Formatting**: The standardization and structuring of cleaned data into specific formats and schemas required by machine learning frameworks, including tokenization, sequence formatting, and conversion to training-ready formats like JSON, CSV, or specialized formats.

**Domain-Specific Adaptation**: The process of customizing and fine-tuning language models to perform optimally within specific domains or industries by training on specialized datasets that contain domain-relevant vocabulary, terminology, and contextual patterns.

**Data Structuring**: The organization and arrangement of training data into optimal formats that facilitate efficient model training, including proper sequence length management, batch organization, and hierarchical data organization for complex training scenarios.

**Data Quality Metrics**: Quantitative measures used to assess the quality, completeness, consistency, and relevance of training datasets, including diversity scores, duplicate detection, bias assessment, and domain coverage analysis.

**Training Data Optimization**: The process of enhancing dataset quality through techniques such as data augmentation, balancing, filtering, and strategic sampling to maximize model performance while minimizing computational requirements and training time.

**Data Pipeline**: An automated workflow system that processes raw data through multiple stages of collection, cleaning, validation, transformation, and formatting to produce production-ready training datasets with proper versioning and lineage tracking.

## Comprehensive Data Preparation Framework for LLM Training

Modern large language model training requires sophisticated data preparation pipelines that can handle massive datasets while ensuring quality, diversity, and domain specificity. This framework demonstrates advanced techniques for collecting, processing, and optimizing training data.

### Advanced Data Collection and Processing Pipeline

````python
import asyncio
import aiohttp
import aiofiles
import json
import re
import hashlib
import logging
import os
import time
from typing import Dict, List, Any, Optional, Union, Tuple, Generator
from dataclasses import dataclass, field
from datetime import datetime, timezone
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import multiprocessing as mp
from collections import defaultdict, Counter
import sqlite3
import pickle

# Data processing libraries
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.stem import WordNetLemmatizer
import spacy
import langdetect
from langdetect import detect
import datasets
from datasets import Dataset, DatasetDict
import transformers
from transformers import AutoTokenizer
import torch

# Web scraping and data sources
import requests
import feedparser
from bs4 import BeautifulSoup
import scrapy
from scrapy.crawler import CrawlerProcess
from newspaper import Article
import wikipedia
import arxiv

# Data quality and validation
from textstat import flesch_reading_ease, flesch_kincaid_grade
import textdistance
from collections import defaultdict
import matplotlib.pyplot as plt
import seaborn as sns

# Advanced text processing
import ftfy  # Fix text encoding issues
import emoji
from cleantext import clean
import contractions

from dotenv import load_dotenv

load_dotenv()

# Download required NLTK data
try:
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
    nltk.download('wordnet', quiet=True)
    nltk.download('averaged_perceptron_tagger', quiet=True)
except:
    pass

# Load spaCy model
try:
    nlp = spacy.load("en_core_web_sm")
except OSError:
    logging.warning("spaCy English model not found. Install with: python -m spacy download en_core_web_sm")
    nlp = None

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class DataSource:
    """Configuration for data sources"""
    name: str
    source_type: str  # 'web', 'api', 'file', 'database'
    url: Optional[str] = None
    api_key: Optional[str] = None
    file_path: Optional[str] = None
    query_params: Dict[str, Any] = field(default_factory=dict)
    rate_limit: float = 1.0  # requests per second
    max_items: Optional[int] = None

@dataclass
class DataQualityMetrics:
    """Metrics for assessing data quality"""
    total_documents: int = 0
    total_tokens: int = 0
    average_document_length: float = 0.0
    duplicate_ratio: float = 0.0
    language_distribution: Dict[str, int] = field(default_factory=dict)
    readability_score: float = 0.0
    vocabulary_size: int = 0
    domain_coverage: Dict[str, float] = field(default_factory=dict)
    quality_score: float = 0.0

@dataclass
class ProcessingConfig:
    """Configuration for data processing pipeline"""
    min_document_length: int = 50
    max_document_length: int = 10000
    target_languages: List[str] = field(default_factory=lambda: ['en'])
    remove_duplicates: bool = True
    similarity_threshold: float = 0.85
    clean_html: bool = True
    fix_encoding: bool = True
    normalize_whitespace: bool = True
    remove_urls: bool = False
    remove_emails: bool = False
    expand_contractions: bool = True
    handle_emojis: str = 'remove'  # 'remove', 'replace', 'keep'
    min_readability_score: float = 30.0
    max_readability_score: float = 100.0

class DataCollector:
    """Advanced data collection from multiple sources"""
    
    def __init__(self, output_dir: str = "collected_data"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.session = None
        self.collected_data = []
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def collect_from_sources(self, sources: List[DataSource]) -> List[Dict[str, Any]]:
        """Collect data from multiple sources concurrently"""
        tasks = []
        
        for source in sources:
            if source.source_type == 'web':
                tasks.append(self._collect_web_data(source))
            elif source.source_type == 'api':
                tasks.append(self._collect_api_data(source))
            elif source.source_type == 'file':
                tasks.append(self._collect_file_data(source))
            elif source.source_type == 'database':
                tasks.append(self._collect_database_data(source))
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        all_data = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"Failed to collect from {sources[i].name}: {result}")
            else:
                all_data.extend(result)
        
        logger.info(f"Collected {len(all_data)} documents from {len(sources)} sources")
        return all_data
    
    async def _collect_web_data(self, source: DataSource) -> List[Dict[str, Any]]:
        """Collect data from web sources"""
        collected = []
        
        try:
            # Rate limiting
            await asyncio.sleep(1.0 / source.rate_limit)
            
            async with self.session.get(source.url) as response:
                if response.status == 200:
                    content = await response.text()
                    
                    # Parse with BeautifulSoup
                    soup = BeautifulSoup(content, 'html.parser')
                    
                    # Extract text content
                    text = soup.get_text()
                    
                    if text.strip():
                        collected.append({
                            'text': text,
                            'source': source.name,
                            'url': source.url,
                            'collected_at': datetime.now(timezone.utc),
                            'metadata': {
                                'title': soup.title.string if soup.title else '',
                                'length': len(text)
                            }
                        })
                
        except Exception as e:
            logger.error(f"Error collecting from {source.url}: {e}")
        
        return collected
    
    async def _collect_api_data(self, source: DataSource) -> List[Dict[str, Any]]:
        """Collect data from API sources"""
        collected = []
        
        try:
            headers = {}
            if source.api_key:
                headers['Authorization'] = f"Bearer {source.api_key}"
            
            async with self.session.get(
                source.url, 
                headers=headers, 
                params=source.query_params
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    # Process API response based on structure
                    if isinstance(data, list):
                        for item in data:
                            if isinstance(item, dict) and 'text' in item:
                                collected.append({
                                    'text': item['text'],
                                    'source': source.name,
                                    'collected_at': datetime.now(timezone.utc),
                                    'metadata': item
                                })
                    elif isinstance(data, dict):
                        if 'data' in data and isinstance(data['data'], list):
                            for item in data['data']:
                                if 'text' in item:
                                    collected.append({
                                        'text': item['text'],
                                        'source': source.name,
                                        'collected_at': datetime.now(timezone.utc),
                                        'metadata': item
                                    })
                
        except Exception as e:
            logger.error(f"Error collecting from API {source.url}: {e}")
        
        return collected
    
    async def _collect_file_data(self, source: DataSource) -> List[Dict[str, Any]]:
        """Collect data from file sources"""
        collected = []
        
        try:
            file_path = Path(source.file_path)
            
            if file_path.suffix.lower() == '.json':
                async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                    content = await f.read()
                    data = json.loads(content)
                    
                    if isinstance(data, list):
                        for item in data:
                            if isinstance(item, dict) and 'text' in item:
                                collected.append({
                                    'text': item['text'],
                                    'source': source.name,
                                    'collected_at': datetime.now(timezone.utc),
                                    'metadata': item
                                })
            
            elif file_path.suffix.lower() == '.csv':
                df = pd.read_csv(file_path)
                for _, row in df.iterrows():
                    if 'text' in row:
                        collected.append({
                            'text': str(row['text']),
                            'source': source.name,
                            'collected_at': datetime.now(timezone.utc),
                            'metadata': row.to_dict()
                        })
            
            elif file_path.suffix.lower() == '.txt':
                async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                    content = await f.read()
                    
                    # Split into chunks (e.g., paragraphs)
                    chunks = content.split('\n\n')
                    for chunk in chunks:
                        if chunk.strip():
                            collected.append({
                                'text': chunk.strip(),
                                'source': source.name,
                                'collected_at': datetime.now(timezone.utc),
                                'metadata': {'length': len(chunk)}
                            })
                
        except Exception as e:
            logger.error(f"Error collecting from file {source.file_path}: {e}")
        
        return collected
    
    async def _collect_database_data(self, source: DataSource) -> List[Dict[str, Any]]:
        """Collect data from database sources"""
        collected = []
        
        try:
            # Simple SQLite example (extend for other databases)
            conn = sqlite3.connect(source.file_path)
            cursor = conn.cursor()
            
            query = source.query_params.get('query', 'SELECT * FROM documents')
            cursor.execute(query)
            
            for row in cursor.fetchall():
                # Assume first column is text
                if row and row[0]:
                    collected.append({
                        'text': str(row[0]),
                        'source': source.name,
                        'collected_at': datetime.now(timezone.utc),
                        'metadata': {'db_row': row}
                    })
            
            conn.close()
                
        except Exception as e:
            logger.error(f"Error collecting from database {source.file_path}: {e}")
        
        return collected

class SpecializedDataCollectors:
    """Specialized collectors for common data sources"""
    
    @staticmethod
    async def collect_arxiv_papers(query: str, max_results: int = 100) -> List[Dict[str, Any]]:
        """Collect papers from arXiv"""
        collected = []
        
        try:
            search = arxiv.Search(
                query=query,
                max_results=max_results,
                sort_by=arxiv.SortCriterion.SubmittedDate
            )
            
            for paper in search.results():
                collected.append({
                    'text': f"{paper.title}\n\n{paper.summary}",
                    'source': 'arxiv',
                    'collected_at': datetime.now(timezone.utc),
                    'metadata': {
                        'title': paper.title,
                        'authors': [str(author) for author in paper.authors],
                        'published': paper.published,
                        'url': paper.entry_id,
                        'categories': paper.categories
                    }
                })
                
        except Exception as e:
            logger.error(f"Error collecting from arXiv: {e}")
        
        return collected
    
    @staticmethod
    async def collect_wikipedia_articles(topics: List[str], max_per_topic: int = 10) -> List[Dict[str, Any]]:
        """Collect Wikipedia articles"""
        collected = []
        
        for topic in topics:
            try:
                # Search for pages related to topic
                search_results = wikipedia.search(topic, results=max_per_topic)
                
                for title in search_results:
                    try:
                        page = wikipedia.page(title)
                        collected.append({
                            'text': f"{page.title}\n\n{page.content}",
                            'source': 'wikipedia',
                            'collected_at': datetime.now(timezone.utc),
                            'metadata': {
                                'title': page.title,
                                'url': page.url,
                                'categories': getattr(page, 'categories', []),
                                'links': page.links[:50]  # Limit links
                            }
                        })
                        
                        # Rate limiting
                        await asyncio.sleep(0.1)
                        
                    except wikipedia.exceptions.DisambiguationError as e:
                        # Use first option from disambiguation
                        if e.options:
                            try:
                                page = wikipedia.page(e.options[0])
                                collected.append({
                                    'text': f"{page.title}\n\n{page.content}",
                                    'source': 'wikipedia',
                                    'collected_at': datetime.now(timezone.utc),
                                    'metadata': {
                                        'title': page.title,
                                        'url': page.url,
                                        'disambiguated_from': title
                                    }
                                })
                            except:
                                continue
                    except:
                        continue
                        
            except Exception as e:
                logger.error(f"Error collecting Wikipedia articles for {topic}: {e}")
        
        return collected
    
    @staticmethod
    async def collect_news_articles(rss_feeds: List[str], max_per_feed: int = 20) -> List[Dict[str, Any]]:
        """Collect news articles from RSS feeds"""
        collected = []
        
        for feed_url in rss_feeds:
            try:
                feed = feedparser.parse(feed_url)
                
                for entry in feed.entries[:max_per_feed]:
                    try:
                        # Download full article
                        article = Article(entry.link)
                        article.download()
                        article.parse()
                        
                        if article.text:
                            collected.append({
                                'text': f"{article.title}\n\n{article.text}",
                                'source': 'news',
                                'collected_at': datetime.now(timezone.utc),
                                'metadata': {
                                    'title': article.title,
                                    'url': entry.link,
                                    'published': getattr(entry, 'published', ''),
                                    'authors': article.authors,
                                    'feed_title': feed.feed.get('title', '')
                                }
                            })
                            
                        # Rate limiting
                        await asyncio.sleep(0.5)
                        
                    except Exception as e:
                        logger.warning(f"Error processing article {entry.link}: {e}")
                        
            except Exception as e:
                logger.error(f"Error processing RSS feed {feed_url}: {e}")
        
        return collected

class DataCleaner:
    """Advanced data cleaning and preprocessing"""
    
    def __init__(self, config: ProcessingConfig):
        self.config = config
        self.lemmatizer = WordNetLemmatizer()
        self.stop_words = set(stopwords.words('english'))
        
    def clean_document(self, text: str) -> Optional[str]:
        """Clean a single document"""
        if not text or len(text.strip()) < self.config.min_document_length:
            return None
        
        # Fix encoding issues
        if self.config.fix_encoding:
            text = ftfy.fix_text(text)
        
        # Clean HTML
        if self.config.clean_html:
            soup = BeautifulSoup(text, 'html.parser')
            text = soup.get_text()
        
        # Expand contractions
        if self.config.expand_contractions:
            text = contractions.fix(text)
        
        # Handle emojis
        if self.config.handle_emojis == 'remove':
            text = emoji.demojize(text)
            text = re.sub(r':[a-z_]+:', '', text)
        elif self.config.handle_emojis == 'replace':
            text = emoji.demojize(text)
        
        # Remove URLs
        if self.config.remove_urls:
            text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        
        # Remove emails
        if self.config.remove_emails:
            text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '', text)
        
        # Normalize whitespace
        if self.config.normalize_whitespace:
            text = re.sub(r'\s+', ' ', text)
            text = text.strip()
        
        # Check length after cleaning
        if len(text) < self.config.min_document_length or len(text) > self.config.max_document_length:
            return None
        
        # Check language
        try:
            detected_lang = detect(text)
            if detected_lang not in self.config.target_languages:
                return None
        except:
            # If language detection fails, keep the text
            pass
        
        # Check readability
        try:
            readability = flesch_reading_ease(text)
            if (readability < self.config.min_readability_score or 
                readability > self.config.max_readability_score):
                return None
        except:
            # If readability calculation fails, keep the text
            pass
        
        return text
    
    def batch_clean(self, documents: List[Dict[str, Any]], 
                   num_workers: int = None) -> List[Dict[str, Any]]:
        """Clean documents in parallel"""
        if num_workers is None:
            num_workers = min(mp.cpu_count(), len(documents))
        
        with ProcessPoolExecutor(max_workers=num_workers) as executor:
            cleaned_texts = list(executor.map(
                self.clean_document,
                [doc['text'] for doc in documents]
            ))
        
        cleaned_docs = []
        for doc, cleaned_text in zip(documents, cleaned_texts):
            if cleaned_text:
                doc['text'] = cleaned_text
                cleaned_docs.append(doc)
        
        logger.info(f"Cleaned {len(cleaned_docs)}/{len(documents)} documents")
        return cleaned_docs

class DuplicateRemover:
    """Advanced duplicate detection and removal"""
    
    def __init__(self, similarity_threshold: float = 0.85):
        self.similarity_threshold = similarity_threshold
        self.vectorizer = TfidfVectorizer(
            max_features=1000,
            stop_words='english',
            ngram_range=(1, 2)
        )
    
    def remove_exact_duplicates(self, documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Remove exact text duplicates"""
        seen_hashes = set()
        unique_docs = []
        
        for doc in documents:
            text_hash = hashlib.md5(doc['text'].encode()).hexdigest()
            if text_hash not in seen_hashes:
                seen_hashes.add(text_hash)
                unique_docs.append(doc)
        
        logger.info(f"Removed {len(documents) - len(unique_docs)} exact duplicates")
        return unique_docs
    
    def remove_near_duplicates(self, documents: List[Dict[str, Any]], 
                              batch_size: int = 1000) -> List[Dict[str, Any]]:
        """Remove near-duplicate documents using TF-IDF similarity"""
        if len(documents) <= 1:
            return documents
        
        # Process in batches to handle large datasets
        unique_docs = []
        
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]
            
            # Extract texts
            texts = [doc['text'] for doc in batch]
            
            # Compute TF-IDF matrix
            try:
                tfidf_matrix = self.vectorizer.fit_transform(texts)
                
                # Compute similarity matrix
                similarity_matrix = cosine_similarity(tfidf_matrix)
                
                # Find duplicates
                to_remove = set()
                for j in range(len(similarity_matrix)):
                    if j in to_remove:
                        continue
                    
                    for k in range(j + 1, len(similarity_matrix)):
                        if k in to_remove:
                            continue
                        
                        if similarity_matrix[j][k] > self.similarity_threshold:
                            # Keep the longer document
                            if len(texts[j]) >= len(texts[k]):
                                to_remove.add(k)
                            else:
                                to_remove.add(j)
                
                # Add non-duplicate documents
                for j, doc in enumerate(batch):
                    if j not in to_remove:
                        unique_docs.append(doc)
                        
            except Exception as e:
                logger.warning(f"Error in duplicate removal for batch {i}: {e}")
                unique_docs.extend(batch)  # Keep all if error occurs
        
        logger.info(f"Removed {len(documents) - len(unique_docs)} near-duplicates")
        return unique_docs
    
    def remove_duplicates(self, documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Remove both exact and near duplicates"""
        # First remove exact duplicates
        documents = self.remove_exact_duplicates(documents)
        
        # Then remove near duplicates
        documents = self.remove_near_duplicates(documents)
        
        return documents

class DataQualityAssessor:
    """Assess and score data quality"""
    
    def __init__(self):
        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
    
    def assess_quality(self, documents: List[Dict[str, Any]]) -> DataQualityMetrics:
        """Comprehensive data quality assessment"""
        metrics = DataQualityMetrics()
        
        if not documents:
            return metrics
        
        texts = [doc['text'] for doc in documents]
        
        # Basic statistics
        metrics.total_documents = len(documents)
        
        # Token statistics
        token_counts = []
        for text in texts:
            tokens = self.tokenizer.encode(text, add_special_tokens=False)
            token_counts.append(len(tokens))
        
        metrics.total_tokens = sum(token_counts)
        metrics.average_document_length = np.mean(token_counts)
        
        # Language distribution
        language_counts = defaultdict(int)
        for text in texts:
            try:
                lang = detect(text)
                language_counts[lang] += 1
            except:
                language_counts['unknown'] += 1
        
        metrics.language_distribution = dict(language_counts)
        
        # Readability assessment
        readability_scores = []
        for text in texts:
            try:
                score = flesch_reading_ease(text)
                readability_scores.append(score)
            except:
                continue
        
        if readability_scores:
            metrics.readability_score = np.mean(readability_scores)
        
        # Vocabulary diversity
        all_words = []
        for text in texts:
            words = word_tokenize(text.lower())
            all_words.extend([word for word in words if word.isalpha()])
        
        metrics.vocabulary_size = len(set(all_words))
        
        # Domain coverage (simplified heuristic)
        domain_keywords = {
            'technology': ['computer', 'software', 'algorithm', 'data', 'digital', 'internet'],
            'science': ['research', 'study', 'experiment', 'theory', 'analysis', 'discovery'],
            'business': ['company', 'market', 'revenue', 'profit', 'customer', 'strategy'],
            'healthcare': ['patient', 'treatment', 'medical', 'health', 'disease', 'therapy'],
            'education': ['student', 'learn', 'teach', 'school', 'knowledge', 'academic']
        }
        
        domain_coverage = defaultdict(int)
        for text in texts:
            text_lower = text.lower()
            for domain, keywords in domain_keywords.items():
                score = sum(1 for keyword in keywords if keyword in text_lower)
                domain_coverage[domain] += score
        
        total_coverage = sum(domain_coverage.values())
        if total_coverage > 0:
            metrics.domain_coverage = {
                domain: count / total_coverage 
                for domain, count in domain_coverage.items()
            }
        
        # Overall quality score (0-1)
        quality_factors = []
        
        # Vocabulary diversity factor
        if metrics.total_tokens > 0:
            vocab_diversity = metrics.vocabulary_size / metrics.total_tokens
            quality_factors.append(min(vocab_diversity * 100, 1.0))
        
        # Language consistency factor
        if 'en' in metrics.language_distribution:
            lang_consistency = metrics.language_distribution['en'] / metrics.total_documents
            quality_factors.append(lang_consistency)
        
        # Readability factor
        if metrics.readability_score > 0:
            readability_factor = min(metrics.readability_score / 100, 1.0)
            quality_factors.append(readability_factor)
        
        # Domain diversity factor
        if metrics.domain_coverage:
            domain_diversity = 1.0 - max(metrics.domain_coverage.values())
            quality_factors.append(domain_diversity)
        
        if quality_factors:
            metrics.quality_score = np.mean(quality_factors)
        
        return metrics

class DataStructurer:
    """Structure data for optimal LLM training"""
    
    def __init__(self, tokenizer_name: str = 'gpt2'):
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
    
    def create_training_examples(self, documents: List[Dict[str, Any]], 
                               max_length: int = 1024,
                               format_type: str = 'completion') -> List[Dict[str, Any]]:
        """Create training examples in various formats"""
        training_examples = []
        
        for doc in documents:
            text = doc['text']
            
            if format_type == 'completion':
                # For completion tasks
                examples = self._create_completion_examples(text, max_length)
                
            elif format_type == 'instruction':
                # For instruction following
                examples = self._create_instruction_examples(doc, max_length)
                
            elif format_type == 'conversation':
                # For chat/conversation
                examples = self._create_conversation_examples(text, max_length)
                
            elif format_type == 'qa':
                # For question-answering
                examples = self._create_qa_examples(text, max_length)
                
            else:
                examples = [{'text': text}]
            
            training_examples.extend(examples)
        
        logger.info(f"Created {len(training_examples)} training examples")
        return training_examples
    
    def _create_completion_examples(self, text: str, max_length: int) -> List[Dict[str, Any]]:
        """Create completion training examples"""
        examples = []
        
        # Split text into sentences
        sentences = sent_tokenize(text)
        
        # Create sliding window examples
        window_size = 3  # Use 3 sentences at a time
        
        for i in range(len(sentences) - window_size + 1):
            context = ' '.join(sentences[i:i + window_size - 1])
            completion = sentences[i + window_size - 1]
            
            # Check token length
            tokens = self.tokenizer.encode(context + completion)
            if len(tokens) <= max_length:
                examples.append({
                    'prompt': context,
                    'completion': completion,
                    'text': context + ' ' + completion
                })
        
        return examples
    
    def _create_instruction_examples(self, doc: Dict[str, Any], 
                                   max_length: int) -> List[Dict[str, Any]]:
        """Create instruction-following examples"""
        examples = []
        text = doc['text']
        
        # Extract title/topic from metadata
        title = doc.get('metadata', {}).get('title', 'Unknown Topic')
        
        # Create various instruction formats
        instructions = [
            f"Write about {title}:",
            f"Explain the topic of {title}:",
            f"Provide information about {title}:",
            f"Describe {title}:"
        ]
        
        for instruction in instructions:
            full_text = f"{instruction}\n\n{text}"
            tokens = self.tokenizer.encode(full_text)
            
            if len(tokens) <= max_length:
                examples.append({
                    'instruction': instruction,
                    'response': text,
                    'text': full_text
                })
                break  # Use first fitting instruction
        
        return examples
    
    def _create_conversation_examples(self, text: str, max_length: int) -> List[Dict[str, Any]]:
        """Create conversation examples"""
        examples = []
        
        # Split into paragraphs
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        
        # Create turn-based conversations
        for i in range(0, len(paragraphs) - 1, 2):
            if i + 1 < len(paragraphs):
                user_message = paragraphs[i]
                assistant_message = paragraphs[i + 1]
                
                conversation = f"User: {user_message}\nAssistant: {assistant_message}"
                tokens = self.tokenizer.encode(conversation)
                
                if len(tokens) <= max_length:
                    examples.append({
                        'messages': [
                            {'role': 'user', 'content': user_message},
                            {'role': 'assistant', 'content': assistant_message}
                        ],
                        'text': conversation
                    })
        
        return examples
    
    def _create_qa_examples(self, text: str, max_length: int) -> List[Dict[str, Any]]:
        """Create question-answering examples"""
        examples = []
        
        # Simple heuristic to create Q&A pairs
        sentences = sent_tokenize(text)
        
        for sentence in sentences:
            # Skip short sentences
            if len(sentence.split()) < 8:
                continue
            
            # Generate questions from statements
            questions = self._generate_questions(sentence)
            
            for question in questions:
                qa_text = f"Question: {question}\nAnswer: {sentence}"
                tokens = self.tokenizer.encode(qa_text)
                
                if len(tokens) <= max_length:
                    examples.append({
                        'question': question,
                        'answer': sentence,
                        'text': qa_text
                    })
        
        return examples
    
    def _generate_questions(self, sentence: str) -> List[str]:
        """Generate questions from a sentence (simplified)"""
        questions = []
        
        # Simple question templates
        if 'is' in sentence.lower():
            questions.append("What " + sentence.lower().replace('is', 'is', 1) + "?")
        
        if any(word in sentence.lower() for word in ['because', 'due to', 'causes']):
            questions.append("Why " + sentence.split('.')[0].lower() + "?")
        
        if any(word in sentence.lower() for word in ['when', 'during', 'in']):
            questions.append("When " + sentence.split('.')[0].lower() + "?")
        
        # Generic question
        questions.append("What can you tell me about this: " + sentence[:50] + "...?")
        
        return questions[:2]  # Limit to 2 questions per sentence
    
    def create_dataset_splits(self, examples: List[Dict[str, Any]], 
                            train_ratio: float = 0.8,
                            val_ratio: float = 0.1,
                            test_ratio: float = 0.1) -> DatasetDict:
        """Create train/validation/test splits"""
        
        # Shuffle examples
        np.random.shuffle(examples)
        
        total = len(examples)
        train_end = int(total * train_ratio)
        val_end = train_end + int(total * val_ratio)
        
        splits = {
            'train': examples[:train_end],
            'validation': examples[train_end:val_end],
            'test': examples[val_end:]
        }
        
        # Convert to HuggingFace datasets
        dataset_dict = DatasetDict({
            split: Dataset.from_list(data)
            for split, data in splits.items()
        })
        
        logger.info(f"Created dataset splits: train={len(splits['train'])}, "
                   f"val={len(splits['validation'])}, test={len(splits['test'])}")
        
        return dataset_dict

class DomainAdaptationPipeline:
    """Pipeline for domain-specific model adaptation"""
    
    def __init__(self, domain_name: str, output_dir: str = "domain_data"):
        self.domain_name = domain_name
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # Domain-specific configurations
        self.domain_configs = {
            'medical': {
                'keywords': ['patient', 'treatment', 'diagnosis', 'therapy', 'clinical'],
                'sources': ['pubmed', 'medical_journals'],
                'min_readability': 40.0,
                'max_readability': 80.0
            },
            'legal': {
                'keywords': ['law', 'court', 'legal', 'contract', 'regulation'],
                'sources': ['legal_databases', 'court_documents'],
                'min_readability': 30.0,
                'max_readability': 70.0
            },
            'technical': {
                'keywords': ['algorithm', 'system', 'implementation', 'architecture'],
                'sources': ['arxiv', 'technical_docs'],
                'min_readability': 35.0,
                'max_readability': 85.0
            },
            'finance': {
                'keywords': ['investment', 'market', 'financial', 'risk', 'portfolio'],
                'sources': ['financial_reports', 'market_data'],
                'min_readability': 45.0,
                'max_readability': 90.0
            }
        }
    
    async def create_domain_dataset(self, target_size: int = 10000) -> DatasetDict:
        """Create a domain-specific dataset"""
        
        domain_config = self.domain_configs.get(self.domain_name, {})
        keywords = domain_config.get('keywords', [])
        
        logger.info(f"Creating {self.domain_name} domain dataset with {target_size} examples")
        
        # Configure data collection
        sources = []
        
        # Add arXiv papers for technical domains
        if self.domain_name in ['technical', 'medical']:
            query = ' OR '.join(keywords)
            arxiv_data = await SpecializedDataCollectors.collect_arxiv_papers(
                query, max_results=min(1000, target_size // 4)
            )
            logger.info(f"Collected {len(arxiv_data)} arXiv papers")
        else:
            arxiv_data = []
        
        # Add Wikipedia articles
        wiki_data = await SpecializedDataCollectors.collect_wikipedia_articles(
            keywords, max_per_topic=50
        )
        logger.info(f"Collected {len(wiki_data)} Wikipedia articles")
        
        # Combine data
        all_data = arxiv_data + wiki_data
        
        # Configure processing for domain
        processing_config = ProcessingConfig(
            min_document_length=100,
            max_document_length=5000,
            min_readability_score=domain_config.get('min_readability', 30.0),
            max_readability_score=domain_config.get('max_readability', 100.0)
        )
        
        # Clean data
        cleaner = DataCleaner(processing_config)
        cleaned_data = cleaner.batch_clean(all_data)
        
        # Remove duplicates
        duplicate_remover = DuplicateRemover(similarity_threshold=0.9)
        unique_data = duplicate_remover.remove_duplicates(cleaned_data)
        
        # Filter for domain relevance
        relevant_data = self._filter_domain_relevant(unique_data, keywords)
        
        # Truncate to target size if necessary
        if len(relevant_data) > target_size:
            np.random.shuffle(relevant_data)
            relevant_data = relevant_data[:target_size]
        
        # Structure for training
        structurer = DataStructurer()
        training_examples = structurer.create_training_examples(
            relevant_data, 
            format_type='instruction'
        )
        
        # Create dataset splits
        dataset = structurer.create_dataset_splits(training_examples)
        
        # Assess quality
        assessor = DataQualityAssessor()
        quality_metrics = assessor.assess_quality(relevant_data)
        
        # Save dataset
        dataset_path = self.output_dir / f"{self.domain_name}_dataset"
        dataset.save_to_disk(str(dataset_path))
        
        # Save quality report
        quality_report = {
            'domain': self.domain_name,
            'total_documents': len(relevant_data),
            'training_examples': len(training_examples),
            'quality_metrics': quality_metrics.__dict__,
            'created_at': datetime.now(timezone.utc).isoformat()
        }
        
        with open(self.output_dir / f"{self.domain_name}_quality_report.json", 'w') as f:
            json.dump(quality_report, f, indent=2)
        
        logger.info(f"Domain dataset created with quality score: {quality_metrics.quality_score:.3f}")
        
        return dataset
    
    def _filter_domain_relevant(self, documents: List[Dict[str, Any]], 
                               keywords: List[str], 
                               min_keyword_count: int = 2) -> List[Dict[str, Any]]:
        """Filter documents for domain relevance"""
        relevant_docs = []
        
        for doc in documents:
            text_lower = doc['text'].lower()
            keyword_count = sum(1 for keyword in keywords if keyword in text_lower)
            
            if keyword_count >= min_keyword_count:
                doc['domain_relevance_score'] = keyword_count / len(keywords)
                relevant_docs.append(doc)
        
        # Sort by relevance score
        relevant_docs.sort(key=lambda x: x.get('domain_relevance_score', 0), reverse=True)
        
        logger.info(f"Filtered to {len(relevant_docs)} domain-relevant documents")
        return relevant_docs

# Practical exercise implementation
async def practical_exercise_data_preparation():
    """Practical Exercise 2: Comprehensive Data Preparation for LLM Training"""
    
    logger.info("=== Practical Exercise: Data Preparation for LLM Training ===")
    
    # Exercise 1: Multi-source data collection
    logger.info("\n1. Multi-source Data Collection")
    
    async with DataCollector("exercise_data") as collector:
        # Define data sources
        sources = [
            DataSource(
                name="sample_web",
                source_type="web",
                url="https://en.wikipedia.org/wiki/Artificial_intelligence"
            ),
            DataSource(
                name="local_file",
                source_type="file",
                file_path="sample_data.json"  # Create sample file
            )
        ]
        
        # Create sample file for demonstration
        sample_data = [
            {"text": "Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals."},
            {"text": "Machine learning is a method of data analysis that automates analytical model building."},
            {"text": "Deep learning is part of a broader family of machine learning methods based on artificial neural networks."}
        ]
        
        os.makedirs("exercise_data", exist_ok=True)
        with open("sample_data.json", "w") as f:
            json.dump(sample_data, f)
        
        # Collect data
        collected_data = await collector.collect_from_sources(sources)
        logger.info(f"Collected {len(collected_data)} documents")
    
    # Exercise 2: Specialized collection
    logger.info("\n2. Specialized Data Collection")
    
    # Collect AI-related Wikipedia articles
    ai_topics = ["machine learning", "neural networks", "natural language processing"]
    wiki_data = await SpecializedDataCollectors.collect_wikipedia_articles(ai_topics, max_per_topic=3)
    
    # Collect AI papers from arXiv
    arxiv_data = await SpecializedDataCollectors.collect_arxiv_papers("artificial intelligence", max_results=5)
    
    all_data = collected_data + wiki_data + arxiv_data
    logger.info(f"Total collected: {len(all_data)} documents")
    
    # Exercise 3: Data cleaning and quality assessment
    logger.info("\n3. Data Cleaning and Quality Assessment")
    
    # Configure cleaning
    config = ProcessingConfig(
        min_document_length=50,
        max_document_length=2000,
        remove_duplicates=True,
        similarity_threshold=0.85
    )
    
    # Clean data
    cleaner = DataCleaner(config)
    cleaned_data = cleaner.batch_clean(all_data)
    
    # Remove duplicates
    duplicate_remover = DuplicateRemover(similarity_threshold=0.85)
    unique_data = duplicate_remover.remove_duplicates(cleaned_data)
    
    # Assess quality
    assessor = DataQualityAssessor()
    quality_metrics = assessor.assess_quality(unique_data)
    
    logger.info(f"Quality Assessment Results:")
    logger.info(f"  Total documents: {quality_metrics.total_documents}")
    logger.info(f"  Total tokens: {quality_metrics.total_tokens}")
    logger.info(f"  Average length: {quality_metrics.average_document_length:.1f}")
    logger.info(f"  Quality score: {quality_metrics.quality_score:.3f}")
    logger.info(f"  Language distribution: {quality_metrics.language_distribution}")
    
    # Exercise 4: Data structuring for training
    logger.info("\n4. Data Structuring for Training")
    
    # Create training examples in different formats
    structurer = DataStructurer()
    
    # Completion format
    completion_examples = structurer.create_training_examples(
        unique_data[:10], format_type='completion'
    )
    
    # Instruction format
    instruction_examples = structurer.create_training_examples(
        unique_data[:10], format_type='instruction'
    )
    
    logger.info(f"Created {len(completion_examples)} completion examples")
    logger.info(f"Created {len(instruction_examples)} instruction examples")
    
    # Create dataset splits
    all_examples = completion_examples + instruction_examples
    dataset = structurer.create_dataset_splits(all_examples)
    
    # Save dataset
    dataset.save_to_disk("exercise_dataset")
    logger.info("Dataset saved to 'exercise_dataset'")
    
    # Exercise 5: Domain-specific adaptation
    logger.info("\n5. Domain-Specific Adaptation")
    
    # Create technical domain dataset
    domain_pipeline = DomainAdaptationPipeline("technical", "domain_datasets")
    domain_dataset = await domain_pipeline.create_domain_dataset(target_size=100)
    
    logger.info(f"Domain dataset created with {len(domain_dataset['train'])} training examples")
    
    # Exercise 6: Generate comprehensive report
    logger.info("\n6. Generating Comprehensive Report")
    
    report = {
        "exercise_summary": {
            "total_sources": len(sources) + 2,  # +2 for specialized collectors
            "total_documents_collected": len(all_data),
            "documents_after_cleaning": len(unique_data),
            "training_examples_created": len(all_examples),
            "dataset_splits": {
                "train": len(dataset['train']),
                "validation": len(dataset['validation']),
                "test": len(dataset['test'])
            }
        },
        "quality_metrics": quality_metrics.__dict__,
        "data_preparation_pipeline": {
            "collection_methods": ["web_scraping", "file_processing", "wikipedia_api", "arxiv_api"],
            "cleaning_steps": ["encoding_fix", "html_removal", "whitespace_normalization", "language_detection"],
            "deduplication": {"method": "tfidf_similarity", "threshold": 0.85},
            "structuring_formats": ["completion", "instruction"]
        },
        "recommendations": [
            "Consider expanding data sources for better domain coverage",
            "Implement more sophisticated deduplication algorithms",
            "Add data augmentation techniques for small datasets",
            "Monitor quality metrics during training data updates"
        ]
    }
    
    # Save report
    with open("data_preparation_report.json", "w") as f:
        json.dump(report, f, indent=2)
    
    logger.info("Exercise completed! Check 'data_preparation_report.json' for detailed results.")
    
    return report

# Main execution
async def main():
    """Main execution for data preparation demonstration"""
    
    logger.info("=== Data Preparation for LLM Training Framework ===")
    
    try:
        # Run practical exercise
        report = await practical_exercise_data_preparation()
        
        logger.info("\n=== Exercise Summary ===")
        logger.info(f"Documents collected: {report['exercise_summary']['total_documents_collected']}")
        logger.info(f"Training examples: {report['exercise_summary']['training_examples_created']}")
        logger.info(f"Quality score: {report['quality_metrics']['quality_score']:.3f}")
        
        # Create visualization of results
        plt.figure(figsize=(12, 8))
        
        # Dataset size distribution
        plt.subplot(2, 2, 1)
        splits = report['exercise_summary']['dataset_splits']
        plt.pie(splits.values(), labels=splits.keys(), autopct='%1.1f%%')
        plt.title('Dataset Split Distribution')
        
        # Language distribution
        plt.subplot(2, 2, 2)
        lang_dist = report['quality_metrics']['language_distribution']
        plt.bar(lang_dist.keys(), lang_dist.values())
        plt.title('Language Distribution')
        plt.xticks(rotation=45)
        
        # Quality metrics
        plt.subplot(2, 2, 3)
        metrics = ['quality_score', 'readability_score']
        values = [report['quality_metrics'].get(m, 0) for m in metrics]
        plt.bar(metrics, values)
        plt.title('Quality Metrics')
        plt.ylim(0, 100)
        
        # Data processing pipeline
        plt.subplot(2, 2, 4)
        pipeline_steps = ['Collection', 'Cleaning', 'Deduplication', 'Structuring']
        step_counts = [
            report['exercise_summary']['total_documents_collected'],
            report['exercise_summary']['documents_after_cleaning'],
            report['exercise_summary']['documents_after_cleaning'],  # Same as cleaning for visualization
            report['exercise_summary']['training_examples_created']
        ]
        plt.plot(pipeline_steps, step_counts, marker='o')
        plt.title('Data Processing Pipeline')
        plt.xticks(rotation=45)
        
        plt.tight_layout()
        plt.savefig('data_preparation_results.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info("Visualization saved as 'data_preparation_results.png'")
        
    except Exception as e:
        logger.error(f"Error in data preparation exercise: {e}")
        import traceback
        logger.error(traceback.format_exc())

if __name__ == "__main__":
    asyncio.run(main())
````

## Conclusion

The comprehensive data preparation framework demonstrates the critical importance of systematic data collection, cleaning, and structuring processes in developing high-quality training datasets for large language models. This methodology establishes the foundation for successful model training and domain adaptation.

**Multi-Source Data Collection** capabilities enable gathering diverse, comprehensive datasets from web sources, APIs, documents, and specialized repositories. The asynchronous collection framework supports scalable data acquisition while respecting rate limits and handling errors gracefully.

**Advanced Data Cleaning** through sophisticated preprocessing pipelines ensures high-quality training data by addressing encoding issues, removing duplicates, filtering by language and readability, and standardizing formats. This cleaning process significantly impacts final model performance.

**Quality Assessment Metrics** provide objective measures for evaluating dataset quality, including vocabulary diversity, readability scores, language distribution, and domain coverage. These metrics guide data collection and cleaning decisions.

**Intelligent Deduplication** using both exact matching and semantic similarity prevents training on redundant content while preserving valuable variations. The TF-IDF based approach effectively identifies near-duplicates at scale.

**Flexible Data Structuring** accommodates various training paradigms including completion, instruction-following, conversation, and question-answering formats. This adaptability supports different model architectures and training objectives.

**Domain-Specific Adaptation** enables creating specialized datasets for particular industries or applications, incorporating domain expertise and terminology while maintaining data quality standards.

**Automated Pipeline Architecture** supports production-scale data preparation with proper error handling, monitoring, and quality control. The modular design enables easy customization and extension for specific requirements.

**Practical Implementation** considerations including memory management, parallel processing, and incremental updates ensure the framework can handle large-scale data preparation tasks efficiently.

This comprehensive approach to data preparation provides the essential foundation for training effective, reliable, and domain-appropriate large language models, establishing best practices for data quality and processing efficiency in modern AI development workflows.