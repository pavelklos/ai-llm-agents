<small>Claude Sonnet 4</small>
# 01. Introduction to Neural Networks and Generative AI

## Key Terms

**Neural Networks (NN)**: Computational models inspired by biological neural networks, consisting of interconnected nodes (neurons) organized in layers that process information through weighted connections and activation functions, enabling pattern recognition and complex decision-making.

**Feedforward Networks**: The simplest type of artificial neural network where information flows in one direction from input to output layers without cycles, commonly used for classification and regression tasks.

**Convolutional Neural Networks (CNN)**: Specialized neural networks designed for processing grid-like data such as images, using convolutional layers that apply filters to detect local features and spatial hierarchies.

**Recurrent Neural Networks (RNN)**: Neural networks with memory capabilities that can process sequential data by maintaining hidden states, allowing information to persist and influence future predictions.

**Transformers**: Advanced neural network architecture based on self-attention mechanisms that can process sequences in parallel, revolutionizing natural language processing and becoming the foundation for modern large language models.

**Generative AI**: Artificial intelligence systems capable of creating new content (text, images, audio, video) by learning patterns from training data and generating novel outputs that resemble the training distribution.

**Fine-tuning**: The process of adapting a pre-trained model to a specific task or domain by continuing training on task-specific data with lower learning rates to preserve learned features while specializing behavior.

**LoRA (Low-Rank Adaptation)**: A parameter-efficient fine-tuning technique that adds trainable low-rank matrices to existing model weights, reducing computational requirements while maintaining performance.

**QLoRA (Quantized LoRA)**: An advanced fine-tuning method combining quantization and LoRA to enable efficient training of large models on consumer hardware by reducing memory requirements.

## Neural Networks and Generative AI Fundamentals

Neural networks represent the foundational architecture driving the artificial intelligence revolution, evolving from simple perceptrons to sophisticated transformer models capable of human-like reasoning and creativity. This comprehensive exploration covers the essential architectures, training methodologies, and generative capabilities that define modern AI systems.

### Complete Neural Network Implementation Framework

````python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset, random_split
from torch.cuda.amp import GradScaler, autocast
import torchvision
import torchvision.transforms as transforms
from torchvision.models import resnet50, vit_b_16
import transformers
from transformers import (
    AutoTokenizer, AutoModel, AutoModelForCausalLM,
    BitsAndBytesConfig, TrainingArguments, Trainer
)
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional, Any, Union
import logging
import json
import os
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
import math
import time
from contextlib import contextmanager
import wandb
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from PIL import Image
import librosa
import cv2
from datasets import Dataset as HFDataset
import peft
from peft import LoraConfig, get_peft_model, TaskType
import bitsandbytes as bnb

from dotenv import load_dotenv

load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class NetworkConfig:
    """Configuration for neural network architectures"""
    input_size: int
    hidden_sizes: List[int]
    output_size: int
    dropout_rate: float = 0.1
    activation: str = "relu"
    batch_norm: bool = True
    device: str = "cuda" if torch.cuda.is_available() else "cpu"

@dataclass
class TrainingConfig:
    """Training configuration parameters"""
    batch_size: int = 32
    learning_rate: float = 1e-3
    num_epochs: int = 100
    weight_decay: float = 1e-4
    gradient_clip_norm: float = 1.0
    early_stopping_patience: int = 10
    use_mixed_precision: bool = True
    save_checkpoint_every: int = 10

class BaseNeuralNetwork(nn.Module, ABC):
    """Abstract base class for neural networks"""
    
    def __init__(self, config: NetworkConfig):
        super().__init__()
        self.config = config
        self.device = torch.device(config.device)
        
    @abstractmethod
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass implementation"""
        pass
    
    def count_parameters(self) -> int:
        """Count trainable parameters"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)
    
    def get_activation_function(self, activation: str) -> nn.Module:
        """Get activation function by name"""
        activations = {
            "relu": nn.ReLU(),
            "gelu": nn.GELU(),
            "swish": nn.SiLU(),
            "tanh": nn.Tanh(),
            "sigmoid": nn.Sigmoid(),
            "leaky_relu": nn.LeakyReLU(0.01)
        }
        return activations.get(activation, nn.ReLU())

class FeedforwardNetwork(BaseNeuralNetwork):
    """Feedforward neural network implementation"""
    
    def __init__(self, config: NetworkConfig):
        super().__init__(config)
        
        layers = []
        input_size = config.input_size
        
        # Hidden layers
        for hidden_size in config.hidden_sizes:
            layers.append(nn.Linear(input_size, hidden_size))
            
            if config.batch_norm:
                layers.append(nn.BatchNorm1d(hidden_size))
            
            layers.append(self.get_activation_function(config.activation))
            
            if config.dropout_rate > 0:
                layers.append(nn.Dropout(config.dropout_rate))
            
            input_size = hidden_size
        
        # Output layer
        layers.append(nn.Linear(input_size, config.output_size))
        
        self.network = nn.Sequential(*layers)
        
        # Initialize weights
        self._initialize_weights()
    
    def _initialize_weights(self):
        """Initialize network weights using Xavier/He initialization"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                if self.config.activation in ["relu", "leaky_relu"]:
                    nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
                else:
                    nn.init.xavier_normal_(module.weight)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass through the network"""
        return self.network(x)

class ConvolutionalNetwork(BaseNeuralNetwork):
    """Convolutional neural network for image processing"""
    
    def __init__(self, config: NetworkConfig, input_channels: int = 3, 
                 kernel_sizes: List[int] = [3, 3, 3], 
                 feature_maps: List[int] = [32, 64, 128]):
        super().__init__(config)
        
        self.input_channels = input_channels
        self.kernel_sizes = kernel_sizes
        self.feature_maps = feature_maps
        
        # Convolutional layers
        conv_layers = []
        in_channels = input_channels
        
        for i, (kernel_size, out_channels) in enumerate(zip(kernel_sizes, feature_maps)):
            conv_layers.extend([
                nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size//2),
                nn.BatchNorm2d(out_channels) if config.batch_norm else nn.Identity(),
                self.get_activation_function(config.activation),
                nn.MaxPool2d(2, 2),
                nn.Dropout2d(config.dropout_rate) if config.dropout_rate > 0 else nn.Identity()
            ])
            in_channels = out_channels
        
        self.conv_layers = nn.Sequential(*conv_layers)
        
        # Calculate flattened size (assuming square input)
        sample_input = torch.randn(1, input_channels, config.input_size, config.input_size)
        conv_output = self.conv_layers(sample_input)
        flattened_size = conv_output.view(conv_output.size(0), -1).size(1)
        
        # Fully connected layers
        fc_layers = []
        input_size = flattened_size
        
        for hidden_size in config.hidden_sizes:
            fc_layers.extend([
                nn.Linear(input_size, hidden_size),
                nn.BatchNorm1d(hidden_size) if config.batch_norm else nn.Identity(),
                self.get_activation_function(config.activation),
                nn.Dropout(config.dropout_rate) if config.dropout_rate > 0 else nn.Identity()
            ])
            input_size = hidden_size
        
        fc_layers.append(nn.Linear(input_size, config.output_size))
        self.fc_layers = nn.Sequential(*fc_layers)
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        """Initialize weights for CNN"""
        for module in self.modules():
            if isinstance(module, (nn.Conv2d, nn.Linear)):
                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass through CNN"""
        x = self.conv_layers(x)
        x = x.view(x.size(0), -1)  # Flatten
        x = self.fc_layers(x)
        return x

class RecurrentNetwork(BaseNeuralNetwork):
    """Recurrent neural network for sequence processing"""
    
    def __init__(self, config: NetworkConfig, sequence_length: int, 
                 rnn_type: str = "LSTM", num_layers: int = 2, bidirectional: bool = True):
        super().__init__(config)
        
        self.sequence_length = sequence_length
        self.rnn_type = rnn_type
        self.num_layers = num_layers
        self.bidirectional = bidirectional
        
        # RNN layer
        if rnn_type == "LSTM":
            self.rnn = nn.LSTM(
                input_size=config.input_size,
                hidden_size=config.hidden_sizes[0],
                num_layers=num_layers,
                dropout=config.dropout_rate if num_layers > 1 else 0,
                bidirectional=bidirectional,
                batch_first=True
            )
        elif rnn_type == "GRU":
            self.rnn = nn.GRU(
                input_size=config.input_size,
                hidden_size=config.hidden_sizes[0],
                num_layers=num_layers,
                dropout=config.dropout_rate if num_layers > 1 else 0,
                bidirectional=bidirectional,
                batch_first=True
            )
        else:
            self.rnn = nn.RNN(
                input_size=config.input_size,
                hidden_size=config.hidden_sizes[0],
                num_layers=num_layers,
                dropout=config.dropout_rate if num_layers > 1 else 0,
                bidirectional=bidirectional,
                batch_first=True
            )
        
        # Calculate RNN output size
        rnn_output_size = config.hidden_sizes[0] * (2 if bidirectional else 1)
        
        # Fully connected layers
        fc_layers = []
        input_size = rnn_output_size
        
        for hidden_size in config.hidden_sizes[1:]:
            fc_layers.extend([
                nn.Linear(input_size, hidden_size),
                nn.BatchNorm1d(hidden_size) if config.batch_norm else nn.Identity(),
                self.get_activation_function(config.activation),
                nn.Dropout(config.dropout_rate) if config.dropout_rate > 0 else nn.Identity()
            ])
            input_size = hidden_size
        
        fc_layers.append(nn.Linear(input_size, config.output_size))
        self.fc_layers = nn.Sequential(*fc_layers)
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        """Initialize RNN weights"""
        for name, param in self.rnn.named_parameters():
            if 'weight_ih' in name:
                nn.init.xavier_uniform_(param.data)
            elif 'weight_hh' in name:
                nn.init.orthogonal_(param.data)
            elif 'bias' in name:
                nn.init.constant_(param.data, 0)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass through RNN"""
        # x shape: (batch_size, sequence_length, input_size)
        rnn_output, _ = self.rnn(x)
        
        # Use last output (or mean pooling)
        if self.rnn_type in ["LSTM", "GRU"]:
            last_output = rnn_output[:, -1, :]  # Take last timestep
        else:
            last_output = torch.mean(rnn_output, dim=1)  # Mean pooling
        
        output = self.fc_layers(last_output)
        return output

class TransformerBlock(nn.Module):
    """Single transformer block implementation"""
    
    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        
        self.attention = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model),
            nn.Dropout(dropout)
        )
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # Self-attention with residual connection
        attn_output, _ = self.attention(x, x, x, attn_mask=mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # Feed-forward with residual connection
        ff_output = self.feed_forward(x)
        x = self.norm2(x + ff_output)
        
        return x

class TransformerNetwork(BaseNeuralNetwork):
    """Transformer network implementation"""
    
    def __init__(self, config: NetworkConfig, d_model: int = 512, num_heads: int = 8, 
                 num_layers: int = 6, max_seq_length: int = 1024, vocab_size: int = 50000):
        super().__init__(config)
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.max_seq_length = max_seq_length
        self.vocab_size = vocab_size
        
        # Embedding layers
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.position_embedding = nn.Embedding(max_seq_length, d_model)
        
        # Transformer blocks
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(d_model, num_heads, d_model * 4, config.dropout_rate)
            for _ in range(num_layers)
        ])
        
        # Output layers
        self.norm = nn.LayerNorm(d_model)
        self.output_projection = nn.Linear(d_model, config.output_size)
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        """Initialize transformer weights"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.normal_(module.weight, std=0.02)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.Embedding):
                nn.init.normal_(module.weight, std=0.02)
    
    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        batch_size, seq_length = input_ids.shape
        
        # Create position indices
        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)
        position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)
        
        # Embeddings
        token_embeds = self.token_embedding(input_ids)
        position_embeds = self.position_embedding(position_ids)
        embeddings = token_embeds + position_embeds
        
        # Apply transformer blocks
        hidden_states = embeddings
        for transformer_block in self.transformer_blocks:
            hidden_states = transformer_block(hidden_states, attention_mask)
        
        # Final normalization and projection
        hidden_states = self.norm(hidden_states)
        
        # Global average pooling for classification
        if attention_mask is not None:
            mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()
            sum_embeddings = torch.sum(hidden_states * mask_expanded, 1)
            sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)
            pooled = sum_embeddings / sum_mask
        else:
            pooled = torch.mean(hidden_states, dim=1)
        
        output = self.output_projection(pooled)
        return output

class GenerativeTextModel(nn.Module):
    """Generative text model based on transformer architecture"""
    
    def __init__(self, vocab_size: int, d_model: int = 768, num_heads: int = 12, 
                 num_layers: int = 12, max_seq_length: int = 1024):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.max_seq_length = max_seq_length
        
        # Embeddings
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.position_embedding = nn.Embedding(max_seq_length, d_model)
        
        # Transformer blocks
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(d_model, num_heads, d_model * 4, dropout=0.1)
            for _ in range(num_layers)
        ])
        
        # Output head
        self.norm = nn.LayerNorm(d_model)
        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)
        
        # Tie embeddings (common practice)
        self.lm_head.weight = self.token_embedding.weight
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        """Initialize weights"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.normal_(module.weight, std=0.02)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.Embedding):
                nn.init.normal_(module.weight, std=0.02)
    
    def forward(self, input_ids: torch.Tensor, 
                attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        batch_size, seq_length = input_ids.shape
        
        # Position embeddings
        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)
        position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)
        
        # Embeddings
        token_embeds = self.token_embedding(input_ids)
        position_embeds = self.position_embedding(position_ids)
        hidden_states = token_embeds + position_embeds
        
        # Create causal mask for autoregressive generation
        causal_mask = torch.triu(
            torch.ones(seq_length, seq_length, device=input_ids.device) * float('-inf'), 
            diagonal=1
        )
        
        # Apply transformer blocks
        for transformer_block in self.transformer_blocks:
            hidden_states = transformer_block(hidden_states, causal_mask)
        
        # Final processing
        hidden_states = self.norm(hidden_states)
        logits = self.lm_head(hidden_states)
        
        return logits
    
    def generate(self, prompt_ids: torch.Tensor, max_length: int = 100, 
                 temperature: float = 1.0, top_k: int = 50) -> torch.Tensor:
        """Generate text autoregressively"""
        self.eval()
        generated = prompt_ids.clone()
        
        with torch.no_grad():
            for _ in range(max_length):
                if generated.size(1) >= self.max_seq_length:
                    break
                
                # Forward pass
                logits = self.forward(generated)
                next_token_logits = logits[:, -1, :] / temperature
                
                # Top-k sampling
                if top_k > 0:
                    top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)
                    next_token_logits = torch.full_like(next_token_logits, float('-inf'))
                    next_token_logits.scatter_(-1, top_k_indices, top_k_logits)
                
                # Sample next token
                probs = F.softmax(next_token_logits, dim=-1)
                next_token = torch.multinomial(probs, 1)
                
                # Append to sequence
                generated = torch.cat([generated, next_token], dim=1)
                
                # Check for end token (assuming 0 is end token)
                if next_token.item() == 0:
                    break
        
        return generated

class LoRAAdapter(nn.Module):
    """Low-Rank Adaptation implementation"""
    
    def __init__(self, original_layer: nn.Linear, rank: int = 16, alpha: float = 16.0):
        super().__init__()
        
        self.original_layer = original_layer
        self.rank = rank
        self.alpha = alpha
        self.scaling = alpha / rank
        
        # Freeze original weights
        for param in self.original_layer.parameters():
            param.requires_grad = False
        
        # LoRA matrices
        self.lora_A = nn.Parameter(torch.randn(rank, original_layer.in_features) * 0.01)
        self.lora_B = nn.Parameter(torch.zeros(original_layer.out_features, rank))
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Original transformation
        original_output = self.original_layer(x)
        
        # LoRA transformation
        lora_output = (x @ self.lora_A.T @ self.lora_B.T) * self.scaling
        
        return original_output + lora_output

class QLoRALinear(nn.Module):
    """Quantized LoRA linear layer"""
    
    def __init__(self, original_layer: nn.Linear, rank: int = 16, alpha: float = 16.0,
                 quantization_bits: int = 4):
        super().__init__()
        
        self.rank = rank
        self.alpha = alpha
        self.scaling = alpha / rank
        self.quantization_bits = quantization_bits
        
        # Quantize original weights
        self.register_buffer('quantized_weight', self._quantize_weights(original_layer.weight))
        self.register_buffer('scale', torch.tensor(1.0))
        self.register_buffer('zero_point', torch.tensor(0.0))
        
        # LoRA adapters
        self.lora_A = nn.Parameter(torch.randn(rank, original_layer.in_features) * 0.01)
        self.lora_B = nn.Parameter(torch.zeros(original_layer.out_features, rank))
        
        if original_layer.bias is not None:
            self.bias = nn.Parameter(original_layer.bias.clone())
        else:
            self.register_parameter('bias', None)
    
    def _quantize_weights(self, weights: torch.Tensor) -> torch.Tensor:
        """Quantize weights to specified bit precision"""
        min_val = weights.min()
        max_val = weights.max()
        
        # Calculate scale and zero point
        qmin = 0
        qmax = (2 ** self.quantization_bits) - 1
        
        scale = (max_val - min_val) / (qmax - qmin)
        zero_point = qmin - min_val / scale
        
        self.scale = scale
        self.zero_point = zero_point
        
        # Quantize
        quantized = torch.round(weights / scale + zero_point)
        quantized = torch.clamp(quantized, qmin, qmax)
        
        return quantized.to(torch.uint8)
    
    def _dequantize_weights(self) -> torch.Tensor:
        """Dequantize weights for computation"""
        return self.scale * (self.quantized_weight.float() - self.zero_point)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Dequantize weights
        weight = self._dequantize_weights()
        
        # Original linear transformation
        output = F.linear(x, weight, self.bias)
        
        # LoRA adaptation
        lora_output = (x @ self.lora_A.T @ self.lora_B.T) * self.scaling
        
        return output + lora_output

class NeuralNetworkTrainer:
    """Comprehensive trainer for neural networks"""
    
    def __init__(self, model: nn.Module, train_config: TrainingConfig):
        self.model = model
        self.config = train_config
        self.device = torch.device(train_config.device if hasattr(train_config, 'device') else 'cuda' if torch.cuda.is_available() else 'cpu')
        
        # Move model to device
        self.model.to(self.device)
        
        # Setup optimizer and scheduler
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=train_config.learning_rate,
            weight_decay=train_config.weight_decay
        )
        
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=train_config.num_epochs
        )
        
        # Mixed precision training
        self.scaler = GradScaler() if train_config.use_mixed_precision else None
        
        # Training history
        self.train_losses = []
        self.val_losses = []
        self.best_val_loss = float('inf')
        self.patience_counter = 0
        
        logger.info(f"Trainer initialized with {self.model.count_parameters():,} parameters")
    
    def train_epoch(self, train_loader: DataLoader, criterion: nn.Module) -> float:
        """Train for one epoch"""
        self.model.train()
        total_loss = 0.0
        num_batches = len(train_loader)
        
        for batch_idx, (data, targets) in enumerate(train_loader):
            data, targets = data.to(self.device), targets.to(self.device)
            
            self.optimizer.zero_grad()
            
            if self.scaler:
                with autocast():
                    outputs = self.model(data)
                    loss = criterion(outputs, targets)
                
                self.scaler.scale(loss).backward()
                
                if self.config.gradient_clip_norm > 0:
                    self.scaler.unscale_(self.optimizer)
                    nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip_norm)
                
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                outputs = self.model(data)
                loss = criterion(outputs, targets)
                loss.backward()
                
                if self.config.gradient_clip_norm > 0:
                    nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip_norm)
                
                self.optimizer.step()
            
            total_loss += loss.item()
            
            if batch_idx % 100 == 0:
                logger.info(f"Batch {batch_idx}/{num_batches}, Loss: {loss.item():.4f}")
        
        return total_loss / num_batches
    
    def validate(self, val_loader: DataLoader, criterion: nn.Module) -> Tuple[float, float]:
        """Validate model"""
        self.model.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, targets in val_loader:
                data, targets = data.to(self.device), targets.to(self.device)
                
                outputs = self.model(data)
                loss = criterion(outputs, targets)
                total_loss += loss.item()
                
                # Calculate accuracy
                _, predicted = torch.max(outputs.data, 1)
                total += targets.size(0)
                correct += (predicted == targets).sum().item()
        
        avg_loss = total_loss / len(val_loader)
        accuracy = 100.0 * correct / total
        
        return avg_loss, accuracy
    
    def train(self, train_loader: DataLoader, val_loader: DataLoader, 
              criterion: nn.Module) -> Dict[str, List[float]]:
        """Complete training loop"""
        logger.info("Starting training...")
        
        for epoch in range(self.config.num_epochs):
            start_time = time.time()
            
            # Training
            train_loss = self.train_epoch(train_loader, criterion)
            
            # Validation
            val_loss, val_accuracy = self.validate(val_loader, criterion)
            
            # Learning rate scheduling
            self.scheduler.step()
            
            # Record metrics
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            
            epoch_time = time.time() - start_time
            current_lr = self.optimizer.param_groups[0]['lr']
            
            logger.info(
                f"Epoch {epoch+1}/{self.config.num_epochs} - "
                f"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, "
                f"Val Acc: {val_accuracy:.2f}%, LR: {current_lr:.6f}, "
                f"Time: {epoch_time:.2f}s"
            )
            
            # Early stopping
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.patience_counter = 0
                self.save_checkpoint(f"best_model_epoch_{epoch+1}.pt")
            else:
                self.patience_counter += 1
                if self.patience_counter >= self.config.early_stopping_patience:
                    logger.info(f"Early stopping at epoch {epoch+1}")
                    break
            
            # Save checkpoint
            if (epoch + 1) % self.config.save_checkpoint_every == 0:
                self.save_checkpoint(f"checkpoint_epoch_{epoch+1}.pt")
        
        return {
            "train_losses": self.train_losses,
            "val_losses": self.val_losses
        }
    
    def save_checkpoint(self, filepath: str):
        """Save model checkpoint"""
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'best_val_loss': self.best_val_loss
        }
        torch.save(checkpoint, filepath)
        logger.info(f"Checkpoint saved: {filepath}")

class GenerativeAIDemo:
    """Demonstration of various generative AI capabilities"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"GenerativeAI Demo initialized on {self.device}")
    
    def create_synthetic_text_dataset(self, vocab_size: int = 1000, 
                                    seq_length: int = 128, num_samples: int = 10000) -> HFDataset:
        """Create synthetic text dataset for demonstration"""
        
        # Generate random sequences
        data = []
        for _ in range(num_samples):
            sequence = torch.randint(0, vocab_size, (seq_length,)).tolist()
            data.append({"input_ids": sequence})
        
        return HFDataset.from_list(data)
    
    def demonstrate_text_generation(self, vocab_size: int = 5000):
        """Demonstrate text generation capabilities"""
        logger.info("Demonstrating text generation...")
        
        # Create model
        model = GenerativeTextModel(
            vocab_size=vocab_size,
            d_model=512,
            num_heads=8,
            num_layers=6,
            max_seq_length=256
        ).to(self.device)
        
        logger.info(f"Text generation model created with {model.token_embedding.weight.numel():,} parameters")
        
        # Generate sample text
        prompt = torch.randint(1, vocab_size, (1, 10)).to(self.device)
        generated = model.generate(prompt, max_length=50, temperature=0.8, top_k=40)
        
        logger.info(f"Generated sequence length: {generated.size(1)}")
        logger.info(f"Sample tokens: {generated[0, :20].tolist()}")
        
        return model
    
    def demonstrate_lora_fine_tuning(self):
        """Demonstrate LoRA fine-tuning"""
        logger.info("Demonstrating LoRA fine-tuning...")
        
        # Create base model
        config = NetworkConfig(input_size=768, hidden_sizes=[1024, 512], output_size=10)
        base_model = FeedforwardNetwork(config).to(self.device)
        
        # Apply LoRA to specific layers
        lora_layers = {}
        for name, module in base_model.named_modules():
            if isinstance(module, nn.Linear) and 'network.0' in name:  # First layer
                lora_layers[name] = LoRAAdapter(module, rank=16, alpha=16.0)
        
        # Replace layers with LoRA adapters
        for name, lora_layer in lora_layers.items():
            parts = name.split('.')
            parent = base_model
            for part in parts[:-1]:
                parent = getattr(parent, part)
            setattr(parent, parts[-1], lora_layer)
        
        # Count trainable parameters
        total_params = sum(p.numel() for p in base_model.parameters())
        trainable_params = sum(p.numel() for p in base_model.parameters() if p.requires_grad)
        
        logger.info(f"Total parameters: {total_params:,}")
        logger.info(f"Trainable parameters: {trainable_params:,}")
        logger.info(f"Trainable ratio: {100 * trainable_params / total_params:.2f}%")
        
        return base_model
    
    def demonstrate_qlora(self):
        """Demonstrate QLoRA quantization"""
        logger.info("Demonstrating QLoRA...")
        
        # Create model with QLoRA layers
        original_layer = nn.Linear(768, 1024)
        qlora_layer = QLoRALinear(original_layer, rank=16, alpha=16.0, quantization_bits=4)
        
        # Test forward pass
        test_input = torch.randn(32, 768)
        output = qlora_layer(test_input)
        
        # Calculate memory savings
        original_size = original_layer.weight.numel() * 4  # 32-bit floats
        quantized_size = qlora_layer.quantized_weight.numel() * 0.5  # 4-bit quantization
        lora_size = (qlora_layer.lora_A.numel() + qlora_layer.lora_B.numel()) * 4  # 32-bit
        
        total_qlora_size = quantized_size + lora_size
        memory_savings = (original_size - total_qlora_size) / original_size * 100
        
        logger.info(f"Original model size: {original_size / 1024 / 1024:.2f} MB")
        logger.info(f"QLoRA model size: {total_qlora_size / 1024 / 1024:.2f} MB")
        logger.info(f"Memory savings: {memory_savings:.1f}%")
        
        return qlora_layer

def create_multimodal_dataset():
    """Create a synthetic multimodal dataset"""
    
    # Text data
    text_samples = [
        "A beautiful sunset over the ocean",
        "A cat sitting on a windowsill",
        "Modern architecture in the city",
        "Children playing in the park",
        "Mountain landscape with snow"
    ]
    
    # Generate synthetic image data (normally would be real images)
    image_data = [torch.randn(3, 224, 224) for _ in range(len(text_samples))]
    
    # Generate synthetic audio data (normally would be real audio)
    audio_data = [torch.randn(16000) for _ in range(len(text_samples))]  # 1 second at 16kHz
    
    return list(zip(text_samples, image_data, audio_data))

# Comprehensive demonstration
async def main():
    """Main demonstration of neural network architectures and generative AI"""
    
    # Initialize logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    logger.info("=== Neural Networks and Generative AI Demonstration ===")
    
    # Device setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"Using device: {device}")
    
    # 1. Feedforward Network
    logger.info("\n1. Feedforward Network Demonstration")
    ff_config = NetworkConfig(
        input_size=784,  # MNIST-like
        hidden_sizes=[512, 256, 128],
        output_size=10,
        dropout_rate=0.2,
        activation="relu"
    )
    ff_model = FeedforwardNetwork(ff_config)
    logger.info(f"Feedforward model parameters: {ff_model.count_parameters():,}")
    
    # 2. Convolutional Network
    logger.info("\n2. Convolutional Network Demonstration")
    cnn_config = NetworkConfig(
        input_size=32,  # 32x32 images
        hidden_sizes=[256, 128],
        output_size=10,
        dropout_rate=0.2
    )
    cnn_model = ConvolutionalNetwork(
        cnn_config, 
        input_channels=3, 
        feature_maps=[32, 64, 128]
    )
    logger.info(f"CNN model parameters: {cnn_model.count_parameters():,}")
    
    # 3. Recurrent Network
    logger.info("\n3. Recurrent Network Demonstration")
    rnn_config = NetworkConfig(
        input_size=100,  # Feature dimension
        hidden_sizes=[256, 128],
        output_size=5,
        dropout_rate=0.1
    )
    rnn_model = RecurrentNetwork(
        rnn_config, 
        sequence_length=50, 
        rnn_type="LSTM", 
        bidirectional=True
    )
    logger.info(f"RNN model parameters: {rnn_model.count_parameters():,}")
    
    # 4. Transformer Network
    logger.info("\n4. Transformer Network Demonstration")
    transformer_config = NetworkConfig(
        input_size=512,
        hidden_sizes=[],
        output_size=10,
        dropout_rate=0.1
    )
    transformer_model = TransformerNetwork(
        transformer_config,
        d_model=512,
        num_heads=8,
        num_layers=6,
        vocab_size=10000
    )
    logger.info(f"Transformer model parameters: {transformer_model.count_parameters():,}")
    
    # 5. Generative AI Demonstrations
    logger.info("\n5. Generative AI Demonstrations")
    gen_ai_demo = GenerativeAIDemo()
    
    # Text generation
    text_gen_model = gen_ai_demo.demonstrate_text_generation(vocab_size=5000)
    
    # LoRA fine-tuning
    lora_model = gen_ai_demo.demonstrate_lora_fine_tuning()
    
    # QLoRA demonstration
    qlora_layer = gen_ai_demo.demonstrate_qlora()
    
    # 6. Training demonstration
    logger.info("\n6. Training Demonstration")
    
    # Create synthetic dataset
    train_data = torch.randn(1000, 784)
    train_labels = torch.randint(0, 10, (1000,))
    val_data = torch.randn(200, 784)
    val_labels = torch.randint(0, 10, (200,))
    
    train_dataset = torch.utils.data.TensorDataset(train_data, train_labels)
    val_dataset = torch.utils.data.TensorDataset(val_data, val_labels)
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    
    # Training configuration
    train_config = TrainingConfig(
        batch_size=32,
        learning_rate=1e-3,
        num_epochs=5,  # Short demo
        early_stopping_patience=3
    )
    
    # Train model
    trainer = NeuralNetworkTrainer(ff_model, train_config)
    criterion = nn.CrossEntropyLoss()
    
    training_history = trainer.train(train_loader, val_loader, criterion)
    
    logger.info("Training completed!")
    logger.info(f"Final train loss: {training_history['train_losses'][-1]:.4f}")
    logger.info(f"Final val loss: {training_history['val_losses'][-1]:.4f}")
    
    # 7. Multimodal data demonstration
    logger.info("\n7. Multimodal Data Demonstration")
    multimodal_data = create_multimodal_dataset()
    logger.info(f"Created multimodal dataset with {len(multimodal_data)} samples")
    for i, (text, image, audio) in enumerate(multimodal_data[:2]):
        logger.info(f"Sample {i+1}: Text='{text}', Image shape={image.shape}, Audio shape={audio.shape}")
    
    logger.info("\n=== Demonstration Complete ===")

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
````

## Conclusion

The comprehensive exploration of neural networks and generative AI demonstrates the remarkable evolution from simple perceptrons to sophisticated transformer architectures capable of human-like reasoning and creativity. This foundation establishes the theoretical and practical knowledge essential for modern AI development.

**Architectural Diversity** across feedforward, convolutional, recurrent, and transformer networks provides specialized solutions for different data types and tasks. Understanding these architectures enables informed decisions about model selection and hybrid approaches for complex applications.

**Generative Capabilities** represent a paradigm shift in AI applications, moving from discriminative tasks to creative content generation across text, images, audio, and video. The transformer architecture has become the cornerstone of modern generative AI systems.

**Parameter-Efficient Fine-tuning** through LoRA and QLoRA techniques democratizes access to large model customization by dramatically reducing computational requirements while maintaining performance. These methods enable practical deployment of specialized models on consumer hardware.

**Training Optimization** techniques including mixed precision, gradient clipping, learning rate scheduling, and early stopping ensure efficient and stable model training. Advanced optimization strategies are crucial for handling the scale and complexity of modern neural networks.

**Practical Implementation** considerations encompass memory management, computational efficiency, hardware optimization, and production deployment. The framework demonstrates enterprise-ready approaches to neural network development and deployment.

**Quantization and Compression** methods like QLoRA showcase the importance of model efficiency in real-world applications. These techniques enable deployment of powerful models on resource-constrained devices while maintaining acceptable performance levels.

**Future Directions** include multimodal architectures, improved attention mechanisms, more efficient training algorithms, and novel generative paradigms. The foundational understanding provided here enables exploration of cutting-edge developments in neural network research.

This comprehensive framework establishes the essential knowledge base for advanced AI development, providing both theoretical understanding and practical implementation skills necessary for creating next-generation neural network architectures and generative AI systems.