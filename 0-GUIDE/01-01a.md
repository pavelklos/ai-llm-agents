<small>Claude 3.7 Sonnet Thinking</small>
# 01. AI API and First Agent

## Key Terms

- **Large Language Models (LLMs)**: Neural network-based models trained on massive text datasets that can generate human-like text responses.
- **API (Application Programming Interface)**: Standardized interfaces that allow different software systems to communicate.
- **Tool-calling/Function-calling**: Capability of LLMs to invoke external functions during inference.
- **Agent Architecture**: Design patterns for autonomous AI systems that can perceive, reason, and act.
- **Context Window**: Maximum text length an LLM can process in a single interaction.
- **Temperature**: Parameter controlling randomness in model outputs (higher = more creative, lower = more deterministic).

## Understanding LLMs and APIs

Large Language Models represent a significant advancement in natural language processing. These transformer-based models process sequences of tokens (word fragments) through multiple attention layers to generate contextually relevant outputs. APIs provide standardized interfaces to interact with these models.

### Major API Providers:

**OpenAI**
- Models: GPT-4, GPT-3.5-Turbo
- High capability but commercial pricing
- Excellent function-calling capabilities
- Strong at following instructions and reasoning

**Anthropic**
- Models: Claude 3 family (Opus, Sonnet, Haiku)
- Focus on safety and helpful behavior
- Longer context windows (up to 200K tokens)
- Tool-use capabilities via Claude 3 API

**Ollama**
- Open-source framework for running models locally
- Support for Llama-2, Mistral, and other open models
- Lower latency but reduced capabilities
- No usage costs but requires local hardware

**HuggingFace**
- Hub for thousands of open-source models
- Inference API for hosted model access
- Wide variety of specialized models
- Transformers library for model integration

## Model Parameters and Provider Differences

```python
# Key parameters when calling LLM APIs
parameters = {
    # Controls randomness: 0.0 = deterministic, 1.0 = maximum randomness
    "temperature": 0.7,
    
    # Alternative to temperature, selects from top probability tokens
    "top_p": 0.95,
    
    # Maximum number of tokens to generate
    "max_tokens": 1024,
    
    # Stop sequences that terminate generation
    "stop": ["\n\n", "Human:", "Assistant:"],
    
    # Penalty for new tokens based on their existing frequency
    "frequency_penalty": 0.0,
    
    # Penalty for repeating tokens
    "presence_penalty": 0.0,
    
    # Controls determinism across API calls
    "seed": 42  # Not supported by all providers
}
```

### Provider Comparison:

| Feature | OpenAI | Anthropic | Ollama | HuggingFace |
|---------|--------|-----------|--------|-------------|
| Pricing | $0.01-0.10/1K tokens | $0.015-0.15/1K tokens | Free (local) | Various |
| Context | 128K tokens | Up to 200K tokens | Model-dependent | Model-dependent |
| Tools | Advanced | Basic (improving) | Limited | Model-dependent |
| Latency | Very low | Low | Depends on hardware | Varies |
| Control | High | Medium | High | High |

## Tool-calling and Agent Architecture

Tool-calling allows LLMs to interact with external systems by invoking functions. This capability transforms models from passive text generators into active computational agents.

### Basic Agent Architecture:

1. **Input**: User query or system trigger
2. **Reasoning**: LLM processes query and decides actions
3. **Tool Selection**: Agent determines which tools to use
4. **Tool Execution**: External functions are called with parameters
5. **Result Interpretation**: LLM processes tool outputs
6. **Response Generation**: Final answer synthesized from reasoning and tool results

## Python Implementation

Let's implement a practical example using the dotenv package for API key management and accessing multiple providers:

```python
import os
import json
import requests
from typing import List, Dict, Any, Callable, Optional
from dotenv import load_dotenv
import anthropic
import openai
import ollama

# Load environment variables
load_dotenv()

class LLMProvider:
    """Base class for LLM provider implementations"""
    
    def generate(self, prompt: str, system_prompt: str = None, 
                 temperature: float = 0.7, max_tokens: int = 1000) -> str:
        """Generate text from prompt"""
        raise NotImplementedError("Subclasses must implement generate()")
        
    def generate_with_tools(self, prompt: str, tools: List[Dict], 
                            system_prompt: str = None, temperature: float = 0.7) -> Dict:
        """Generate text with tool-calling capabilities"""
        raise NotImplementedError("Subclasses must implement generate_with_tools()")

class OpenAIProvider(LLMProvider):
    """OpenAI API provider implementation"""
    
    def __init__(self, model: str = "gpt-3.5-turbo"):
        self.client = openai.OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
        self.model = model
        
    def generate(self, prompt: str, system_prompt: str = None, 
                 temperature: float = 0.7, max_tokens: int = 1000) -> str:
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens
        )
        
        return response.choices[0].message.content
    
    def generate_with_tools(self, prompt: str, tools: List[Dict], 
                           system_prompt: str = None, temperature: float = 0.7) -> Dict:
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            tools=tools,
            temperature=temperature
        )
        
        return {
            "content": response.choices[0].message.content,
            "tool_calls": response.choices[0].message.tool_calls
        }

class AnthropicProvider(LLMProvider):
    """Anthropic API provider implementation"""
    
    def __init__(self, model: str = "claude-3-sonnet-20240229"):
        self.client = anthropic.Anthropic(api_key=os.environ.get("ANTHROPIC_API_KEY"))
        self.model = model
        
    def generate(self, prompt: str, system_prompt: str = None, 
                 temperature: float = 0.7, max_tokens: int = 1000) -> str:
        message = self.client.messages.create(
            model=self.model,
            max_tokens=max_tokens,
            temperature=temperature,
            system=system_prompt,
            messages=[{"role": "user", "content": prompt}]
        )
        
        return message.content[0].text
    
    def generate_with_tools(self, prompt: str, tools: List[Dict], 
                           system_prompt: str = None, temperature: float = 0.7) -> Dict:
        # Convert OpenAI-style tool definitions to Anthropic format
        anthropic_tools = []
        for tool in tools:
            anthropic_tools.append({
                "name": tool["function"]["name"],
                "description": tool["function"].get("description", ""),
                "input_schema": tool["function"]["parameters"]
            })
            
        message = self.client.messages.create(
            model=self.model,
            temperature=temperature,
            system=system_prompt,
            messages=[{"role": "user", "content": prompt}],
            tools=anthropic_tools
        )
        
        tool_calls = []
        content = None
        
        for content_block in message.content:
            if content_block.type == "text":
                content = content_block.text
            elif content_block.type == "tool_use":
                tool_calls.append({
                    "name": content_block.name,
                    "arguments": content_block.input
                })
                
        return {
            "content": content,
            "tool_calls": tool_calls
        }

class OllamaProvider(LLMProvider):
    """Ollama API provider implementation"""
    
    def __init__(self, model: str = "llama2"):
        self.model = model
        self.api_base = "http://localhost:11434/api"
        
    def generate(self, prompt: str, system_prompt: str = None, 
                 temperature: float = 0.7, max_tokens: int = 1000) -> str:
        headers = {"Content-Type": "application/json"}
        data = {
            "model": self.model,
            "prompt": prompt,
            "temperature": temperature,
            "num_predict": max_tokens,
        }
        
        if system_prompt:
            data["system"] = system_prompt
            
        response = requests.post(f"{self.api_base}/generate", headers=headers, json=data)
        response.raise_for_status()
        
        result = ""
        for line in response.text.strip().split('\n'):
            try:
                chunk = json.loads(line)
                if "response" in chunk:
                    result += chunk["response"]
            except:
                pass
                
        return result
    
    def generate_with_tools(self, prompt: str, tools: List[Dict], 
                           system_prompt: str = None, temperature: float = 0.7) -> Dict:
        # Ollama doesn't natively support tool calling, so we'll implement a basic version
        tool_descriptions = []
        for tool in tools:
            params = json.dumps(tool["function"]["parameters"], indent=2)
            desc = f"""
            Tool Name: {tool["function"]["name"]}
            Description: {tool["function"].get("description", "")}
            Parameters: {params}
            """
            tool_descriptions.append(desc)
            
        enhanced_prompt = f"""
        {prompt}
        
        You have access to the following tools:
        
        {'\n'.join(tool_descriptions)}
        
        To use a tool, respond in the following JSON format:
        {{
          "thought": "your reasoning process",
          "tool": "tool_name",
          "parameters": {{
            "param1": "value1",
            "param2": "value2"
          }}
        }}
        
        If you don't need to use a tool, respond normally.
        """
        
        enhanced_system = system_prompt or "You are a helpful AI assistant with access to tools."
        
        response = self.generate(enhanced_prompt, enhanced_system, temperature, max_tokens)
        
        # Try to extract JSON if present
        try:
            # Look for JSON-like structure within the response
            start_idx = response.find('{')
            end_idx = response.rfind('}') + 1
            
            if start_idx >= 0 and end_idx > start_idx:
                json_str = response[start_idx:end_idx]
                tool_call = json.loads(json_str)
                
                return {
                    "content": response.replace(json_str, "").strip(),
                    "tool_calls": [
                        {
                            "name": tool_call.get("tool"),
                            "arguments": tool_call.get("parameters", {})
                        }
                    ] if "tool" in tool_call else []
                }
        except:
            pass
            
        return {
            "content": response,
            "tool_calls": []
        }

# Agent implementation
class Agent:
    """Simple LLM-based agent with tool-calling capabilities"""
    
    def __init__(self, llm_provider: LLMProvider, system_prompt: str = None):
        self.llm = llm_provider
        self.system_prompt = system_prompt or "You are a helpful AI assistant."
        self.tools = {}
        
    def register_tool(self, name: str, function: Callable, description: str = "", 
                     parameters: Dict = None):
        """Register a tool/function for the agent to use"""
        self.tools[name] = {
            "function": function,
            "schema": {
                "name": name,
                "description": description,
                "parameters": parameters or {}
            }
        }
        
    def get_tools_schema(self) -> List[Dict]:
        """Get OpenAI-compatible tools schema"""
        return [
            {
                "type": "function",
                "function": tool["schema"]
            } for tool in self.tools.values()
        ]
        
    def run(self, prompt: str, temperature: float = 0.7) -> str:
        """Run the agent with access to tools"""
        if not self.tools:
            # No tools registered, just generate text
            return self.llm.generate(prompt, self.system_prompt, temperature)
        
        # Generate response with tools
        response = self.llm.generate_with_tools(
            prompt, 
            self.get_tools_schema(),
            self.system_prompt,
            temperature
        )
        
        content = response.get("content", "")
        tool_calls = response.get("tool_calls", [])
        
        if not tool_calls:
            return content
            
        # Execute tool calls and format results
        results = []
        for call in tool_calls:
            if isinstance(call, dict):
                tool_name = call.get("name")
                arguments = call.get("arguments")
                if isinstance(arguments, str):
                    try:
                        arguments = json.loads(arguments)
                    except:
                        arguments = {}
            else:
                # Handle OpenAI's structured format
                tool_name = call.function.name
                try:
                    arguments = json.loads(call.function.arguments)
                except:
                    arguments = {}
                
            if tool_name in self.tools:
                try:
                    result = self.tools[tool_name]["function"](**arguments)
                    results.append(f"Tool: {tool_name}\nResult: {result}")
                except Exception as e:
                    results.append(f"Tool: {tool_name}\nError: {str(e)}")
            else:
                results.append(f"Tool: {tool_name}\nError: Tool not found")
                
        # If we have tool results, send them back to the LLM for interpretation
        if results:
            follow_up_prompt = f"""
            I used the tools you requested. Here are the results:
            
            {"\n\n".join(results)}
            
            Based on these results, please provide your final response to the original query:
            "{prompt}"
            """
            
            final_response = self.llm.generate(
                follow_up_prompt,
                self.system_prompt,
                temperature
            )
            
            return final_response
            
        return content

# Example usage
def weather_api(location: str) -> str:
    """Simulated weather API function"""
    # In a real implementation, this would call an actual weather API
    weather_data = {
        "New York": {"temp": 72, "conditions": "Partly Cloudy"},
        "London": {"temp": 65, "conditions": "Rainy"},
        "Tokyo": {"temp": 80, "conditions": "Sunny"},
        "Sydney": {"temp": 70, "conditions": "Clear"}
    }
    
    if location in weather_data:
        data = weather_data[location]
        return f"Temperature: {data['temp']}°F, Conditions: {data['conditions']}"
    else:
        return f"Weather data for {location} not found."

def calculator(operation: str, x: float, y: float) -> str:
    """Simple calculator function"""
    if operation == "add":
        return str(x + y)
    elif operation == "subtract":
        return str(x - y)
    elif operation == "multiply":
        return str(x * y)
    elif operation == "divide":
        if y == 0:
            return "Error: Division by zero"
        return str(x / y)
    else:
        return f"Unsupported operation: {operation}"

# Main demonstration
if __name__ == "__main__":
    # Select provider (uncomment the one you want to use)
    provider = OpenAIProvider(model="gpt-3.5-turbo")
    # provider = AnthropicProvider(model="claude-3-sonnet-20240229")
    # provider = OllamaProvider(model="llama2")
    
    # Create agent
    agent = Agent(
        provider, 
        system_prompt="You are a helpful assistant that can use tools to answer questions."
    )
    
    # Register tools
    agent.register_tool(
        name="get_weather",
        function=weather_api,
        description="Get the current weather for a location",
        parameters={
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "The city name, e.g., 'New York', 'London'"
                }
            },
            "required": ["location"]
        }
    )
    
    agent.register_tool(
        name="calculator",
        function=calculator,
        description="Perform simple calculations",
        parameters={
            "type": "object",
            "properties": {
                "operation": {
                    "type": "string",
                    "enum": ["add", "subtract", "multiply", "divide"],
                    "description": "The mathematical operation to perform"
                },
                "x": {
                    "type": "number",
                    "description": "First number"
                },
                "y": {
                    "type": "number",
                    "description": "Second number"
                }
            },
            "required": ["operation", "x", "y"]
        }
    )
    
    # Test the agent
    queries = [
        "What's the weather like in Tokyo?",
        "Can you calculate 135 divided by 15?",
        "If it's 72°F in New York and 65°F in London, what's the average temperature?",
        "Tell me a joke about programming."
    ]
    
    for query in queries:
        print(f"\n>>> QUERY: {query}")
        response = agent.run(query)
        print(f"AGENT: {response}")
```

## Practical Exercise: Creating a Simple Agent

Let's build a task management agent that can add, list, and complete tasks:

```python
import os
from datetime import datetime
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv
import openai
import json

# Load environment variables
load_dotenv()

# Initialize OpenAI client
client = openai.OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

# Task storage (in a real application, this would be a database)
tasks = []

# Tool functions
def add_task(title: str, description: str, due_date: Optional[str] = None, priority: Optional[str] = None) -> str:
    """Add a new task to the task list"""
    task_id = len(tasks) + 1
    task = {
        "id": task_id,
        "title": title,
        "description": description,
        "due_date": due_date,
        "priority": priority or "medium",
        "completed": False,
        "created_at": datetime.now().isoformat()
    }
    tasks.append(task)
    return f"Task added with ID: {task_id}"

def list_tasks(filter_by: Optional[str] = None) -> str:
    """List all tasks or filter by status"""
    if not tasks:
        return "No tasks found."
        
    filtered_tasks = tasks
    if filter_by == "completed":
        filtered_tasks = [t for t in tasks if t["completed"]]
    elif filter_by == "pending":
        filtered_tasks = [t for t in tasks if not t["completed"]]
        
    result = []
    for task in filtered_tasks:
        status = "✅" if task["completed"] else "⏳"
        due = f", Due: {task['due_date']}" if task["due_date"] else ""
        result.append(f"{task['id']}. {status} {task['title']} (Priority: {task['priority']}{due})")
        
    return "\n".join(result) if result else "No matching tasks found."

def complete_task(task_id: int) -> str:
    """Mark a task as completed"""
    for task in tasks:
        if task["id"] == task_id:
            if task["completed"]:
                return f"Task {task_id} is already marked as completed."
            task["completed"] = True
            return f"Task {task_id} marked as completed."
    return f"Task with ID {task_id} not found."

def delete_task(task_id: int) -> str:
    """Delete a task from the list"""
    global tasks
    for i, task in enumerate(tasks):
        if task["id"] == task_id:
            removed = tasks.pop(i)
            return f"Task '{removed['title']}' with ID {task_id} was deleted."
    return f"Task with ID {task_id} not found."

# Tool definitions for the API
tools = [
    {
        "type": "function",
        "function": {
            "name": "add_task",
            "description": "Add a new task to the task list",
            "parameters": {
                "type": "object",
                "properties": {
                    "title": {
                        "type": "string",
                        "description": "Short title of the task"
                    },
                    "description": {
                        "type": "string",
                        "description": "Detailed description of the task"
                    },
                    "due_date": {
                        "type": "string",
                        "description": "Due date in YYYY-MM-DD format (optional)"
                    },
                    "priority": {
                        "type": "string",
                        "enum": ["low", "medium", "high"],
                        "description": "Task priority (optional, defaults to medium)"
                    }
                },
                "required": ["title", "description"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "list_tasks",
            "description": "List all tasks or filter by status",
            "parameters": {
                "type": "object",
                "properties": {
                    "filter_by": {
                        "type": "string",
                        "enum": ["all", "completed", "pending"],
                        "description": "Filter tasks by status (optional, defaults to 'all')"
                    }
                }
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "complete_task",
            "description": "Mark a task as completed",
            "parameters": {
                "type": "object",
                "properties": {
                    "task_id": {
                        "type": "integer",
                        "description": "ID of the task to mark as completed"
                    }
                },
                "required": ["task_id"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "delete_task",
            "description": "Delete a task from the list",
            "parameters": {
                "type": "object",
                "properties": {
                    "task_id": {
                        "type": "integer",
                        "description": "ID of the task to delete"
                    }
                },
                "required": ["task_id"]
            }
        }
    }
]

class TaskAgent:
    """Task management agent using LLM and tool-calling"""
    
    def __init__(self, model: str = "gpt-3.5-turbo"):
        self.model = model
        self.messages = [
            {
                "role": "system", 
                "content": """
                You are a task management assistant.
                You can help users manage their tasks by adding, listing, completing, and deleting tasks.
                Always use the provided tools to interact with the task system.
                Be helpful and concise in your responses.
                """
            }
        ]
        
    def process_message(self, user_message: str) -> str:
        """Process user message and return agent response"""
        # Add user message to conversation history
        self.messages.append({"role": "user", "content": user_message})
        
        # First, get response from LLM
        response = client.chat.completions.create(
            model=self.model,
            messages=self.messages,
            tools=tools,
            tool_choice="auto"
        )
        
        assistant_message = response.choices[0].message
        self.messages.append(assistant_message)
        
        # Check if the model wants to call a function
        if assistant_message.tool_calls:
            # Process each tool call
            for tool_call in assistant_message.tool_calls:
                function_name = tool_call.function.name
                function_args = json.loads(tool_call.function.arguments)
                
                # Get the appropriate function
                available_functions = {
                    "add_task": add_task,
                    "list_tasks": list_tasks,
                    "complete_task": complete_task,
                    "delete_task": delete_task
                }
                
                function_to_call = available_functions.get(function_name)
                if function_to_call:
                    # Call the function
                    function_response = function_to_call(**function_args)
                    
                    # Add function response to messages
                    self.messages.append({
                        "tool_call_id": tool_call.id,
                        "role": "tool",
                        "name": function_name,
                        "content": function_response
                    })
            
            # Get a new response from the model
            second_response = client.chat.completions.create(
                model=self.model,
                messages=self.messages
            )
            
            final_response = second_response.choices[0].message.content
            self.messages.append({"role": "assistant", "content": final_response})
            return final_response
        else:
            # Model did not call a function
            return assistant_message.content

if __name__ == "__main__":
    agent = TaskAgent()
    print("Task Management Assistant (type 'exit' to quit)")
    
    while True:
        user_input = input("\nYou: ")
        if user_input.lower() in ["exit", "quit"]:
            break
            
        response = agent.process_message(user_input)
        print(f"\nAssistant: {response}")
```

## Conclusion

In this comprehensive introduction to AI APIs and agents, we've explored the fundamental technologies powering modern AI systems. We've compared major LLM providers including OpenAI, Anthropic, Ollama, and HuggingFace, understanding their differences in capabilities, pricing, and integration patterns.

We've covered essential model parameters that control generation behavior and implemented a versatile framework for interacting with different provider APIs using a consistent interface. The agent architecture we've built demonstrates how to leverage tool-calling capabilities to create AI systems that can reason about and interact with external functions.

The practical task management agent shows a complete example of combining conversational AI with tool-calling to create useful applications. This foundation serves as a starting point for more complex agent systems that can handle autonomous workflows, interact with databases, and perform specialized tasks.

As you continue developing AI agents, remember to:
1. Consider the trade-offs between different model providers based on your specific needs
2. Design clear tool interfaces with well-defined parameters
3. Implement proper error handling for tool execution
4. Structure agent reasoning to make effective use of available tools
5. Iterate on prompts and system messages to improve agent performance

These fundamentals will serve as building blocks for more advanced agent architectures in subsequent sections.