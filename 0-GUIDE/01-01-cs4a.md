<small>Claude Sonnet 4</small>
# 01. AI API and First Agent

## Key Terms

**Large Language Models (LLMs)**: Advanced neural networks trained on vast amounts of text data that can understand and generate human-like text, capable of performing various language tasks including conversation, code generation, analysis, and reasoning.

**API (Application Programming Interface)**: A standardized interface that allows different software applications to communicate with each other, enabling developers to access LLM capabilities through structured requests and responses over HTTP protocols.

**Tool-calling (Function Calling)**: A capability that allows LLMs to invoke external functions, APIs, or tools during conversation, enabling agents to perform actions beyond text generation such as calculations, database queries, or API interactions.

**Agent Architecture**: The structural design pattern for AI systems that includes components for reasoning, planning, memory, and tool usage, enabling autonomous decision-making and task execution.

**Tokenization**: The process of converting text into numerical tokens that can be processed by neural networks, where each token represents a word, subword, or character depending on the tokenization strategy.

**Context Window**: The maximum number of tokens an LLM can process in a single request, determining how much conversation history and input the model can consider when generating responses.

**Temperature**: A parameter controlling the randomness of LLM outputs, where lower values produce more deterministic responses and higher values increase creativity and variability.

**Prompt Engineering**: The practice of crafting effective input prompts to guide LLM behavior and achieve desired outputs, involving techniques like few-shot learning, chain-of-thought reasoning, and role-playing.

## Understanding LLMs and API Providers

The landscape of Large Language Model providers offers diverse capabilities, pricing models, and integration approaches. Understanding the fundamental differences between providers enables informed decisions for agent development and deployment strategies.

### Comprehensive LLM Provider Integration Framework

````python
import os
import json
import asyncio
import time
from typing import Dict, List, Any, Optional, Union, Callable
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
from datetime import datetime
import logging
from functools import wraps
from enum import Enum

import openai
import anthropic
import requests
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
from dotenv import load_dotenv

load_dotenv()

class ModelProvider(Enum):
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    OLLAMA = "ollama"
    HUGGINGFACE = "huggingface"

@dataclass
class ModelCapabilities:
    """Defines capabilities and limitations of a model"""
    max_tokens: int
    supports_functions: bool
    supports_vision: bool
    supports_streaming: bool
    context_window: int
    input_cost_per_token: float
    output_cost_per_token: float
    rate_limit_rpm: int
    rate_limit_tpm: int

@dataclass
class ChatMessage:
    """Standardized message format across providers"""
    role: str  # system, user, assistant, function
    content: str
    function_call: Optional[Dict[str, Any]] = None
    tool_calls: Optional[List[Dict[str, Any]]] = None
    name: Optional[str] = None

@dataclass
class FunctionDefinition:
    """Standardized function definition for tool calling"""
    name: str
    description: str
    parameters: Dict[str, Any]
    required: List[str] = field(default_factory=list)

class LLMProviderInterface(ABC):
    """Abstract base class for LLM providers"""
    
    def __init__(self, api_key: str, model_name: str):
        self.api_key = api_key
        self.model_name = model_name
        self.capabilities = self._get_model_capabilities()
    
    @abstractmethod
    async def chat_completion(
        self,
        messages: List[ChatMessage],
        temperature: float = 0.7,
        max_tokens: int = 1000,
        functions: Optional[List[FunctionDefinition]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """Generate chat completion"""
        pass
    
    @abstractmethod
    async def stream_completion(
        self,
        messages: List[ChatMessage],
        temperature: float = 0.7,
        max_tokens: int = 1000,
        **kwargs
    ) -> AsyncIterator[str]:
        """Stream chat completion"""
        pass
    
    @abstractmethod
    def _get_model_capabilities(self) -> ModelCapabilities:
        """Get model capabilities and limitations"""
        pass
    
    @abstractmethod
    def count_tokens(self, text: str) -> int:
        """Count tokens in text"""
        pass

class OpenAIProvider(LLMProviderInterface):
    """OpenAI API provider implementation"""
    
    def __init__(self, api_key: str = None, model_name: str = "gpt-4-turbo-preview"):
        self.client = openai.OpenAI(api_key=api_key or os.getenv('OPENAI_API_KEY'))
        super().__init__(api_key, model_name)
    
    async def chat_completion(
        self,
        messages: List[ChatMessage],
        temperature: float = 0.7,
        max_tokens: int = 1000,
        functions: Optional[List[FunctionDefinition]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """Generate OpenAI chat completion"""
        
        # Convert messages to OpenAI format
        openai_messages = [
            {
                "role": msg.role,
                "content": msg.content,
                **({"function_call": msg.function_call} if msg.function_call else {}),
                **({"tool_calls": msg.tool_calls} if msg.tool_calls else {}),
                **({"name": msg.name} if msg.name else {})
            }
            for msg in messages
        ]
        
        # Prepare function calling parameters
        completion_kwargs = {
            "model": self.model_name,
            "messages": openai_messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            **kwargs
        }
        
        if functions:
            # Convert to OpenAI tools format
            tools = [
                {
                    "type": "function",
                    "function": {
                        "name": func.name,
                        "description": func.description,
                        "parameters": func.parameters
                    }
                }
                for func in functions
            ]
            completion_kwargs["tools"] = tools
            completion_kwargs["tool_choice"] = "auto"
        
        try:
            response = await asyncio.to_thread(
                self.client.chat.completions.create,
                **completion_kwargs
            )
            
            return {
                "content": response.choices[0].message.content,
                "function_calls": response.choices[0].message.tool_calls,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                },
                "model": response.model,
                "finish_reason": response.choices[0].finish_reason
            }
            
        except Exception as e:
            return {"error": str(e)}
    
    async def stream_completion(
        self,
        messages: List[ChatMessage],
        temperature: float = 0.7,
        max_tokens: int = 1000,
        **kwargs
    ):
        """Stream OpenAI chat completion"""
        
        openai_messages = [{"role": msg.role, "content": msg.content} for msg in messages]
        
        try:
            stream = await asyncio.to_thread(
                self.client.chat.completions.create,
                model=self.model_name,
                messages=openai_messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=True,
                **kwargs
            )
            
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            yield f"Error: {str(e)}"
    
    def _get_model_capabilities(self) -> ModelCapabilities:
        """Get OpenAI model capabilities"""
        
        model_specs = {
            "gpt-4-turbo-preview": ModelCapabilities(
                max_tokens=4096,
                supports_functions=True,
                supports_vision=True,
                supports_streaming=True,
                context_window=128000,
                input_cost_per_token=0.00001,
                output_cost_per_token=0.00003,
                rate_limit_rpm=500,
                rate_limit_tpm=30000
            ),
            "gpt-3.5-turbo": ModelCapabilities(
                max_tokens=4096,
                supports_functions=True,
                supports_vision=False,
                supports_streaming=True,
                context_window=16384,
                input_cost_per_token=0.0000015,
                output_cost_per_token=0.000002,
                rate_limit_rpm=3500,
                rate_limit_tpm=60000
            )
        }
        
        return model_specs.get(self.model_name, model_specs["gpt-3.5-turbo"])
    
    def count_tokens(self, text: str) -> int:
        """Count tokens using tiktoken"""
        try:
            import tiktoken
            encoding = tiktoken.encoding_for_model(self.model_name)
            return len(encoding.encode(text))
        except ImportError:
            # Fallback approximation
            return len(text.split()) * 1.3

class AnthropicProvider(LLMProviderInterface):
    """Anthropic Claude API provider implementation"""
    
    def __init__(self, api_key: str = None, model_name: str = "claude-3-sonnet-20240229"):
        self.client = anthropic.Anthropic(api_key=api_key or os.getenv('ANTHROPIC_API_KEY'))
        super().__init__(api_key, model_name)
    
    async def chat_completion(
        self,
        messages: List[ChatMessage],
        temperature: float = 0.7,
        max_tokens: int = 1000,
        functions: Optional[List[FunctionDefinition]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """Generate Anthropic chat completion"""
        
        # Separate system message from conversation
        system_message = ""
        conversation_messages = []
        
        for msg in messages:
            if msg.role == "system":
                system_message = msg.content
            else:
                conversation_messages.append({
                    "role": msg.role,
                    "content": msg.content
                })
        
        completion_kwargs = {
            "model": self.model_name,
            "messages": conversation_messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            **kwargs
        }
        
        if system_message:
            completion_kwargs["system"] = system_message
        
        # Anthropic doesn't support function calling in the same way as OpenAI
        # We'll simulate it by including function descriptions in the system message
        if functions:
            function_descriptions = "\n\n".join([
                f"Function: {func.name}\nDescription: {func.description}\nParameters: {json.dumps(func.parameters, indent=2)}"
                for func in functions
            ])
            
            system_message_with_functions = f"{system_message}\n\nAvailable functions:\n{function_descriptions}\n\nTo call a function, respond with JSON in the format: {{\"function_call\": {{\"name\": \"function_name\", \"arguments\": {{...}}}}}}"
            completion_kwargs["system"] = system_message_with_functions
        
        try:
            response = await asyncio.to_thread(
                self.client.messages.create,
                **completion_kwargs
            )
            
            content = response.content[0].text if response.content else ""
            
            # Parse potential function calls
            function_calls = None
            if functions and content.strip().startswith('{'):
                try:
                    parsed = json.loads(content)
                    if "function_call" in parsed:
                        function_calls = [parsed["function_call"]]
                except json.JSONDecodeError:
                    pass
            
            return {
                "content": content,
                "function_calls": function_calls,
                "usage": {
                    "prompt_tokens": response.usage.input_tokens,
                    "completion_tokens": response.usage.output_tokens,
                    "total_tokens": response.usage.input_tokens + response.usage.output_tokens
                },
                "model": response.model,
                "finish_reason": response.stop_reason
            }
            
        except Exception as e:
            return {"error": str(e)}
    
    async def stream_completion(
        self,
        messages: List[ChatMessage],
        temperature: float = 0.7,
        max_tokens: int = 1000,
        **kwargs
    ):
        """Stream Anthropic chat completion"""
        
        system_message = ""
        conversation_messages = []
        
        for msg in messages:
            if msg.role == "system":
                system_message = msg.content
            else:
                conversation_messages.append({"role": msg.role, "content": msg.content})
        
        completion_kwargs = {
            "model": self.model_name,
            "messages": conversation_messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": True,
            **kwargs
        }
        
        if system_message:
            completion_kwargs["system"] = system_message
        
        try:
            async with self.client.messages.stream(**completion_kwargs) as stream:
                async for text in stream.text_stream:
                    yield text
                    
        except Exception as e:
            yield f"Error: {str(e)}"
    
    def _get_model_capabilities(self) -> ModelCapabilities:
        """Get Anthropic model capabilities"""
        
        model_specs = {
            "claude-3-opus-20240229": ModelCapabilities(
                max_tokens=4096,
                supports_functions=False,  # Simulated through prompting
                supports_vision=True,
                supports_streaming=True,
                context_window=200000,
                input_cost_per_token=0.000015,
                output_cost_per_token=0.000075,
                rate_limit_rpm=1000,
                rate_limit_tpm=80000
            ),
            "claude-3-sonnet-20240229": ModelCapabilities(
                max_tokens=4096,
                supports_functions=False,
                supports_vision=True,
                supports_streaming=True,
                context_window=200000,
                input_cost_per_token=0.000003,
                output_cost_per_token=0.000015,
                rate_limit_rpm=1000,
                rate_limit_tpm=80000
            )
        }
        
        return model_specs.get(self.model_name, model_specs["claude-3-sonnet-20240229"])
    
    def count_tokens(self, text: str) -> int:
        """Count tokens (approximation for Anthropic)"""
        return len(text.split()) * 1.2

class OllamaProvider(LLMProviderInterface):
    """Ollama local LLM provider implementation"""
    
    def __init__(self, api_key: str = None, model_name: str = "llama2", base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        super().__init__(api_key, model_name)
    
    async def chat_completion(
        self,
        messages: List[ChatMessage],
        temperature: float = 0.7,
        max_tokens: int = 1000,
        functions: Optional[List[FunctionDefinition]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """Generate Ollama chat completion"""
        
        # Convert messages to Ollama format
        ollama_messages = [
            {"role": msg.role, "content": msg.content}
            for msg in messages
        ]
        
        payload = {
            "model": self.model_name,
            "messages": ollama_messages,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens
            },
            "stream": False
        }
        
        # Add function calling simulation
        if functions:
            system_prompt = "You are an AI assistant with access to functions. When you need to call a function, respond with JSON in the format: {\"function_call\": {\"name\": \"function_name\", \"arguments\": {...}}}\n\nAvailable functions:\n"
            for func in functions:
                system_prompt += f"- {func.name}: {func.description}\n  Parameters: {json.dumps(func.parameters)}\n"
            
            # Add system message if not present or prepend to existing
            if not ollama_messages or ollama_messages[0]["role"] != "system":
                ollama_messages.insert(0, {"role": "system", "content": system_prompt})
            else:
                ollama_messages[0]["content"] = system_prompt + "\n\n" + ollama_messages[0]["content"]
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.base_url}/api/chat",
                    json=payload
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        content = result.get("message", {}).get("content", "")
                        
                        # Parse potential function calls
                        function_calls = None
                        if functions and content.strip().startswith('{'):
                            try:
                                parsed = json.loads(content)
                                if "function_call" in parsed:
                                    function_calls = [parsed["function_call"]]
                            except json.JSONDecodeError:
                                pass
                        
                        return {
                            "content": content,
                            "function_calls": function_calls,
                            "usage": {
                                "prompt_tokens": result.get("prompt_eval_count", 0),
                                "completion_tokens": result.get("eval_count", 0),
                                "total_tokens": result.get("prompt_eval_count", 0) + result.get("eval_count", 0)
                            },
                            "model": self.model_name,
                            "finish_reason": "stop"
                        }
                    else:
                        return {"error": f"HTTP {response.status}: {await response.text()}"}
                        
        except Exception as e:
            return {"error": str(e)}
    
    async def stream_completion(
        self,
        messages: List[ChatMessage],
        temperature: float = 0.7,
        max_tokens: int = 1000,
        **kwargs
    ):
        """Stream Ollama chat completion"""
        
        ollama_messages = [
            {"role": msg.role, "content": msg.content}
            for msg in messages
        ]
        
        payload = {
            "model": self.model_name,
            "messages": ollama_messages,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens
            },
            "stream": True
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.base_url}/api/chat",
                    json=payload
                ) as response:
                    async for line in response.content:
                        if line:
                            try:
                                chunk = json.loads(line.decode('utf-8'))
                                if "message" in chunk and "content" in chunk["message"]:
                                    yield chunk["message"]["content"]
                            except json.JSONDecodeError:
                                continue
                                
        except Exception as e:
            yield f"Error: {str(e)}"
    
    def _get_model_capabilities(self) -> ModelCapabilities:
        """Get Ollama model capabilities"""
        
        # Default capabilities for local models
        return ModelCapabilities(
            max_tokens=2048,
            supports_functions=False,  # Simulated through prompting
            supports_vision=False,
            supports_streaming=True,
            context_window=4096,
            input_cost_per_token=0.0,  # Local deployment
            output_cost_per_token=0.0,
            rate_limit_rpm=0,  # No API limits
            rate_limit_tpm=0
        )
    
    def count_tokens(self, text: str) -> int:
        """Count tokens (approximation)"""
        return len(text.split()) * 1.3

class HuggingFaceProvider(LLMProviderInterface):
    """HuggingFace models provider implementation"""
    
    def __init__(self, api_key: str = None, model_name: str = "microsoft/DialoGPT-medium"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.tokenizer = None
        self.model = None
        self.pipeline = None
        super().__init__(api_key, model_name)
        self._load_model()
    
    def _load_model(self):
        """Load HuggingFace model and tokenizer"""
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForCausalLM.from_pretrained(self.model_name)
            
            # Add padding token if not present
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            self.pipeline = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                device=0 if torch.cuda.is_available() else -1
            )
            
        except Exception as e:
            logging.error(f"Failed to load HuggingFace model: {e}")
    
    async def chat_completion(
        self,
        messages: List[ChatMessage],
        temperature: float = 0.7,
        max_tokens: int = 1000,
        functions: Optional[List[FunctionDefinition]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """Generate HuggingFace chat completion"""
        
        if not self.pipeline:
            return {"error": "Model not loaded"}
        
        # Convert messages to prompt text
        prompt = self._messages_to_prompt(messages)
        
        # Add function calling simulation
        if functions:
            function_descriptions = "\n".join([
                f"Function {func.name}: {func.description}"
                for func in functions
            ])
            prompt = f"Available functions:\n{function_descriptions}\n\n{prompt}"
        
        try:
            result = await asyncio.to_thread(
                self.pipeline,
                prompt,
                max_length=len(prompt.split()) + max_tokens,
                temperature=temperature,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                **kwargs
            )
            
            generated_text = result[0]['generated_text']
            content = generated_text[len(prompt):].strip()
            
            # Parse potential function calls
            function_calls = None
            if functions and "function_call" in content.lower():
                # Simple pattern matching for function calls
                import re
                pattern = r'function_call:\s*(\w+)\s*\((.*?)\)'
                match = re.search(pattern, content)
                if match:
                    func_name = match.group(1)
                    args_str = match.group(2)
                    try:
                        args = json.loads(f'{{{args_str}}}')
                        function_calls = [{"name": func_name, "arguments": args}]
                    except:
                        pass
            
            return {
                "content": content,
                "function_calls": function_calls,
                "usage": {
                    "prompt_tokens": len(self.tokenizer.encode(prompt)),
                    "completion_tokens": len(self.tokenizer.encode(content)),
                    "total_tokens": len(self.tokenizer.encode(generated_text))
                },
                "model": self.model_name,
                "finish_reason": "stop"
            }
            
        except Exception as e:
            return {"error": str(e)}
    
    async def stream_completion(
        self,
        messages: List[ChatMessage],
        temperature: float = 0.7,
        max_tokens: int = 1000,
        **kwargs
    ):
        """Stream HuggingFace chat completion (simulated)"""
        
        result = await self.chat_completion(messages, temperature, max_tokens, **kwargs)
        
        if "error" in result:
            yield result["error"]
            return
        
        # Simulate streaming by yielding words
        content = result["content"]
        words = content.split()
        
        for word in words:
            yield word + " "
            await asyncio.sleep(0.1)  # Simulate streaming delay
    
    def _messages_to_prompt(self, messages: List[ChatMessage]) -> str:
        """Convert messages to prompt format"""
        prompt_parts = []
        
        for msg in messages:
            if msg.role == "system":
                prompt_parts.append(f"System: {msg.content}")
            elif msg.role == "user":
                prompt_parts.append(f"Human: {msg.content}")
            elif msg.role == "assistant":
                prompt_parts.append(f"Assistant: {msg.content}")
        
        prompt_parts.append("Assistant:")
        return "\n".join(prompt_parts)
    
    def _get_model_capabilities(self) -> ModelCapabilities:
        """Get HuggingFace model capabilities"""
        
        return ModelCapabilities(
            max_tokens=1024,
            supports_functions=False,  # Simulated through prompting
            supports_vision=False,
            supports_streaming=False,  # Simulated
            context_window=2048,
            input_cost_per_token=0.0,  # Local deployment
            output_cost_per_token=0.0,
            rate_limit_rpm=0,
            rate_limit_tpm=0
        )
    
    def count_tokens(self, text: str) -> int:
        """Count tokens using tokenizer"""
        if self.tokenizer:
            return len(self.tokenizer.encode(text))
        return len(text.split()) * 1.3

class AdvancedLLMAgent:
    """Advanced LLM agent with tool calling capabilities"""
    
    def __init__(self, provider: LLMProviderInterface, system_prompt: str = None):
        self.provider = provider
        self.system_prompt = system_prompt or "You are a helpful AI assistant."
        self.conversation_history: List[ChatMessage] = []
        self.available_functions: Dict[str, Callable] = {}
        self.function_definitions: List[FunctionDefinition] = []
        
        # Performance tracking
        self.usage_stats = {
            "total_tokens": 0,
            "total_requests": 0,
            "function_calls": 0,
            "errors": 0
        }
    
    def register_function(self, func: Callable, description: str, parameters: Dict[str, Any]):
        """Register a function for tool calling"""
        
        function_def = FunctionDefinition(
            name=func.__name__,
            description=description,
            parameters=parameters,
            required=list(parameters.get("required", []))
        )
        
        self.available_functions[func.__name__] = func
        self.function_definitions.append(function_def)
        
        logging.info(f"Registered function: {func.__name__}")
    
    async def chat(self, message: str, temperature: float = 0.7, max_tokens: int = 1000) -> str:
        """Send message and get response with potential function calling"""
        
        # Add user message to history
        user_message = ChatMessage(role="user", content=message)
        
        # Prepare conversation with system prompt
        messages = []
        if self.system_prompt:
            messages.append(ChatMessage(role="system", content=self.system_prompt))
        
        messages.extend(self.conversation_history)
        messages.append(user_message)
        
        try:
            # Get response from LLM
            response = await self.provider.chat_completion(
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                functions=self.function_definitions if self.available_functions else None
            )
            
            if "error" in response:
                self.usage_stats["errors"] += 1
                return f"Error: {response['error']}"
            
            # Update usage stats
            self.usage_stats["total_requests"] += 1
            if "usage" in response:
                self.usage_stats["total_tokens"] += response["usage"]["total_tokens"]
            
            content = response.get("content", "")
            function_calls = response.get("function_calls")
            
            # Handle function calls
            if function_calls:
                self.usage_stats["function_calls"] += len(function_calls)
                
                for func_call in function_calls:
                    func_name = func_call.get("name")
                    func_args = func_call.get("arguments", {})
                    
                    if func_name in self.available_functions:
                        try:
                            # Execute function
                            if asyncio.iscoroutinefunction(self.available_functions[func_name]):
                                func_result = await self.available_functions[func_name](**func_args)
                            else:
                                func_result = self.available_functions[func_name](**func_args)
                            
                            # Add function call and result to conversation
                            function_message = ChatMessage(
                                role="assistant",
                                content="",
                                function_call=func_call
                            )
                            
                            function_result_message = ChatMessage(
                                role="function",
                                name=func_name,
                                content=json.dumps(func_result)
                            )
                            
                            messages.extend([function_message, function_result_message])
                            
                            # Get follow-up response
                            follow_up_response = await self.provider.chat_completion(
                                messages=messages,
                                temperature=temperature,
                                max_tokens=max_tokens
                            )
                            
                            if "usage" in follow_up_response:
                                self.usage_stats["total_tokens"] += follow_up_response["usage"]["total_tokens"]
                            
                            content = follow_up_response.get("content", content)
                            
                        except Exception as e:
                            content += f"\n\nError executing function {func_name}: {str(e)}"
                    else:
                        content += f"\n\nFunction {func_name} not found."
            
            # Add messages to conversation history
            self.conversation_history.append(user_message)
            if content:
                assistant_message = ChatMessage(role="assistant", content=content)
                self.conversation_history.append(assistant_message)
            
            return content
            
        except Exception as e:
            self.usage_stats["errors"] += 1
            return f"Error: {str(e)}"
    
    async def stream_chat(self, message: str, temperature: float = 0.7, max_tokens: int = 1000):
        """Stream chat response"""
        
        user_message = ChatMessage(role="user", content=message)
        messages = []
        
        if self.system_prompt:
            messages.append(ChatMessage(role="system", content=self.system_prompt))
        
        messages.extend(self.conversation_history)
        messages.append(user_message)
        
        try:
            async for chunk in self.provider.stream_completion(
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            ):
                yield chunk
                
        except Exception as e:
            yield f"Error: {str(e)}"
    
    def clear_history(self):
        """Clear conversation history"""
        self.conversation_history = []
    
    def get_usage_stats(self) -> Dict[str, Any]:
        """Get usage statistics"""
        return {
            **self.usage_stats,
            "provider": self.provider.__class__.__name__,
            "model": self.provider.model_name,
            "conversation_length": len(self.conversation_history)
        }

# Tool calling examples
def calculate_math(expression: str) -> Dict[str, Any]:
    """Calculate mathematical expression safely"""
    try:
        # Simple eval with safety restrictions
        allowed_names = {
            k: v for k, v in __builtins__.items()
            if k in ['abs', 'round', 'min', 'max', 'sum', 'pow']
        }
        allowed_names.update({'__builtins__': {}})
        
        result = eval(expression, allowed_names)
        return {"result": result, "expression": expression}
    except Exception as e:
        return {"error": str(e), "expression": expression}

async def get_weather(city: str) -> Dict[str, Any]:
    """Get weather information for a city (mock implementation)"""
    # This would typically call a real weather API
    mock_weather = {
        "temperature": "22°C",
        "condition": "Sunny",
        "humidity": "65%",
        "wind": "10 km/h"
    }
    return {"city": city, "weather": mock_weather}

def get_current_time() -> Dict[str, Any]:
    """Get current time"""
    return {"current_time": datetime.now().isoformat()}

# Usage example
async def main():
    """Example usage of LLM providers and agent"""
    
    print("Testing LLM Providers and Agent Framework")
    
    # Test OpenAI
    print("\n=== OpenAI Provider ===")
    openai_provider = OpenAIProvider(model_name="gpt-3.5-turbo")
    openai_agent = AdvancedLLMAgent(
        provider=openai_provider,
        system_prompt="You are a helpful assistant with access to tools."
    )
    
    # Register functions
    openai_agent.register_function(
        calculate_math,
        "Calculate mathematical expressions",
        {
            "type": "object",
            "properties": {
                "expression": {"type": "string", "description": "Mathematical expression to evaluate"}
            },
            "required": ["expression"]
        }
    )
    
    openai_agent.register_function(
        get_current_time,
        "Get the current time",
        {"type": "object", "properties": {}}
    )
    
    # Test conversation
    response = await openai_agent.chat("What's 25 * 34? Also, what time is it?")
    print(f"OpenAI Response: {response}")
    
    # Test Anthropic
    print("\n=== Anthropic Provider ===")
    anthropic_provider = AnthropicProvider(model_name="claude-3-sonnet-20240229")
    anthropic_agent = AdvancedLLMAgent(
        provider=anthropic_provider,
        system_prompt="You are a helpful assistant."
    )
    
    response = await anthropic_agent.chat("Explain quantum computing in simple terms.")
    print(f"Anthropic Response: {response[:200]}...")
    
    # Test Ollama (if available)
    print("\n=== Ollama Provider ===")
    try:
        ollama_provider = OllamaProvider(model_name="llama2")
        ollama_agent = AdvancedLLMAgent(
            provider=ollama_provider,
            system_prompt="You are a helpful assistant."
        )
        
        response = await ollama_agent.chat("Hello! How are you?")
        print(f"Ollama Response: {response}")
        
    except Exception as e:
        print(f"Ollama not available: {e}")
    
    # Print usage statistics
    print("\n=== Usage Statistics ===")
    print(f"OpenAI Stats: {openai_agent.get_usage_stats()}")
    print(f"Anthropic Stats: {anthropic_agent.get_usage_stats()}")

if __name__ == "__main__":
    asyncio.run(main())
````

## Conclusion

The implementation of a comprehensive LLM provider framework demonstrates the complexity and sophistication required for modern AI agent development. This approach enables seamless integration across multiple providers while maintaining consistency in agent behavior and functionality.

**Provider Abstraction** allows developers to switch between different LLM providers without changing application logic, providing flexibility for cost optimization, performance tuning, and capability matching. The standardized interface ensures consistent behavior regardless of the underlying model provider.

**Function Calling Integration** represents a crucial advancement in agent capabilities, enabling LLMs to interact with external systems, perform calculations, and access real-time data. The implementation handles various provider approaches to tool calling, from native support to prompt-based simulation.

**Advanced Architecture** incorporates modern software engineering practices including async/await patterns, comprehensive error handling, usage tracking, and extensible design patterns. The modular structure facilitates maintenance and feature additions while ensuring robust production deployment.

**Cost and Performance Optimization** through provider-specific optimizations, token counting, rate limiting, and usage statistics enables informed decisions about model selection and resource allocation. The framework supports both cloud-based and local deployment scenarios.

**Real-World Considerations** include API key management, error recovery, conversation history management, and security considerations for function execution. The implementation provides a foundation for production-grade agent development while maintaining flexibility for experimentation and prototyping.

This comprehensive framework enables developers to focus on agent logic and user experience rather than provider-specific integration details, accelerating development while ensuring robustness and scalability for production deployment scenarios.