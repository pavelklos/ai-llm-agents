<small>Claude Sonnet 4 **(Scientific Paper Explorer pro In≈æen√Ωry)**</small>
# Scientific Paper Explorer for Engineers

## Kl√≠ƒçov√© Koncepty

### RAG (Retrieval-Augmented Generation)
RAG je architektura, kter√° kombinuje vyhled√°v√°n√≠ relevantn√≠ch dokument≈Ø s generov√°n√≠m odpovƒõd√≠ pomoc√≠ LLM. Umo≈æ≈àuje AI syst√©m≈Øm pracovat s aktu√°ln√≠mi a specifick√Ωmi daty bez nutnosti p≈ôetr√©nov√°n√≠ modelu.

### LlamaIndex
Framework pro budov√°n√≠ RAG aplikac√≠, kter√Ω poskytuje n√°stroje pro indexov√°n√≠, dotazov√°n√≠ a integraci r≈Øzn√Ωch datov√Ωch zdroj≈Ø s LLM modely.

### ArXiv API
Ve≈ôejn√© API pro p≈ô√≠stup k vƒõdeck√Ωm publikac√≠m z arXiv.org datab√°ze, umo≈æ≈àuj√≠c√≠ programov√© vyhled√°v√°n√≠ a stahov√°n√≠ vƒõdeck√Ωch ƒçl√°nk≈Ø.

### Embedding Comparison
Technika porovn√°v√°n√≠ vektorov√Ωch reprezentac√≠ textu pro nalezen√≠ s√©manticky podobn√Ωch dokument≈Ø nebo pas√°≈æ√≠.

### Visual Search
Vizu√°ln√≠ rozhran√≠ pro vyhled√°v√°n√≠ a proch√°zen√≠ v√Ωsledk≈Ø, ƒçasto s grafick√Ωmi prvky pro lep≈°√≠ u≈æivatelsk√Ω z√°≈æitek.

### Streamlit UI
Python framework pro rychl√© vytv√°≈ôen√≠ webov√Ωch aplikac√≠ s interaktivn√≠mi komponenty pro data science a ML projekty.

## Komplexn√≠ Vysvƒõtlen√≠ Projektu

Scientific Paper Explorer je pokroƒçil√Ω RAG syst√©m navr≈æen√Ω specificky pro in≈æen√Ωry a v√Ωzkumn√≠ky. Projekt ≈ôe≈°√≠ probl√©m efektivn√≠ho vyhled√°v√°n√≠ a extrakce relevantn√≠ch informac√≠ z obrovsk√©ho mno≈æstv√≠ vƒõdeck√Ωch publikac√≠.

### Hlavn√≠ C√≠le:
- **Inteligentn√≠ vyhled√°v√°n√≠**: Naj√≠t relevantn√≠ ƒçl√°nky na z√°kladƒõ technick√Ωch dotaz≈Ø
- **Extrakce metod**: Automaticky identifikovat a extrahovat specifick√© metodologie
- **Anal√Ωza evaluac√≠**: Porovnat v√Ωsledky a metriky r≈Øzn√Ωch p≈ô√≠stup≈Ø
- **Vizu√°ln√≠ exploraci**: Poskytovat intuitivn√≠ rozhran√≠ pro proch√°zen√≠ v√Ωsledk≈Ø

### Technick√© V√Ωzvy:
- Zpracov√°n√≠ velk√©ho objemu vƒõdeck√Ωch text≈Ø
- S√©mantick√© porozumƒõn√≠ technick√Ωm term√≠n≈Øm
- Efektivn√≠ indexov√°n√≠ a vyhled√°v√°n√≠
- P≈ôesn√° extrakce strukturovan√Ωch informac√≠

### Potenci√°ln√≠ Dopad:
- Urychlen√≠ v√Ωzkumn√©ho procesu
- Lep≈°√≠ p≈ôehled o souƒçasn√©m stavu technologi√≠
- Identifikace v√Ωzkumn√Ωch mezer
- Podpora evidence-based rozhodov√°n√≠

## Komplexn√≠ Implementace v Pythonu

````python
streamlit==1.29.0
llama-index==0.9.15
openai==1.3.7
arxiv==1.4.8
sentence-transformers==2.2.2
chromadb==0.4.18
plotly==5.17.0
pandas==2.1.4
numpy==1.24.3
python-dotenv==1.0.0
````

````python
import os
from dataclasses import dataclass
from typing import List, Optional

@dataclass
class Config:
    OPENAI_API_KEY: str = os.getenv("OPENAI_API_KEY", "")
    EMBEDDING_MODEL: str = "sentence-transformers/all-MiniLM-L6-v2"
    LLM_MODEL: str = "gpt-3.5-turbo"
    CHROMA_PERSIST_DIR: str = "./chroma_db"
    MAX_PAPERS_PER_QUERY: int = 50
    CHUNK_SIZE: int = 1024
    CHUNK_OVERLAP: int = 200
    
    def validate(self) -> bool:
        if not self.OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY mus√≠ b√Ωt nastaven")
        return True

CONFIG = Config()
````

````python
import arxiv
import logging
from typing import List, Dict, Optional
from dataclasses import dataclass
from datetime import datetime

@dataclass
class Paper:
    title: str
    authors: List[str]
    abstract: str
    published: datetime
    arxiv_id: str
    pdf_url: str
    categories: List[str]

class ArxivClient:
    def __init__(self, max_results: int = 50):
        self.max_results = max_results
        self.client = arxiv.Client()
        
    def search_papers(self, query: str, categories: Optional[List[str]] = None) -> List[Paper]:
        """Vyhled√° ƒçl√°nky na ArXiv podle dotazu"""
        try:
            # Sestaven√≠ vyhled√°vac√≠ho dotazu
            search_query = query
            if categories:
                cat_query = " OR ".join([f"cat:{cat}" for cat in categories])
                search_query = f"({query}) AND ({cat_query})"
            
            search = arxiv.Search(
                query=search_query,
                max_results=self.max_results,
                sort_by=arxiv.SortCriterion.Relevance,
                sort_order=arxiv.SortOrder.Descending
            )
            
            papers = []
            for result in self.client.results(search):
                paper = Paper(
                    title=result.title,
                    authors=[author.name for author in result.authors],
                    abstract=result.summary,
                    published=result.published,
                    arxiv_id=result.entry_id.split('/')[-1],
                    pdf_url=result.pdf_url,
                    categories=[cat for cat in result.categories]
                )
                papers.append(paper)
                
            logging.info(f"Nalezeno {len(papers)} ƒçl√°nk≈Ø pro dotaz: {query}")
            return papers
            
        except Exception as e:
            logging.error(f"Chyba p≈ôi vyhled√°v√°n√≠ na ArXiv: {e}")
            return []
    
    def get_engineering_categories(self) -> List[str]:
        """Vr√°t√≠ seznam in≈æen√Ωrsk√Ωch kategori√≠ na ArXiv"""
        return [
            "cs.AI",  # Artificial Intelligence
            "cs.LG",  # Machine Learning
            "cs.CV",  # Computer Vision
            "cs.RO",  # Robotics
            "cs.SY",  # Systems and Control
            "eess.SP", # Signal Processing
            "math.OC", # Optimization and Control
            "stat.ML"  # Machine Learning (Statistics)
        ]
````

````python
import chromadb
import numpy as np
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Any, Optional
from llama_index import Document, VectorStoreIndex, ServiceContext
from llama_index.vector_stores import ChromaVectorStore
from llama_index.storage.storage_context import StorageContext
import logging

class EmbeddingManager:
    def __init__(self, model_name: str, persist_dir: str):
        self.model = SentenceTransformer(model_name)
        self.persist_dir = persist_dir
        self.chroma_client = chromadb.PersistentClient(path=persist_dir)
        self.collection = self.chroma_client.get_or_create_collection(
            name="scientific_papers",
            metadata={"hnsw:space": "cosine"}
        )
        
    def add_papers(self, papers: List[Any]) -> None:
        """P≈ôid√° ƒçl√°nky do vektorov√© datab√°ze"""
        try:
            documents = []
            metadatas = []
            ids = []
            
            for i, paper in enumerate(papers):
                # Kombinace title a abstract pro lep≈°√≠ embedding
                content = f"N√°zev: {paper.title}\n\nAbstrakt: {paper.abstract}"
                
                documents.append(content)
                metadatas.append({
                    "title": paper.title,
                    "authors": ", ".join(paper.authors),
                    "arxiv_id": paper.arxiv_id,
                    "categories": ", ".join(paper.categories),
                    "published": paper.published.isoformat(),
                    "pdf_url": paper.pdf_url
                })
                ids.append(f"paper_{paper.arxiv_id}_{i}")
            
            # Generov√°n√≠ embedding≈Ø
            embeddings = self.model.encode(documents).tolist()
            
            # P≈ôid√°n√≠ do ChromaDB
            self.collection.add(
                documents=documents,
                embeddings=embeddings,
                metadatas=metadatas,
                ids=ids
            )
            
            logging.info(f"P≈ôid√°no {len(papers)} ƒçl√°nk≈Ø do vektorov√© datab√°ze")
            
        except Exception as e:
            logging.error(f"Chyba p≈ôi p≈ôid√°v√°n√≠ ƒçl√°nk≈Ø: {e}")
    
    def search_similar(self, query: str, n_results: int = 5) -> List[Dict]:
        """Vyhled√° podobn√© ƒçl√°nky na z√°kladƒõ dotazu"""
        try:
            query_embedding = self.model.encode([query]).tolist()
            
            results = self.collection.query(
                query_embeddings=query_embedding,
                n_results=n_results,
                include=['documents', 'metadatas', 'distances']
            )
            
            similar_papers = []
            for i in range(len(results['documents'][0])):
                similar_papers.append({
                    'document': results['documents'][0][i],
                    'metadata': results['metadatas'][0][i],
                    'similarity': 1 - results['distances'][0][i]  # Konverze distance na similarity
                })
                
            return similar_papers
            
        except Exception as e:
            logging.error(f"Chyba p≈ôi vyhled√°v√°n√≠: {e}")
            return []
    
    def get_collection_stats(self) -> Dict[str, Any]:
        """Vr√°t√≠ statistiky kolekce"""
        try:
            count = self.collection.count()
            return {
                "total_papers": count,
                "collection_name": self.collection.name
            }
        except Exception as e:
            logging.error(f"Chyba p≈ôi z√≠sk√°v√°n√≠ statistik: {e}")
            return {"total_papers": 0, "collection_name": "unknown"}
````

````python
import openai
from typing import List, Dict, Any, Optional
import json
import logging
from config import CONFIG

class LLMAnalyzer:
    def __init__(self, api_key: str, model: str = "gpt-3.5-turbo"):
        openai.api_key = api_key
        self.model = model
    
    def extract_methods(self, paper_content: str) -> Dict[str, Any]:
        """Extrahuje metodologie z ƒçl√°nku"""
        prompt = f"""
        Analyzuj n√°sleduj√≠c√≠ vƒõdeck√Ω ƒçl√°nek a extrahuj hlavn√≠ metodologie a p≈ô√≠stupy.
        
        ƒål√°nek:
        {paper_content[:3000]}...
        
        Vra≈• odpovƒõƒè ve form√°tu JSON s n√°sleduj√≠c√≠mi kl√≠ƒçi:
        - "main_methods": seznam hlavn√≠ch metod
        - "algorithms": seznam algoritm≈Ø
        - "technologies": seznam pou≈æit√Ωch technologi√≠
        - "evaluation_metrics": seznam evaluaƒçn√≠ch metrik
        
        Odpovƒõz pouze JSON bez dal≈°√≠ho textu.
        """
        
        try:
            response = openai.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1
            )
            
            result = json.loads(response.choices[0].message.content)
            return result
            
        except Exception as e:
            logging.error(f"Chyba p≈ôi extrakci metod: {e}")
            return {
                "main_methods": [],
                "algorithms": [],
                "technologies": [],
                "evaluation_metrics": []
            }
    
    def compare_papers(self, papers: List[Dict]) -> str:
        """Porovn√° v√≠ce ƒçl√°nk≈Ø"""
        papers_summary = "\n\n".join([
            f"ƒål√°nek {i+1}: {paper['metadata']['title']}\n"
            f"Auto≈ôi: {paper['metadata']['authors']}\n"
            f"Abstrakt: {paper['document'][:500]}..."
            for i, paper in enumerate(papers[:3])
        ])
        
        prompt = f"""
        Porovnej n√°sleduj√≠c√≠ vƒõdeck√© ƒçl√°nky z hlediska:
        1. Pou≈æit√Ωch metod a p≈ô√≠stup≈Ø
        2. V√Ωsledk≈Ø a v√Ωkonu
        3. Inovac√≠ a p≈ô√≠nos≈Ø
        4. Omezen√≠ a nev√Ωhod
        
        ƒål√°nky:
        {papers_summary}
        
        Napi≈° strukturovan√© porovn√°n√≠ v ƒçe≈°tinƒõ.
        """
        
        try:
            response = openai.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logging.error(f"Chyba p≈ôi porovn√°v√°n√≠ ƒçl√°nk≈Ø: {e}")
            return "Chyba p≈ôi anal√Ωze ƒçl√°nk≈Ø."
    
    def answer_question(self, question: str, context_papers: List[Dict]) -> str:
        """Odpov√≠d√° na ot√°zky na z√°kladƒõ kontextu ƒçl√°nk≈Ø"""
        context = "\n\n".join([
            f"ƒål√°nek: {paper['metadata']['title']}\n{paper['document'][:1000]}..."
            for paper in context_papers[:5]
        ])
        
        prompt = f"""
        Na z√°kladƒõ n√°sleduj√≠c√≠ch vƒõdeck√Ωch ƒçl√°nk≈Ø odpovƒõz na ot√°zku:
        
        Ot√°zka: {question}
        
        Kontext z ƒçl√°nk≈Ø:
        {context}
        
        Poskytni podrobnou odpovƒõƒè v ƒçe≈°tinƒõ s odkazy na konkr√©tn√≠ ƒçl√°nky.
        """
        
        try:
            response = openai.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logging.error(f"Chyba p≈ôi odpov√≠d√°n√≠ na ot√°zku: {e}")
            return "Nepoda≈ôilo se zpracovat ot√°zku."
````

````python
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
from datetime import datetime, timedelta
import logging
from typing import List, Dict

from config import CONFIG
from arxiv_client import ArxivClient, Paper
from embedding_manager import EmbeddingManager
from llm_analyzer import LLMAnalyzer

# Konfigurace loggingu
logging.basicConfig(level=logging.INFO)

@st.cache_resource
def initialize_components():
    """Inicializuje komponenty aplikace"""
    try:
        CONFIG.validate()
        arxiv_client = ArxivClient(max_results=CONFIG.MAX_PAPERS_PER_QUERY)
        embedding_manager = EmbeddingManager(
            model_name=CONFIG.EMBEDDING_MODEL,
            persist_dir=CONFIG.CHROMA_PERSIST_DIR
        )
        llm_analyzer = LLMAnalyzer(
            api_key=CONFIG.OPENAI_API_KEY,
            model=CONFIG.LLM_MODEL
        )
        return arxiv_client, embedding_manager, llm_analyzer
    except Exception as e:
        st.error(f"Chyba p≈ôi inicializaci: {e}")
        return None, None, None

def create_papers_dataframe(papers: List[Paper]) -> pd.DataFrame:
    """Vytvo≈ô√≠ DataFrame z ƒçl√°nk≈Ø pro vizualizaci"""
    data = []
    for paper in papers:
        data.append({
            'title': paper.title,
            'authors': ', '.join(paper.authors[:3]),  # Prvn√≠ 3 auto≈ôi
            'published': paper.published.date(),
            'categories': ', '.join(paper.categories),
            'arxiv_id': paper.arxiv_id,
            'year': paper.published.year
        })
    return pd.DataFrame(data)

def visualize_papers_timeline(df: pd.DataFrame):
    """Vytvo≈ô√≠ timeline vizualizaci ƒçl√°nk≈Ø"""
    if df.empty:
        return
    
    yearly_counts = df.groupby('year').size().reset_index(name='count')
    
    fig = px.bar(
        yearly_counts, 
        x='year', 
        y='count',
        title='Distribuce ƒçl√°nk≈Ø podle roku publikace',
        labels={'year': 'Rok', 'count': 'Poƒçet ƒçl√°nk≈Ø'}
    )
    
    st.plotly_chart(fig, use_container_width=True)

def visualize_similarity_scores(results: List[Dict]):
    """Vizualizuje sk√≥re podobnosti"""
    if not results:
        return
    
    titles = [r['metadata']['title'][:50] + '...' for r in results]
    similarities = [r['similarity'] for r in results]
    
    fig = go.Figure(data=go.Bar(
        x=similarities,
        y=titles,
        orientation='h',
        text=[f"{s:.3f}" for s in similarities],
        textposition='auto'
    ))
    
    fig.update_layout(
        title='M√≠ra podobnosti nalezen√Ωch ƒçl√°nk≈Ø',
        xaxis_title='Sk√≥re podobnosti',
        yaxis_title='ƒål√°nky',
        height=400
    )
    
    st.plotly_chart(fig, use_container_width=True)

def main():
    st.set_page_config(
        page_title="Scientific Paper Explorer",
        page_icon="üî¨",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("üî¨ Scientific Paper Explorer pro In≈æen√Ωry")
    st.markdown("*Inteligentn√≠ vyhled√°v√°n√≠ a anal√Ωza vƒõdeck√Ωch publikac√≠*")
    
    # Inicializace komponent
    arxiv_client, embedding_manager, llm_analyzer = initialize_components()
    
    if not all([arxiv_client, embedding_manager, llm_analyzer]):
        st.error("Nepoda≈ôilo se inicializovat aplikaci. Zkontrolujte konfiguraci.")
        return
    
    # Sidebar
    with st.sidebar:
        st.header("‚öôÔ∏è Nastaven√≠")
        
        # Statistiky datab√°ze
        stats = embedding_manager.get_collection_stats()
        st.metric("ƒål√°nky v datab√°zi", stats['total_papers'])
        
        st.markdown("---")
        
        # Kategorie
        categories = arxiv_client.get_engineering_categories()
        selected_categories = st.multiselect(
            "Vyberte kategorie:",
            categories,
            default=["cs.AI", "cs.LG"],
            help="Zvolte oblasti pro vyhled√°v√°n√≠"
        )
    
    # Hlavn√≠ rozhran√≠
    col1, col2 = st.columns([2, 1])
    
    with col1:
        search_query = st.text_input(
            "üîç Vyhled√°vac√≠ dotaz:",
            placeholder="nap≈ô. 'neural networks for computer vision'",
            help="Zadejte technick√Ω dotaz v angliƒçtinƒõ"
        )
    
    with col2:
        search_mode = st.selectbox(
            "Re≈æim vyhled√°v√°n√≠:",
            ["S√©mantick√©", "ArXiv API"],
            help="S√©mantick√© - v lok√°ln√≠ datab√°zi, ArXiv API - nov√© ƒçl√°nky"
        )
    
    if st.button("üöÄ Vyhledat", type="primary"):
        if search_query:
            with st.spinner("Vyhled√°v√°m ƒçl√°nky..."):
                
                if search_mode == "ArXiv API":
                    # Vyhled√°n√≠ nov√Ωch ƒçl√°nk≈Ø
                    papers = arxiv_client.search_papers(search_query, selected_categories)
                    
                    if papers:
                        st.success(f"Nalezeno {len(papers)} nov√Ωch ƒçl√°nk≈Ø")
                        
                        # P≈ôid√°n√≠ do datab√°ze
                        with st.spinner("P≈ôid√°v√°m ƒçl√°nky do datab√°ze..."):
                            embedding_manager.add_papers(papers)
                        
                        # Vytvo≈ôen√≠ DataFrame pro vizualizaci
                        df = create_papers_dataframe(papers)
                        
                        # Zobrazen√≠ timeline
                        st.subheader("üìä Timeline publikac√≠")
                        visualize_papers_timeline(df)
                        
                        # Zobrazen√≠ ƒçl√°nk≈Ø
                        st.subheader("üìÑ Nalezen√© ƒçl√°nky")
                        for i, paper in enumerate(papers[:10]):
                            with st.expander(f"{i+1}. {paper.title}"):
                                st.write(f"**Auto≈ôi:** {', '.join(paper.authors)}")
                                st.write(f"**Publikov√°no:** {paper.published.date()}")
                                st.write(f"**Kategorie:** {', '.join(paper.categories)}")
                                st.write(f"**ArXiv ID:** {paper.arxiv_id}")
                                st.write("**Abstrakt:**")
                                st.write(paper.abstract)
                                st.markdown(f"[üìë PDF]({paper.pdf_url})")
                    else:
                        st.warning("Nebyl nalezen ≈æ√°dn√Ω ƒçl√°nek pro dan√Ω dotaz")
                
                else:
                    # S√©mantick√© vyhled√°v√°n√≠ v datab√°zi
                    results = embedding_manager.search_similar(search_query, n_results=10)
                    
                    if results:
                        st.success(f"Nalezeno {len(results)} podobn√Ωch ƒçl√°nk≈Ø")
                        
                        # Vizualizace podobnosti
                        st.subheader("üìä M√≠ra podobnosti")
                        visualize_similarity_scores(results)
                        
                        # Zobrazen√≠ v√Ωsledk≈Ø
                        st.subheader("üéØ Nejpodobnƒõj≈°√≠ ƒçl√°nky")
                        for i, result in enumerate(results):
                            with st.expander(
                                f"{i+1}. {result['metadata']['title']} "
                                f"(podobnost: {result['similarity']:.3f})"
                            ):
                                st.write(f"**Auto≈ôi:** {result['metadata']['authors']}")
                                st.write(f"**ArXiv ID:** {result['metadata']['arxiv_id']}")
                                st.write(f"**Kategorie:** {result['metadata']['categories']}")
                                st.write("**Obsah:**")
                                st.write(result['document'][:1000] + "...")
                                
                                # Anal√Ωza metodologi√≠
                                if st.button(f"üî¨ Analyzovat metody #{i+1}", key=f"analyze_{i}"):
                                    with st.spinner("Analyzuji metodologie..."):
                                        methods = llm_analyzer.extract_methods(result['document'])
                                        
                                        col_a, col_b = st.columns(2)
                                        with col_a:
                                            if methods['main_methods']:
                                                st.write("**Hlavn√≠ metody:**")
                                                for method in methods['main_methods']:
                                                    st.write(f"‚Ä¢ {method}")
                                            
                                            if methods['algorithms']:
                                                st.write("**Algoritmy:**")
                                                for algo in methods['algorithms']:
                                                    st.write(f"‚Ä¢ {algo}")
                                        
                                        with col_b:
                                            if methods['technologies']:
                                                st.write("**Technologie:**")
                                                for tech in methods['technologies']:
                                                    st.write(f"‚Ä¢ {tech}")
                                            
                                            if methods['evaluation_metrics']:
                                                st.write("**Evaluaƒçn√≠ metriky:**")
                                                for metric in methods['evaluation_metrics']:
                                                    st.write(f"‚Ä¢ {metric}")
                        
                        # Porovn√°n√≠ ƒçl√°nk≈Ø
                        if len(results) > 1:
                            st.subheader("‚öñÔ∏è Porovn√°n√≠ ƒçl√°nk≈Ø")
                            if st.button("üìä Porovnat top 3 ƒçl√°nky"):
                                with st.spinner("Porovn√°v√°m ƒçl√°nky..."):
                                    comparison = llm_analyzer.compare_papers(results[:3])
                                    st.write(comparison)
                        
                        # Q&A sekce
                        st.subheader("‚ùì Zeptejte se na ƒçl√°nky")
                        question = st.text_input(
                            "Polo≈æte ot√°zku souvisej√≠c√≠ s nalezen√Ωmi ƒçl√°nky:",
                            placeholder="nap≈ô. 'Jak√© jsou hlavn√≠ v√Ωhody navr≈æen√Ωch metod?'"
                        )
                        
                        if question and st.button("üí¨ Odpovƒõdƒõt"):
                            with st.spinner("Generuji odpovƒõƒè..."):
                                answer = llm_analyzer.answer_question(question, results)
                                st.write(answer)
                    
                    else:
                        st.warning("Nebyl nalezen ≈æ√°dn√Ω podobn√Ω ƒçl√°nek v datab√°zi")
        else:
            st.warning("Zadejte vyhled√°vac√≠ dotaz")
    
    # Footer
    st.markdown("---")
    st.markdown(
        "**Scientific Paper Explorer** | "
        "Vytvo≈ôeno s ‚ù§Ô∏è pomoc√≠ Streamlit, LlamaIndex a OpenAI"
    )

if __name__ == "__main__":
    main()
````

````python
# Zkop√≠rujte tento soubor jako .env a vypl≈àte hodnoty
OPENAI_API_KEY=your_openai_api_key_here
````

````python
import subprocess
import sys
import os

def setup_environment():
    """Nastav√≠ prost≈ôed√≠ a spust√≠ aplikaci"""
    print("üîß Nastavuji prost≈ôed√≠...")
    
    # Instalace z√°vislost√≠
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-r", "requirements.txt"])
    
    # Kontrola .env souboru
    if not os.path.exists(".env"):
        print("‚ö†Ô∏è  Vytvo≈ôte .env soubor s OPENAI_API_KEY")
        print("P≈ô√≠klad najdete v .env.example")
        return False
    
    return True

def main():
    if setup_environment():
        print("üöÄ Spou≈°t√≠m Scientific Paper Explorer...")
        subprocess.run([sys.executable, "-m", "streamlit", "run", "app.py"])
    else:
        print("‚ùå Nepoda≈ôilo se nastavit prost≈ôed√≠")

if __name__ == "__main__":
    main()
````

## Shrnut√≠ Projektu

Scientific Paper Explorer p≈ôedstavuje pokroƒçil√Ω RAG syst√©m specificky navr≈æen√Ω pro pot≈ôeby in≈æen√Ωr≈Ø a v√Ωzkumn√≠k≈Ø. Projekt kombinuje s√≠lu modern√≠ch AI technologi√≠ s intuitivn√≠m u≈æivatelsk√Ωm rozhran√≠m.

### Kl√≠ƒçov√© Hodnoty:
- **Efektivita**: Dramaticky sni≈æuje ƒças pot≈ôebn√Ω pro vyhled√°v√°n√≠ relevantn√≠ch publikac√≠
- **P≈ôesnost**: S√©mantick√© vyhled√°v√°n√≠ poskytuje relevantnƒõj≈°√≠ v√Ωsledky ne≈æ klasick√© fulltextov√©
- **Inteligence**: LLM anal√Ωza extrahuje strukturovan√© informace a umo≈æ≈àuje komplexn√≠ dotazy
- **≈†k√°lovatelnost**: Architektura podporuje r≈Øst datab√°ze bez ztr√°ty v√Ωkonu

### Technologick√© Inovace:
- **Hybridn√≠ vyhled√°v√°n√≠**: Kombinace ArXiv API s lok√°ln√≠ vektorovou datab√°z√≠
- **Multimod√°ln√≠ anal√Ωza**: Zpracov√°n√≠ textu, metadat i vizu√°ln√≠ch prvk≈Ø
- **Inteligentn√≠ extrakce**: Automatick√° identifikace metodologi√≠ a evaluaƒçn√≠ch metrik
- **Interaktivn√≠ Q&A**: Mo≈ænost pokl√°dat komplexn√≠ ot√°zky nad kolekc√≠ ƒçl√°nk≈Ø

Projekt demonstruje praktickou aplikaci RAG architektury v re√°ln√©m prost≈ôed√≠ a poskytuje z√°klad pro dal≈°√≠ roz≈°√≠≈ôen√≠ smƒõrem k plnƒõ autonomn√≠mu v√Ωzkumn√©mu asistentovi.