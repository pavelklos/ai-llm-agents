<small>Claude Sonnet 4 **(Multilingvn칤 Zpravodajsk칳 Pr콢zkumn칤k s RAG)**</small>
# Multilingual News Explorer

## Kl칤캜ov칠 Koncepty

### RAG (Retrieval-Augmented Generation)
RAG architektura kombinuje vyhled치v치n칤 relevantn칤ch informac칤 z rozs치hl칠 datab치ze s generov치n칤m odpov캩d칤 pomoc칤 jazykov칳ch model콢. Umo쮄갓je poskytovat aktu치ln칤 a fakticky p콏esn칠 informace zalo쬰n칠 na konkr칠tn칤ch zdroj칤ch.

### News API
Rozhran칤 pro p콏칤stup k aktu치ln칤m zpravodajsk칳m 캜l치nk콢m z tis칤c콢 zdroj콢 po cel칠m sv캩t캩. Poskytuje strukturovan칳 p콏칤stup k novink치m s metadaty jako jsou datum, zdroj, kategorie a jazyk.

### Multilingvn칤 Embeddings
Vektorov칠 reprezentace textu, kter칠 zachycuj칤 s칠mantick칳 v칳znam nap콏칤캜 r콢zn칳mi jazyky. Umo쮄갓j칤 porovn치vat a vyhled치vat podobn칳 obsah bez ohledu na jazyk p콢vodn칤ho textu.

### Milvus
Vysoce v칳konn치 open-source vektorov치 datab치ze optimalizovan치 pro ukl치d치n칤 a vyhled치v치n칤 miliard vektor콢. Podporuje distribuovan칠 nasazen칤 a je ide치ln칤 pro real-time aplikace.

### GPT-4 Turbo
Nejnov캩j코칤 verze OpenAI modelu s vylep코enou rychlost칤, ni쮄뫆셠i n치klady a schopnost칤 zpracov치vat dlouh칠 kontexty. Exceluje v anal칳ze a sumarizaci komplexn칤ho obsahu.

## Komplexn칤 Vysv캩tlen칤 Projektu

### Popis a C칤le
Multilingvn칤 zpravodajsk칳 pr콢zkumn칤k p콏edstavuje pokro캜il칳 syst칠m pro anal칳zu glob치ln칤ch zpr치v nap콏칤캜 jazykov칳mi bari칠rami. Hlavn칤m c칤lem je poskytovat u쬴vatel콢m komplexn칤 p콏ehled zpravodajstv칤 s automatickou detekc칤 p콏edpojatosti a inteligentn칤 sumarizac칤.

### V칳zvy a 콎e코en칤
- **Jazykov치 diverzita**: Multilingvn칤 embeddings umo쮄갓j칤 jednotn칠 zpracov치n칤 r콢zn칳ch jazyk콢
- **Objektivita**: AI anal칳za detekuje a upozor켿uje na mo쬹ou p콏edpojatost
- **맒치lovatelnost**: Milvus zaji코콘uje rychl칠 vyhled치v치n칤 v milionech 캜l치nk콢
- **Aktu치lnost**: Real-time integrace s News API pro nejnov캩j코칤 informace

### Technologick칠 Inovace
Syst칠m vyu쮂셨치 nejmodern캩j코칤 NLP technologie pro cross-lingv치ln칤 porozum캩n칤, automatickou detekci bias a inteligentn칤 clustering podobn칳ch zpr치v nap콏칤캜 r콢zn칳mi zdroji.

### Dopad a Potenci치l
Projekt m콢쬰 revolucionizovat zp콢sob, jak칳m lid칠 konzumuj칤 zpr치vy, podporuje medi치ln칤 gramotnost a pom치h치 bojovat proti dezinformac칤m prost콏ednictv칤m transparentn칤 anal칳zy zdroj콢.

## Komplexn칤 Implementace v Pythonu

### Instalace z치vislost칤

````python
# requirements.txt
langchain==0.1.20
openai==1.12.0
pymilvus==2.3.4
sentence-transformers==2.2.2
newsapi-python==0.2.7
streamlit==1.31.0
pandas==2.1.4
numpy==1.24.3
requests==2.31.0
python-dotenv==1.0.0
feedparser==6.0.10
beautifulsoup4==4.12.2
langdetect==1.0.9
plotly==5.17.0
wordcloud==1.9.2
textblob==0.17.1
transformers==4.36.2
torch==2.1.2
````

### Hlavn칤 implementace

````python
import os
import logging
import asyncio
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
import json
import hashlib
from concurrent.futures import ThreadPoolExecutor

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from wordcloud import WordCloud
import matplotlib.pyplot as plt

from newsapi import NewsApiClient
from sentence_transformers import SentenceTransformer
from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from langdetect import detect
from textblob import TextBlob
import requests
from bs4 import BeautifulSoup

# Konfigurace logov치n칤
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class NewsArticle:
    """Reprezentace zpravodajsk칠ho 캜l치nku"""
    id: str
    title: str
    content: str
    source: str
    author: Optional[str]
    published_at: datetime
    url: str
    language: str
    category: Optional[str]
    country: Optional[str]
    embedding: Optional[List[float]] = None
    bias_score: Optional[float] = None
    sentiment_score: Optional[float] = None

class MultilingualEmbeddings:
    """Spr치va multilingvn칤ch embedding콢"""
    
    def __init__(self, model_name: str = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"):
        self.model = SentenceTransformer(model_name)
        self.dimension = self.model.get_sentence_embedding_dimension()
        logger.info(f"Na캜ten multilingvn칤 model: {model_name}, dimenze: {self.dimension}")
    
    def encode_texts(self, texts: List[str]) -> List[List[float]]:
        """Zak칩duje texty do vektorov칠 reprezentace"""
        try:
            embeddings = self.model.encode(texts, convert_to_tensor=False)
            return embeddings.tolist()
        except Exception as e:
            logger.error(f"Chyba p콏i k칩dov치n칤 text콢: {e}")
            raise

class MilvusVectorStore:
    """Spr치va Milvus vektorov칠 datab치ze"""
    
    def __init__(self, collection_name: str = "news_articles", 
                 host: str = "localhost", port: str = "19530"):
        self.collection_name = collection_name
        self.host = host
        self.port = port
        self.collection = None
        self._connect()
        self._setup_collection()
    
    def _connect(self):
        """P콏ipojen칤 k Milvus datab치zi"""
        try:
            connections.connect("default", host=self.host, port=self.port)
            logger.info(f"P콏ipojeno k Milvus na {self.host}:{self.port}")
        except Exception as e:
            logger.error(f"Chyba p콏i p콏ipojov치n칤 k Milvus: {e}")
            raise
    
    def _setup_collection(self):
        """Vytvo콏칤 kolekci pokud neexistuje"""
        try:
            if utility.has_collection(self.collection_name):
                self.collection = Collection(self.collection_name)
                logger.info(f"Na캜tena existuj칤c칤 kolekce: {self.collection_name}")
            else:
                # Definice sch칠matu
                fields = [
                    FieldSchema(name="id", dtype=DataType.VARCHAR, max_length=100, is_primary=True),
                    FieldSchema(name="title", dtype=DataType.VARCHAR, max_length=1000),
                    FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=10000),
                    FieldSchema(name="source", dtype=DataType.VARCHAR, max_length=200),
                    FieldSchema(name="author", dtype=DataType.VARCHAR, max_length=200),
                    FieldSchema(name="published_at", dtype=DataType.VARCHAR, max_length=50),
                    FieldSchema(name="url", dtype=DataType.VARCHAR, max_length=500),
                    FieldSchema(name="language", dtype=DataType.VARCHAR, max_length=10),
                    FieldSchema(name="category", dtype=DataType.VARCHAR, max_length=50),
                    FieldSchema(name="country", dtype=DataType.VARCHAR, max_length=10),
                    FieldSchema(name="bias_score", dtype=DataType.FLOAT),
                    FieldSchema(name="sentiment_score", dtype=DataType.FLOAT),
                    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=768)
                ]
                
                schema = CollectionSchema(fields, description="Zpravodajsk칠 캜l치nky s embeddings")
                self.collection = Collection(self.collection_name, schema)
                
                # Vytvo콏en칤 indexu
                index_params = {
                    "metric_type": "COSINE",
                    "index_type": "IVF_FLAT",
                    "params": {"nlist": 1024}
                }
                self.collection.create_index("embedding", index_params)
                logger.info(f"Vytvo콏ena nov치 kolekce: {self.collection_name}")
        except Exception as e:
            logger.error(f"Chyba p콏i vytv치콏en칤 kolekce: {e}")
            raise
    
    def insert_articles(self, articles: List[NewsArticle]):
        """Vlo쮂 캜l치nky do datab치ze"""
        try:
            data = []
            for article in articles:
                if article.embedding:
                    data.append([
                        article.id,
                        article.title,
                        article.content[:9999],  # Omezen칤 d칠lky
                        article.source,
                        article.author or "",
                        article.published_at.isoformat(),
                        article.url,
                        article.language,
                        article.category or "",
                        article.country or "",
                        article.bias_score or 0.0,
                        article.sentiment_score or 0.0,
                        article.embedding
                    ])
            
            if data:
                # Transpozice dat pro Milvus form치t
                transposed_data = list(map(list, zip(*data)))
                self.collection.insert(transposed_data)
                self.collection.flush()
                logger.info(f"Vlo쬰no {len(data)} 캜l치nk콢 do datab치ze")
        except Exception as e:
            logger.error(f"Chyba p콏i vkl치d치n칤 캜l치nk콢: {e}")
            raise
    
    def search_similar(self, query_vector: List[float], top_k: int = 10, 
                      filters: Optional[Dict] = None) -> List[Dict]:
        """Vyhled치 podobn칠 캜l치nky"""
        try:
            self.collection.load()
            
            search_params = {"metric_type": "COSINE", "params": {"nprobe": 10}}
            
            results = self.collection.search(
                data=[query_vector],
                anns_field="embedding",
                param=search_params,
                limit=top_k,
                output_fields=["id", "title", "content", "source", "published_at", 
                             "language", "bias_score", "sentiment_score", "url"]
            )
            
            articles = []
            for result in results[0]:
                article_data = {
                    "id": result.entity.get("id"),
                    "title": result.entity.get("title"),
                    "content": result.entity.get("content"),
                    "source": result.entity.get("source"),
                    "published_at": result.entity.get("published_at"),
                    "language": result.entity.get("language"),
                    "bias_score": result.entity.get("bias_score"),
                    "sentiment_score": result.entity.get("sentiment_score"),
                    "url": result.entity.get("url"),
                    "similarity_score": result.distance
                }
                articles.append(article_data)
            
            return articles
        except Exception as e:
            logger.error(f"Chyba p콏i vyhled치v치n칤: {e}")
            return []

class NewsDataCollector:
    """Sb캩r zpravodajsk칳ch dat"""
    
    def __init__(self, news_api_key: str):
        self.news_api = NewsApiClient(api_key=news_api_key)
        self.session = requests.Session()
    
    def fetch_top_headlines(self, language: str = "en", country: str = None, 
                          category: str = None, page_size: int = 100) -> List[Dict]:
        """Na캜te hlavn칤 zpr치vy"""
        try:
            response = self.news_api.get_top_headlines(
                language=language,
                country=country,
                category=category,
                page_size=page_size
            )
            return response.get("articles", [])
        except Exception as e:
            logger.error(f"Chyba p콏i na캜칤t치n칤 zpr치v: {e}")
            return []
    
    def fetch_everything(self, query: str, language: str = None, 
                        from_date: datetime = None, page_size: int = 100) -> List[Dict]:
        """Vyhled치 캜l치nky podle dotazu"""
        try:
            response = self.news_api.get_everything(
                q=query,
                language=language,
                from_param=from_date.isoformat() if from_date else None,
                page_size=page_size,
                sort_by="publishedAt"
            )
            return response.get("articles", [])
        except Exception as e:
            logger.error(f"Chyba p콏i vyhled치v치n칤 캜l치nk콢: {e}")
            return []
    
    def extract_full_content(self, url: str) -> Optional[str]:
        """Extrahuje pln칳 obsah 캜l치nku z URL"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Odstran캩n칤 ne쮂멳ouc칤ch element콢
            for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                tag.decompose()
            
            # Hled치n칤 hlavn칤ho obsahu
            content_selectors = [
                'article', '.article-content', '.post-content', 
                '.entry-content', '[role="main"]', 'main'
            ]
            
            content = ""
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    content = ' '.join([elem.get_text(strip=True) for elem in elements])
                    break
            
            if not content:
                # Fallback na v코echny odstavce
                paragraphs = soup.find_all('p')
                content = ' '.join([p.get_text(strip=True) for p in paragraphs])
            
            return content[:5000] if content else None  # Omezen칤 d칠lky
            
        except Exception as e:
            logger.warning(f"Nepoda콏ilo se extrahovat obsah z {url}: {e}")
            return None

class BiasDetector:
    """Detekce p콏edpojatosti v textu"""
    
    def __init__(self, openai_api_key: str):
        self.llm = ChatOpenAI(
            api_key=openai_api_key,
            model="gpt-4-turbo-preview",
            temperature=0.1
        )
    
    def analyze_bias(self, text: str) -> Tuple[float, str]:
        """Analyzuje p콏edpojatost v textu"""
        try:
            system_prompt = """Jsi expert na anal칳zu medi치ln칤 p콏edpojatosti. Analyzuj n치sleduj칤c칤 text a:
1. Ohodno콘 p콏edpojatost na 코k치le 0-1 (0 = neutr치ln칤, 1 = siln캩 p콏edpojat칳)
2. Identifikuj typ p콏edpojatosti (politick치, kulturn칤, ekonomick치, atd.)
3. Uve캞 konkr칠tn칤 p콏칤klady p콏edpojat칳ch formulac칤

Odpov캩z ve form치tu JSON:
{
    "bias_score": 0.0-1.0,
    "bias_type": "typ p콏edpojatosti",
    "explanation": "vysv캩tlen칤 s p콏칤klady"
}"""
            
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=f"Text k anal칳ze: {text[:2000]}")
            ]
            
            response = self.llm(messages)
            result = json.loads(response.content)
            
            return result.get("bias_score", 0.0), result.get("explanation", "")
            
        except Exception as e:
            logger.error(f"Chyba p콏i anal칳ze p콏edpojatosti: {e}")
            return 0.0, "Anal칳za se nezda콏ila"
    
    def analyze_sentiment(self, text: str) -> float:
        """Analyzuje sentiment textu"""
        try:
            blob = TextBlob(text)
            return blob.sentiment.polarity
        except Exception as e:
            logger.warning(f"Chyba p콏i anal칳ze sentimentu: {e}")
            return 0.0

class GPT4Summarizer:
    """Sumarizace pomoc칤 GPT-4 Turbo"""
    
    def __init__(self, openai_api_key: str):
        self.llm = ChatOpenAI(
            api_key=openai_api_key,
            model="gpt-4-turbo-preview",
            temperature=0.3
        )
    
    def summarize_articles(self, articles: List[Dict], query: str) -> str:
        """Sumarizuje 캜l치nky k dan칠mu dotazu"""
        try:
            # P콏칤prava obsahu
            content_parts = []
            for i, article in enumerate(articles[:5], 1):  # Omezen칤 na 5 캜l치nk콢
                content_parts.append(
                    f"캛L츼NEK {i}:\nZdroj: {article.get('source', 'Nezn치m칳')}\n"
                    f"Jazyk: {article.get('language', 'Nezn치m칳')}\n"
                    f"N치zor/P콏edpojatost: {article.get('bias_score', 0):.2f}\n"
                    f"Obsah: {article.get('content', article.get('title', ''))[:1000]}\n"
                )
            
            combined_content = "\n\n".join(content_parts)
            
            system_prompt = f"""Jsi expert na zpravodajskou anal칳zu. Na z치klad캩 poskytnut칳ch 캜l치nk콢 vytvo콏:

1. HLAVN칈 SHRNUT칈 (2-3 odstavce)
2. KL칈캛OV칄 BODY (odr치쬶y)
3. R콡ZN칄 PERSPEKTIVY (pokud existuj칤)
4. POTENCI츼LN칈 P콎EDPOJATOST (upozorn캩n칤)

Dotaz u쬴vatele: {query}

Zam캩콏 se na objektivitu a uve캞 r콢zn칠 칰hly pohledu."""

            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=combined_content)
            ]
            
            response = self.llm(messages)
            return response.content
            
        except Exception as e:
            logger.error(f"Chyba p콏i sumarizaci: {e}")
            return "Nepoda콏ilo se vytvo콏it shrnut칤."
    
    def translate_query(self, query: str, target_language: str = "en") -> str:
        """P콏elo쮂 dotaz do c칤lov칠ho jazyka"""
        try:
            if target_language == "en":
                system_prompt = "P콏elo n치sleduj칤c칤 dotaz do angli캜tiny. Vra콘 pouze p콏eklad bez dal코칤ch koment치콏콢."
            else:
                system_prompt = f"P콏elo n치sleduj칤c칤 dotaz do jazyka: {target_language}. Vra콘 pouze p콏eklad."
            
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=query)
            ]
            
            response = self.llm(messages)
            return response.content.strip()
            
        except Exception as e:
            logger.warning(f"Chyba p콏i p콏ekladu: {e}")
            return query

class MultilingualNewsExplorer:
    """Hlavn칤 t콏칤da pro multilingvn칤 zpravodajsk칳 pr콢zkumn칤k"""
    
    def __init__(self, news_api_key: str, openai_api_key: str):
        self.embeddings = MultilingualEmbeddings()
        self.vector_store = MilvusVectorStore()
        self.news_collector = NewsDataCollector(news_api_key)
        self.bias_detector = BiasDetector(openai_api_key)
        self.summarizer = GPT4Summarizer(openai_api_key)
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=100
        )
    
    def _generate_article_id(self, title: str, url: str) -> str:
        """Generuje unik치tn칤 ID 캜l치nku"""
        return hashlib.md5(f"{title}{url}".encode()).hexdigest()
    
    def _detect_language(self, text: str) -> str:
        """Detekuje jazyk textu"""
        try:
            return detect(text)
        except:
            return "unknown"
    
    def process_raw_articles(self, raw_articles: List[Dict]) -> List[NewsArticle]:
        """Zpracuje surov치 data 캜l치nk콢"""
        processed_articles = []
        
        for raw_article in raw_articles:
            try:
                # Extrakce z치kladn칤ch 칰daj콢
                title = raw_article.get("title", "")
                url = raw_article.get("url", "")
                
                if not title or not url:
                    continue
                
                # Z칤sk치n칤 pln칠ho obsahu
                content = self.news_collector.extract_full_content(url)
                if not content:
                    content = raw_article.get("description", title)
                
                # Detekce jazyka
                language = self._detect_language(content)
                
                # Vytvo콏en칤 캜l치nku
                article = NewsArticle(
                    id=self._generate_article_id(title, url),
                    title=title,
                    content=content,
                    source=raw_article.get("source", {}).get("name", "Unknown"),
                    author=raw_article.get("author"),
                    published_at=datetime.fromisoformat(
                        raw_article.get("publishedAt", "").replace("Z", "+00:00")
                    ),
                    url=url,
                    language=language,
                    category=None,
                    country=None
                )
                
                processed_articles.append(article)
                
            except Exception as e:
                logger.warning(f"Chyba p콏i zpracov치n칤 캜l치nku: {e}")
                continue
        
        return processed_articles
    
    def analyze_articles(self, articles: List[NewsArticle]) -> List[NewsArticle]:
        """Analyzuje 캜l치nky (embeddings, bias, sentiment)"""
        analyzed_articles = []
        
        # Generov치n칤 embedding콢
        contents = [f"{article.title} {article.content}" for article in articles]
        embeddings = self.embeddings.encode_texts(contents)
        
        for article, embedding in zip(articles, embeddings):
            try:
                # P콏i콏azen칤 embeddingu
                article.embedding = embedding
                
                # Anal칳za p콏edpojatosti a sentimentu
                bias_score, _ = self.bias_detector.analyze_bias(article.content)
                sentiment_score = self.bias_detector.analyze_sentiment(article.content)
                
                article.bias_score = bias_score
                article.sentiment_score = sentiment_score
                
                analyzed_articles.append(article)
                
            except Exception as e:
                logger.warning(f"Chyba p콏i anal칳ze 캜l치nku {article.id}: {e}")
                continue
        
        return analyzed_articles
    
    def collect_and_store_news(self, languages: List[str] = ["en", "cs", "de", "fr"], 
                              categories: List[str] = None):
        """Sb칤r치 a ukl치d치 zpr치vy z r콢zn칳ch zdroj콢"""
        all_articles = []
        
        for language in languages:
            try:
                # Z칤sk치n칤 hlavn칤ch zpr치v
                raw_articles = self.news_collector.fetch_top_headlines(
                    language=language,
                    page_size=50
                )
                
                # Zpracov치n칤 a anal칳za
                processed_articles = self.process_raw_articles(raw_articles)
                analyzed_articles = self.analyze_articles(processed_articles)
                
                all_articles.extend(analyzed_articles)
                
                logger.info(f"Zpracov치no {len(analyzed_articles)} 캜l치nk콢 v jazyce {language}")
                
            except Exception as e:
                logger.error(f"Chyba p콏i zpracov치n칤 jazyka {language}: {e}")
                continue
        
        # Ulo쬰n칤 do datab치ze
        if all_articles:
            self.vector_store.insert_articles(all_articles)
        
        return all_articles
    
    def explore_topic(self, query: str, languages: List[str] = None, 
                     max_results: int = 20) -> Tuple[str, List[Dict], Dict]:
        """Pr콢zkum t칠matu nap콏칤캜 jazyky"""
        try:
            # P콏eklad dotazu do r콢zn칳ch jazyk콢
            translated_queries = [query]
            if languages:
                for lang in languages:
                    translated_query = self.summarizer.translate_query(query, lang)
                    translated_queries.append(translated_query)
            
            # Vyhled치n칤 relevantn칤ch 캜l치nk콢
            all_results = []
            for translated_query in translated_queries:
                query_embedding = self.embeddings.encode_texts([translated_query])[0]
                results = self.vector_store.search_similar(
                    query_vector=query_embedding,
                    top_k=max_results // len(translated_queries)
                )
                all_results.extend(results)
            
            # Odstran캩n칤 duplik치t콢
            unique_results = {}
            for result in all_results:
                if result["id"] not in unique_results:
                    unique_results[result["id"]] = result
            
            final_results = list(unique_results.values())[:max_results]
            
            # Generov치n칤 shrnut칤
            summary = self.summarizer.summarize_articles(final_results, query)
            
            # Statistiky
            stats = self._generate_statistics(final_results)
            
            return summary, final_results, stats
            
        except Exception as e:
            logger.error(f"Chyba p콏i pr콢zkumu t칠matu: {e}")
            return "Chyba p콏i zpracov치n칤 dotazu.", [], {}
    
    def _generate_statistics(self, articles: List[Dict]) -> Dict:
        """Generuje statistiky o 캜l치nc칤ch"""
        if not articles:
            return {}
        
        # Jazykov칠 rozlo쬰n칤
        language_counts = {}
        source_counts = {}
        bias_scores = []
        sentiment_scores = []
        
        for article in articles:
            # Jazyky
            lang = article.get("language", "unknown")
            language_counts[lang] = language_counts.get(lang, 0) + 1
            
            # Zdroje
            source = article.get("source", "Unknown")
            source_counts[source] = source_counts.get(source, 0) + 1
            
            # Sk칩re
            if article.get("bias_score") is not None:
                bias_scores.append(article["bias_score"])
            if article.get("sentiment_score") is not None:
                sentiment_scores.append(article["sentiment_score"])
        
        return {
            "total_articles": len(articles),
            "languages": language_counts,
            "sources": source_counts,
            "avg_bias_score": np.mean(bias_scores) if bias_scores else 0,
            "avg_sentiment_score": np.mean(sentiment_scores) if sentiment_scores else 0,
            "bias_scores": bias_scores,
            "sentiment_scores": sentiment_scores
        }

# Streamlit aplikace
def main():
    """Hlavn칤 Streamlit aplikace"""
    st.set_page_config(
        page_title="Multilingvn칤 Zpravodajsk칳 Pr콢zkumn칤k",
        page_icon="游깴",
        layout="wide"
    )
    
    st.title("游깴 Multilingvn칤 Zpravodajsk칳 Pr콢zkumn칤k")
    st.markdown("*Pr콢zkum glob치ln칤ch zpr치v s AI anal칳zou p콏edpojatosti*")
    st.markdown("---")
    
    # Na캜ten칤 API kl칤캜콢
    news_api_key = st.secrets.get("NEWS_API_KEY", "")
    openai_api_key = st.secrets.get("OPENAI_API_KEY", "")
    
    if not news_api_key or not openai_api_key:
        st.error("Pros칤m, nastavte API kl칤캜e v Streamlit secrets")
        st.stop()
    
    # Inicializace syst칠mu
    if 'explorer' not in st.session_state:
        with st.spinner("Inicializuji syst칠m..."):
            try:
                st.session_state.explorer = MultilingualNewsExplorer(
                    news_api_key=news_api_key,
                    openai_api_key=openai_api_key
                )
                st.success("Syst칠m p콏ipraven!")
            except Exception as e:
                st.error(f"Chyba p콏i inicializaci: {e}")
                st.stop()
    
    # Bo캜n칤 panel
    with st.sidebar:
        st.header("丘뙖잺 Nastaven칤")
        
        languages = st.multiselect(
            "Jazyky:",
            ["en", "cs", "de", "fr", "es", "it", "ru", "zh"],
            default=["en", "cs"]
        )
        
        max_results = st.slider("Max. v칳sledk콢:", 5, 50, 20)
        
        st.markdown("---")
        
        if st.button("游댃 Aktualizovat datab치zi zpr치v"):
            with st.spinner("Sb칤r치m nov칠 zpr치vy..."):
                articles = st.session_state.explorer.collect_and_store_news(languages)
                st.success(f"Zpracov치no {len(articles)} nov칳ch 캜l치nk콢")
    
    # Hlavn칤 obsah
    st.header("游댌 Pr콢zkum t칠matu")
    
    query = st.text_input(
        "Zadejte t칠ma pro pr콢zkum:",
        placeholder="nap콏. klimatick칠 zm캩ny, ekonomick치 krize, technologie AI..."
    )
    
    if st.button("游 Prozkoumat", type="primary") and query:
        with st.spinner("Analyzuji zpr치vy nap콏칤캜 jazyky..."):
            summary, articles, stats = st.session_state.explorer.explore_topic(
                query=query,
                languages=languages,
                max_results=max_results
            )
        
        if articles:
            # Zobrazen칤 shrnut칤
            st.subheader("游늶 Inteligentn칤 shrnut칤")
            st.markdown(summary)
            
            # Statistiky
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("Celkem 캜l치nk콢", stats.get("total_articles", 0))
            
            with col2:
                st.metric("Pr콢m캩rn치 p콏edpojatost", 
                         f"{stats.get('avg_bias_score', 0):.2f}")
            
            with col3:
                st.metric("Pr콢m캩rn칳 sentiment", 
                         f"{stats.get('avg_sentiment_score', 0):.2f}")
            
            with col4:
                st.metric("Po캜et jazyk콢", len(stats.get("languages", {})))
            
            # Grafy
            col1, col2 = st.columns(2)
            
            with col1:
                if stats.get("languages"):
                    fig_lang = px.pie(
                        values=list(stats["languages"].values()),
                        names=list(stats["languages"].keys()),
                        title="Rozlo쬰n칤 podle jazyk콢"
                    )
                    st.plotly_chart(fig_lang, use_container_width=True)
            
            with col2:
                if stats.get("bias_scores"):
                    fig_bias = px.histogram(
                        x=stats["bias_scores"],
                        nbins=20,
                        title="Distribuce p콏edpojatosti"
                    )
                    st.plotly_chart(fig_bias, use_container_width=True)
            
            # Seznam 캜l치nk콢
            st.subheader("游닗 Nalezen칠 캜l치nky")
            
            for i, article in enumerate(articles):
                with st.expander(
                    f"{i+1}. {article['title'][:100]}... "
                    f"({article['language'].upper()}) - "
                    f"P콏edpojatost: {article.get('bias_score', 0):.2f}"
                ):
                    col1, col2 = st.columns([3, 1])
                    
                    with col1:
                        st.write(f"**Zdroj:** {article['source']}")
                        st.write(f"**Datum:** {article['published_at']}")
                        st.write(f"**Obsah:** {article['content'][:500]}...")
                        st.write(f"**URL:** {article['url']}")
                    
                    with col2:
                        st.metric("Podobnost", f"{article.get('similarity_score', 0):.3f}")
                        st.metric("P콏edpojatost", f"{article.get('bias_score', 0):.2f}")
                        st.metric("Sentiment", f"{article.get('sentiment_score', 0):.2f}")
        else:
            st.warning("Nebyli nalezeny 쮂멳n칠 relevantn칤 캜l치nky.")

if __name__ == "__main__":
    main()
````

### Konfigurace a spu코t캩n칤

````python
# .streamlit/secrets.toml
NEWS_API_KEY = "your_news_api_key_here"
OPENAI_API_KEY = "your_openai_api_key_here"
````

````bash
# Spu코t캩n칤 Milvus (Docker)
docker run -p 19530:19530 -p 9091:9091 milvusdb/milvus:latest

# Spu코t캩n칤 aplikace
streamlit run multilingual_news_explorer.py
````

## Shrnut칤 Projektu

Multilingvn칤 zpravodajsk칳 pr콢zkumn칤k p콏edstavuje pokro캜il칠 AI 콏e코en칤 pro glob치ln칤 anal칳zu zpravodajstv칤. Syst칠m kombinuje nejmodern캩j코칤 technologie pro p콏ekon치n칤 jazykov칳ch bari칠r a poskytov치n칤 objektivn칤ho p콏ehledu sv캩tov칳ch ud치lost칤.

**Kl칤캜ov칠 inovace:**
- Cross-lingv치ln칤 vyhled치v치n칤 s multilingvn칤mi embeddings
- Automatick치 detekce p콏edpojatosti pomoc칤 GPT-4 Turbo
- Real-time integrace s glob치ln칤mi zpravodajsk칳mi zdroji
- Inteligentn칤 sumarizace nap콏칤캜 r콢zn칳mi perspektivami

**Technick칠 v칳hody:**
- 맒치lovateln치 architektura s Milvus datab치z칤
- Pokro캜il치 NLP anal칳za sentimentu a biasu
- Multilingvn칤 podpora pro des칤tky jazyk콢
- Interaktivn칤 vizualizace v칳sledk콢

**Spole캜ensk칳 dopad:**
Projekt m콢쬰 v칳znamn캩 p콏isp캩t k medi치ln칤 gramotnosti, pom치h치 u쬴vatel콢m rozpoznat p콏edpojatost v m칠di칤ch a poskytuje komplexn칤 pohled na glob치ln칤 ud치losti bez jazykov칳ch omezen칤.