<small>Claude Sonnet 4 **(Historical Archive Explorer - RAG syst√©m pro pr≈Øzkum historick√Ωch archiv≈Ø)**</small>
# Historical Archive Explorer

## Kl√≠ƒçov√© koncepty projektu

### RAG (Retrieval-Augmented Generation)
Hybridn√≠ p≈ô√≠stup kombinuj√≠c√≠ vyhled√°v√°n√≠ relevantn√≠ch informac√≠ z datab√°ze s generativn√≠ AI. RAG nejprve najde souvisej√≠c√≠ dokumenty a pot√© je pou≈æije jako kontext pro generov√°n√≠ odpovƒõdi.

### Llama 3
Open-source velk√Ω jazykov√Ω model od Meta, kter√Ω poskytuje pokroƒçil√© schopnosti porozumƒõn√≠ textu a generov√°n√≠ odpovƒõd√≠. Ide√°ln√≠ pro lok√°ln√≠ nasazen√≠ a pr√°ci s citliv√Ωmi historick√Ωmi daty.

### OCR (Optical Character Recognition)
Technologie pro rozpozn√°v√°n√≠ textu ze skenovan√Ωch dokument≈Ø a obr√°zk≈Ø. Kl√≠ƒçov√° pro zpracov√°n√≠ historick√Ωch novin, knih a rukopis≈Ø v digit√°ln√≠ podobƒõ.

### Vector Search (Vektorov√© vyhled√°v√°n√≠)
Metoda vyhled√°v√°n√≠ zalo≈æen√° na s√©mantick√© podobnosti, kde jsou texty p≈ôevedeny na vektory a vyhled√°v√°n√≠ prob√≠h√° na z√°kladƒõ vzd√°lenosti v multidimenzion√°ln√≠m prostoru.

### Timeline Generation
Automatick√© vytv√°≈ôen√≠ ƒçasov√Ωch os a chronologick√Ωch p≈ôehled≈Ø na z√°kladƒõ extrahovan√Ωch dat z historick√Ωch dokument≈Ø.

## Komplexn√≠ vysvƒõtlen√≠ projektu

Historical Archive Explorer je pokroƒçil√Ω RAG syst√©m navr≈æen√Ω pro digitalizaci, indexaci a inteligentn√≠ pr≈Øzkum historick√Ωch archiv≈Ø. Projekt ≈ôe≈°√≠ probl√©m nedostupnosti a obt√≠≈æn√©ho prohled√°v√°n√≠ historick√Ωch dokument≈Ø, kter√© jsou ƒçasto ulo≈æeny pouze ve fyzick√© podobƒõ nebo jako naskenovan√© obr√°zky bez mo≈ænosti textov√©ho vyhled√°v√°n√≠.

### Hlavn√≠ c√≠le projektu:
- **Digitalizace**: P≈ôevod fyzick√Ωch dokument≈Ø do digit√°ln√≠ podoby pomoc√≠ OCR
- **Indexace**: Vytvo≈ôen√≠ s√©mantick√Ωch index≈Ø pro efektivn√≠ vyhled√°v√°n√≠
- **Kontextu√°ln√≠ vyhled√°v√°n√≠**: Umo≈ænƒõn√≠ dotaz≈Ø v p≈ôirozen√©m jazyce
- **ƒåasov√° anal√Ωza**: Automatick√© vytv√°≈ôen√≠ chronologi√≠ a ƒçasov√Ωch trend≈Ø
- **Zachov√°n√≠ kontextu**: Udr≈æen√≠ historick√©ho a kulturn√≠ho kontextu p≈ôi odpovƒõd√≠ch

### Technick√© v√Ωzvy:
- Kvalita OCR pro star√© a po≈°kozen√© dokumenty
- Zpracov√°n√≠ r≈Øzn√Ωch jazyk≈Ø a historick√Ωch variant ƒçe≈°tiny
- ≈†k√°lovatelnost pro velk√© archivy
- P≈ôesnost s√©mantick√©ho vyhled√°v√°n√≠

### Potenci√°ln√≠ dopad:
Projekt m≈Ø≈æe revolutionizovat pr√°ci historik≈Ø, novin√°≈ô≈Ø a v√Ωzkumn√≠k≈Ø t√≠m, ≈æe uƒçin√≠ historick√© prameny dostupnƒõj≈°√≠mi a prohled√°vatelƒõj≈°√≠mi.

## Komplexn√≠ implementace projektu

````python
langchain==0.1.0
llama-cpp-python==0.2.20
chromadb==0.4.18
pytesseract==0.3.10
Pillow==10.1.0
streamlit==1.29.0
pandas==2.1.4
numpy==1.25.2
python-dotenv==1.0.0
requests==2.31.0
beautifulsoup4==4.12.2
plotly==5.17.0
sentence-transformers==2.2.2
````

````python
import pytesseract
from PIL import Image
import os
import logging
from typing import List, Dict, Optional
import cv2
import numpy as np

class OCRProcessor:
    """Zpracov√°n√≠ OCR pro historick√© dokumenty"""
    
    def __init__(self, tesseract_path: Optional[str] = None):
        if tesseract_path:
            pytesseract.pytesseract.tesseract_cmd = tesseract_path
        
        self.logger = logging.getLogger(__name__)
        
        # Konfigurace pro ƒçesk√© texty
        self.czech_config = '--psm 6 -l ces+eng'
        
    def preprocess_image(self, image_path: str) -> np.ndarray:
        """P≈ôedzpracov√°n√≠ obr√°zku pro lep≈°√≠ OCR v√Ωsledky"""
        try:
            # Naƒçten√≠ obr√°zku
            img = cv2.imread(image_path)
            
            # P≈ôevod na ≈°edou
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            
            # Zlep≈°en√≠ kontrastu
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
            enhanced = clahe.apply(gray)
            
            # Odstranƒõn√≠ ≈°umu
            denoised = cv2.medianBlur(enhanced, 3)
            
            # Prahov√°n√≠
            _, thresh = cv2.threshold(denoised, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
            
            return thresh
            
        except Exception as e:
            self.logger.error(f"Chyba p≈ôi p≈ôedzpracov√°n√≠ obr√°zku {image_path}: {e}")
            return cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    
    def extract_text(self, image_path: str) -> Dict[str, str]:
        """Extrakce textu z obr√°zku"""
        try:
            # P≈ôedzpracov√°n√≠
            processed_img = self.preprocess_image(image_path)
            
            # OCR s konfidenc√≠
            data = pytesseract.image_to_data(
                processed_img, 
                config=self.czech_config, 
                output_type=pytesseract.Output.DICT
            )
            
            # Filtrov√°n√≠ slov s n√≠zkou konfidenc√≠
            confident_words = []
            for i, conf in enumerate(data['conf']):
                if int(conf) > 30:  # Minim√°ln√≠ confidence
                    word = data['text'][i].strip()
                    if word:
                        confident_words.append(word)
            
            text = ' '.join(confident_words)
            
            # Z√°kladn√≠ text z cel√©ho obr√°zku
            full_text = pytesseract.image_to_string(
                processed_img, 
                config=self.czech_config
            )
            
            return {
                'full_text': full_text,
                'confident_text': text,
                'source_file': os.path.basename(image_path)
            }
            
        except Exception as e:
            self.logger.error(f"Chyba p≈ôi OCR zpracov√°n√≠ {image_path}: {e}")
            return {
                'full_text': '',
                'confident_text': '',
                'source_file': os.path.basename(image_path),
                'error': str(e)
            }
    
    def batch_process(self, image_directory: str) -> List[Dict[str, str]]:
        """D√°vkov√© zpracov√°n√≠ obr√°zk≈Ø"""
        results = []
        supported_formats = ('.png', '.jpg', '.jpeg', '.tiff', '.bmp')
        
        for filename in os.listdir(image_directory):
            if filename.lower().endswith(supported_formats):
                image_path = os.path.join(image_directory, filename)
                result = self.extract_text(image_path)
                results.append(result)
                self.logger.info(f"Zpracov√°n soubor: {filename}")
        
        return results
````

````python
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
import uuid
from typing import List, Dict, Optional
import logging

class HistoricalVectorStore:
    """Vektorov√© √∫lo≈æi≈°tƒõ pro historick√© dokumenty"""
    
    def __init__(self, persist_directory: str = "./chroma_db"):
        self.client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(anonymized_telemetry=False)
        )
        
        # ƒåesk√© embedding modely
        self.embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        
        self.collection = self.client.get_or_create_collection(
            name="historical_documents",
            metadata={"description": "Historick√© dokumenty a noviny"}
        )
        
        self.logger = logging.getLogger(__name__)
    
    def add_documents(self, documents: List[Dict[str, str]]) -> None:
        """P≈ôid√°n√≠ dokument≈Ø do vektorov√©ho √∫lo≈æi≈°tƒõ"""
        try:
            texts = []
            metadatas = []
            ids = []
            
            for doc in documents:
                if doc.get('confident_text') and len(doc['confident_text'].strip()) > 10:
                    text = doc['confident_text']
                    
                    # Vytvo≈ôen√≠ metadat
                    metadata = {
                        'source_file': doc.get('source_file', 'unknown'),
                        'full_text': doc.get('full_text', '')[:1000],  # Omezen√≠ d√©lky
                        'length': len(text),
                        'has_error': 'error' in doc
                    }
                    
                    texts.append(text)
                    metadatas.append(metadata)
                    ids.append(str(uuid.uuid4()))
            
            if texts:
                # Generov√°n√≠ embedding≈Ø
                embeddings = self.embedding_model.encode(texts).tolist()
                
                # P≈ôid√°n√≠ do Chroma DB
                self.collection.add(
                    embeddings=embeddings,
                    documents=texts,
                    metadatas=metadatas,
                    ids=ids
                )
                
                self.logger.info(f"P≈ôid√°no {len(texts)} dokument≈Ø do vektorov√©ho √∫lo≈æi≈°tƒõ")
            
        except Exception as e:
            self.logger.error(f"Chyba p≈ôi p≈ôid√°v√°n√≠ dokument≈Ø: {e}")
    
    def search(self, query: str, n_results: int = 5) -> List[Dict]:
        """Vyhled√°v√°n√≠ v historick√Ωch dokumentech"""
        try:
            # Generov√°n√≠ embedding pro dotaz
            query_embedding = self.embedding_model.encode([query]).tolist()
            
            # Vyhled√°v√°n√≠
            results = self.collection.query(
                query_embeddings=query_embedding,
                n_results=n_results,
                include=['documents', 'metadatas', 'distances']
            )
            
            # Form√°tov√°n√≠ v√Ωsledk≈Ø
            formatted_results = []
            for i in range(len(results['documents'][0])):
                formatted_results.append({
                    'document': results['documents'][0][i],
                    'metadata': results['metadatas'][0][i],
                    'similarity': 1 - results['distances'][0][i],  # P≈ôevod distance na similarity
                })
            
            return formatted_results
            
        except Exception as e:
            self.logger.error(f"Chyba p≈ôi vyhled√°v√°n√≠: {e}")
            return []
    
    def get_statistics(self) -> Dict:
        """Statistiky √∫lo≈æi≈°tƒõ"""
        try:
            count = self.collection.count()
            return {
                'total_documents': count,
                'collection_name': self.collection.name
            }
        except Exception as e:
            self.logger.error(f"Chyba p≈ôi z√≠sk√°v√°n√≠ statistik: {e}")
            return {'total_documents': 0}
````

````python
from llama_cpp import Llama
import json
from typing import List, Dict, Optional
import logging

class LlamaHistoricalQA:
    """Integrace Llama 3 pro historick√© dotazy"""
    
    def __init__(self, model_path: str, context_window: int = 4096):
        try:
            self.llm = Llama(
                model_path=model_path,
                n_ctx=context_window,
                n_threads=4,
                verbose=False
            )
            self.logger = logging.getLogger(__name__)
            self.logger.info("Llama model √∫spƒõ≈°nƒõ naƒçten")
            
        except Exception as e:
            self.logger.error(f"Chyba p≈ôi naƒç√≠t√°n√≠ Llama modelu: {e}")
            raise
    
    def create_historical_prompt(self, query: str, context_documents: List[Dict]) -> str:
        """Vytvo≈ôen√≠ promptu pro historick√© dotazy"""
        
        context_text = "\n\n".join([
            f"Dokument {i+1} (ze souboru {doc['metadata']['source_file']}):\n{doc['document']}"
            for i, doc in enumerate(context_documents)
        ])
        
        prompt = f"""Jsi expert na ƒçeskou historii a archivn√≠ dokumenty. Na z√°kladƒõ poskytnut√Ωch historick√Ωch dokument≈Ø odpovƒõz na n√°sleduj√≠c√≠ dotaz v ƒçe≈°tinƒõ.

HISTORICK√â DOKUMENTY:
{context_text}

DOTAZ: {query}

INSTRUKCE:
- Odpovƒõz pouze na z√°kladƒõ poskytnut√Ωch dokument≈Ø
- Pokud informace v dokumentech nejsou, jasnƒõ to uveƒè
- Zachovej historick√Ω kontext a uv√°dej zdroje
- Pou≈æ√≠vej form√°ln√≠, ale srozumiteln√Ω jazyk
- Pokud jsou v dokumentech data, uveƒè je p≈ôesnƒõ

ODPOVƒöƒé:"""
        
        return prompt
    
    def generate_answer(self, prompt: str, max_tokens: int = 512) -> str:
        """Generov√°n√≠ odpovƒõdi pomoc√≠ Llama"""
        try:
            response = self.llm(
                prompt,
                max_tokens=max_tokens,
                temperature=0.3,
                top_p=0.9,
                stop=["DOTAZ:", "INSTRUKCE:", "U≈æivatel:"],
                echo=False
            )
            
            answer = response['choices'][0]['text'].strip()
            return answer
            
        except Exception as e:
            self.logger.error(f"Chyba p≈ôi generov√°n√≠ odpovƒõdi: {e}")
            return f"Omlouv√°me se, do≈°lo k chybƒõ p≈ôi zpracov√°n√≠ dotazu: {str(e)}"
    
    def extract_dates(self, text: str) -> List[str]:
        """Extrakce dat z textu pro timeline"""
        import re
        
        # Vzory pro ƒçesk√° data
        date_patterns = [
            r'\d{1,2}\.\s*\d{1,2}\.\s*\d{4}',  # DD.MM.YYYY
            r'\d{4}',  # YYYY
            r'\d{1,2}\.\s*\d{4}',  # MM.YYYY
        ]
        
        dates = []
        for pattern in date_patterns:
            matches = re.findall(pattern, text)
            dates.extend(matches)
        
        return list(set(dates))  # Odstranƒõn√≠ duplik√°t≈Ø
````

````python
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
import re
from typing import List, Dict, Optional
import logging

class HistoricalTimelineGenerator:
    """Gener√°tor ƒçasov√Ωch os z historick√Ωch dat"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def parse_czech_date(self, date_str: str) -> Optional[datetime]:
        """Parsov√°n√≠ ƒçesk√Ωch form√°t≈Ø dat"""
        date_str = date_str.strip()
        
        # R≈Øzn√© form√°ty dat
        formats = [
            '%d.%m.%Y',
            '%d. %m. %Y',
            '%m.%Y',
            '%m. %Y',
            '%Y'
        ]
        
        for fmt in formats:
            try:
                if fmt == '%Y':
                    return datetime.strptime(date_str, fmt)
                elif fmt in ['%m.%Y', '%m. %Y']:
                    return datetime.strptime(date_str, fmt)
                else:
                    return datetime.strptime(date_str, fmt)
            except ValueError:
                continue
        
        return None
    
    def extract_events_from_documents(self, documents: List[Dict]) -> List[Dict]:
        """Extrakce ud√°lost√≠ z dokument≈Ø"""
        events = []
        
        for doc in documents:
            text = doc.get('document', '')
            source = doc.get('metadata', {}).get('source_file', 'Nezn√°m√Ω zdroj')
            
            # Extrakce dat
            date_pattern = r'(\d{1,2}\.\s*\d{1,2}\.\s*\d{4}|\d{4}|\d{1,2}\.\s*\d{4})'
            dates = re.findall(date_pattern, text)
            
            # Extrakce vƒõt s daty
            sentences = text.split('.')
            
            for sentence in sentences:
                sentence = sentence.strip()
                if len(sentence) > 20:  # Minim√°ln√≠ d√©lka vƒõty
                    for date_str in dates:
                        if date_str in sentence:
                            parsed_date = self.parse_czech_date(date_str)
                            if parsed_date:
                                events.append({
                                    'date': parsed_date,
                                    'date_str': date_str,
                                    'event': sentence[:200] + '...' if len(sentence) > 200 else sentence,
                                    'source': source,
                                    'full_text': text[:500]
                                })
                                break
        
        return events
    
    def create_timeline_chart(self, events: List[Dict]) -> go.Figure:
        """Vytvo≈ôen√≠ interaktivn√≠ ƒçasov√© osy"""
        if not events:
            fig = go.Figure()
            fig.add_annotation(
                text="Nebyly nalezeny ≈æ√°dn√© datovan√© ud√°losti",
                xref="paper", yref="paper",
                x=0.5, y=0.5, showarrow=False
            )
            return fig
        
        # P≈ôevod na DataFrame
        df = pd.DataFrame(events)
        df = df.sort_values('date')
        
        # Vytvo≈ôen√≠ timeline grafu
        fig = go.Figure()
        
        # P≈ôid√°n√≠ bod≈Ø na ƒçasovou osu
        fig.add_trace(go.Scatter(
            x=df['date'],
            y=[1] * len(df),
            mode='markers+text',
            marker=dict(
                size=12,
                color='blue',
                line=dict(width=2, color='darkblue')
            ),
            text=df['date_str'],
            textposition='top center',
            hovertemplate='<b>Datum:</b> %{text}<br>' +
                         '<b>Ud√°lost:</b> %{customdata[0]}<br>' +
                         '<b>Zdroj:</b> %{customdata[1]}<extra></extra>',
            customdata=list(zip(df['event'], df['source'])),
            name='Historick√© ud√°losti'
        ))
        
        # Form√°tov√°n√≠ grafu
        fig.update_layout(
            title='ƒåasov√° osa historick√Ωch ud√°lost√≠',
            xaxis_title='Datum',
            yaxis=dict(
                showticklabels=False,
                showgrid=False,
                zeroline=False,
                range=[0.5, 1.5]
            ),
            showlegend=False,
            height=400,
            margin=dict(t=50, b=50, l=50, r=50)
        )
        
        return fig
    
    def generate_summary_stats(self, events: List[Dict]) -> Dict:
        """Generov√°n√≠ souhrnn√Ωch statistik"""
        if not events:
            return {'total_events': 0}
        
        df = pd.DataFrame(events)
        
        # Extrakce rok≈Ø
        years = [event['date'].year for event in events]
        year_counts = pd.Series(years).value_counts()
        
        return {
            'total_events': len(events),
            'date_range': f"{min(years)} - {max(years)}",
            'most_active_year': year_counts.index[0] if not year_counts.empty else None,
            'events_in_most_active_year': year_counts.iloc[0] if not year_counts.empty else 0,
            'unique_sources': len(df['source'].unique())
        }
````

````python
import streamlit as st
import os
import logging
from datetime import datetime
import pandas as pd

from ocr_processor import OCRProcessor
from vector_store import HistoricalVectorStore
from llm_integration import LlamaHistoricalQA
from timeline_generator import HistoricalTimelineGenerator

# Konfigurace loggingu
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class HistoricalArchiveExplorer:
    """Hlavn√≠ aplikace pro pr≈Øzkum historick√Ωch archiv≈Ø"""
    
    def __init__(self):
        self.ocr_processor = None
        self.vector_store = None
        self.llm_qa = None
        self.timeline_generator = HistoricalTimelineGenerator()
    
    def initialize_components(self):
        """Inicializace komponent"""
        try:
            # OCR processor
            self.ocr_processor = OCRProcessor()
            
            # Vector store
            self.vector_store = HistoricalVectorStore()
            
            # LLM (pouze pokud je dostupn√Ω model)
            model_path = st.session_state.get('llm_model_path')
            if model_path and os.path.exists(model_path):
                self.llm_qa = LlamaHistoricalQA(model_path)
            
            return True
            
        except Exception as e:
            st.error(f"Chyba p≈ôi inicializaci: {e}")
            return False
    
    def create_sample_data(self):
        """Vytvo≈ôen√≠ uk√°zkov√Ωch dat pro demonstraci"""
        sample_documents = [
            {
                'confident_text': 'Dne 28. ≈ô√≠jna 1918 byla vyhl√°≈°ena samostatnost ƒåeskoslovensk√© republiky. Tom√°≈° Garrigue Masaryk se stal prvn√≠m prezidentem.',
                'full_text': 'Dne 28. ≈ô√≠jna 1918 byla vyhl√°≈°ena samostatnost ƒåeskoslovensk√© republiky. Tom√°≈° Garrigue Masaryk se stal prvn√≠m prezidentem nov√©ho st√°tu.',
                'source_file': 'noviny_1918_10.jpg'
            },
            {
                'confident_text': 'V√°lka skonƒçila 11. listopadu 1918 podpisem p≈ô√≠mƒõ≈ô√≠ v Compi√®gne. Pro ƒåechy a Slov√°ky to znamenalo svobodu.',
                'full_text': 'Prvn√≠ svƒõtov√° v√°lka skonƒçila 11. listopadu 1918 podpisem p≈ô√≠mƒõ≈ô√≠ v Compi√®gne. Pro ƒåechy a Slov√°ky to znamenalo koneƒçnƒõ svobodu od rakousko-uhersk√© nadvl√°dy.',
                'source_file': 'zpravy_1918_11.jpg'
            },
            {
                'confident_text': 'V roce 1920 byla p≈ôijata nov√° √∫stava ƒåeskoslovensk√© republiky. Zemi ƒçekalo obdob√≠ budov√°n√≠ demokracie.',
                'full_text': 'V roce 1920 byla p≈ôijata nov√° √∫stava ƒåeskoslovensk√© republiky, kter√° ustanovila parlamentn√≠ syst√©m. Zemi ƒçekalo obdob√≠ budov√°n√≠ modern√≠ demokracie.',
                'source_file': 'ustava_1920.jpg'
            }
        ]
        
        return sample_documents

def main():
    st.set_page_config(
        page_title="Historical Archive Explorer",
        page_icon="üìö",
        layout="wide"
    )
    
    st.title("üìö Historical Archive Explorer")
    st.subtitle("RAG syst√©m pro pr≈Øzkum historick√Ωch archiv≈Ø")
    
    # Inicializace session state
    if 'app' not in st.session_state:
        st.session_state.app = HistoricalArchiveExplorer()
    
    app = st.session_state.app
    
    # Sidebar pro konfiguraci
    with st.sidebar:
        st.header("‚öôÔ∏è Konfigurace")
        
        # Nastaven√≠ Llama modelu
        st.subheader("LLM Model")
        model_path = st.text_input(
            "Cesta k Llama modelu (voliteln√©)",
            placeholder="/path/to/llama-model.gguf",
            help="Pokud m√°te lok√°ln√≠ Llama model, zadejte cestu k nƒõmu"
        )
        
        if model_path:
            st.session_state.llm_model_path = model_path
        
        # Tlaƒç√≠tko pro inicializaci
        if st.button("üîÑ Inicializovat syst√©m"):
            with st.spinner("Inicializace komponent..."):
                if app.initialize_components():
                    st.success("‚úÖ Syst√©m inicializov√°n!")
                    
                    # Naƒçten√≠ uk√°zkov√Ωch dat
                    sample_data = app.create_sample_data()
                    app.vector_store.add_documents(sample_data)
                    st.info("üìÑ Naƒçtena uk√°zkov√° data")
        
        # Statistiky
        if app.vector_store:
            st.subheader("üìä Statistiky")
            stats = app.vector_store.get_statistics()
            st.metric("Dokumenty v datab√°zi", stats.get('total_documents', 0))
    
    # Hlavn√≠ rozhran√≠
    tabs = st.tabs(["üîç Vyhled√°v√°n√≠", "üìÅ Nahr√°v√°n√≠ dokument≈Ø", "üìà ƒåasov√° osa", "‚ÑπÔ∏è O projektu"])
    
    # Tab 1: Vyhled√°v√°n√≠
    with tabs[0]:
        st.header("üîç Vyhled√°v√°n√≠ v historick√Ωch archivech")
        
        if not app.vector_store:
            st.warning("‚ö†Ô∏è Nejprve inicializujte syst√©m v postrann√≠m panelu")
            return
        
        # Vyhled√°vac√≠ formul√°≈ô
        query = st.text_input(
            "Zadejte dotaz:",
            placeholder="Nap≈ô: Kdy byla vyhl√°≈°ena samostatnost ƒåeskoslovenska?"
        )
        
        col1, col2 = st.columns([1, 4])
        with col1:
            search_button = st.button("üîç Vyhledat", type="primary")
        with col2:
            n_results = st.slider("Poƒçet v√Ωsledk≈Ø", 1, 10, 5)
        
        if search_button and query:
            with st.spinner("Vyhled√°v√°n√≠ v archivech..."):
                # Vyhled√°n√≠ dokument≈Ø
                results = app.vector_store.search(query, n_results)
                
                if results:
                    st.success(f"‚úÖ Nalezeno {len(results)} relevantn√≠ch dokument≈Ø")
                    
                    # Zobrazen√≠ v√Ωsledk≈Ø
                    for i, result in enumerate(results, 1):
                        with st.expander(f"üìÑ Dokument {i} (podobnost: {result['similarity']:.2%})"):
                            st.write("**Obsah:**")
                            st.write(result['document'])
                            st.write(f"**Zdroj:** {result['metadata']['source_file']}")
                    
                    # Generov√°n√≠ odpovƒõdi pomoc√≠ LLM (pokud je k dispozici)
                    if app.llm_qa:
                        st.subheader("ü§ñ AI odpovƒõƒè")
                        with st.spinner("Generov√°n√≠ odpovƒõdi..."):
                            prompt = app.llm_qa.create_historical_prompt(query, results)
                            answer = app.llm_qa.generate_answer(prompt)
                            st.write(answer)
                    else:
                        st.info("üí° Pro AI odpovƒõdi zadejte cestu k Llama modelu v konfiguraci")
                else:
                    st.warning("üîç Nebyly nalezeny ≈æ√°dn√© relevantn√≠ dokumenty")
    
    # Tab 2: Nahr√°v√°n√≠ dokument≈Ø
    with tabs[1]:
        st.header("üìÅ Nahr√°v√°n√≠ a zpracov√°n√≠ dokument≈Ø")
        
        if not app.ocr_processor:
            st.warning("‚ö†Ô∏è Nejprve inicializujte syst√©m v postrann√≠m panelu")
            return
        
        uploaded_files = st.file_uploader(
            "Vyberte obr√°zky historick√Ωch dokument≈Ø",
            type=['png', 'jpg', 'jpeg', 'tiff', 'bmp'],
            accept_multiple_files=True
        )
        
        if uploaded_files:
            if st.button("üìù Zpracovat dokumenty"):
                progress_bar = st.progress(0)
                processed_docs = []
                
                for i, uploaded_file in enumerate(uploaded_files):
                    # Ulo≈æen√≠ doƒçasn√©ho souboru
                    temp_path = f"temp_{uploaded_file.name}"
                    with open(temp_path, "wb") as f:
                        f.write(uploaded_file.getbuffer())
                    
                    # OCR zpracov√°n√≠
                    result = app.ocr_processor.extract_text(temp_path)
                    processed_docs.append(result)
                    
                    # Cleanup
                    os.remove(temp_path)
                    
                    # Aktualizace progress baru
                    progress_bar.progress((i + 1) / len(uploaded_files))
                
                # P≈ôid√°n√≠ do vektorov√© datab√°ze
                app.vector_store.add_documents(processed_docs)
                
                st.success(f"‚úÖ Zpracov√°no {len(processed_docs)} dokument≈Ø")
                
                # Zobrazen√≠ v√Ωsledk≈Ø
                for doc in processed_docs:
                    if doc.get('confident_text'):
                        with st.expander(f"üìÑ {doc['source_file']}"):
                            st.write("**Rozpoznan√Ω text:**")
                            st.write(doc['confident_text'])
    
    # Tab 3: ƒåasov√° osa
    with tabs[2]:
        st.header("üìà Generov√°n√≠ ƒçasov√© osy")
        
        if not app.vector_store:
            st.warning("‚ö†Ô∏è Nejprve inicializujte syst√©m")
            return
        
        if st.button("üìÖ Vytvo≈ôit ƒçasovou osu"):
            with st.spinner("Anal√Ωza dokument≈Ø a vytv√°≈ôen√≠ ƒçasov√© osy..."):
                # Z√≠sk√°n√≠ v≈°ech dokument≈Ø
                all_results = app.vector_store.search("", n_results=50)
                
                # Extrakce ud√°lost√≠
                events = app.timeline_generator.extract_events_from_documents(all_results)
                
                if events:
                    # Zobrazen√≠ ƒçasov√© osy
                    fig = app.timeline_generator.create_timeline_chart(events)
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # Statistiky
                    stats = app.timeline_generator.generate_summary_stats(events)
                    
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("Celkem ud√°lost√≠", stats['total_events'])
                    with col2:
                        st.metric("ƒåasov√© rozpƒõt√≠", stats.get('date_range', 'N/A'))
                    with col3:
                        st.metric("Nejaktivnƒõj≈°√≠ rok", stats.get('most_active_year', 'N/A'))
                    
                    # Detail ud√°lost√≠
                    st.subheader("üìã Seznam ud√°lost√≠")
                    events_df = pd.DataFrame([
                        {
                            'Datum': event['date_str'],
                            'Ud√°lost': event['event'][:100] + '...' if len(event['event']) > 100 else event['event'],
                            'Zdroj': event['source']
                        }
                        for event in sorted(events, key=lambda x: x['date'])
                    ])
                    st.dataframe(events_df, use_container_width=True)
                else:
                    st.info("üìÖ V dokumentech nebyly nalezeny datovan√© ud√°losti")
    
    # Tab 4: O projektu
    with tabs[3]:
        st.header("‚ÑπÔ∏è O projektu Historical Archive Explorer")
        
        st.markdown("""
        ### üéØ √öƒçel projektu
        Historical Archive Explorer je pokroƒçil√Ω RAG (Retrieval-Augmented Generation) syst√©m 
        navr≈æen√Ω pro digitalizaci, indexaci a inteligentn√≠ pr≈Øzkum historick√Ωch archiv≈Ø.
        
        ### üîß Kl√≠ƒçov√© technologie
        - **RAG architektura**: Kombinace vyhled√°v√°n√≠ a generativn√≠ AI
        - **Llama 3**: Open-source jazykov√Ω model pro lok√°ln√≠ nasazen√≠
        - **OCR (Tesseract)**: Rozpozn√°v√°n√≠ textu z historick√Ωch dokument≈Ø
        - **ChromaDB**: Vektorov√° datab√°ze pro s√©mantick√© vyhled√°v√°n√≠
        - **Streamlit**: Modern√≠ webov√© rozhran√≠
        
        ### üìà Funkce
        1. **OCR zpracov√°n√≠**: P≈ôevod skenovan√Ωch dokument≈Ø na text
        2. **Vektorov√© vyhled√°v√°n√≠**: S√©mantick√© hled√°n√≠ v archivech
        3. **AI asistent**: Inteligentn√≠ odpovƒõdi na historick√© dotazy
        4. **ƒåasov√© osy**: Automatick√© vytv√°≈ôen√≠ chronologi√≠
        
        ### üöÄ Vyu≈æit√≠
        - Historick√Ω v√Ωzkum
        - Archivn√≠ pr√°ce
        - Novin√°≈ôsk√° investigace
        - Vzdƒõl√°v√°n√≠
        """)

if __name__ == "__main__":
    main()
````

````python
"""
Instalaƒçn√≠ skript pro Historical Archive Explorer
"""

import subprocess
import sys
import os

def install_requirements():
    """Instalace po≈æadovan√Ωch bal√≠ƒçk≈Ø"""
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-r", "requirements.txt"])
        print("‚úÖ V≈°echny bal√≠ƒçky byly √∫spƒõ≈°nƒõ nainstalov√°ny")
    except subprocess.CalledProcessError as e:
        print(f"‚ùå Chyba p≈ôi instalaci bal√≠ƒçk≈Ø: {e}")
        return False
    return True

def setup_directories():
    """Vytvo≈ôen√≠ pot≈ôebn√Ωch adres√°≈ô≈Ø"""
    directories = ["data", "models", "temp", "chroma_db"]
    
    for directory in directories:
        if not os.path.exists(directory):
            os.makedirs(directory)
            print(f"üìÅ Vytvo≈ôen adres√°≈ô: {directory}")
    
    return True

def download_sample_model():
    """Sta≈æen√≠ uk√°zkov√©ho modelu (voliteln√©)"""
    print("""
üì• Pro plnou funkcionalnost doporuƒçujeme st√°hnout Llama model:

1. Nav≈°tivte: https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML
2. St√°hnƒõte soubor s p≈ô√≠ponou .gguf
3. Um√≠stƒõte do slo≈æky 'models/'
4. Zadejte cestu v aplikaci

Alternativnƒõ m≈Ø≈æete pou≈æ√≠t p≈ô√≠kaz:
wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.q4_0.gguf -O models/llama-model.gguf
    """)

def main():
    print("üöÄ Nastaven√≠ Historical Archive Explorer")
    print("=" * 50)
    
    # Instalace bal√≠ƒçk≈Ø
    print("1. Instalace Python bal√≠ƒçk≈Ø...")
    if not install_requirements():
        return
    
    # Vytvo≈ôen√≠ adres√°≈ô≈Ø
    print("\n2. Vytv√°≈ôen√≠ adres√°≈ôov√© struktury...")
    setup_directories()
    
    # Informace o modelu
    print("\n3. Informace o jazykov√©m modelu...")
    download_sample_model()
    
    print("\n‚úÖ Nastaven√≠ dokonƒçeno!")
    print("\nüöÄ Spu≈°tƒõn√≠ aplikace:")
    print("streamlit run src/main_app.py")

if __name__ == "__main__":
    main()
````

## Shrnut√≠ projektu

Historical Archive Explorer p≈ôedstavuje pokroƒçil√© ≈ôe≈°en√≠ pro digitalizaci a pr≈Øzkum historick√Ωch archiv≈Ø pomoc√≠ modern√≠ch AI technologi√≠. Projekt kombinuje **OCR zpracov√°n√≠**, **vektorov√© vyhled√°v√°n√≠** a **generativn√≠ AI** do komplexn√≠ho syst√©mu.

### Kl√≠ƒçov√© v√Ωhody:
- **Dostupnost**: Zp≈ô√≠stupnƒõn√≠ historick√Ωch dokument≈Ø ≈°ir≈°√≠ ve≈ôejnosti
- **Efektivita**: Rychl√© vyhled√°v√°n√≠ v rozs√°hl√Ωch archivech
- **P≈ôesnost**: S√©mantick√© porozumƒõn√≠ historick√©mu kontextu
- **≈†k√°lovatelnost**: Mo≈ænost roz≈°√≠≈ôen√≠ na tis√≠ce dokument≈Ø

### Technick√© p≈ôednosti:
- **Lok√°ln√≠ nasazen√≠** s Llama 3 zaji≈°≈•uje soukrom√≠ dat
- **Modul√°rn√≠ architektura** umo≈æ≈àuje snadn√© roz≈°√≠≈ôen√≠
- **Modern√≠ UI** s real-time anal√Ωzou a vizualizacemi
- **Podpora ƒçe≈°tiny** vƒçetnƒõ historick√Ωch variant

Projekt m√° potenci√°l revolutionizovat pr√°ci s historick√Ωmi prameny a uƒçinit na≈°i minulost dostupnƒõj≈°√≠ pro v√Ωzkum i vzdƒõl√°v√°n√≠.