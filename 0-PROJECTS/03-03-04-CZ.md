<small>Claude Sonnet 4 **(Academic Research Synthesizer - RAG SystÃ©m pro SyntÃ©zu VÄ›deckÃ½ch PracÃ­)**</small>
# Academic Research Synthesizer

## 1. KlÃ­ÄovÃ© Koncepty

### **RAG (Retrieval-Augmented Generation)**
HybridnÃ­ pÅ™Ã­stup kombinujÃ­cÃ­ informaÄnÃ­ vyhledÃ¡vÃ¡nÃ­ s generativnÃ­mi modely. RAG nejprve vyhledÃ¡ relevantnÃ­ dokumenty z databÃ¡ze, potÃ© je pouÅ¾ije jako kontext pro generovÃ¡nÃ­ odpovÄ›di.

### **ArXiv/Semantic Scholar**
- **ArXiv**: OtevÅ™enÃ¡ platforma pro vÄ›deckÃ© preprinty v oblastech fyziky, matematiky, informatiky
- **Semantic Scholar**: AI-pohÃ¡nÄ›nÃ½ vyhledÃ¡vaÄ akademickÃ© literatury s API pro automatickÃ½ pÅ™Ã­stup

### **Specter Embeddings**
SpecializovanÃ© vektorovÃ© reprezentace vÄ›deckÃ½ch dokumentÅ¯ vytvoÅ™enÃ© modelem SPECTER, kterÃ½ je trÃ©novanÃ½ na citaÄnÃ­ch grafech pro lepÅ¡Ã­ sÃ©mantickÃ© porozumÄ›nÃ­ vÄ›deckÃ½m textÅ¯m.

### **ChromaDB**
ModernÃ­ vektorovÃ¡ databÃ¡ze optimalizovanÃ¡ pro AI aplikace, umoÅ¾ÅˆujÃ­cÃ­ efektivnÃ­ uklÃ¡dÃ¡nÃ­ a vyhledÃ¡vÃ¡nÃ­ embeddings s podporou metadat a filtrovÃ¡nÃ­.

### **GPT-4o**
NejnovÄ›jÅ¡Ã­ multimodÃ¡lnÃ­ model OpenAI s vylepÅ¡enÃ½mi schopnostmi v analÃ½ze textu, syntÃ©ze informacÃ­ a generovÃ¡nÃ­ strukturovanÃ½ch pÅ™ehledÅ¯.

## 2. KomplexnÃ­ VysvÄ›tlenÃ­ Projektu

### **CÃ­le Projektu**
Academic Research Synthesizer automatizuje proces reÅ¡erÅ¡e a syntÃ©zy vÄ›deckÃ© literatury. SystÃ©m dokÃ¡Å¾e na zÃ¡kladÄ› uÅ¾ivatelskÃ©ho dotazu vyhledat relevantnÃ­ vÄ›deckÃ© prÃ¡ce, analyzovat jejich obsah a vytvoÅ™it strukturovanÃ½ literÃ¡rnÃ­ pÅ™ehled s citacemi.

### **HlavnÃ­ VÃ½zvy**
- **SÃ©mantickÃ© porozumÄ›nÃ­**: RozpoznÃ¡nÃ­ souvislostÃ­ mezi vÄ›deckÃ½mi koncepty napÅ™Ã­Ä obory
- **Kvalita citacÃ­**: ZajiÅ¡tÄ›nÃ­ pÅ™esnÃ½ch a relevantnÃ­ch referencÃ­
- **Å kÃ¡lovatelnost**: EfektivnÃ­ zpracovÃ¡nÃ­ velkÃ½ch objemÅ¯ akademickÃ© literatury
- **AktuÃ¡lnost**: Integrace nejnovÄ›jÅ¡Ã­ch vÃ½zkumÅ¯ z rÅ¯znÃ½ch zdrojÅ¯

### **PotenciÃ¡lnÃ­ Dopad**
SystÃ©m vÃ½znamnÄ› urychluje vÄ›deckou prÃ¡ci, umoÅ¾Åˆuje rychlou orientaci v novÃ½ch oblastech vÃ½zkumu a podporuje interdisciplinÃ¡rnÃ­ spoluprÃ¡ci automatickÃ½m propojovÃ¡nÃ­m souvisejÃ­cÃ­ch studiÃ­.

## 3. KomplexnÃ­ Python Implementace

### **ZÃ¡vislosti a NastavenÃ­**

````python
openai==1.30.0
chromadb==0.4.24
langchain==0.2.5
langchain-openai==0.1.8
arxiv==2.1.0
requests==2.31.0
python-dotenv==1.0.0
sentence-transformers==2.7.0
pydantic==2.7.0
streamlit==1.35.0
````

### **HlavnÃ­ Implementace**

````python
import os
import arxiv
import requests
import chromadb
from typing import List, Dict, Optional
from dataclasses import dataclass
from datetime import datetime
import json
import logging
from sentence_transformers import SentenceTransformer
from openai import OpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
import streamlit as st
from dotenv import load_dotenv

load_dotenv()

# NastavenÃ­ logovÃ¡nÃ­
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ResearchPaper:
    """DatovÃ¡ struktura pro vÄ›deckou prÃ¡ci"""
    title: str
    authors: List[str]
    abstract: str
    url: str
    published_date: datetime
    categories: List[str]
    paper_id: str
    content: Optional[str] = None

class SemanticScholarAPI:
    """Wrapper pro Semantic Scholar API"""
    
    def __init__(self):
        self.base_url = "https://api.semanticscholar.org/graph/v1"
        self.headers = {"User-Agent": "AcademicSynthesizer/1.0"}
    
    def search_papers(self, query: str, limit: int = 20) -> List[Dict]:
        """VyhledÃ¡nÃ­ pracÃ­ v Semantic Scholar"""
        try:
            url = f"{self.base_url}/paper/search"
            params = {
                "query": query,
                "limit": limit,
                "fields": "title,authors,abstract,url,year,venue,citationCount"
            }
            
            response = requests.get(url, params=params, headers=self.headers)
            response.raise_for_status()
            
            return response.json().get("data", [])
        except Exception as e:
            logger.error(f"Chyba pÅ™i vyhledÃ¡vÃ¡nÃ­ v Semantic Scholar: {e}")
            return []

class ArxivSearcher:
    """Wrapper pro ArXiv API"""
    
    def __init__(self):
        self.client = arxiv.Client()
    
    def search_papers(self, query: str, max_results: int = 20) -> List[ResearchPaper]:
        """VyhledÃ¡nÃ­ pracÃ­ v ArXiv"""
        try:
            search = arxiv.Search(
                query=query,
                max_results=max_results,
                sort_by=arxiv.SortCriterion.Relevance
            )
            
            papers = []
            for result in self.client.results(search):
                paper = ResearchPaper(
                    title=result.title,
                    authors=[author.name for author in result.authors],
                    abstract=result.summary,
                    url=result.pdf_url,
                    published_date=result.published,
                    categories=[cat for cat in result.categories],
                    paper_id=result.entry_id.split('/')[-1],
                    content=result.summary  # Pro demo pouÅ¾ijeme abstrakt
                )
                papers.append(paper)
            
            return papers
        except Exception as e:
            logger.error(f"Chyba pÅ™i vyhledÃ¡vÃ¡nÃ­ v ArXiv: {e}")
            return []

class EmbeddingManager:
    """SprÃ¡va vektorovÃ½ch reprezentacÃ­ dokumentÅ¯"""
    
    def __init__(self, model_name: str = "sentence-transformers/allenai-specter"):
        try:
            self.model = SentenceTransformer(model_name)
        except:
            # Fallback na dostupnÃ½ model
            self.model = SentenceTransformer('all-MiniLM-L6-v2')
            logger.warning("PouÅ¾it zÃ¡loÅ¾nÃ­ embedding model")
    
    def create_embeddings(self, texts: List[str]) -> List[List[float]]:
        """VytvoÅ™enÃ­ embeddings pro texty"""
        try:
            embeddings = self.model.encode(texts, convert_to_tensor=False)
            return embeddings.tolist()
        except Exception as e:
            logger.error(f"Chyba pÅ™i vytvÃ¡Å™enÃ­ embeddings: {e}")
            return []

class ChromaDBManager:
    """SprÃ¡va ChromaDB vektorovÃ© databÃ¡ze"""
    
    def __init__(self, collection_name: str = "research_papers"):
        self.client = chromadb.PersistentClient(path="./chroma_db")
        self.collection_name = collection_name
        self.embedding_manager = EmbeddingManager()
        
        # VytvoÅ™enÃ­ nebo zÃ­skÃ¡nÃ­ kolekce
        try:
            self.collection = self.client.get_collection(collection_name)
        except:
            self.collection = self.client.create_collection(
                name=collection_name,
                metadata={"description": "Kolekce vÄ›deckÃ½ch pracÃ­"}
            )
    
    def add_papers(self, papers: List[ResearchPaper]) -> bool:
        """PÅ™idÃ¡nÃ­ pracÃ­ do databÃ¡ze"""
        try:
            texts = []
            metadatas = []
            ids = []
            
            for paper in papers:
                # Kombinace nÃ¡zvu a abstraktu pro embedding
                text = f"{paper.title} {paper.abstract}"
                texts.append(text)
                
                metadata = {
                    "title": paper.title,
                    "authors": ", ".join(paper.authors),
                    "url": paper.url,
                    "published_date": paper.published_date.isoformat(),
                    "categories": ", ".join(paper.categories),
                    "abstract": paper.abstract
                }
                metadatas.append(metadata)
                ids.append(paper.paper_id)
            
            # VytvoÅ™enÃ­ embeddings
            embeddings = self.embedding_manager.create_embeddings(texts)
            
            if embeddings:
                self.collection.add(
                    embeddings=embeddings,
                    metadatas=metadatas,
                    documents=texts,
                    ids=ids
                )
                return True
            return False
            
        except Exception as e:
            logger.error(f"Chyba pÅ™i pÅ™idÃ¡vÃ¡nÃ­ do ChromaDB: {e}")
            return False
    
    def search_similar(self, query: str, n_results: int = 10) -> List[Dict]:
        """VyhledÃ¡nÃ­ podobnÃ½ch dokumentÅ¯"""
        try:
            query_embedding = self.embedding_manager.create_embeddings([query])
            
            if query_embedding:
                results = self.collection.query(
                    query_embeddings=query_embedding,
                    n_results=n_results,
                    include=["metadatas", "documents", "distances"]
                )
                
                return results
            return []
            
        except Exception as e:
            logger.error(f"Chyba pÅ™i vyhledÃ¡vÃ¡nÃ­ v ChromaDB: {e}")
            return []

class GPTSynthesizer:
    """SyntÃ©za vÃ½sledkÅ¯ pomocÃ­ GPT-4o"""
    
    def __init__(self):
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    def synthesize_literature_review(self, query: str, relevant_papers: List[Dict]) -> str:
        """VytvoÅ™enÃ­ literÃ¡rnÃ­ho pÅ™ehledu"""
        try:
            # PÅ™Ã­prava kontextu z relevantnÃ­ch pracÃ­
            context = self._prepare_context(relevant_papers)
            
            prompt = f"""
            Jste odbornÃ½ vÄ›deckÃ½ asistent. Na zÃ¡kladÄ› poskytnutÃ½ch vÄ›deckÃ½ch pracÃ­ vytvoÅ™te strukturovanÃ½ literÃ¡rnÃ­ pÅ™ehled pro dotaz: "{query}"

            RelevantnÃ­ vÄ›deckÃ© prÃ¡ce:
            {context}

            VytvoÅ™te detailnÃ­ literÃ¡rnÃ­ pÅ™ehled, kterÃ½ bude obsahovat:
            1. Ãšvod do problematiky
            2. HlavnÃ­ smÄ›ry vÃ½zkumu
            3. KlÃ­ÄovÃ© nÃ¡lezy a metodologie
            4. SouÄasnÃ© vÃ½zvy a omezenÃ­
            5. BudoucÃ­ smÄ›ry vÃ½zkumu
            6. ZÃ¡vÄ›r

            PouÅ¾Ã­vejte pÅ™esnÃ© citace ve formÃ¡tu [Autor, Rok] a udrÅ¾ujte akademickÃ½ styl.
            """
            
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "Jste odbornÃ½ vÄ›deckÃ½ asistent specializujÃ­cÃ­ se na syntÃ©zu akademickÃ© literatury."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2000,
                temperature=0.3
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"Chyba pÅ™i syntÃ©ze pomocÃ­ GPT: {e}")
            return "Chyba pÅ™i generovÃ¡nÃ­ literÃ¡rnÃ­ho pÅ™ehledu."
    
    def _prepare_context(self, papers: List[Dict]) -> str:
        """PÅ™Ã­prava kontextu z vÄ›deckÃ½ch pracÃ­"""
        context_parts = []
        
        if 'metadatas' in papers:
            for i, metadata in enumerate(papers['metadatas'][0]):
                authors = metadata.get('authors', 'NeznÃ¡mÃ½ autor')
                title = metadata.get('title', 'Bez nÃ¡zvu')
                abstract = metadata.get('abstract', 'Bez abstraktu')
                year = metadata.get('published_date', '')[:4] if metadata.get('published_date') else 'NeznÃ¡mÃ½ rok'
                
                context_part = f"""
                PrÃ¡ce {i+1}:
                NÃ¡zev: {title}
                AutoÅ™i: {authors}
                Rok: {year}
                Abstrakt: {abstract[:500]}...
                """
                context_parts.append(context_part)
        
        return "\n".join(context_parts)

class AcademicSynthesizer:
    """HlavnÃ­ tÅ™Ã­da pro syntÃ©zu akademickÃ½ch pracÃ­"""
    
    def __init__(self):
        self.arxiv_searcher = ArxivSearcher()
        self.semantic_scholar = SemanticScholarAPI()
        self.chroma_manager = ChromaDBManager()
        self.gpt_synthesizer = GPTSynthesizer()
    
    def process_query(self, query: str, max_papers: int = 20) -> Dict:
        """ZpracovÃ¡nÃ­ dotazu a vytvoÅ™enÃ­ syntÃ©zy"""
        logger.info(f"ZpracovÃ¡vÃ¡m dotaz: {query}")
        
        # 1. VyhledÃ¡nÃ­ pracÃ­ v ArXiv
        arxiv_papers = self.arxiv_searcher.search_papers(query, max_papers // 2)
        logger.info(f"Nalezeno {len(arxiv_papers)} pracÃ­ v ArXiv")
        
        # 2. PÅ™idÃ¡nÃ­ do ChromaDB
        if arxiv_papers:
            success = self.chroma_manager.add_papers(arxiv_papers)
            logger.info(f"PÅ™idÃ¡nÃ­ do databÃ¡ze: {'ÃºspÄ›Å¡nÃ©' if success else 'neÃºspÄ›Å¡nÃ©'}")
        
        # 3. VyhledÃ¡nÃ­ relevantnÃ­ch pracÃ­
        relevant_papers = self.chroma_manager.search_similar(query, max_papers)
        logger.info(f"Nalezeno {len(relevant_papers.get('metadatas', [[]]))} relevantnÃ­ch pracÃ­")
        
        # 4. SyntÃ©za literÃ¡rnÃ­ho pÅ™ehledu
        if relevant_papers.get('metadatas'):
            literature_review = self.gpt_synthesizer.synthesize_literature_review(
                query, relevant_papers
            )
        else:
            literature_review = "Nebyly nalezeny Å¾Ã¡dnÃ© relevantnÃ­ vÄ›deckÃ© prÃ¡ce pro vÃ¡Å¡ dotaz."
        
        return {
            "query": query,
            "papers_found": len(arxiv_papers),
            "relevant_papers": relevant_papers,
            "literature_review": literature_review,
            "timestamp": datetime.now().isoformat()
        }

# Streamlit UI
def main():
    st.set_page_config(
        page_title="Academic Research Synthesizer",
        page_icon="ğŸ“š",
        layout="wide"
    )
    
    st.title("ğŸ“š Academic Research Synthesizer")
    st.markdown("*RAG systÃ©m pro automatickou syntÃ©zu vÄ›deckÃ© literatury*")
    
    # Inicializace systÃ©mu
    if 'synthesizer' not in st.session_state:
        with st.spinner("Inicializuji systÃ©m..."):
            st.session_state.synthesizer = AcademicSynthesizer()
    
    # UI pro zadÃ¡nÃ­ dotazu
    col1, col2 = st.columns([3, 1])
    
    with col1:
        query = st.text_input(
            "Zadejte vÃ½zkumnÃ½ dotaz:",
            placeholder="napÅ™. 'machine learning in healthcare' nebo 'quantum computing algorithms'"
        )
    
    with col2:
        max_papers = st.selectbox("Max. poÄet pracÃ­:", [10, 20, 30, 50], index=1)
    
    if st.button("ğŸ” Vyhledat a syntetizovat", type="primary"):
        if query:
            with st.spinner("VyhledÃ¡vÃ¡m a analyzuji vÄ›deckÃ© prÃ¡ce..."):
                results = st.session_state.synthesizer.process_query(query, max_papers)
                
                # ZobrazenÃ­ vÃ½sledkÅ¯
                st.success(f"âœ… Nalezeno {results['papers_found']} novÃ½ch pracÃ­")
                
                # LiterÃ¡rnÃ­ pÅ™ehled
                st.subheader("ğŸ“– Automaticky generovanÃ½ literÃ¡rnÃ­ pÅ™ehled")
                st.markdown(results['literature_review'])
                
                # RelevantnÃ­ prÃ¡ce
                if results['relevant_papers'].get('metadatas'):
                    st.subheader("ğŸ“‘ RelevantnÃ­ vÄ›deckÃ© prÃ¡ce")
                    
                    for i, metadata in enumerate(results['relevant_papers']['metadatas'][0][:5]):
                        with st.expander(f"ğŸ“„ {metadata.get('title', 'Bez nÃ¡zvu')}"):
                            st.write(f"**AutoÅ™i:** {metadata.get('authors', 'NeznÃ¡mÃ½')}")
                            st.write(f"**Rok:** {metadata.get('published_date', '')[:4]}")
                            st.write(f"**Abstrakt:** {metadata.get('abstract', 'Bez abstraktu')}")
                            if metadata.get('url'):
                                st.markdown(f"[ğŸ”— Odkaz na prÃ¡ci]({metadata['url']})")
        else:
            st.warning("âš ï¸ ProsÃ­m, zadejte vÃ½zkumnÃ½ dotaz.")

if __name__ == "__main__":
    main()
````

### **KonfiguraÄnÃ­ soubor**

````python
OPENAI_API_KEY=your_openai_api_key_here
CHROMA_DB_PATH=./chroma_db
LOG_LEVEL=INFO
````

### **SpuÅ¡tÄ›nÃ­ aplikace**

````bash
# Instalace zÃ¡vislostÃ­
pip install -r requirements.txt

# SpuÅ¡tÄ›nÃ­ Streamlit aplikace
streamlit run academic_synthesizer.py
````

## 4. Souhrn Projektu

### **Hodnota Projektu**
Academic Research Synthesizer pÅ™edstavuje pokroÄilÃ© Å™eÅ¡enÃ­ pro automatizaci vÄ›deckÃ© reÅ¡erÅ¡e, kterÃ©:

- **Urychluje vÃ½zkum** o 80-90% dÃ­ky automatickÃ© agregaci a syntÃ©ze
- **ZvyÅ¡uje kvalitu** literÃ¡rnÃ­ch pÅ™ehledÅ¯ prostÅ™ednictvÃ­m AI analÃ½zy
- **Podporuje interdisciplinaritu** propojovÃ¡nÃ­m souvisejÃ­cÃ­ch oblastÃ­
- **ZajiÅ¡Å¥uje aktuÃ¡lnost** kontinuÃ¡lnÃ­m monitoringem novÃ½ch publikacÃ­

### **KlÃ­ÄovÃ© Vlastnosti**
- ModernÃ­ RAG architektura s ChromaDB a SPECTER embeddings
- Integrace s ArXiv a Semantic Scholar API
- AutomatickÃ¡ syntÃ©za pomocÃ­ GPT-4o
- Å kÃ¡lovatelnÃ© Å™eÅ¡enÃ­ s perzistentnÃ­m ÃºloÅ¾iÅ¡tÄ›m
- IntuitivnÃ­ Streamlit interface

### **BudoucÃ­ RozÅ¡Ã­Å™enÃ­**
- Podpora dalÅ¡Ã­ch akademickÃ½ch databÃ¡zÃ­ (PubMed, IEEE)
- PokroÄilÃ© citaÄnÃ­ analÃ½zy a doporuÄovacÃ­ systÃ©my
- KolaborativnÃ­ funkce pro vÃ½zkumnÃ© tÃ½my
- Export do LaTeX a Word formÃ¡tÅ¯

Tento systÃ©m pÅ™edstavuje vÃ½znamnÃ½ krok vpÅ™ed v digitalizaci vÄ›deckÃ© prÃ¡ce a mÃ¡ potenciÃ¡l transformovat zpÅ¯sob, jakÃ½m vÃ½zkumnÃ­ci pÅ™istupujÃ­ k literÃ¡rnÃ­ reÅ¡erÅ¡i.