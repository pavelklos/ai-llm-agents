<small>Claude Sonnet 4 **(Podcast Content Discovery and Analysis with RAG)**</small>
# Podcast Content Discovery and Analysis

## Key Concepts Explanation

### Retrieval-Augmented Generation (RAG)
A technique that combines information retrieval with generative AI to provide intelligent podcast insights by first retrieving relevant data from episode transcripts, host profiles, and listener feedback, then using an LLM to generate comprehensive content analysis and personalized recommendations.

### Episode Transcripts
Text versions of podcast audio content generated through speech-to-text technology, containing the complete spoken content including speaker identification, timestamps, and contextual information that enables semantic search and content analysis.

### Host Information
Comprehensive profiles of podcast hosts including their expertise areas, speaking style, guest networks, career background, and content themes that influence show quality and audience appeal.

### Topic Categorization
Automated classification system that organizes podcast content into thematic categories such as technology, business, health, entertainment, using natural language processing to identify main subjects and subtopics discussed.

### Listener Reviews
User-generated feedback and ratings that provide insights into podcast quality, audience engagement, content value, and listening experience, serving as valuable signals for recommendation algorithms.

### Audio Processing
Technical analysis of audio files including speech recognition, speaker diarization, audio quality assessment, and acoustic feature extraction that enables automated content understanding and indexing.

### Recommendation Engine
AI-powered system that suggests relevant podcasts and episodes based on user preferences, listening history, content similarity, and collaborative filtering to enhance content discovery.

## Comprehensive Project Explanation

The Podcast Content Discovery and Analysis platform is an AI-powered system that revolutionizes how listeners discover, analyze, and engage with podcast content. By leveraging RAG technology, this platform combines comprehensive podcast databases with intelligent analysis to provide personalized recommendations, content insights, and deep understanding of the podcasting landscape.

### Objectives
- **Intelligent Discovery**: Enable users to find relevant podcast content through semantic search and AI-powered recommendations based on interests and preferences
- **Content Analysis**: Provide deep insights into podcast themes, quality, and trends through automated transcript analysis and topic modeling
- **Personalization**: Create tailored listening experiences through user behavior analysis and preference learning
- **Quality Assessment**: Evaluate podcast content quality through multi-dimensional analysis including audio quality, content depth, and audience engagement
- **Trend Identification**: Identify emerging topics, popular themes, and industry trends across the podcasting landscape

### Challenges
- **Scale and Volume**: Processing millions of podcast episodes with varying audio quality and content types while maintaining real-time performance
- **Accuracy in Transcription**: Dealing with diverse accents, audio quality issues, background noise, and technical terminology in automated transcription
- **Content Understanding**: Accurately interpreting context, humor, sarcasm, and nuanced discussions that require deep semantic understanding
- **Personalization Balance**: Providing diverse recommendations while avoiding filter bubbles and maintaining content discovery serendipity
- **Real-time Processing**: Handling continuous content updates, new episode releases, and dynamic user preferences with minimal latency

### Potential Impact
- **Enhanced Discovery**: Transforming podcast discovery from keyword-based search to intelligent, context-aware content matching
- **Creator Insights**: Providing podcast creators with audience analytics, content performance metrics, and optimization recommendations
- **Market Intelligence**: Offering industry insights, trend analysis, and competitive intelligence for podcast networks and advertisers
- **Accessibility**: Making podcast content more accessible through searchable transcripts and content summaries
- **Educational Value**: Enabling knowledge extraction and learning path creation from educational podcast content

## Comprehensive Project Example with Python Implementation

### Dependencies and Setup

````python
# requirements.txt
langchain==0.1.0
openai==1.3.0
chromadb==0.4.18
pandas==2.1.4
numpy==1.24.3
requests==2.31.0
python-dotenv==1.0.0
pydantic==2.5.0
fastapi==0.104.1
uvicorn==0.24.0
spotipy==2.22.1
librosa==0.10.1
speech-recognition==3.10.0
pydub==0.25.1
scikit-learn==1.3.2
nltk==3.8.1
transformers==4.35.2
sentence-transformers==2.2.2
feedparser==6.0.10
beautifulsoup4==4.12.2
plotly==5.17.0
streamlit==1.28.1
wordcloud==1.9.2
textblob==0.17.1
pyaudio==0.2.11
whisper==1.1.10
````

### Core Implementation

````python
# main.py
import os
import json
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple, Union
from dataclasses import dataclass, asdict, field
from pydantic import BaseModel
import asyncio
import logging
from pathlib import Path
from enum import Enum
import uuid
import re

from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import Document
import chromadb
import openai
from dotenv import load_dotenv

import librosa
import speech_recognition as sr
from pydub import AudioSegment
import whisper
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import nltk
from textblob import TextBlob
from wordcloud import WordCloud
import plotly.graph_objects as go
import plotly.express as px

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Download required NLTK data
try:
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
    nltk.download('vader_lexicon', quiet=True)
except:
    pass

class PodcastCategory(Enum):
    TECHNOLOGY = "technology"
    BUSINESS = "business"
    HEALTH = "health"
    ENTERTAINMENT = "entertainment"
    EDUCATION = "education"
    NEWS = "news"
    SPORTS = "sports"
    COMEDY = "comedy"
    SCIENCE = "science"
    HISTORY = "history"

class AudioQuality(Enum):
    EXCELLENT = "excellent"
    GOOD = "good"
    FAIR = "fair"
    POOR = "poor"

@dataclass
class Host:
    host_id: str
    name: str
    bio: str
    expertise_areas: List[str]
    social_media: Dict[str, str]
    years_experience: int
    speaking_style: str
    notable_guests: List[str]
    education_background: Optional[str]
    career_highlights: List[str]

@dataclass
class Podcast:
    podcast_id: str
    title: str
    description: str
    category: PodcastCategory
    subcategories: List[str]
    hosts: List[Host]
    publisher: str
    language: str
    country: str
    total_episodes: int
    average_duration: int  # minutes
    release_frequency: str
    first_episode_date: datetime
    last_episode_date: datetime
    total_downloads: int
    average_rating: float
    total_reviews: int
    website: Optional[str]
    rss_feed: Optional[str]
    social_media: Dict[str, str]
    artwork_url: Optional[str]

@dataclass
class Episode:
    episode_id: str
    podcast_id: str
    title: str
    description: str
    episode_number: Optional[int]
    season_number: Optional[int]
    duration: int  # seconds
    release_date: datetime
    audio_url: str
    transcript: Optional[str]
    summary: Optional[str]
    topics: List[str]
    guests: List[str]
    key_quotes: List[str]
    audio_quality: AudioQuality
    download_count: int
    rating: Optional[float]
    sentiment_score: Optional[float]
    keywords: List[str]

@dataclass
class Transcript:
    transcript_id: str
    episode_id: str
    full_text: str
    segments: List[Dict[str, Union[str, float, int]]]  # speaker, text, start_time, end_time
    speaker_count: int
    word_count: int
    confidence_score: float
    processing_date: datetime
    language_detected: str

@dataclass
class ListenerReview:
    review_id: str
    podcast_id: str
    episode_id: Optional[str]
    user_id: str
    rating: int  # 1-5
    review_text: Optional[str]
    review_date: datetime
    helpful_votes: int
    verified_listener: bool
    listening_time: Optional[int]  # seconds listened
    sentiment: Optional[str]  # positive, negative, neutral

@dataclass
class ContentAnalysis:
    analysis_id: str
    episode_id: str
    main_topics: List[Tuple[str, float]]  # (topic, relevance_score)
    sentiment_analysis: Dict[str, float]  # positive, negative, neutral scores
    key_entities: List[str]
    complexity_score: float  # 0-1, readability/complexity
    engagement_indicators: Dict[str, float]
    quality_metrics: Dict[str, float]
    audio_features: Dict[str, float]
    transcript_quality: float
    content_uniqueness: float

@dataclass
class UserProfile:
    user_id: str
    listening_history: List[str]  # episode_ids
    preferred_categories: List[PodcastCategory]
    favorite_hosts: List[str]
    average_session_length: int  # minutes
    listening_times: List[str]  # preferred times
    device_types: List[str]
    subscription_tier: str
    created_date: datetime
    last_active: datetime
    engagement_score: float

class PodcastDataCollector:
    """Collects and manages podcast data from various sources"""
    
    def __init__(self):
        self.whisper_model = None
        self.sentence_transformer = SentenceTransformer('all-MiniLM-L6-v2')
        self.sample_data = self._generate_sample_data()
    
    def _generate_sample_data(self):
        """Generate comprehensive sample podcast data"""
        
        # Sample hosts
        hosts = [
            Host(
                host_id="host_001",
                name="Alex Chen",
                bio="Technology entrepreneur and venture capitalist with 15 years in Silicon Valley",
                expertise_areas=["artificial intelligence", "startup funding", "tech trends"],
                social_media={"twitter": "@alexchen_vc", "linkedin": "alexchen"},
                years_experience=15,
                speaking_style="analytical and conversational",
                notable_guests=["Elon Musk", "Reid Hoffman", "Satya Nadella"],
                education_background="Stanford MBA, MIT Computer Science",
                career_highlights=["Founded 2 companies", "Partner at top VC firm", "Tech advisor"]
            ),
            Host(
                host_id="host_002",
                name="Dr. Sarah Mitchell",
                bio="Clinical psychologist and wellness expert specializing in mental health",
                expertise_areas=["mental health", "wellness", "psychology", "mindfulness"],
                social_media={"instagram": "@drsarahmitchell", "website": "sarahmitchell.com"},
                years_experience=12,
                speaking_style="empathetic and educational",
                notable_guests=["Tony Robbins", "BrenÃ© Brown", "Dr. Andrew Huberman"],
                education_background="PhD Clinical Psychology, Harvard Medical School",
                career_highlights=["Published researcher", "Bestselling author", "Clinic founder"]
            ),
            Host(
                host_id="host_003",
                name="Marcus Johnson",
                bio="Former NBA player turned business coach and motivational speaker",
                expertise_areas=["sports psychology", "leadership", "motivation", "business"],
                social_media={"twitter": "@marcusjcoach", "instagram": "@marcusj_motivation"},
                years_experience=8,
                speaking_style="energetic and inspiring",
                notable_guests=["LeBron James", "Gary Vaynerchuk", "Tim Ferriss"],
                education_background="MBA Northwestern Kellogg",
                career_highlights=["10-year NBA career", "Fortune 500 consultant", "Motivational speaker"]
            )
        ]
        
        # Sample podcasts
        podcasts = [
            Podcast(
                podcast_id="podcast_001",
                title="Tech Innovators",
                description="Deep dive conversations with technology leaders shaping the future",
                category=PodcastCategory.TECHNOLOGY,
                subcategories=["AI", "startups", "venture capital", "innovation"],
                hosts=[hosts[0]],
                publisher="TechCast Media",
                language="English",
                country="United States",
                total_episodes=156,
                average_duration=45,
                release_frequency="Weekly",
                first_episode_date=datetime(2021, 3, 15),
                last_episode_date=datetime(2024, 5, 28),
                total_downloads=2500000,
                average_rating=4.7,
                total_reviews=1240,
                website="techinnovators.com",
                rss_feed="feeds.techinnovators.com/rss",
                social_media={"twitter": "@techinnovators", "youtube": "TechInnovatorsPod"},
                artwork_url="artwork_tech.jpg"
            ),
            Podcast(
                podcast_id="podcast_002",
                title="Mind & Wellness",
                description="Science-based insights for mental health and personal wellness",
                category=PodcastCategory.HEALTH,
                subcategories=["mental health", "wellness", "psychology", "self-improvement"],
                hosts=[hosts[1]],
                publisher="Wellness Media Group",
                language="English",
                country="United States",
                total_episodes=89,
                average_duration=38,
                release_frequency="Bi-weekly",
                first_episode_date=datetime(2022, 1, 10),
                last_episode_date=datetime(2024, 5, 25),
                total_downloads=1800000,
                average_rating=4.8,
                total_reviews=956,
                website="mindwellnesspod.com",
                rss_feed="feeds.mindwellness.com/rss",
                social_media={"instagram": "@mindwellnesspod", "facebook": "MindWellnessPodcast"},
                artwork_url="artwork_wellness.jpg"
            ),
            Podcast(
                podcast_id="podcast_003",
                title="Champion Mindset",
                description="Success strategies from elite athletes and business leaders",
                category=PodcastCategory.BUSINESS,
                subcategories=["leadership", "motivation", "success", "sports psychology"],
                hosts=[hosts[2]],
                publisher="Champion Media",
                language="English",
                country="United States",
                total_episodes=124,
                average_duration=52,
                release_frequency="Twice weekly",
                first_episode_date=datetime(2020, 9, 1),
                last_episode_date=datetime(2024, 5, 30),
                total_downloads=3200000,
                average_rating=4.6,
                total_reviews=1580,
                website="championmindsetpod.com",
                rss_feed="feeds.championmindset.com/rss",
                social_media={"twitter": "@championmindset", "youtube": "ChampionMindsetPod"},
                artwork_url="artwork_champion.jpg"
            )
        ]
        
        # Sample episodes
        episodes = [
            Episode(
                episode_id="ep_001",
                podcast_id="podcast_001",
                title="The Future of AI with OpenAI's CTO",
                description="Deep discussion about artificial intelligence trends and implications",
                episode_number=152,
                season_number=3,
                duration=2580,  # 43 minutes
                release_date=datetime(2024, 5, 15),
                audio_url="audio/tech_152.mp3",
                transcript="In this episode, we explore the revolutionary developments in artificial intelligence...",
                summary="Discussion covers GPT-4 developments, AI safety, and future applications",
                topics=["artificial intelligence", "machine learning", "AI safety", "technology ethics"],
                guests=["Mira Murati"],
                key_quotes=["AI will transform every industry", "Safety must be paramount"],
                audio_quality=AudioQuality.EXCELLENT,
                download_count=125000,
                rating=4.8,
                sentiment_score=0.75,
                keywords=["AI", "GPT-4", "safety", "innovation", "future"]
            ),
            Episode(
                episode_id="ep_002",
                podcast_id="podcast_002",
                title="Managing Anxiety in the Digital Age",
                description="Practical strategies for dealing with modern anxiety triggers",
                episode_number=45,
                season_number=2,
                duration=2280,  # 38 minutes
                release_date=datetime(2024, 5, 10),
                audio_url="audio/wellness_45.mp3",
                transcript="Today we're discussing how digital technology impacts our mental health...",
                summary="Explores digital wellness, anxiety management techniques, and mindful technology use",
                topics=["anxiety", "digital wellness", "mental health", "mindfulness"],
                guests=["Dr. Anna Cal Newport"],
                key_quotes=["Technology should serve us, not control us", "Mindful consumption is key"],
                audio_quality=AudioQuality.GOOD,
                download_count=89000,
                rating=4.9,
                sentiment_score=0.65,
                keywords=["anxiety", "digital", "wellness", "mindfulness", "mental health"]
            ),
            Episode(
                episode_id="ep_003",
                podcast_id="podcast_003",
                title="From NBA to Business Success",
                description="How elite sports performance translates to business achievement",
                episode_number=98,
                season_number=4,
                duration=3120,  # 52 minutes
                release_date=datetime(2024, 5, 28),
                audio_url="audio/champion_98.mp3",
                transcript="The discipline and mindset that made me successful in basketball...",
                summary="Explores performance psychology, leadership lessons, and business strategy",
                topics=["leadership", "performance psychology", "business strategy", "motivation"],
                guests=["Chris Paul"],
                key_quotes=["Excellence is a habit", "Leadership is about elevating others"],
                audio_quality=AudioQuality.EXCELLENT,
                download_count=156000,
                rating=4.7,
                sentiment_score=0.85,
                keywords=["leadership", "performance", "NBA", "business", "success"]
            )
        ]
        
        # Sample reviews
        reviews = [
            ListenerReview(
                review_id="review_001",
                podcast_id="podcast_001",
                episode_id="ep_001",
                user_id="user_001",
                rating=5,
                review_text="Incredible insights into AI development. Alex asks great questions.",
                review_date=datetime(2024, 5, 16),
                helpful_votes=23,
                verified_listener=True,
                listening_time=2580,
                sentiment="positive"
            ),
            ListenerReview(
                review_id="review_002",
                podcast_id="podcast_002",
                episode_id="ep_002",
                user_id="user_002",
                rating=5,
                review_text="Dr. Mitchell's advice on digital wellness really helped me. Practical and actionable.",
                review_date=datetime(2024, 5, 12),
                helpful_votes=18,
                verified_listener=True,
                listening_time=2280,
                sentiment="positive"
            ),
            ListenerReview(
                review_id="review_003",
                podcast_id="podcast_003",
                episode_id="ep_003",
                user_id="user_003",
                rating=4,
                review_text="Great motivation and solid business advice. Could use more specific examples.",
                review_date=datetime(2024, 5, 29),
                helpful_votes=12,
                verified_listener=True,
                listening_time=2800,
                sentiment="positive"
            )
        ]
        
        return {
            'hosts': hosts,
            'podcasts': podcasts,
            'episodes': episodes,
            'reviews': reviews
        }
    
    async def process_audio_file(self, audio_path: str) -> Dict[str, any]:
        """Process audio file to extract features and transcript"""
        
        try:
            # Load audio file
            audio_data, sample_rate = librosa.load(audio_path, sr=22050)
            
            # Extract audio features
            features = {
                'duration': len(audio_data) / sample_rate,
                'tempo': float(librosa.beat.tempo(y=audio_data, sr=sample_rate)[0]),
                'zero_crossing_rate': float(np.mean(librosa.feature.zero_crossing_rate(audio_data))),
                'spectral_centroid': float(np.mean(librosa.feature.spectral_centroid(y=audio_data, sr=sample_rate))),
                'mfcc_mean': float(np.mean(librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=13))),
                'energy': float(np.mean(librosa.feature.rms(y=audio_data)))
            }
            
            # Assess audio quality
            quality_score = self._assess_audio_quality(features)
            
            # Generate transcript (simplified - would use Whisper in production)
            transcript = await self._generate_transcript(audio_path)
            
            return {
                'features': features,
                'quality_score': quality_score,
                'transcript': transcript
            }
            
        except Exception as e:
            logger.error(f"Error processing audio file: {str(e)}")
            return {'error': str(e)}
    
    def _assess_audio_quality(self, features: Dict[str, float]) -> float:
        """Assess audio quality based on features"""
        
        # Simplified quality assessment
        quality_factors = []
        
        # Energy level (should be moderate)
        energy = features.get('energy', 0)
        if 0.01 < energy < 0.1:
            quality_factors.append(0.8)
        else:
            quality_factors.append(0.4)
        
        # Zero crossing rate (speech typically has moderate ZCR)
        zcr = features.get('zero_crossing_rate', 0)
        if 0.05 < zcr < 0.2:
            quality_factors.append(0.9)
        else:
            quality_factors.append(0.5)
        
        # Spectral centroid (brightness indicator)
        centroid = features.get('spectral_centroid', 0)
        if 1000 < centroid < 4000:
            quality_factors.append(0.8)
        else:
            quality_factors.append(0.6)
        
        return np.mean(quality_factors)
    
    async def _generate_transcript(self, audio_path: str) -> str:
        """Generate transcript from audio (simplified)"""
        
        # This would use Whisper or similar in production
        # For demo purposes, return sample transcript
        return "This is a sample transcript generated from the audio file. In production, this would use Whisper or similar speech-to-text technology."
    
    async def analyze_episode_content(self, episode: Episode) -> ContentAnalysis:
        """Analyze episode content for topics, sentiment, and quality"""
        
        if not episode.transcript:
            return ContentAnalysis(
                analysis_id=str(uuid.uuid4()),
                episode_id=episode.episode_id,
                main_topics=[],
                sentiment_analysis={},
                key_entities=[],
                complexity_score=0.5,
                engagement_indicators={},
                quality_metrics={},
                audio_features={},
                transcript_quality=0.0,
                content_uniqueness=0.5
            )
        
        # Sentiment analysis
        blob = TextBlob(episode.transcript)
        sentiment = {
            'polarity': blob.sentiment.polarity,
            'subjectivity': blob.sentiment.subjectivity,
            'positive': max(0, blob.sentiment.polarity),
            'negative': max(0, -blob.sentiment.polarity),
            'neutral': 1 - abs(blob.sentiment.polarity)
        }
        
        # Topic extraction (simplified)
        topics = self._extract_topics(episode.transcript)
        
        # Entity extraction (simplified)
        entities = self._extract_entities(episode.transcript)
        
        # Complexity score
        complexity = self._calculate_complexity(episode.transcript)
        
        # Quality metrics
        quality_metrics = {
            'transcript_length': len(episode.transcript),
            'word_count': len(episode.transcript.split()),
            'sentence_count': len(episode.transcript.split('.')),
            'average_sentence_length': len(episode.transcript.split()) / max(1, len(episode.transcript.split('.'))),
            'keyword_density': len(episode.keywords) / max(1, len(episode.transcript.split())) * 100
        }
        
        return ContentAnalysis(
            analysis_id=str(uuid.uuid4()),
            episode_id=episode.episode_id,
            main_topics=topics,
            sentiment_analysis=sentiment,
            key_entities=entities,
            complexity_score=complexity,
            engagement_indicators={'sentiment_strength': abs(sentiment['polarity'])},
            quality_metrics=quality_metrics,
            audio_features={},
            transcript_quality=0.8,
            content_uniqueness=0.7
        )
    
    def _extract_topics(self, text: str) -> List[Tuple[str, float]]:
        """Extract main topics from text"""
        
        # Simplified topic extraction using keyword frequency
        words = text.lower().split()
        word_freq = {}
        
        # Filter out common words
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'can', 'that', 'this', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they'}
        
        for word in words:
            word = re.sub(r'[^\w\s]', '', word)
            if len(word) > 3 and word not in stop_words:
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # Return top topics with scores
        sorted_topics = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        total_words = sum(word_freq.values())
        
        return [(topic, count/total_words) for topic, count in sorted_topics[:10]]
    
    def _extract_entities(self, text: str) -> List[str]:
        """Extract named entities from text (simplified)"""
        
        # This would use spaCy or similar NER in production
        # For demo, extract capitalized words as potential entities
        words = text.split()
        entities = []
        
        for word in words:
            word = re.sub(r'[^\w\s]', '', word)
            if word.istitle() and len(word) > 2:
                entities.append(word)
        
        # Remove duplicates and return top entities
        unique_entities = list(set(entities))
        return unique_entities[:20]
    
    def _calculate_complexity(self, text: str) -> float:
        """Calculate text complexity score"""
        
        words = text.split()
        sentences = text.split('.')
        
        if not words or not sentences:
            return 0.5
        
        # Average word length
        avg_word_length = np.mean([len(word) for word in words])
        
        # Average sentence length
        avg_sentence_length = len(words) / len(sentences)
        
        # Complexity based on word and sentence length
        complexity = min(1.0, (avg_word_length / 10) * 0.5 + (avg_sentence_length / 20) * 0.5)
        
        return complexity

class PodcastRecommendationEngine:
    """AI-powered podcast recommendation system"""
    
    def __init__(self):
        self.sentence_transformer = SentenceTransformer('all-MiniLM-L6-v2')
        self.user_profiles = {}
        self.content_embeddings = {}
    
    async def generate_recommendations(self, 
                                    user_profile: UserProfile, 
                                    podcasts: List[Podcast],
                                    episodes: List[Episode]) -> List[Dict[str, any]]:
        """Generate personalized podcast recommendations"""
        
        recommendations = []
        
        # Content-based filtering
        content_recs = await self._content_based_filtering(user_profile, podcasts, episodes)
        recommendations.extend(content_recs)
        
        # Collaborative filtering (simplified)
        collab_recs = await self._collaborative_filtering(user_profile, podcasts)
        recommendations.extend(collab_recs)
        
        # Trending content
        trending_recs = await self._trending_recommendations(podcasts, episodes)
        recommendations.extend(trending_recs)
        
        # Remove duplicates and rank
        unique_recs = {}
        for rec in recommendations:
            rec_id = rec.get('podcast_id') or rec.get('episode_id')
            if rec_id not in unique_recs:
                unique_recs[rec_id] = rec
            else:
                # Combine scores for duplicates
                unique_recs[rec_id]['score'] = max(unique_recs[rec_id]['score'], rec['score'])
        
        # Sort by score and return top recommendations
        final_recs = list(unique_recs.values())
        final_recs.sort(key=lambda x: x['score'], reverse=True)
        
        return final_recs[:20]
    
    async def _content_based_filtering(self, 
                                     user_profile: UserProfile, 
                                     podcasts: List[Podcast],
                                     episodes: List[Episode]) -> List[Dict[str, any]]:
        """Content-based recommendation using user preferences"""
        
        recommendations = []
        
        # Get user's preferred categories
        preferred_categories = user_profile.preferred_categories
        
        for podcast in podcasts:
            score = 0.0
            
            # Category match
            if podcast.category in preferred_categories:
                score += 0.3
            
            # Rating boost
            score += (podcast.average_rating / 5.0) * 0.2
            
            # Popularity boost
            normalized_downloads = min(1.0, podcast.total_downloads / 1000000)
            score += normalized_downloads * 0.1
            
            # Recent activity boost
            days_since_last_episode = (datetime.now() - podcast.last_episode_date).days
            if days_since_last_episode < 30:
                score += 0.2
            
            if score > 0.3:  # Threshold for recommendation
                recommendations.append({
                    'type': 'podcast',
                    'podcast_id': podcast.podcast_id,
                    'title': podcast.title,
                    'score': score,
                    'reason': f'Matches your interest in {podcast.category.value}'
                })
        
        return recommendations
    
    async def _collaborative_filtering(self, 
                                     user_profile: UserProfile, 
                                     podcasts: List[Podcast]) -> List[Dict[str, any]]:
        """Collaborative filtering based on similar users (simplified)"""
        
        recommendations = []
        
        # This would implement proper collaborative filtering in production
        # For demo, recommend highly-rated podcasts not in user's history
        
        for podcast in podcasts:
            if podcast.podcast_id not in user_profile.listening_history:
                if podcast.average_rating >= 4.5:
                    recommendations.append({
                        'type': 'podcast',
                        'podcast_id': podcast.podcast_id,
                        'title': podcast.title,
                        'score': podcast.average_rating / 5.0 * 0.8,
                        'reason': 'Highly rated by listeners like you'
                    })
        
        return recommendations
    
    async def _trending_recommendations(self, 
                                      podcasts: List[Podcast], 
                                      episodes: List[Episode]) -> List[Dict[str, any]]:
        """Recommend trending content"""
        
        recommendations = []
        
        # Recent episodes with high engagement
        recent_episodes = [ep for ep in episodes 
                          if (datetime.now() - ep.release_date).days < 7]
        
        for episode in recent_episodes:
            if episode.download_count > 50000:  # High download threshold
                recommendations.append({
                    'type': 'episode',
                    'episode_id': episode.episode_id,
                    'podcast_id': episode.podcast_id,
                    'title': episode.title,
                    'score': min(1.0, episode.download_count / 100000) * 0.7,
                    'reason': 'Trending this week'
                })
        
        return recommendations

class PodcastDiscoveryRAG:
    """RAG system for podcast content discovery and analysis"""
    
    def __init__(self):
        # Initialize OpenAI
        openai.api_key = os.getenv("OPENAI_API_KEY")
        
        # Initialize components
        self.embeddings = OpenAIEmbeddings()
        self.llm = ChatOpenAI(model="gpt-4", temperature=0.3)
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        
        # Initialize vector store
        self.client = chromadb.PersistentClient(path="./podcast_discovery_db")
        self.vectorstore = Chroma(
            client=self.client,
            collection_name="podcast_knowledge",
            embedding_function=self.embeddings
        )
        
        self.data_collector = PodcastDataCollector()
        self.recommendation_engine = PodcastRecommendationEngine()
    
    async def initialize_knowledge_base(self):
        """Initialize the knowledge base with podcast data"""
        logger.info("Initializing podcast discovery knowledge base...")
        
        documents = []
        
        # Add podcast information
        for podcast in self.data_collector.sample_data['podcasts']:
            doc_content = f"""
            Podcast: {podcast.title}
            Category: {podcast.category.value}
            Description: {podcast.description}
            Hosts: {', '.join([h.name for h in podcast.hosts])}
            Publisher: {podcast.publisher}
            Total Episodes: {podcast.total_episodes}
            Average Duration: {podcast.average_duration} minutes
            Release Frequency: {podcast.release_frequency}
            Average Rating: {podcast.average_rating}/5 ({podcast.total_reviews} reviews)
            Total Downloads: {podcast.total_downloads:,}
            Subcategories: {', '.join(podcast.subcategories)}
            Language: {podcast.language}
            Country: {podcast.country}
            """
            documents.append(Document(
                page_content=doc_content,
                metadata={
                    "type": "podcast",
                    "podcast_id": podcast.podcast_id,
                    "title": podcast.title,
                    "category": podcast.category.value,
                    "rating": podcast.average_rating,
                    "total_episodes": podcast.total_episodes
                }
            ))
        
        # Add episode information
        for episode in self.data_collector.sample_data['episodes']:
            doc_content = f"""
            Episode: {episode.title}
            Podcast ID: {episode.podcast_id}
            Description: {episode.description}
            Duration: {episode.duration // 60} minutes
            Release Date: {episode.release_date.strftime('%Y-%m-%d')}
            Topics: {', '.join(episode.topics)}
            Guests: {', '.join(episode.guests)}
            Key Quotes: {'; '.join(episode.key_quotes)}
            Keywords: {', '.join(episode.keywords)}
            Rating: {episode.rating}/5
            Downloads: {episode.download_count:,}
            Summary: {episode.summary}
            """
            if episode.transcript:
                doc_content += f"\nTranscript Sample: {episode.transcript[:500]}..."
            
            documents.append(Document(
                page_content=doc_content,
                metadata={
                    "type": "episode",
                    "episode_id": episode.episode_id,
                    "podcast_id": episode.podcast_id,
                    "title": episode.title,
                    "topics": episode.topics,
                    "rating": episode.rating or 0,
                    "duration": episode.duration
                }
            ))
        
        # Add host information
        for host in self.data_collector.sample_data['hosts']:
            doc_content = f"""
            Host: {host.name}
            Bio: {host.bio}
            Expertise Areas: {', '.join(host.expertise_areas)}
            Years of Experience: {host.years_experience}
            Speaking Style: {host.speaking_style}
            Notable Guests: {', '.join(host.notable_guests)}
            Education: {host.education_background}
            Career Highlights: {', '.join(host.career_highlights)}
            """
            documents.append(Document(
                page_content=doc_content,
                metadata={
                    "type": "host",
                    "host_id": host.host_id,
                    "name": host.name,
                    "expertise_areas": host.expertise_areas,
                    "years_experience": host.years_experience
                }
            ))
        
        # Add general podcast knowledge
        podcast_concepts = [
            {
                "title": "Podcast Discovery Best Practices",
                "content": "Effective podcast discovery involves understanding listener preferences, content quality indicators, and trending topics. Key factors include host expertise, production quality, consistency, and audience engagement."
            },
            {
                "title": "Content Quality Assessment",
                "content": "Podcast quality can be evaluated through multiple dimensions including audio production quality, content depth, host preparation, guest quality, and listener engagement metrics."
            },
            {
                "title": "Personalization Strategies",
                "content": "Personalized podcast recommendations combine content-based filtering (matching content to preferences), collaborative filtering (similar user behavior), and contextual factors like listening time and device type."
            }
        ]
        
        for concept in podcast_concepts:
            documents.append(Document(
                page_content=f"Title: {concept['title']}\nContent: {concept['content']}",
                metadata={
                    "type": "concept",
                    "title": concept['title']
                }
            ))
        
        # Add documents to vector store
        if documents:
            self.vectorstore.add_documents(documents)
        
        logger.info(f"Added {len(documents)} documents to knowledge base")
    
    def retrieve_relevant_information(self, query: str, k: int = 8) -> List[Document]:
        """Retrieve relevant podcast information"""
        return self.vectorstore.similarity_search(query, k=k)
    
    async def search_podcasts(self, query: str, filters: Dict[str, any] = None) -> Dict[str, any]:
        """Search for podcasts using semantic search"""
        
        # Retrieve relevant documents
        relevant_docs = self.retrieve_relevant_information(query, k=10)
        context = "\n".join([doc.page_content for doc in relevant_docs])
        
        # Filter results based on criteria
        filtered_podcasts = []
        filtered_episodes = []
        
        for doc in relevant_docs:
            if doc.metadata.get('type') == 'podcast':
                podcast = next((p for p in self.data_collector.sample_data['podcasts'] 
                              if p.podcast_id == doc.metadata.get('podcast_id')), None)
                if podcast and self._matches_filters(podcast, filters):
                    filtered_podcasts.append(podcast)
            
            elif doc.metadata.get('type') == 'episode':
                episode = next((e for e in self.data_collector.sample_data['episodes'] 
                              if e.episode_id == doc.metadata.get('episode_id')), None)
                if episode:
                    filtered_episodes.append(episode)
        
        # Generate search insights
        prompt = ChatPromptTemplate.from_template("""
        Analyze these podcast search results for the query: "{query}"
        
        Relevant Content: {context}
        
        Found {podcast_count} podcasts and {episode_count} episodes.
        
        Provide search insights including:
        1. Most relevant podcasts with brief explanations
        2. Episode recommendations related to the query
        3. Topic themes and patterns identified
        4. Host expertise relevant to the query
        5. Content quality indicators
        6. Suggestions for refining the search
        
        Focus on helping users discover the most valuable content for their interests.
        """)
        
        chain = prompt | self.llm
        response = await chain.ainvoke({
            "query": query,
            "context": context,
            "podcast_count": len(filtered_podcasts),
            "episode_count": len(filtered_episodes)
        })
        
        return {
            "query": query,
            "search_insights": response.content,
            "podcasts": [asdict(p) for p in filtered_podcasts],
            "episodes": [asdict(e) for e in filtered_episodes],
            "search_timestamp": datetime.now().isoformat()
        }
    
    def _matches_filters(self, podcast: Podcast, filters: Dict[str, any]) -> bool:
        """Check if podcast matches search filters"""
        
        if not filters:
            return True
        
        # Category filter
        if 'category' in filters and podcast.category.value != filters['category']:
            return False
        
        # Rating filter
        if 'min_rating' in filters and podcast.average_rating < filters['min_rating']:
            return False
        
        # Duration filter
        if 'max_duration' in filters and podcast.average_duration > filters['max_duration']:
            return False
        
        # Language filter
        if 'language' in filters and podcast.language.lower() != filters['language'].lower():
            return False
        
        return True
    
    async def analyze_episode_content(self, episode_id: str) -> Dict[str, any]:
        """Provide detailed episode content analysis"""
        
        episode = next((e for e in self.data_collector.sample_data['episodes'] 
                       if e.episode_id == episode_id), None)
        
        if not episode:
            return {"error": "Episode not found"}
        
        # Perform content analysis
        content_analysis = await self.data_collector.analyze_episode_content(episode)
        
        # Get relevant context
        query = f"episode analysis {episode.title} {' '.join(episode.topics)}"
        relevant_docs = self.retrieve_relevant_information(query, k=6)
        context = "\n".join([doc.page_content for doc in relevant_docs])
        
        # Generate comprehensive analysis
        prompt = ChatPromptTemplate.from_template("""
        Provide comprehensive analysis of this podcast episode:
        
        Episode: {title}
        Description: {description}
        Duration: {duration} minutes
        Topics: {topics}
        Guests: {guests}
        Key Quotes: {quotes}
        
        Content Analysis:
        - Main Topics: {main_topics}
        - Sentiment: {sentiment}
        - Complexity Score: {complexity}
        - Quality Metrics: {quality}
        
        Context: {context}
        
        Provide detailed analysis including:
        1. Content summary and key insights
        2. Topic depth and expertise level
        3. Conversation quality and flow
        4. Guest contributions and perspectives
        5. Actionable takeaways for listeners
        6. Comparison with similar content
        7. Listening recommendations (who should listen)
        8. Follow-up content suggestions
        
        Make the analysis helpful for potential listeners to decide if this episode matches their interests.
        """)
        
        chain = prompt | self.llm
        response = await chain.ainvoke({
            "title": episode.title,
            "description": episode.description,
            "duration": episode.duration // 60,
            "topics": ', '.join(episode.topics),
            "guests": ', '.join(episode.guests) if episode.guests else 'No guests',
            "quotes": '; '.join(episode.key_quotes) if episode.key_quotes else 'No key quotes available',
            "main_topics": str(content_analysis.main_topics[:5]),
            "sentiment": str(content_analysis.sentiment_analysis),
            "complexity": content_analysis.complexity_score,
            "quality": str(content_analysis.quality_metrics),
            "context": context
        })
        
        return {
            "episode": asdict(episode),
            "content_analysis": asdict(content_analysis),
            "comprehensive_analysis": response.content,
            "analysis_date": datetime.now().isoformat()
        }
    
    async def get_personalized_recommendations(self, user_id: str) -> Dict[str, any]:
        """Get personalized podcast recommendations"""
        
        # Create sample user profile (in production, would retrieve from database)
        user_profile = UserProfile(
            user_id=user_id,
            listening_history=["ep_001"],
            preferred_categories=[PodcastCategory.TECHNOLOGY, PodcastCategory.BUSINESS],
            favorite_hosts=["host_001"],
            average_session_length=45,
            listening_times=["morning", "commute"],
            device_types=["smartphone", "car"],
            subscription_tier="premium",
            created_date=datetime.now() - timedelta(days=30),
            last_active=datetime.now(),
            engagement_score=0.8
        )
        
        # Generate recommendations
        recommendations = await self.recommendation_engine.generate_recommendations(
            user_profile,
            self.data_collector.sample_data['podcasts'],
            self.data_collector.sample_data['episodes']
        )
        
        # Get context for recommendation explanation
        query = f"personalized recommendations {' '.join([cat.value for cat in user_profile.preferred_categories])}"
        relevant_docs = self.retrieve_relevant_information(query, k=6)
        context = "\n".join([doc.page_content for doc in relevant_docs])
        
        # Generate recommendation explanations
        rec_summaries = []
        for rec in recommendations[:5]:
            rec_summaries.append(f"- {rec['title']}: {rec['reason']} (Score: {rec['score']:.2f})")
        
        prompt = ChatPromptTemplate.from_template("""
        Explain these personalized podcast recommendations for the user:
        
        User Profile:
        - Preferred Categories: {categories}
        - Listening History: {history_count} episodes
        - Average Session: {session_length} minutes
        - Engagement Score: {engagement}
        
        Top Recommendations:
        {recommendations}
        
        Context: {context}
        
        Provide personalized explanation including:
        1. Why these recommendations match user preferences
        2. Discovery opportunities (new categories/hosts)
        3. Optimal listening schedule suggestions
        4. Content progression pathway
        5. Quality indicators for recommended content
        6. Tips for maximizing listening experience
        
        Make recommendations actionable and personalized to the user's interests and habits.
        """)
        
        chain = prompt | self.llm
        response = await chain.ainvoke({
            "categories": ', '.join([cat.value for cat in user_profile.preferred_categories]),
            "history_count": len(user_profile.listening_history),
            "session_length": user_profile.average_session_length,
            "engagement": user_profile.engagement_score,
            "recommendations": '\n'.join(rec_summaries),
            "context": context
        })
        
        return {
            "user_profile": asdict(user_profile),
            "recommendations": recommendations,
            "personalized_explanation": response.content,
            "recommendation_date": datetime.now().isoformat()
        }

class PodcastDiscoveryAssistant:
    """Main assistant orchestrating podcast content discovery and analysis"""
    
    def __init__(self):
        self.rag_system = PodcastDiscoveryRAG()
        self.initialized = False
    
    async def initialize(self):
        """Initialize the podcast discovery assistant"""
        if not self.initialized:
            await self.rag_system.initialize_knowledge_base()
            self.initialized = True
            logger.info("Podcast Discovery Assistant initialized successfully")
    
    async def search_content(self, query: str, filters: Dict[str, any] = None) -> Dict[str, any]:
        """Search for podcast content"""
        
        if not self.initialized:
            await self.initialize()
        
        return await self.rag_system.search_podcasts(query, filters)
    
    async def analyze_episode(self, episode_id: str) -> Dict[str, any]:
        """Analyze specific episode content"""
        
        if not self.initialized:
            await self.initialize()
        
        return await self.rag_system.analyze_episode_content(episode_id)
    
    async def get_recommendations(self, user_id: str) -> Dict[str, any]:
        """Get personalized recommendations"""
        
        if not self.initialized:
            await self.initialize()
        
        return await self.rag_system.get_personalized_recommendations(user_id)

# Example usage and testing
async def main():
    """Main function demonstrating the Podcast Discovery Assistant"""
    
    print("ð§ Initializing Podcast Content Discovery and Analysis Assistant...")
    assistant = PodcastDiscoveryAssistant()
    
    try:
        # Initialize the system
        await assistant.initialize()
        print("â Podcast Discovery Assistant initialized successfully")
        
        # Search for AI-related content
        print("\nð Searching for AI-related podcast content...")
        search_results = await assistant.search_content(
            "artificial intelligence machine learning", 
            {"category": "technology", "min_rating": 4.0}
        )
        
        print("â Search Complete:")
        print(f"Found {len(search_results['podcasts'])} podcasts and {len(search_results['episodes'])} episodes")
        print(f"Search insights preview: {search_results['search_insights'][:200]}...")
        
        # Analyze specific episode
        print("\nð Analyzing episode content...")
        episode_analysis = await assistant.analyze_episode("ep_001")
        
        print("â Episode Analysis Complete:")
        if 'episode' in episode_analysis:
            episode = episode_analysis['episode']
            print(f"Episode: {episode['title']}")
            print(f"Duration: {episode['duration']//60} minutes")
            print(f"Topics: {', '.join(episode['topics'])}")
            print(f"Analysis preview: {episode_analysis['comprehensive_analysis'][:200]}...")
        
        # Get personalized recommendations
        print("\nð¯ Getting personalized recommendations...")
        recommendations = await assistant.get_recommendations("user_123")
        
        print("â Recommendations Generated:")
        print(f"Found {len(recommendations['recommendations'])} recommendations")
        if recommendations['recommendations']:
            top_rec = recommendations['recommendations'][0]
            print(f"Top recommendation: {top_rec['title']} (Score: {top_rec['score']:.2f})")
        print(f"Explanation preview: {recommendations['personalized_explanation'][:200]}...")
        
        print("\nð§ Podcast Content Discovery and Analysis demonstration completed successfully!")
        
    except Exception as e:
        logger.error(f"Error in main execution: {str(e)}")
        print(f"â Error: {str(e)}")

if __name__ == "__main__":
    # Set up environment variables
    os.environ.setdefault("OPENAI_API_KEY", "your-openai-api-key-here")
    
    # Run the main function
    asyncio.run(main())
````

## Project Summary

The **Podcast Content Discovery and Analysis** platform represents a revolutionary advancement in audio content intelligence, combining RAG technology with sophisticated audio processing and machine learning to transform how users discover, analyze, and engage with podcast content. This system democratizes access to deep podcast insights previously available only to major platforms and content creators.

### Key Value Propositions

**Intelligent Content Discovery**: Leverages semantic search and AI-powered analysis to help users find relevant podcast content beyond simple keyword matching, enabling discovery of hidden gems and niche content that matches specific interests and needs.

**Deep Content Analysis**: Provides comprehensive episode analysis including topic extraction, sentiment analysis, quality assessment, and content complexity scoring, helping listeners make informed decisions about their time investment.

**Personalized Recommendations**: Combines content-based filtering, collaborative filtering, and contextual factors to deliver highly personalized podcast recommendations that evolve with user preferences and listening behavior.

**Creator Insights**: Offers podcast creators valuable analytics about content performance, audience engagement, and optimization opportunities to improve their shows and grow their audience.

### Technical Excellence

The implementation showcases advanced audio processing and NLP engineering with **LangChain** for RAG orchestration, **Whisper** for speech-to-text conversion, **SentenceTransformers** for content embeddings, **LibROSA** for audio analysis, and **ChromaDB** for podcast knowledge storage. The architecture supports real-time transcription, multi-dimensional content analysis, and scalable recommendation generation.

### Impact and Applications

This platform serves podcast listeners, content creators, podcast networks, advertisers, and researchers. Applications include podcast discovery platforms, creator analytics dashboards, advertising optimization, academic research into audio content trends, and accessibility improvements through searchable transcripts.

The project demonstrates how AI can transform podcast consumption from passive browsing to intelligent content curation, making the vast podcast landscape more navigable and valuable. Through automated analysis and personalized recommendations, this system enhances content discovery while providing creators with actionable insights to improve their content and reach their target audience more effectively.