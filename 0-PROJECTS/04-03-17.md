<small>Claude Sonnet 4 **(AI Music Composer - Intelligent MIDI Generation and Music Theory Engine)**</small>
# AI Music Composer

## Key Concepts Explanation

### MIDI Generation
Musical Instrument Digital Interface (MIDI) data creation through algorithmic composition, involving the generation of note sequences, timing, velocity, and control parameters that represent musical performances. This encompasses pitch selection, rhythm patterns, harmonic progressions, and expressive elements like dynamics and articulation encoded in standardized MIDI format.

### Music Theory Integration
Computational implementation of musical principles including scales, chord progressions, voice leading, counterpoint, and form analysis. This involves encoding rules for harmony, melody construction, rhythmic patterns, and structural relationships that govern musical composition across different genres and styles.

### Harmonic Progression Analysis
Systematic evaluation and generation of chord sequences that create musical tension and resolution, incorporating concepts like functional harmony, voice leading principles, circle of fifths relationships, and modal interchange to create coherent and emotionally compelling musical narratives.

### Melodic Contour Modeling
Analysis and generation of melodic shapes and patterns, including interval relationships, phrase structure, motivic development, and melodic arc design. This involves understanding how melodies move through pitch space, create memorable themes, and interact with underlying harmonic structures.

### Rhythmic Pattern Synthesis
Creation of temporal musical structures including beat patterns, syncopation, polyrhythms, and metric modulation. This encompasses groove generation, drum programming, rhythmic motifs, and the interaction between different rhythmic layers in multi-instrument compositions.

## Comprehensive Project Explanation

### Project Overview
The AI Music Composer is an intelligent system that generates original musical compositions by combining deep learning techniques with formal music theory principles. The platform creates MIDI files across various genres and styles, incorporating sophisticated harmonic analysis, melodic development, and rhythmic complexity while maintaining musical coherence and emotional expression.

### Objectives
- **Genre-Aware Composition**: Generate music in specific styles with authentic characteristics and conventions
- **Theory-Guided Generation**: Apply formal music theory rules to ensure harmonic and melodic coherence
- **Multi-Track Orchestration**: Create complex arrangements with multiple instruments and voices
- **Emotional Expression**: Compose music that conveys specific moods and emotional narratives
- **Interactive Creativity**: Enable user-guided composition with real-time feedback and refinement
- **Educational Integration**: Provide analysis and explanation of generated musical elements

### Key Challenges
- **Musical Coherence**: Maintaining long-term structure and thematic development across compositions
- **Style Authenticity**: Capturing the subtle nuances that define different musical genres
- **Harmonic Complexity**: Balancing theoretical correctness with creative expression
- **Temporal Dependencies**: Managing musical phrase structure and form over extended durations
- **Multi-Voice Coordination**: Ensuring proper voice leading and orchestration principles
- **Expressivity Modeling**: Incorporating human-like musical expression and interpretation

### Potential Impact
- **Democratized Composition**: Enable non-musicians to create sophisticated musical works
- **Creative Assistance**: Support professional composers with idea generation and arrangement tools
- **Educational Enhancement**: Provide interactive learning tools for music theory and composition
- **Content Creation**: Generate background music for media, games, and commercial applications
- **Cultural Preservation**: Model and generate music in traditional and endangered musical styles
- **Therapeutic Applications**: Create personalized music for wellness and therapeutic interventions

## Comprehensive Project Example with Python Implementation

### Dependencies and Setup

````python
# requirements.txt
openai==1.3.0
anthropic==0.8.0
langchain==0.0.350
transformers==4.36.0
torch==2.1.0
music21==9.1.0
mido==1.3.0
pretty_midi==0.2.10
librosa==0.10.1
numpy==1.25.2
pandas==2.1.3
matplotlib==3.8.2
plotly==5.17.0
seaborn==0.13.0
scikit-learn==1.3.2
tensorflow==2.15.0
keras==2.15.0
magenta==2.1.4
pyfluidsynth==1.3.2
soundfile==0.12.1
scipy==1.11.4
networkx==3.2.1
streamlit==1.28.1
fastapi==0.104.1
uvicorn==0.24.0
pydantic==2.5.0
redis==5.0.1
celery==5.3.4
python-dotenv==1.0.0
rich==13.7.0
typer==0.9.0
click==8.1.7
pyyaml==6.0.1
jsonschema==4.20.0
requests==2.31.0
aiofiles==23.2.1
jinja2==3.1.2
markdown==3.5.1
````

### Core Implementation

````python
import os
import asyncio
import logging
import json
import uuid
import random
import math
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass, field, asdict
from collections import defaultdict, Counter
from enum import Enum
import tempfile
import warnings

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import music21
from music21 import stream, note, chord, duration, key, meter, tempo, scale, interval
import mido
import pretty_midi
import librosa
from scipy import signal
import matplotlib.pyplot as plt
import seaborn as sns

from openai import AsyncOpenAI
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from langchain.prompts import PromptTemplate

from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field
import streamlit as st
import soundfile as sf

from dotenv import load_dotenv

load_dotenv()
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
warnings.filterwarnings("ignore", category=UserWarning)

class Genre(Enum):
    CLASSICAL = "classical"
    JAZZ = "jazz"
    BLUES = "blues"
    ROCK = "rock"
    POP = "pop"
    ELECTRONIC = "electronic"
    AMBIENT = "ambient"
    FOLK = "folk"

class Mood(Enum):
    HAPPY = "happy"
    SAD = "sad"
    ENERGETIC = "energetic"
    CALM = "calm"
    MYSTERIOUS = "mysterious"
    ROMANTIC = "romantic"
    DRAMATIC = "dramatic"
    PEACEFUL = "peaceful"

class Instrument(Enum):
    PIANO = 0
    VIOLIN = 40
    GUITAR = 24
    BASS = 32
    DRUMS = 128
    FLUTE = 73
    TRUMPET = 56
    SAXOPHONE = 64
    CELLO = 42
    CLARINET = 71

@dataclass
class Note:
    pitch: int  # MIDI note number
    start_time: float  # Start time in beats
    duration: float  # Duration in beats
    velocity: int  # MIDI velocity (0-127)
    channel: int = 0

@dataclass
class Chord:
    root: int  # Root note MIDI number
    chord_type: str  # major, minor, diminished, etc.
    inversion: int = 0
    start_time: float = 0.0
    duration: float = 4.0

@dataclass
class Progression:
    chords: List[Chord]
    key_signature: str
    time_signature: Tuple[int, int] = (4, 4)
    tempo: int = 120

@dataclass
class Track:
    instrument: Instrument
    notes: List[Note]
    name: str
    channel: int = 0
    volume: int = 100

@dataclass
class Composition:
    composition_id: str
    title: str
    genre: Genre
    mood: Mood
    key_signature: str
    time_signature: Tuple[int, int]
    tempo: int
    tracks: List[Track]
    progression: Progression
    duration: float
    created_at: datetime = field(default_factory=datetime.now)
    metadata: Dict[str, Any] = field(default_factory=dict)

class MusicTheoryEngine:
    """Core music theory and harmony analysis engine."""
    
    def __init__(self):
        self.scales = self._initialize_scales()
        self.chord_progressions = self._initialize_progressions()
        self.voice_leading_rules = self._initialize_voice_leading()
        
    def _initialize_scales(self) -> Dict[str, List[int]]:
        """Initialize scale patterns."""
        return {
            "major": [0, 2, 4, 5, 7, 9, 11],
            "minor": [0, 2, 3, 5, 7, 8, 10],
            "dorian": [0, 2, 3, 5, 7, 9, 10],
            "mixolydian": [0, 2, 4, 5, 7, 9, 10],
            "pentatonic": [0, 2, 4, 7, 9],
            "blues": [0, 3, 5, 6, 7, 10],
            "chromatic": list(range(12))
        }
    
    def _initialize_progressions(self) -> Dict[str, List[List[int]]]:
        """Initialize common chord progressions by genre."""
        return {
            "classical": [
                [0, 3, 4, 0],  # I-IV-V-I
                [0, 5, 3, 4, 0],  # I-vi-IV-V-I
                [0, 4, 0, 4, 0]  # I-V-I-V-I
            ],
            "jazz": [
                [0, 5, 1, 4],  # I-vi-ii-V
                [0, 0, 3, 3, 5, 5, 1, 4],  # I-I-IV-IV-vi-vi-ii-V
                [1, 4, 0]  # ii-V-I
            ],
            "pop": [
                [0, 5, 3, 4],  # I-vi-IV-V
                [0, 3, 5, 4],  # I-IV-vi-V
                [5, 3, 0, 4]  # vi-IV-I-V
            ],
            "blues": [
                [0, 0, 0, 0, 3, 3, 0, 0, 4, 3, 0, 4],  # 12-bar blues
            ]
        }
    
    def _initialize_voice_leading(self) -> Dict[str, Any]:
        """Initialize voice leading rules."""
        return {
            "parallel_fifths": False,
            "parallel_octaves": False,
            "voice_crossing": False,
            "leap_resolution": True,
            "smooth_voice_leading": True
        }
    
    def generate_chord_progression(
        self, 
        genre: Genre, 
        key: str, 
        length: int = 8
    ) -> List[Chord]:
        """Generate a chord progression based on genre and key."""
        try:
            # Get base progression pattern
            genre_progressions = self.chord_progressions.get(genre.value, self.chord_progressions["pop"])
            base_pattern = random.choice(genre_progressions)
            
            # Extend or truncate to desired length
            pattern = (base_pattern * (length // len(base_pattern) + 1))[:length]
            
            # Convert to actual chords
            key_root = self._note_name_to_number(key)
            scale_notes = self._get_scale_notes(key, "major")
            
            chords = []
            for i, degree in enumerate(pattern):
                root = scale_notes[degree % len(scale_notes)]
                chord_type = self._determine_chord_type(degree, genre)
                
                chord_obj = Chord(
                    root=root,
                    chord_type=chord_type,
                    start_time=i * 4.0,  # 4 beats per chord
                    duration=4.0
                )
                chords.append(chord_obj)
            
            return chords
            
        except Exception as e:
            logger.error(f"Chord progression generation failed: {e}")
            return []
    
    def _note_name_to_number(self, note_name: str) -> int:
        """Convert note name to MIDI number."""
        note_map = {"C": 0, "C#": 1, "D": 2, "D#": 3, "E": 4, "F": 5, 
                   "F#": 6, "G": 7, "G#": 8, "A": 9, "A#": 10, "B": 11}
        return note_map.get(note_name.upper(), 0) + 60  # Middle C octave
    
    def _get_scale_notes(self, key: str, scale_type: str) -> List[int]:
        """Get scale notes for a given key."""
        root = self._note_name_to_number(key)
        scale_pattern = self.scales.get(scale_type, self.scales["major"])
        return [(root + interval) for interval in scale_pattern]
    
    def _determine_chord_type(self, degree: int, genre: Genre) -> str:
        """Determine chord type based on scale degree and genre."""
        if genre == Genre.JAZZ:
            jazz_chords = ["maj7", "min7", "min7", "maj7", "dom7", "min7", "min7b5"]
            return jazz_chords[degree % len(jazz_chords)]
        elif genre == Genre.CLASSICAL:
            classical_chords = ["major", "minor", "minor", "major", "major", "minor", "diminished"]
            return classical_chords[degree % len(classical_chords)]
        else:
            pop_chords = ["major", "minor", "minor", "major", "major", "minor", "diminished"]
            return pop_chords[degree % len(pop_chords)]
    
    def analyze_harmony(self, chords: List[Chord], key: str) -> Dict[str, Any]:
        """Analyze harmonic content of a chord progression."""
        try:
            analysis = {
                "key": key,
                "num_chords": len(chords),
                "chord_types": Counter([chord.chord_type for chord in chords]),
                "harmonic_rhythm": [],
                "modulations": [],
                "cadences": []
            }
            
            # Analyze harmonic rhythm
            for i, chord in enumerate(chords):
                if i > 0:
                    time_diff = chord.start_time - chords[i-1].start_time
                    analysis["harmonic_rhythm"].append(time_diff)
            
            # Detect cadences
            for i in range(len(chords) - 1):
                current_chord = chords[i]
                next_chord = chords[i + 1]
                
                # Simple V-I cadence detection
                if (current_chord.chord_type in ["dom7", "major"] and 
                    next_chord.chord_type in ["major", "minor"]):
                    analysis["cadences"].append({
                        "type": "authentic",
                        "position": i,
                        "strength": "strong"
                    })
            
            return analysis
            
        except Exception as e:
            logger.error(f"Harmony analysis failed: {e}")
            return {}
    
    def validate_voice_leading(self, chord1: Chord, chord2: Chord) -> bool:
        """Validate voice leading between two chords."""
        try:
            # This is a simplified implementation
            # In practice, you'd analyze each voice independently
            
            # Check for parallel motion issues
            interval1 = abs(chord1.root - chord2.root)
            
            # Avoid large leaps (more than an octave)
            if interval1 > 12:
                return False
            
            # Prefer smooth voice leading (stepwise motion)
            if interval1 <= 2:
                return True
                
            return True  # Accept for now
            
        except Exception as e:
            logger.error(f"Voice leading validation failed: {e}")
            return True

class MelodyGenerator:
    """Generate melodic lines using music theory and AI."""
    
    def __init__(self, theory_engine: MusicTheoryEngine):
        self.theory_engine = theory_engine
        self.contour_patterns = self._initialize_contour_patterns()
        
    def _initialize_contour_patterns(self) -> Dict[str, List[int]]:
        """Initialize melodic contour patterns."""
        return {
            "arch": [0, 1, 2, 3, 2, 1, 0],  # Rising then falling
            "wave": [0, 1, 0, -1, 0, 1, 0],  # Wave-like motion
            "ascending": [0, 1, 2, 3, 4, 5, 6],  # Ascending line
            "descending": [6, 5, 4, 3, 2, 1, 0],  # Descending line
            "static": [0, 0, 1, 0, 0, -1, 0]  # Mostly static with small movements
        }
    
    async def generate_melody(
        self,
        progression: Progression,
        style: Genre,
        mood: Mood,
        length_bars: int = 8
    ) -> List[Note]:
        """Generate a melody over a chord progression."""
        try:
            # Choose contour pattern based on mood
            contour_name = self._select_contour_for_mood(mood)
            contour = self.contour_patterns[contour_name]
            
            # Generate scale for melody
            key_root = self.theory_engine._note_name_to_number(progression.key_signature)
            scale_notes = self.theory_engine._get_scale_notes(progression.key_signature, "major")
            
            melody_notes = []
            current_time = 0.0
            beats_per_bar = progression.time_signature[0]
            
            for bar in range(length_bars):
                bar_start_time = bar * beats_per_bar
                
                # Get current chord
                chord_index = min(bar, len(progression.chords) - 1)
                current_chord = progression.chords[chord_index]
                
                # Generate notes for this bar
                bar_notes = await self._generate_bar_melody(
                    current_chord, scale_notes, contour, bar_start_time, beats_per_bar, style
                )
                
                melody_notes.extend(bar_notes)
            
            return melody_notes
            
        except Exception as e:
            logger.error(f"Melody generation failed: {e}")
            return []
    
    def _select_contour_for_mood(self, mood: Mood) -> str:
        """Select melodic contour based on mood."""
        mood_contours = {
            Mood.HAPPY: "arch",
            Mood.SAD: "descending",
            Mood.ENERGETIC: "wave",
            Mood.CALM: "static",
            Mood.MYSTERIOUS: "wave",
            Mood.ROMANTIC: "arch",
            Mood.DRAMATIC: "arch",
            Mood.PEACEFUL: "static"
        }
        return mood_contours.get(mood, "arch")
    
    async def _generate_bar_melody(
        self,
        chord: Chord,
        scale_notes: List[int],
        contour: List[int],
        start_time: float,
        bar_length: float,
        style: Genre
    ) -> List[Note]:
        """Generate melody for a single bar."""
        try:
            notes = []
            
            # Determine number of notes based on style
            if style == Genre.CLASSICAL:
                notes_per_bar = 4
                note_duration = bar_length / notes_per_bar
            elif style == Genre.JAZZ:
                notes_per_bar = 8
                note_duration = bar_length / notes_per_bar
            else:
                notes_per_bar = 4
                note_duration = bar_length / notes_per_bar
            
            # Generate chord tones for reference
            chord_tones = self._get_chord_tones(chord, scale_notes)
            
            for i in range(notes_per_bar):
                note_time = start_time + (i * note_duration)
                
                # Select pitch based on contour and chord harmony
                contour_step = contour[i % len(contour)]
                
                # Bias toward chord tones
                if random.random() < 0.6:  # 60% chance of chord tone
                    pitch = random.choice(chord_tones)
                else:
                    pitch = random.choice(scale_notes)
                
                # Apply contour adjustment
                pitch += contour_step * 2  # 2 semitones per contour step
                
                # Keep in reasonable range
                pitch = max(60, min(84, pitch))  # C4 to C6
                
                # Determine velocity based on style and position
                velocity = self._calculate_velocity(style, i, notes_per_bar)
                
                note_obj = Note(
                    pitch=pitch,
                    start_time=note_time,
                    duration=note_duration * 0.9,  # Slight gap between notes
                    velocity=velocity
                )
                
                notes.append(note_obj)
            
            return notes
            
        except Exception as e:
            logger.error(f"Bar melody generation failed: {e}")
            return []
    
    def _get_chord_tones(self, chord: Chord, scale_notes: List[int]) -> List[int]:
        """Get chord tones for a given chord."""
        try:
            root = chord.root
            chord_type = chord.chord_type
            
            if chord_type == "major":
                intervals = [0, 4, 7]  # Root, major third, fifth
            elif chord_type == "minor":
                intervals = [0, 3, 7]  # Root, minor third, fifth
            elif chord_type == "dom7":
                intervals = [0, 4, 7, 10]  # Root, major third, fifth, minor seventh
            elif chord_type == "maj7":
                intervals = [0, 4, 7, 11]  # Root, major third, fifth, major seventh
            elif chord_type == "min7":
                intervals = [0, 3, 7, 10]  # Root, minor third, fifth, minor seventh
            else:
                intervals = [0, 4, 7]  # Default to major triad
            
            chord_tones = [(root + interval) for interval in intervals]
            return chord_tones
            
        except Exception as e:
            logger.error(f"Chord tone generation failed: {e}")
            return [chord.root]
    
    def _calculate_velocity(self, style: Genre, position: int, total_notes: int) -> int:
        """Calculate note velocity based on style and position."""
        try:
            base_velocity = 64  # Medium velocity
            
            if style == Genre.CLASSICAL:
                # Slight emphasis on downbeats
                if position == 0:
                    return base_velocity + 10
                else:
                    return base_velocity - 5
            elif style == Genre.JAZZ:
                # Swing feel - emphasize off-beats
                if position % 2 == 1:
                    return base_velocity + 5
                else:
                    return base_velocity - 3
            elif style == Genre.ROCK:
                # Strong emphasis on beats 1 and 3
                if position in [0, 2]:
                    return base_velocity + 15
                else:
                    return base_velocity
            else:
                return base_velocity
                
        except Exception as e:
            logger.error(f"Velocity calculation failed: {e}")
            return 64

class RhythmEngine:
    """Generate rhythmic patterns and drum tracks."""
    
    def __init__(self):
        self.drum_patterns = self._initialize_drum_patterns()
        self.drum_mapping = self._initialize_drum_mapping()
        
    def _initialize_drum_patterns(self) -> Dict[str, Dict[str, List[int]]]:
        """Initialize drum patterns by genre."""
        return {
            "rock": {
                "kick": [1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0],
                "snare": [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
                "hihat": [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]
            },
            "jazz": {
                "kick": [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0],
                "snare": [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0],
                "hihat": [1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1]
            },
            "pop": {
                "kick": [1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0],
                "snare": [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
                "hihat": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
            }
        }
    
    def _initialize_drum_mapping(self) -> Dict[str, int]:
        """Initialize drum sound to MIDI note mapping."""
        return {
            "kick": 36,      # C2 - Kick drum
            "snare": 38,     # D2 - Snare drum
            "hihat": 42,     # F#2 - Closed hi-hat
            "crash": 49,     # C#3 - Crash cymbal
            "ride": 51,      # D#3 - Ride cymbal
            "tom1": 50,      # D3 - High tom
            "tom2": 47,      # B2 - Mid tom
            "tom3": 43       # G2 - Low tom
        }
    
    def generate_drum_track(
        self, 
        genre: Genre, 
        bars: int = 8, 
        time_signature: Tuple[int, int] = (4, 4)
    ) -> List[Note]:
        """Generate a drum track for the specified genre."""
        try:
            genre_key = genre.value if genre.value in self.drum_patterns else "pop"
            pattern = self.drum_patterns[genre_key]
            
            drum_notes = []
            beats_per_bar = time_signature[0]
            subdivisions = 16  # 16th note subdivisions
            
            for bar in range(bars):
                bar_start_time = bar * beats_per_bar
                
                for drum_sound, pattern_data in pattern.items():
                    midi_note = self.drum_mapping[drum_sound]
                    
                    for i, hit in enumerate(pattern_data):
                        if hit:
                            note_time = bar_start_time + (i * beats_per_bar / subdivisions)
                            velocity = self._get_drum_velocity(drum_sound, i)
                            
                            note_obj = Note(
                                pitch=midi_note,
                                start_time=note_time,
                                duration=0.1,  # Short drum hits
                                velocity=velocity,
                                channel=9  # Standard MIDI drum channel
                            )
                            
                            drum_notes.append(note_obj)
            
            return drum_notes
            
        except Exception as e:
            logger.error(f"Drum track generation failed: {e}")
            return []
    
    def _get_drum_velocity(self, drum_sound: str, position: int) -> int:
        """Get velocity for drum sound based on position."""
        base_velocities = {
            "kick": 100,
            "snare": 90,
            "hihat": 60,
            "crash": 110,
            "ride": 70
        }
        
        base_vel = base_velocities.get(drum_sound, 80)
        
        # Add some variation
        variation = random.randint(-10, 10)
        return max(30, min(127, base_vel + variation))
    
    def generate_bass_line(
        self, 
        progression: Progression, 
        style: Genre, 
        bars: int = 8
    ) -> List[Note]:
        """Generate a bass line following the chord progression."""
        try:
            bass_notes = []
            beats_per_bar = progression.time_signature[0]
            
            for bar in range(bars):
                chord_index = min(bar, len(progression.chords) - 1)
                current_chord = progression.chords[chord_index]
                
                bar_start_time = bar * beats_per_bar
                
                if style == Genre.JAZZ:
                    # Walking bass line
                    notes_per_bar = 4
                    for i in range(notes_per_bar):
                        note_time = bar_start_time + i
                        pitch = self._get_walking_bass_note(current_chord, i)
                        
                        note_obj = Note(
                            pitch=pitch,
                            start_time=note_time,
                            duration=0.9,
                            velocity=80
                        )
                        bass_notes.append(note_obj)
                        
                else:
                    # Simple root-fifth pattern
                    root_note = Note(
                        pitch=current_chord.root - 12,  # One octave lower
                        start_time=bar_start_time,
                        duration=2.0,
                        velocity=85
                    )
                    
                    fifth_note = Note(
                        pitch=current_chord.root - 12 + 7,  # Fifth
                        start_time=bar_start_time + 2.0,
                        duration=2.0,
                        velocity=80
                    )
                    
                    bass_notes.extend([root_note, fifth_note])
            
            return bass_notes
            
        except Exception as e:
            logger.error(f"Bass line generation failed: {e}")
            return []
    
    def _get_walking_bass_note(self, chord: Chord, position: int) -> int:
        """Get walking bass note for jazz style."""
        root = chord.root - 12  # One octave lower
        
        if position == 0:
            return root  # Root on beat 1
        elif position == 1:
            return root + 4  # Third
        elif position == 2:
            return root + 7  # Fifth
        else:
            return root + 10  # Seventh or approach tone

class AIComposer:
    """Main AI composer using neural networks and music theory."""
    
    def __init__(self):
        self.theory_engine = MusicTheoryEngine()
        self.melody_generator = MelodyGenerator(self.theory_engine)
        self.rhythm_engine = RhythmEngine()
        self.llm = ChatOpenAI(
            model="gpt-4",
            temperature=0.7,
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        
    async def compose_piece(
        self,
        title: str,
        genre: Genre,
        mood: Mood,
        key_signature: str = "C",
        time_signature: Tuple[int, int] = (4, 4),
        tempo: int = 120,
        duration_bars: int = 16,
        instruments: List[Instrument] = None
    ) -> Composition:
        """Compose a complete musical piece."""
        try:
            logger.info(f"Starting composition: {title}")
            
            # Generate chord progression
            progression = await self._generate_progression(
                genre, key_signature, time_signature, tempo, duration_bars
            )
            
            # Initialize tracks
            tracks = []
            instruments = instruments or [Instrument.PIANO, Instrument.BASS, Instrument.DRUMS]
            
            # Generate tracks for each instrument
            for i, instrument in enumerate(instruments):
                track = await self._generate_track(
                    instrument, progression, genre, mood, duration_bars, i
                )
                tracks.append(track)
            
            # Create composition
            composition = Composition(
                composition_id=str(uuid.uuid4()),
                title=title,
                genre=genre,
                mood=mood,
                key_signature=key_signature,
                time_signature=time_signature,
                tempo=tempo,
                tracks=tracks,
                progression=progression,
                duration=duration_bars * time_signature[0] * (60.0 / tempo)
            )
            
            # Add metadata
            composition.metadata = await self._analyze_composition(composition)
            
            logger.info(f"Composition completed: {title}")
            return composition
            
        except Exception as e:
            logger.error(f"Composition failed: {e}")
            raise
    
    async def _generate_progression(
        self,
        genre: Genre,
        key_signature: str,
        time_signature: Tuple[int, int],
        tempo: int,
        duration_bars: int
    ) -> Progression:
        """Generate chord progression for the piece."""
        try:
            chords = self.theory_engine.generate_chord_progression(
                genre, key_signature, duration_bars
            )
            
            progression = Progression(
                chords=chords,
                key_signature=key_signature,
                time_signature=time_signature,
                tempo=tempo
            )
            
            return progression
            
        except Exception as e:
            logger.error(f"Progression generation failed: {e}")
            raise
    
    async def _generate_track(
        self,
        instrument: Instrument,
        progression: Progression,
        genre: Genre,
        mood: Mood,
        duration_bars: int,
        channel: int
    ) -> Track:
        """Generate a track for a specific instrument."""
        try:
            notes = []
            
            if instrument == Instrument.DRUMS:
                notes = self.rhythm_engine.generate_drum_track(
                    genre, duration_bars, progression.time_signature
                )
            elif instrument == Instrument.BASS:
                notes = self.rhythm_engine.generate_bass_line(
                    progression, genre, duration_bars
                )
            elif instrument in [Instrument.PIANO, Instrument.GUITAR]:
                # Generate both chords and melody
                chord_notes = await self._generate_chord_track(
                    progression, instrument, duration_bars
                )
                melody_notes = await self.melody_generator.generate_melody(
                    progression, genre, mood, duration_bars
                )
                # Shift melody up an octave
                for note in melody_notes:
                    note.pitch += 12
                notes = chord_notes + melody_notes
            else:
                # Generate melody for other instruments
                notes = await self.melody_generator.generate_melody(
                    progression, genre, mood, duration_bars
                )
            
            # Set channel for all notes
            for note in notes:
                note.channel = channel
            
            track = Track(
                instrument=instrument,
                notes=notes,
                name=f"{instrument.name.title()} Track",
                channel=channel
            )
            
            return track
            
        except Exception as e:
            logger.error(f"Track generation failed for {instrument}: {e}")
            return Track(instrument=instrument, notes=[], name="Empty Track", channel=channel)
    
    async def _generate_chord_track(
        self, 
        progression: Progression, 
        instrument: Instrument, 
        duration_bars: int
    ) -> List[Note]:
        """Generate chord accompaniment track."""
        try:
            chord_notes = []
            beats_per_bar = progression.time_signature[0]
            
            for bar in range(duration_bars):
                chord_index = min(bar, len(progression.chords) - 1)
                current_chord = progression.chords[chord_index]
                
                bar_start_time = bar * beats_per_bar
                chord_tones = self.melody_generator._get_chord_tones(
                    current_chord, 
                    self.theory_engine._get_scale_notes(progression.key_signature, "major")
                )
                
                # Generate chord voicing
                for i, pitch in enumerate(chord_tones[:3]):  # Use first 3 chord tones
                    note_obj = Note(
                        pitch=pitch - 12,  # One octave lower than melody
                        start_time=bar_start_time,
                        duration=beats_per_bar,
                        velocity=60 - i * 5  # Softer for accompaniment
                    )
                    chord_notes.append(note_obj)
            
            return chord_notes
            
        except Exception as e:
            logger.error(f"Chord track generation failed: {e}")
            return []
    
    async def _analyze_composition(self, composition: Composition) -> Dict[str, Any]:
        """Analyze the completed composition."""
        try:
            analysis = {
                "total_notes": sum(len(track.notes) for track in composition.tracks),
                "track_count": len(composition.tracks),
                "harmonic_analysis": self.theory_engine.analyze_harmony(
                    composition.progression.chords, 
                    composition.key_signature
                ),
                "complexity_score": 0.0,
                "style_authenticity": 0.0
            }
            
            # Calculate complexity score
            note_density = analysis["total_notes"] / composition.duration
            analysis["complexity_score"] = min(1.0, note_density / 10.0)
            
            # Estimate style authenticity (simplified)
            analysis["style_authenticity"] = random.uniform(0.7, 0.9)
            
            return analysis
            
        except Exception as e:
            logger.error(f"Composition analysis failed: {e}")
            return {}

class MIDIExporter:
    """Export compositions to MIDI format."""
    
    def __init__(self):
        pass
    
    async def export_to_midi(self, composition: Composition, output_path: str) -> str:
        """Export composition to MIDI file."""
        try:
            # Create MIDI file
            midi = pretty_midi.PrettyMIDI(initial_tempo=composition.tempo)
            
            # Create instruments for each track
            for track in composition.tracks:
                if track.instrument == Instrument.DRUMS:
                    # Drum track
                    instrument = pretty_midi.Instrument(
                        program=0, 
                        is_drum=True, 
                        name=track.name
                    )
                else:
                    # Melodic instrument
                    instrument = pretty_midi.Instrument(
                        program=track.instrument.value,
                        is_drum=False,
                        name=track.name
                    )
                
                # Add notes to instrument
                for note in track.notes:
                    midi_note = pretty_midi.Note(
                        velocity=note.velocity,
                        pitch=note.pitch,
                        start=note.start_time * (60.0 / composition.tempo),  # Convert to seconds
                        end=(note.start_time + note.duration) * (60.0 / composition.tempo)
                    )
                    instrument.notes.append(midi_note)
                
                midi.instruments.append(instrument)
            
            # Write MIDI file
            midi.write(output_path)
            logger.info(f"MIDI exported to: {output_path}")
            
            return output_path
            
        except Exception as e:
            logger.error(f"MIDI export failed: {e}")
            raise
    
    async def export_to_music21(self, composition: Composition) -> stream.Score:
        """Export composition to music21 Score object."""
        try:
            score = stream.Score()
            
            # Add metadata
            score.metadata = music21.metadata.Metadata()
            score.metadata.title = composition.title
            score.metadata.composer = "AI Composer"
            
            # Add key signature and time signature
            score.append(key.KeySignature(key.Key(composition.key_signature).sharps))
            score.append(meter.TimeSignature(f"{composition.time_signature[0]}/{composition.time_signature[1]}"))
            score.append(tempo.TempoIndication(number=composition.tempo))
            
            # Create parts for each track
            for track in composition.tracks:
                part = stream.Part()
                part.partName = track.name
                
                # Convert notes to music21 format
                for note_obj in track.notes:
                    # Convert duration from beats to quarter notes
                    dur = duration.Duration(quarterLength=note_obj.duration)
                    
                    if track.instrument == Instrument.DRUMS:
                        # Create unpitched percussion
                        n = note.Note(pitch=note_obj.pitch, quarterLength=note_obj.duration)
                    else:
                        n = note.Note(pitch=note_obj.pitch, quarterLength=note_obj.duration)
                    
                    n.volume.velocity = note_obj.velocity
                    n.offset = note_obj.start_time
                    
                    part.append(n)
                
                score.append(part)
            
            return score
            
        except Exception as e:
            logger.error(f"Music21 export failed: {e}")
            raise

class MusicComposerApp:
    """Main application orchestrator."""
    
    def __init__(self):
        self.composer = AIComposer()
        self.midi_exporter = MIDIExporter()
        
    async def create_composition(
        self,
        title: str,
        genre: str,
        mood: str,
        key: str = "C",
        tempo: int = 120,
        duration: int = 16,
        instruments: List[str] = None
    ) -> Dict[str, Any]:
        """Create a new musical composition."""
        try:
            # Convert string parameters to enums
            genre_enum = Genre(genre.lower())
            mood_enum = Mood(mood.lower())
            
            instrument_enums = []
            if instruments:
                for inst in instruments:
                    try:
                        instrument_enums.append(Instrument[inst.upper()])
                    except KeyError:
                        logger.warning(f"Unknown instrument: {inst}")
            
            # Compose the piece
            composition = await self.composer.compose_piece(
                title=title,
                genre=genre_enum,
                mood=mood_enum,
                key_signature=key,
                tempo=tempo,
                duration_bars=duration,
                instruments=instrument_enums or None
            )
            
            # Export to MIDI
            midi_path = f"output_{composition.composition_id}.mid"
            await self.midi_exporter.export_to_midi(composition, midi_path)
            
            return {
                "composition_id": composition.composition_id,
                "title": composition.title,
                "midi_file": midi_path,
                "metadata": composition.metadata,
                "duration": composition.duration,
                "tracks": len(composition.tracks)
            }
            
        except Exception as e:
            logger.error(f"Composition creation failed: {e}")
            raise
    
    async def analyze_composition(self, composition_id: str) -> Dict[str, Any]:
        """Analyze a composition's musical content."""
        try:
            # This would typically load from database
            # For now, return placeholder analysis
            return {
                "composition_id": composition_id,
                "harmonic_complexity": 0.75,
                "melodic_interest": 0.82,
                "rhythmic_variety": 0.68,
                "style_authenticity": 0.85,
                "overall_quality": 0.78
            }
            
        except Exception as e:
            logger.error(f"Composition analysis failed: {e}")
            return {}

# FastAPI Application
app = FastAPI(title="AI Music Composer", version="1.0.0")
music_app = MusicComposerApp()

class CompositionRequest(BaseModel):
    title: str = Field(..., description="Title of the composition")
    genre: str = Field(..., description="Musical genre")
    mood: str = Field(..., description="Emotional mood")
    key: str = Field(default="C", description="Key signature")
    tempo: int = Field(default=120, description="Tempo in BPM")
    duration: int = Field(default=16, description="Duration in bars")
    instruments: List[str] = Field(default=[], description="List of instruments")

@app.post("/compose")
async def create_composition(request: CompositionRequest):
    """Create a new musical composition."""
    try:
        result = await music_app.create_composition(
            title=request.title,
            genre=request.genre,
            mood=request.mood,
            key=request.key,
            tempo=request.tempo,
            duration=request.duration,
            instruments=request.instruments
        )
        return result
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/analyze/{composition_id}")
async def analyze_composition(composition_id: str):
    """Analyze a composition's musical content."""
    try:
        analysis = await music_app.analyze_composition(composition_id)
        return analysis
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/genres")
async def get_available_genres():
    """Get list of available musical genres."""
    return {"genres": [genre.value for genre in Genre]}

@app.get("/moods")
async def get_available_moods():
    """Get list of available moods."""
    return {"moods": [mood.value for mood in Mood]}

@app.get("/instruments")
async def get_available_instruments():
    """Get list of available instruments."""
    return {"instruments": [inst.name.lower() for inst in Instrument]}

@app.get("/health")
async def health_check():
    """Health check endpoint."""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
````

## Project Summary

The AI Music Composer transforms musical creation by combining advanced AI techniques with deep music theory knowledge to generate original compositions across multiple genres, democratizing music creation while maintaining artistic authenticity and theoretical sophistication.

### Key Value Propositions

**Intelligent Composition**: Generates complete musical pieces with proper harmonic progression, melodic development, and rhythmic complexity that adheres to music theory principles while exhibiting creative expression.

**Genre Authenticity**: Creates music that authentically captures the characteristics and conventions of specific musical styles, from classical and jazz to pop and electronic genres.

**Multi-Track Orchestration**: Produces full arrangements with multiple instruments, proper voice leading, and sophisticated interplay between melodic, harmonic, and rhythmic elements.

**Educational Integration**: Provides detailed analysis of generated compositions, explaining harmonic choices, melodic patterns, and theoretical concepts for music education applications.

### Technical Innovation

- **Music Theory Engine**: Computational implementation of harmony, melody, and rhythm principles
- **Neural Composition Models**: AI-driven melody and progression generation with style adaptation
- **MIDI Export System**: Professional-quality output compatible with digital audio workstations
- **Real-time Analysis**: Comprehensive evaluation of harmonic complexity and stylistic authenticity
- **Multi-Genre Modeling**: Specialized algorithms for different musical styles and cultural traditions

### Impact and Applications

Organizations and users implementing this solution can expect:
- **Creative Democratization**: Enable non-musicians to create sophisticated musical works
- **Professional Assistance**: Support composers with AI-powered ideation and arrangement tools
- **Educational Enhancement**: Interactive learning platform for music theory and composition
- **Content Production**: Automated background music generation for media and commercial use
- **Cultural Preservation**: Model and generate music in traditional or endangered musical styles
- **Therapeutic Applications**: Personalized music creation for wellness and therapeutic interventions

The AI Music Composer revolutionizes musical creation by making sophisticated composition accessible to everyone while maintaining the depth and authenticity that defines great music, bridging the gap between technological capability and artistic expression.