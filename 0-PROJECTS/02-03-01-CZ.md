<small>Claude Sonnet 4 **(AI-LLM Multi-Agent Systems: AutonomnÃ­ VÃ½zkumnÃ½ TÃ½m)**</small>
# Autonomous Research Team

## KlÃ­ÄovÃ© Koncepty

### Multi-Agent Systems (VÃ­ceagentnÃ­ systÃ©my)
SystÃ©my sklÃ¡dajÃ­cÃ­ se z vÃ­ce autonomnÃ­ch AI agentÅ¯, kteÅ™Ã­ spolupracujÃ­ na dosaÅ¾enÃ­ spoleÄnÃ½ch cÃ­lÅ¯. KaÅ¾dÃ½ agent mÃ¡ specializovanÃ© schopnosti a mÅ¯Å¾e komunikovat s ostatnÃ­mi agenty.

### LLM Agents (LLM Agenti)
AutonomnÃ­ software entity vyuÅ¾Ã­vajÃ­cÃ­ velkÃ© jazykovÃ© modely (Large Language Models) pro rozhodovÃ¡nÃ­, komunikaci a vykonÃ¡vÃ¡nÃ­ ÃºkolÅ¯. Jsou schopni plÃ¡novÃ¡nÃ­, reasoning a adaptace.

### Collaborative Research (KolaborativnÃ­ vÃ½zkum)
Proces, kdy vÃ­ce agentÅ¯ spolupracuje na vÃ½zkumnÃ½ch Ãºkolech, sdÃ­lÃ­ poznatky a kombinuje svÃ© schopnosti pro dosaÅ¾enÃ­ lepÅ¡Ã­ch vÃ½sledkÅ¯ neÅ¾ jednotlivci.

### Semantic Search (SÃ©mantickÃ© vyhledÃ¡vÃ¡nÃ­)
VyhledÃ¡vÃ¡nÃ­ zaloÅ¾enÃ© na pochopenÃ­ vÃ½znamu a kontextu, nikoli pouze na klÃ­ÄovÃ½ch slovech. VyuÅ¾Ã­vÃ¡ embeddingy a vektorovÃ© databÃ¡ze.

### Knowledge Synthesis (SyntÃ©za znalostÃ­)
Proces kombinovÃ¡nÃ­ informacÃ­ z rÅ¯znÃ½ch zdrojÅ¯ do koherentnÃ­ch a uÅ¾iteÄnÃ½ch poznatkÅ¯. Zahrnuje sumarizaci, analÃ½zu a vytvÃ¡Å™enÃ­ novÃ½ch zÃ¡vÄ›rÅ¯.

## KomplexnÃ­ VysvÄ›tlenÃ­ Projektu

### CÃ­le Projektu
AutonomnÃ­ vÃ½zkumnÃ½ tÃ½m pÅ™edstavuje pokroÄilÃ½ multi-agentnÃ­ systÃ©m navrÅ¾enÃ½ pro automatizaci akademickÃ©ho vÃ½zkumu. SystÃ©m se sklÃ¡dÃ¡ z specializovanÃ½ch AI agentÅ¯, kteÅ™Ã­ spolupracujÃ­ na:

1. **AutomatickÃ©m vyhledÃ¡vÃ¡nÃ­** relevantnÃ­ch akademickÃ½ch ÄlÃ¡nkÅ¯
2. **AnalÃ½ze a hodnocenÃ­** kvality vÃ½zkumnÃ½ch pracÃ­
3. **Extrakci klÃ­ÄovÃ½ch poznatkÅ¯** z vÄ›deckÃ½ch textÅ¯
4. **SyntÃ©ze poznatkÅ¯** do koherentnÃ­ch souhrnÅ¯
5. **GenerovÃ¡nÃ­ novÃ½ch vÃ½zkumnÃ½ch hypotÃ©z**

### ArchitektonickÃ© VÃ½zvy
- **Koordinace agentÅ¯**: ZajiÅ¡tÄ›nÃ­ efektivnÃ­ komunikace a spoluprÃ¡ce
- **Kvalita dat**: OvÄ›Å™ovÃ¡nÃ­ spolehlivosti zdrojÅ¯ a informacÃ­
- **Å kÃ¡lovatelnost**: ZpracovÃ¡nÃ­ velkÃ½ch objemÅ¯ akademickÃ½ch dat
- **Konzistence vÃ½sledkÅ¯**: ZajiÅ¡tÄ›nÃ­ koherence napÅ™Ã­Ä rÅ¯znÃ½mi agenty

### PotenciÃ¡lnÃ­ Dopad
SystÃ©m mÅ¯Å¾e revolucionizovat zpÅ¯sob, jakÃ½m se provÃ¡dÃ­ akademickÃ½ vÃ½zkum, umoÅ¾Åˆuje rychlejÅ¡Ã­ objevovÃ¡nÃ­ souvislostÃ­ mezi vÃ½zkumnÃ½mi oblastmi a automatizuje ÄasovÄ› nÃ¡roÄnÃ© Ãºkoly jako je literatura review.

## KomplexnÃ­ Implementace v Pythonu

```python
# requirements.txt
"""
crewai==0.28.8
langchain==0.1.13
langchain-openai==0.1.1
chromadb==0.4.24
requests==2.31.0
beautifulsoup4==4.12.3
pydantic==2.6.4
python-dotenv==1.0.1
arxiv==2.1.0
"""

import os
import asyncio
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
from dataclasses import dataclass
from enum import Enum

import requests
from bs4 import BeautifulSoup
import arxiv
import chromadb
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from crewai import Agent, Task, Crew, Process
from langchain_openai import ChatOpenAI
from pydantic import BaseModel
import json

# Konfigurace loggingu
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ResearchStatus(Enum):
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"

@dataclass
class ResearchPaper:
    """DatovÃ¡ struktura pro vÃ½zkumnÃ½ ÄlÃ¡nek"""
    title: str
    authors: List[str]
    abstract: str
    url: str
    published_date: str
    keywords: List[str]
    content: Optional[str] = None
    quality_score: Optional[float] = None

class VectorDatabase:
    """SprÃ¡va vektorovÃ© databÃ¡ze pro sÃ©mantickÃ© vyhledÃ¡vÃ¡nÃ­"""
    
    def __init__(self, collection_name: str = "research_papers"):
        self.client = chromadb.Client()
        self.collection_name = collection_name
        self.embeddings = OpenAIEmbeddings()
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        
        try:
            self.collection = self.client.get_collection(collection_name)
        except:
            self.collection = self.client.create_collection(collection_name)
    
    def add_papers(self, papers: List[ResearchPaper]) -> None:
        """PÅ™idÃ¡nÃ­ ÄlÃ¡nkÅ¯ do vektorovÃ© databÃ¡ze"""
        documents = []
        metadatas = []
        ids = []
        
        for i, paper in enumerate(papers):
            text_chunks = self.text_splitter.split_text(
                f"{paper.title}\n{paper.abstract}\n{paper.content or ''}"
            )
            
            for j, chunk in enumerate(text_chunks):
                documents.append(chunk)
                metadatas.append({
                    "title": paper.title,
                    "authors": ", ".join(paper.authors),
                    "url": paper.url,
                    "published_date": paper.published_date
                })
                ids.append(f"{i}_{j}")
        
        if documents:
            embeddings = self.embeddings.embed_documents(documents)
            self.collection.add(
                embeddings=embeddings,
                documents=documents,
                metadatas=metadatas,
                ids=ids
            )
    
    def search(self, query: str, n_results: int = 10) -> List[Dict]:
        """SÃ©mantickÃ© vyhledÃ¡vÃ¡nÃ­ v databÃ¡zi"""
        query_embedding = self.embeddings.embed_query(query)
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results
        )
        return results

class ArxivSearcher:
    """Agent pro vyhledÃ¡vÃ¡nÃ­ ÄlÃ¡nkÅ¯ na arXiv"""
    
    def __init__(self):
        self.client = arxiv.Client()
    
    def search_papers(self, query: str, max_results: int = 50) -> List[ResearchPaper]:
        """VyhledÃ¡nÃ­ ÄlÃ¡nkÅ¯ podle dotazu"""
        search = arxiv.Search(
            query=query,
            max_results=max_results,
            sort_by=arxiv.SortCriterion.SubmittedDate
        )
        
        papers = []
        for result in self.client.results(search):
            paper = ResearchPaper(
                title=result.title,
                authors=[author.name for author in result.authors],
                abstract=result.summary,
                url=result.entry_id,
                published_date=result.published.strftime("%Y-%m-%d"),
                keywords=result.categories
            )
            papers.append(paper)
        
        logger.info(f"Nalezeno {len(papers)} ÄlÃ¡nkÅ¯ pro dotaz: {query}")
        return papers

class QualityAnalyzer:
    """Agent pro hodnocenÃ­ kvality vÃ½zkumnÃ½ch ÄlÃ¡nkÅ¯"""
    
    def __init__(self, llm):
        self.llm = llm
    
    def analyze_quality(self, paper: ResearchPaper) -> float:
        """AnalÃ½za kvality ÄlÃ¡nku"""
        prompt = f"""
        Analyzuj kvalitu nÃ¡sledujÃ­cÃ­ho vÃ½zkumnÃ©ho ÄlÃ¡nku na Å¡kÃ¡le 0-10:
        
        NÃ¡zev: {paper.title}
        AutoÅ™i: {', '.join(paper.authors)}
        Abstrakt: {paper.abstract}
        
        HodnoÅ¥ podle kritÃ©riÃ­:
        - Novost a originalita
        - MetodologickÃ¡ rigorÃ³znost
        - Jasnost prezentace
        - Relevance vÃ½sledkÅ¯
        
        VraÅ¥ pouze ÄÃ­selnÃ© hodnocenÃ­ (0-10).
        """
        
        try:
            response = self.llm.invoke(prompt)
            score = float(response.content.strip())
            return max(0, min(10, score))
        except:
            return 5.0  # VÃ½chozÃ­ hodnocenÃ­

class KnowledgeSynthesizer:
    """Agent pro syntÃ©zu znalostÃ­ z vÃ­ce ÄlÃ¡nkÅ¯"""
    
    def __init__(self, llm):
        self.llm = llm
    
    def synthesize_research(self, papers: List[ResearchPaper], 
                          research_question: str) -> str:
        """SyntÃ©za poznatkÅ¯ z vÃ½zkumnÃ½ch ÄlÃ¡nkÅ¯"""
        # VÃ½bÄ›r nejkvalitnÄ›jÅ¡Ã­ch ÄlÃ¡nkÅ¯
        top_papers = sorted(papers, 
                          key=lambda x: x.quality_score or 0, 
                          reverse=True)[:10]
        
        papers_summary = "\n\n".join([
            f"**{paper.title}**\n{paper.abstract}"
            for paper in top_papers
        ])
        
        prompt = f"""
        Na zÃ¡kladÄ› nÃ¡sledujÃ­cÃ­ch vÃ½zkumnÃ½ch ÄlÃ¡nkÅ¯ zodpozvÄ›z vÃ½zkumnou otÃ¡zku:
        
        VÃZKUMNÃ OTÃZKA: {research_question}
        
        ÄŒLÃNKY:
        {papers_summary}
        
        VytvoÅ™ komplexnÃ­ syntÃ©zu, kterÃ¡:
        1. Shrne klÃ­ÄovÃ© poznatky
        2. Identifikuje spoleÄnÃ© vzory a trendy
        3. PoukÃ¡Å¾e na rozpory nebo mezery
        4. Navrhne smÄ›ry budoucÃ­ho vÃ½zkumu
        
        OdpovÄ›Ä strukturuj pomocÃ­ jasnÃ½ch nadpisÅ¯ a odstavcÅ¯.
        """
        
        response = self.llm.invoke(prompt)
        return response.content

class ResearchOrchestrator:
    """HlavnÃ­ orchestrÃ¡tor vÃ½zkumnÃ©ho tÃ½mu"""
    
    def __init__(self, openai_api_key: str):
        os.environ["OPENAI_API_KEY"] = openai_api_key
        
        self.llm = ChatOpenAI(temperature=0.3, model="gpt-4")
        self.vector_db = VectorDatabase()
        self.arxiv_searcher = ArxivSearcher()
        self.quality_analyzer = QualityAnalyzer(self.llm)
        self.knowledge_synthesizer = KnowledgeSynthesizer(self.llm)
        
        # Definice CrewAI agentÅ¯
        self.research_coordinator = Agent(
            role="VÃ½zkumnÃ½ koordinÃ¡tor",
            goal="Koordinovat vÃ½zkumnÃ½ proces a zajistit kvalitu vÃ½sledkÅ¯",
            backstory="Jsi zkuÅ¡enÃ½ vÃ½zkumnÃ­k s expertÃ­zou v koordinaci "
                     "multidisciplinÃ¡rnÃ­ch vÃ½zkumnÃ½ch projektÅ¯.",
            llm=self.llm,
            verbose=True
        )
        
        self.literature_analyst = Agent(
            role="Analytik literatury",
            goal="Analyzovat a hodnotit vÃ½zkumnou literaturu",
            backstory="SpecializujeÅ¡ se na kritickou analÃ½zu vÄ›deckÃ½ch "
                     "publikacÃ­ a hodnocenÃ­ jejich kvality.",
            llm=self.llm,
            verbose=True
        )
        
        self.synthesis_expert = Agent(
            role="Expert na syntÃ©zu znalostÃ­",
            goal="Kombinovat poznatky z rÅ¯znÃ½ch zdrojÅ¯ do koherentnÃ­ch zÃ¡vÄ›rÅ¯",
            backstory="MÃ¡Å¡ schopnost nachÃ¡zet souvislosti mezi rÅ¯znÃ½mi "
                     "vÃ½zkumnÃ½mi oblastmi a vytvÃ¡Å™et novÃ© poznatky.",
            llm=self.llm,
            verbose=True
        )
    
    async def conduct_research(self, research_question: str, 
                             search_terms: List[str]) -> Dict[str, Any]:
        """HlavnÃ­ vÃ½zkumnÃ½ proces"""
        logger.info(f"Zahajuji vÃ½zkum: {research_question}")
        
        # FÃ¡ze 1: VyhledÃ¡vÃ¡nÃ­ ÄlÃ¡nkÅ¯
        all_papers = []
        for term in search_terms:
            papers = self.arxiv_searcher.search_papers(term, max_results=20)
            all_papers.extend(papers)
        
        logger.info(f"Celkem nalezeno {len(all_papers)} ÄlÃ¡nkÅ¯")
        
        # FÃ¡ze 2: HodnocenÃ­ kvality
        for paper in all_papers:
            paper.quality_score = self.quality_analyzer.analyze_quality(paper)
        
        # FÃ¡ze 3: UloÅ¾enÃ­ do vektorovÃ© databÃ¡ze
        self.vector_db.add_papers(all_papers)
        
        # FÃ¡ze 4: CrewAI Ãºkoly
        research_task = Task(
            description=f"""
            Analyzuj nÃ¡sledujÃ­cÃ­ vÃ½zkumnou otÃ¡zku a navrhni strukturovanÃ½ 
            pÅ™Ã­stup k jejÃ­mu zodpovÄ›zenÃ­: {research_question}
            
            ZvaÅ¾ dostupnÃ© ÄlÃ¡nky a jejich relevanci.
            """,
            agent=self.research_coordinator
        )
        
        analysis_task = Task(
            description=f"""
            ProveÄ detailnÃ­ analÃ½zu kvality a relevance nalezenÃ½ch ÄlÃ¡nkÅ¯ 
            pro vÃ½zkumnou otÃ¡zku: {research_question}
            
            Identifikuj nejdÅ¯leÅ¾itÄ›jÅ¡Ã­ zdroje a jejich pÅ™Ã­nos.
            """,
            agent=self.literature_analyst
        )
        
        synthesis_task = Task(
            description=f"""
            VytvoÅ™ komplexnÃ­ syntÃ©zu poznatkÅ¯ odpovÃ­dajÃ­cÃ­ na: {research_question}
            
            Integrace: hlavnÃ­ poznatky z nejkvalitnÄ›jÅ¡Ã­ch zdrojÅ¯
            AnalÃ½za: trendy, vzory a mezery v souÄasnÃ©m vÃ½zkumu
            DoporuÄenÃ­: smÄ›ry budoucÃ­ho vÃ½zkumu
            """,
            agent=self.synthesis_expert
        )
        
        # SpuÅ¡tÄ›nÃ­ CrewAI procesu
        crew = Crew(
            agents=[self.research_coordinator, self.literature_analyst, 
                   self.synthesis_expert],
            tasks=[research_task, analysis_task, synthesis_task],
            process=Process.sequential,
            verbose=True
        )
        
        crew_result = crew.kickoff()
        
        # FÃ¡ze 5: FinÃ¡lnÃ­ syntÃ©za pomocÃ­ vlastnÃ­ho synthesizeru
        synthesis = self.knowledge_synthesizer.synthesize_research(
            all_papers, research_question
        )
        
        return {
            "research_question": research_question,
            "papers_found": len(all_papers),
            "high_quality_papers": len([p for p in all_papers if p.quality_score >= 7]),
            "crew_analysis": crew_result,
            "knowledge_synthesis": synthesis,
            "timestamp": datetime.now().isoformat(),
            "papers_summary": [
                {
                    "title": paper.title,
                    "quality_score": paper.quality_score,
                    "authors": paper.authors[:3]  # PrvnÃ­ 3 autoÅ™i
                }
                for paper in sorted(all_papers, 
                                  key=lambda x: x.quality_score or 0, 
                                  reverse=True)[:10]
            ]
        }
    
    def search_knowledge_base(self, query: str) -> List[Dict]:
        """VyhledÃ¡vÃ¡nÃ­ v existujÃ­cÃ­ znalostnÃ­ bÃ¡zi"""
        return self.vector_db.search(query)

# DemonstraÄnÃ­ pouÅ¾itÃ­
async def main():
    """HlavnÃ­ demonstraÄnÃ­ funkce"""
    
    # POZOR: Nastavte svÅ¯j OpenAI API klÃ­Ä
    API_KEY = "your-openai-api-key-here"
    
    if API_KEY == "your-openai-api-key-here":
        print("âš ï¸ Nastavte prosÃ­m vÃ¡Å¡ OpenAI API klÃ­Ä!")
        return
    
    # Inicializace vÃ½zkumnÃ©ho tÃ½mu
    research_team = ResearchOrchestrator(API_KEY)
    
    # Definice vÃ½zkumnÃ© otÃ¡zky
    research_question = "JakÃ© jsou nejnovÄ›jÅ¡Ã­ trendy v oblasti transformerovÃ½ch modelÅ¯ pro zpracovÃ¡nÃ­ pÅ™irozenÃ©ho jazyka?"
    
    search_terms = [
        "transformer models natural language processing",
        "attention mechanisms deep learning",
        "large language models architecture",
        "BERT GPT transformer improvements"
    ]
    
    print("ğŸ”¬ SpouÅ¡tÃ­m autonomnÃ­ vÃ½zkumnÃ½ tÃ½m...")
    print(f"ğŸ“‹ VÃ½zkumnÃ¡ otÃ¡zka: {research_question}")
    
    try:
        # ProvedenÃ­ vÃ½zkumu
        results = await research_team.conduct_research(
            research_question, search_terms
        )
        
        # ZobrazenÃ­ vÃ½sledkÅ¯
        print("\n" + "="*80)
        print("ğŸ“Š VÃSLEDKY VÃZKUMU")
        print("="*80)
        
        print(f"ğŸ” Nalezeno ÄlÃ¡nkÅ¯: {results['papers_found']}")
        print(f"â­ Vysoce kvalitnÃ­ch: {results['high_quality_papers']}")
        
        print(f"\nğŸ“ˆ TOP 5 ÄŒLÃNKÅ®:")
        for i, paper in enumerate(results['papers_summary'][:5], 1):
            print(f"{i}. {paper['title'][:60]}...")
            print(f"   Kvalita: {paper['quality_score']:.1f}/10")
            print(f"   AutoÅ™i: {', '.join(paper['authors'])}")
            print()
        
        print("ğŸ§  SYNTÃ‰ZA POZNATKÅ®:")
        print("-" * 40)
        print(results['knowledge_synthesis'])
        
        # UloÅ¾enÃ­ vÃ½sledkÅ¯
        with open(f"research_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json", 
                  'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ VÃ½sledky uloÅ¾eny do souboru")
        
    except Exception as e:
        logger.error(f"Chyba bÄ›hem vÃ½zkumu: {e}")
        print(f"âŒ VÃ½zkum selhal: {e}")

if __name__ == "__main__":
    asyncio.run(main())
```

### Instalace a NastavenÃ­

```bash
# Instalace zÃ¡vislostÃ­
pip install -r requirements.txt

# NastavenÃ­ promÄ›nnÃ½ch prostÅ™edÃ­
echo "OPENAI_API_KEY=your-api-key-here" > .env
```

## ShrnutÃ­ Projektu

### KlÃ­ÄovÃ© VÃ½hody
- **Automatizace vÃ½zkumu**: Eliminace manuÃ¡lnÃ­ho vyhledÃ¡vÃ¡nÃ­ a analÃ½zy
- **KvalitnÃ­ hodnocenÃ­**: AI-powered hodnocenÃ­ kvality zdrojÅ¯
- **SÃ©mantickÃ© vyhledÃ¡vÃ¡nÃ­**: InteligentnÃ­ objevovÃ¡nÃ­ relevantnÃ­ch ÄlÃ¡nkÅ¯
- **KolaborativnÃ­ pÅ™Ã­stup**: SpecializovanÃ­ agenti pro rÅ¯znÃ© aspekty vÃ½zkumu
- **Å kÃ¡lovatelnost**: Schopnost zpracovat tisÃ­ce ÄlÃ¡nkÅ¯ souÄasnÄ›

### TechnologickÃ© Inovace
Projekt kombinuje nejmodernÄ›jÅ¡Ã­ technologie vÄetnÄ› CrewAI pro orchestraci agentÅ¯, ChromaDB pro vektorovÃ© vyhledÃ¡vÃ¡nÃ­, a LangChain pro zpracovÃ¡nÃ­ jazykÅ¯, ÄÃ­mÅ¾ vytvÃ¡Å™Ã­ robustnÃ­ a efektivnÃ­ vÃ½zkumnÃ½ systÃ©m.

### BudoucÃ­ RozÅ¡Ã­Å™enÃ­
SystÃ©m lze rozÅ¡Ã­Å™it o podporu dalÅ¡Ã­ch databÃ¡zÃ­ (PubMed, Google Scholar), real-time monitoring novÃ½ch publikacÃ­, a integraci s institucionalnÃ­mi repozitÃ¡Å™i pro jeÅ¡tÄ› kompletnÄ›jÅ¡Ã­ vÃ½zkumnÃ© pokrytÃ­.