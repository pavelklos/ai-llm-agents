<small>Claude Sonnet 4 **(AI-LLM Multi-Agent Systems: Autonomní Výzkumný Tým)**</small>
# Autonomous Research Team

## Klíčové Koncepty

### Multi-Agent Systems (Víceagentní systémy)
Systémy skládající se z více autonomních AI agentů, kteří spolupracují na dosažení společných cílů. Každý agent má specializované schopnosti a může komunikovat s ostatními agenty.

### LLM Agents (LLM Agenti)
Autonomní software entity využívající velké jazykové modely (Large Language Models) pro rozhodování, komunikaci a vykonávání úkolů. Jsou schopni plánování, reasoning a adaptace.

### Collaborative Research (Kolaborativní výzkum)
Proces, kdy více agentů spolupracuje na výzkumných úkolech, sdílí poznatky a kombinuje své schopnosti pro dosažení lepších výsledků než jednotlivci.

### Semantic Search (Sémantické vyhledávání)
Vyhledávání založené na pochopení významu a kontextu, nikoli pouze na klíčových slovech. Využívá embeddingy a vektorové databáze.

### Knowledge Synthesis (Syntéza znalostí)
Proces kombinování informací z různých zdrojů do koherentních a užitečných poznatků. Zahrnuje sumarizaci, analýzu a vytváření nových závěrů.

## Komplexní Vysvětlení Projektu

### Cíle Projektu
Autonomní výzkumný tým představuje pokročilý multi-agentní systém navržený pro automatizaci akademického výzkumu. Systém se skládá z specializovaných AI agentů, kteří spolupracují na:

1. **Automatickém vyhledávání** relevantních akademických článků
2. **Analýze a hodnocení** kvality výzkumných prací
3. **Extrakci klíčových poznatků** z vědeckých textů
4. **Syntéze poznatků** do koherentních souhrnů
5. **Generování nových výzkumných hypotéz**

### Architektonické Výzvy
- **Koordinace agentů**: Zajištění efektivní komunikace a spolupráce
- **Kvalita dat**: Ověřování spolehlivosti zdrojů a informací
- **Škálovatelnost**: Zpracování velkých objemů akademických dat
- **Konzistence výsledků**: Zajištění koherence napříč různými agenty

### Potenciální Dopad
Systém může revolucionizovat způsob, jakým se provádí akademický výzkum, umožňuje rychlejší objevování souvislostí mezi výzkumnými oblastmi a automatizuje časově náročné úkoly jako je literatura review.

## Komplexní Implementace v Pythonu

```python
# requirements.txt
"""
crewai==0.28.8
langchain==0.1.13
langchain-openai==0.1.1
chromadb==0.4.24
requests==2.31.0
beautifulsoup4==4.12.3
pydantic==2.6.4
python-dotenv==1.0.1
arxiv==2.1.0
"""

import os
import asyncio
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
from dataclasses import dataclass
from enum import Enum

import requests
from bs4 import BeautifulSoup
import arxiv
import chromadb
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from crewai import Agent, Task, Crew, Process
from langchain_openai import ChatOpenAI
from pydantic import BaseModel
import json

# Konfigurace loggingu
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ResearchStatus(Enum):
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"

@dataclass
class ResearchPaper:
    """Datová struktura pro výzkumný článek"""
    title: str
    authors: List[str]
    abstract: str
    url: str
    published_date: str
    keywords: List[str]
    content: Optional[str] = None
    quality_score: Optional[float] = None

class VectorDatabase:
    """Správa vektorové databáze pro sémantické vyhledávání"""
    
    def __init__(self, collection_name: str = "research_papers"):
        self.client = chromadb.Client()
        self.collection_name = collection_name
        self.embeddings = OpenAIEmbeddings()
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        
        try:
            self.collection = self.client.get_collection(collection_name)
        except:
            self.collection = self.client.create_collection(collection_name)
    
    def add_papers(self, papers: List[ResearchPaper]) -> None:
        """Přidání článků do vektorové databáze"""
        documents = []
        metadatas = []
        ids = []
        
        for i, paper in enumerate(papers):
            text_chunks = self.text_splitter.split_text(
                f"{paper.title}\n{paper.abstract}\n{paper.content or ''}"
            )
            
            for j, chunk in enumerate(text_chunks):
                documents.append(chunk)
                metadatas.append({
                    "title": paper.title,
                    "authors": ", ".join(paper.authors),
                    "url": paper.url,
                    "published_date": paper.published_date
                })
                ids.append(f"{i}_{j}")
        
        if documents:
            embeddings = self.embeddings.embed_documents(documents)
            self.collection.add(
                embeddings=embeddings,
                documents=documents,
                metadatas=metadatas,
                ids=ids
            )
    
    def search(self, query: str, n_results: int = 10) -> List[Dict]:
        """Sémantické vyhledávání v databázi"""
        query_embedding = self.embeddings.embed_query(query)
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results
        )
        return results

class ArxivSearcher:
    """Agent pro vyhledávání článků na arXiv"""
    
    def __init__(self):
        self.client = arxiv.Client()
    
    def search_papers(self, query: str, max_results: int = 50) -> List[ResearchPaper]:
        """Vyhledání článků podle dotazu"""
        search = arxiv.Search(
            query=query,
            max_results=max_results,
            sort_by=arxiv.SortCriterion.SubmittedDate
        )
        
        papers = []
        for result in self.client.results(search):
            paper = ResearchPaper(
                title=result.title,
                authors=[author.name for author in result.authors],
                abstract=result.summary,
                url=result.entry_id,
                published_date=result.published.strftime("%Y-%m-%d"),
                keywords=result.categories
            )
            papers.append(paper)
        
        logger.info(f"Nalezeno {len(papers)} článků pro dotaz: {query}")
        return papers

class QualityAnalyzer:
    """Agent pro hodnocení kvality výzkumných článků"""
    
    def __init__(self, llm):
        self.llm = llm
    
    def analyze_quality(self, paper: ResearchPaper) -> float:
        """Analýza kvality článku"""
        prompt = f"""
        Analyzuj kvalitu následujícího výzkumného článku na škále 0-10:
        
        Název: {paper.title}
        Autoři: {', '.join(paper.authors)}
        Abstrakt: {paper.abstract}
        
        Hodnoť podle kritérií:
        - Novost a originalita
        - Metodologická rigoróznost
        - Jasnost prezentace
        - Relevance výsledků
        
        Vrať pouze číselné hodnocení (0-10).
        """
        
        try:
            response = self.llm.invoke(prompt)
            score = float(response.content.strip())
            return max(0, min(10, score))
        except:
            return 5.0  # Výchozí hodnocení

class KnowledgeSynthesizer:
    """Agent pro syntézu znalostí z více článků"""
    
    def __init__(self, llm):
        self.llm = llm
    
    def synthesize_research(self, papers: List[ResearchPaper], 
                          research_question: str) -> str:
        """Syntéza poznatků z výzkumných článků"""
        # Výběr nejkvalitnějších článků
        top_papers = sorted(papers, 
                          key=lambda x: x.quality_score or 0, 
                          reverse=True)[:10]
        
        papers_summary = "\n\n".join([
            f"**{paper.title}**\n{paper.abstract}"
            for paper in top_papers
        ])
        
        prompt = f"""
        Na základě následujících výzkumných článků zodpozvěz výzkumnou otázku:
        
        VÝZKUMNÁ OTÁZKA: {research_question}
        
        ČLÁNKY:
        {papers_summary}
        
        Vytvoř komplexní syntézu, která:
        1. Shrne klíčové poznatky
        2. Identifikuje společné vzory a trendy
        3. Poukáže na rozpory nebo mezery
        4. Navrhne směry budoucího výzkumu
        
        Odpověď strukturuj pomocí jasných nadpisů a odstavců.
        """
        
        response = self.llm.invoke(prompt)
        return response.content

class ResearchOrchestrator:
    """Hlavní orchestrátor výzkumného týmu"""
    
    def __init__(self, openai_api_key: str):
        os.environ["OPENAI_API_KEY"] = openai_api_key
        
        self.llm = ChatOpenAI(temperature=0.3, model="gpt-4")
        self.vector_db = VectorDatabase()
        self.arxiv_searcher = ArxivSearcher()
        self.quality_analyzer = QualityAnalyzer(self.llm)
        self.knowledge_synthesizer = KnowledgeSynthesizer(self.llm)
        
        # Definice CrewAI agentů
        self.research_coordinator = Agent(
            role="Výzkumný koordinátor",
            goal="Koordinovat výzkumný proces a zajistit kvalitu výsledků",
            backstory="Jsi zkušený výzkumník s expertízou v koordinaci "
                     "multidisciplinárních výzkumných projektů.",
            llm=self.llm,
            verbose=True
        )
        
        self.literature_analyst = Agent(
            role="Analytik literatury",
            goal="Analyzovat a hodnotit výzkumnou literaturu",
            backstory="Specializuješ se na kritickou analýzu vědeckých "
                     "publikací a hodnocení jejich kvality.",
            llm=self.llm,
            verbose=True
        )
        
        self.synthesis_expert = Agent(
            role="Expert na syntézu znalostí",
            goal="Kombinovat poznatky z různých zdrojů do koherentních závěrů",
            backstory="Máš schopnost nacházet souvislosti mezi různými "
                     "výzkumnými oblastmi a vytvářet nové poznatky.",
            llm=self.llm,
            verbose=True
        )
    
    async def conduct_research(self, research_question: str, 
                             search_terms: List[str]) -> Dict[str, Any]:
        """Hlavní výzkumný proces"""
        logger.info(f"Zahajuji výzkum: {research_question}")
        
        # Fáze 1: Vyhledávání článků
        all_papers = []
        for term in search_terms:
            papers = self.arxiv_searcher.search_papers(term, max_results=20)
            all_papers.extend(papers)
        
        logger.info(f"Celkem nalezeno {len(all_papers)} článků")
        
        # Fáze 2: Hodnocení kvality
        for paper in all_papers:
            paper.quality_score = self.quality_analyzer.analyze_quality(paper)
        
        # Fáze 3: Uložení do vektorové databáze
        self.vector_db.add_papers(all_papers)
        
        # Fáze 4: CrewAI úkoly
        research_task = Task(
            description=f"""
            Analyzuj následující výzkumnou otázku a navrhni strukturovaný 
            přístup k jejímu zodpovězení: {research_question}
            
            Zvaž dostupné články a jejich relevanci.
            """,
            agent=self.research_coordinator
        )
        
        analysis_task = Task(
            description=f"""
            Proveď detailní analýzu kvality a relevance nalezených článků 
            pro výzkumnou otázku: {research_question}
            
            Identifikuj nejdůležitější zdroje a jejich přínos.
            """,
            agent=self.literature_analyst
        )
        
        synthesis_task = Task(
            description=f"""
            Vytvoř komplexní syntézu poznatků odpovídající na: {research_question}
            
            Integrace: hlavní poznatky z nejkvalitnějších zdrojů
            Analýza: trendy, vzory a mezery v současném výzkumu
            Doporučení: směry budoucího výzkumu
            """,
            agent=self.synthesis_expert
        )
        
        # Spuštění CrewAI procesu
        crew = Crew(
            agents=[self.research_coordinator, self.literature_analyst, 
                   self.synthesis_expert],
            tasks=[research_task, analysis_task, synthesis_task],
            process=Process.sequential,
            verbose=True
        )
        
        crew_result = crew.kickoff()
        
        # Fáze 5: Finální syntéza pomocí vlastního synthesizeru
        synthesis = self.knowledge_synthesizer.synthesize_research(
            all_papers, research_question
        )
        
        return {
            "research_question": research_question,
            "papers_found": len(all_papers),
            "high_quality_papers": len([p for p in all_papers if p.quality_score >= 7]),
            "crew_analysis": crew_result,
            "knowledge_synthesis": synthesis,
            "timestamp": datetime.now().isoformat(),
            "papers_summary": [
                {
                    "title": paper.title,
                    "quality_score": paper.quality_score,
                    "authors": paper.authors[:3]  # První 3 autoři
                }
                for paper in sorted(all_papers, 
                                  key=lambda x: x.quality_score or 0, 
                                  reverse=True)[:10]
            ]
        }
    
    def search_knowledge_base(self, query: str) -> List[Dict]:
        """Vyhledávání v existující znalostní bázi"""
        return self.vector_db.search(query)

# Demonstrační použití
async def main():
    """Hlavní demonstrační funkce"""
    
    # POZOR: Nastavte svůj OpenAI API klíč
    API_KEY = "your-openai-api-key-here"
    
    if API_KEY == "your-openai-api-key-here":
        print("⚠️ Nastavte prosím váš OpenAI API klíč!")
        return
    
    # Inicializace výzkumného týmu
    research_team = ResearchOrchestrator(API_KEY)
    
    # Definice výzkumné otázky
    research_question = "Jaké jsou nejnovější trendy v oblasti transformerových modelů pro zpracování přirozeného jazyka?"
    
    search_terms = [
        "transformer models natural language processing",
        "attention mechanisms deep learning",
        "large language models architecture",
        "BERT GPT transformer improvements"
    ]
    
    print("🔬 Spouštím autonomní výzkumný tým...")
    print(f"📋 Výzkumná otázka: {research_question}")
    
    try:
        # Provedení výzkumu
        results = await research_team.conduct_research(
            research_question, search_terms
        )
        
        # Zobrazení výsledků
        print("\n" + "="*80)
        print("📊 VÝSLEDKY VÝZKUMU")
        print("="*80)
        
        print(f"🔍 Nalezeno článků: {results['papers_found']}")
        print(f"⭐ Vysoce kvalitních: {results['high_quality_papers']}")
        
        print(f"\n📈 TOP 5 ČLÁNKŮ:")
        for i, paper in enumerate(results['papers_summary'][:5], 1):
            print(f"{i}. {paper['title'][:60]}...")
            print(f"   Kvalita: {paper['quality_score']:.1f}/10")
            print(f"   Autoři: {', '.join(paper['authors'])}")
            print()
        
        print("🧠 SYNTÉZA POZNATKŮ:")
        print("-" * 40)
        print(results['knowledge_synthesis'])
        
        # Uložení výsledků
        with open(f"research_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json", 
                  'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\n💾 Výsledky uloženy do souboru")
        
    except Exception as e:
        logger.error(f"Chyba během výzkumu: {e}")
        print(f"❌ Výzkum selhal: {e}")

if __name__ == "__main__":
    asyncio.run(main())
```

### Instalace a Nastavení

```bash
# Instalace závislostí
pip install -r requirements.txt

# Nastavení proměnných prostředí
echo "OPENAI_API_KEY=your-api-key-here" > .env
```

## Shrnutí Projektu

### Klíčové Výhody
- **Automatizace výzkumu**: Eliminace manuálního vyhledávání a analýzy
- **Kvalitní hodnocení**: AI-powered hodnocení kvality zdrojů
- **Sémantické vyhledávání**: Inteligentní objevování relevantních článků
- **Kolaborativní přístup**: Specializovaní agenti pro různé aspekty výzkumu
- **Škálovatelnost**: Schopnost zpracovat tisíce článků současně

### Technologické Inovace
Projekt kombinuje nejmodernější technologie včetně CrewAI pro orchestraci agentů, ChromaDB pro vektorové vyhledávání, a LangChain pro zpracování jazyků, čímž vytváří robustní a efektivní výzkumný systém.

### Budoucí Rozšíření
Systém lze rozšířit o podporu dalších databází (PubMed, Google Scholar), real-time monitoring nových publikací, a integraci s institucionalními repozitáři pro ještě kompletnější výzkumné pokrytí.