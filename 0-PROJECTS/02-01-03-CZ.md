<small>Claude Sonnet 4 **(SÃ­Å¥ KolaborativnÃ­ch VÃ½zkumnÃ½ch AsistentÅ¯)**</small>
# Collaborative Research Assistant Network

## KlÃ­ÄovÃ© Koncepty

### Multi-Agent SystÃ©m (MAS)
DistribuovanÃ½ systÃ©m obsahujÃ­cÃ­ vÃ­ce autonomnÃ­ch agentÅ¯, kteÅ™Ã­ spolupracujÃ­ na dosaÅ¾enÃ­ spoleÄnÃ½ch cÃ­lÅ¯. KaÅ¾dÃ½ agent mÃ¡ specifickÃ© dovednosti a mÅ¯Å¾e komunikovat s ostatnÃ­mi agenty.

### Knowledge Discovery (ObjevovÃ¡nÃ­ znalostÃ­)
Proces automatizovanÃ©ho extrahovÃ¡nÃ­ uÅ¾iteÄnÃ½ch informacÃ­ a vzorÅ¯ z velkÃ½ch datovÃ½ch sad, vÄetnÄ› vÄ›deckÃ½ch publikacÃ­ a vÃ½zkumnÃ½ch dat.

### Literature Review (PÅ™ehled literatury)
SystematickÃ© vyhledÃ¡vÃ¡nÃ­, hodnocenÃ­ a syntÃ©za existujÃ­cÃ­ vÄ›deckÃ© literatury k danÃ©mu tÃ©matu pro identifikaci souÄasnÃ©ho stavu poznÃ¡nÃ­.

### Hypothesis Generation (GenerovÃ¡nÃ­ hypotÃ©z)
AutomatizovanÃ½ proces vytvÃ¡Å™enÃ­ novÃ½ch vÄ›deckÃ½ch hypotÃ©z na zÃ¡kladÄ› analÃ½zy existujÃ­cÃ­ch dat a literatury.

### Peer Review (VzÃ¡jemnÃ© hodnocenÃ­)
Proces hodnocenÃ­ vÄ›deckÃ© prÃ¡ce odbornÃ­ky ze stejnÃ©ho oboru za ÃºÄelem zajiÅ¡tÄ›nÃ­ kvality a validity vÃ½zkumu.

### Academic Research Coordination (Koordinace akademickÃ©ho vÃ½zkumu)
SystematickÃ© Å™Ã­zenÃ­ a organizace vÃ½zkumnÃ½ch aktivit, vÄetnÄ› plÃ¡novÃ¡nÃ­, alokace zdrojÅ¯ a sledovÃ¡nÃ­ pokroku.

## KomplexnÃ­ VysvÄ›tlenÃ­ Projektu

### CÃ­le Projektu
SÃ­Å¥ KolaborativnÃ­ch VÃ½zkumnÃ½ch AsistentÅ¯ je pokroÄilÃ½ multi-agent systÃ©m navrÅ¾enÃ½ pro automatizaci a zlepÅ¡enÃ­ akademickÃ©ho vÃ½zkumu. HlavnÃ­ cÃ­le zahrnujÃ­:

- **Automatizace rutinnÃ­ch ÃºkolÅ¯**: SystematickÃ© vyhledÃ¡vÃ¡nÃ­ literatury, extrakce dat a jejich analÃ½za
- **ZvÃ½Å¡enÃ­ kvality vÃ½zkumu**: Peer review proces a validace hypotÃ©z
- **Akcelerace objevÅ¯**: RychlejÅ¡Ã­ identifikace vÃ½zkumnÃ½ch mezer a generovÃ¡nÃ­ novÃ½ch hypotÃ©z
- **Koordinace tÃ½movÃ© prÃ¡ce**: EfektivnÃ­ spoluprÃ¡ce mezi vÃ½zkumnÃ­ky a institucemi

### ArchitektonickÃ© VÃ½zvy
- **Å kÃ¡lovatelnost**: SystÃ©m musÃ­ zvlÃ¡dat rostoucÃ­ objem vÄ›deckÃ© literatury
- **Heterogenita dat**: Integrace rÅ¯znÃ½ch formÃ¡tÅ¯ a zdrojÅ¯ vÄ›deckÃ½ch dat
- **Kvalita vÃ½stupÅ¯**: ZajiÅ¡tÄ›nÃ­ vÄ›rohodnosti a relevance generovanÃ½ch hypotÃ©z
- **Koordinace agentÅ¯**: EfektivnÃ­ komunikace a synchronizace mezi agenty

### PotenciÃ¡lnÃ­ Dopad
Projekt mÅ¯Å¾e revolucionizovat akademickÃ½ vÃ½zkum zkrÃ¡cenÃ­m Äasu potÅ™ebnÃ©ho pro literature review, zlepÅ¡enÃ­m kvality hypotÃ©z a umoÅ¾nÄ›nÃ­m vÄ›tÅ¡Ã­ mezioborovÃ© spoluprÃ¡ce.

## KomplexnÃ­ PÅ™Ã­klad s Python ImplementacÃ­

### ZÃ¡vislosti a Instalace

````python
langchain==0.1.0
openai==1.10.0
anthropic==0.15.0
chromadb==0.4.22
faiss-cpu==1.7.4
sentence-transformers==2.2.2
pandas==2.0.3
numpy==1.24.3
scikit-learn==1.3.0
requests==2.31.0
beautifulsoup4==4.12.2
arxiv==1.4.8
python-dotenv==1.0.0
streamlit==1.29.0
````

### HlavnÃ­ Implementace SystÃ©mu

````python
import os
import asyncio
import logging
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from datetime import datetime
import json

from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain.tools import Tool
from langchain.schema import Document
import pandas as pd
import numpy as np
import arxiv
import requests
from bs4 import BeautifulSoup
from sentence_transformers import SentenceTransformer
import chromadb
from dotenv import load_dotenv

load_dotenv()

# Konfigurace loggingu
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ResearchPaper:
    """TÅ™Ã­da reprezentujÃ­cÃ­ vÄ›deckÃ½ ÄlÃ¡nek"""
    title: str
    authors: List[str]
    abstract: str
    url: str
    published_date: datetime
    keywords: List[str]
    doi: Optional[str] = None
    
@dataclass
class Hypothesis:
    """TÅ™Ã­da reprezentujÃ­cÃ­ vÄ›deckou hypotÃ©zu"""
    id: str
    description: str
    confidence: float
    supporting_evidence: List[str]
    research_area: str
    generated_by: str
    timestamp: datetime

class BaseAgent:
    """ZÃ¡kladnÃ­ tÅ™Ã­da pro vÅ¡echny agenty"""
    
    def __init__(self, name: str, description: str):
        self.name = name
        self.description = description
        self.llm = ChatOpenAI(model="gpt-4", temperature=0.7)
        self.embeddings = OpenAIEmbeddings()
        
    async def process(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """ZÃ¡kladnÃ­ metoda pro zpracovÃ¡nÃ­ ÃºkolÅ¯"""
        raise NotImplementedError("MusÃ­ bÃ½t implementovÃ¡no v podtÅ™Ã­dÄ›")

class LiteratureReviewAgent(BaseAgent):
    """Agent pro systematickÃ½ pÅ™ehled literatury"""
    
    def __init__(self):
        super().__init__(
            name="Literature Review Agent",
            description="SpecializovanÃ½ agent pro vyhledÃ¡vÃ¡nÃ­ a analÃ½zu vÄ›deckÃ© literatury"
        )
        self.arxiv_client = arxiv.Client()
        self.vector_store = None
        self._setup_vector_store()
        
    def _setup_vector_store(self):
        """Inicializace vektorovÃ©ho ÃºloÅ¾iÅ¡tÄ›"""
        try:
            self.vector_store = Chroma(
                collection_name="research_papers",
                embedding_function=self.embeddings,
                persist_directory="./chroma_db"
            )
        except Exception as e:
            logger.error(f"Chyba pÅ™i inicializaci vector store: {e}")
    
    async def search_arxiv(self, query: str, max_results: int = 20) -> List[ResearchPaper]:
        """VyhledÃ¡vÃ¡nÃ­ ÄlÃ¡nkÅ¯ na ArXiv"""
        try:
            search = arxiv.Search(
                query=query,
                max_results=max_results,
                sort_by=arxiv.SortCriterion.SubmittedDate
            )
            
            papers = []
            for result in self.arxiv_client.results(search):
                paper = ResearchPaper(
                    title=result.title,
                    authors=[author.name for author in result.authors],
                    abstract=result.summary,
                    url=result.entry_id,
                    published_date=result.published,
                    keywords=[],
                    doi=result.doi
                )
                papers.append(paper)
                
            return papers
        except Exception as e:
            logger.error(f"Chyba pÅ™i vyhledÃ¡vÃ¡nÃ­ na ArXiv: {e}")
            return []
    
    def extract_keywords(self, text: str) -> List[str]:
        """Extrakce klÃ­ÄovÃ½ch slov z textu"""
        prompt = f"""
        Analyzuj nÃ¡sledujÃ­cÃ­ vÄ›deckÃ½ text a extrahuj 5-10 nejdÅ¯leÅ¾itÄ›jÅ¡Ã­ch klÃ­ÄovÃ½ch slov:
        
        {text[:1000]}...
        
        VraÅ¥ klÃ­ÄovÃ¡ slova jako seznam oddÄ›lenÃ½ ÄÃ¡rkami.
        """
        
        try:
            response = self.llm.predict(prompt)
            keywords = [kw.strip() for kw in response.split(',')]
            return keywords[:10]
        except Exception as e:
            logger.error(f"Chyba pÅ™i extrakci klÃ­ÄovÃ½ch slov: {e}")
            return []
    
    async def process(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """ZpracovÃ¡nÃ­ Ãºkolu literature review"""
        query = task.get('query', '')
        max_results = task.get('max_results', 20)
        
        papers = await self.search_arxiv(query, max_results)
        
        # Extrakce klÃ­ÄovÃ½ch slov a uloÅ¾enÃ­ do vector store
        for paper in papers:
            paper.keywords = self.extract_keywords(paper.abstract)
            
            # VytvoÅ™enÃ­ dokumentu pro vector store
            doc = Document(
                page_content=f"{paper.title}\n\n{paper.abstract}",
                metadata={
                    "title": paper.title,
                    "authors": ", ".join(paper.authors),
                    "url": paper.url,
                    "keywords": ", ".join(paper.keywords)
                }
            )
            
            if self.vector_store:
                self.vector_store.add_documents([doc])
        
        return {
            "agent": self.name,
            "papers_found": len(papers),
            "papers": papers,
            "status": "completed"
        }

class HypothesisGenerationAgent(BaseAgent):
    """Agent pro generovÃ¡nÃ­ vÄ›deckÃ½ch hypotÃ©z"""
    
    def __init__(self):
        super().__init__(
            name="Hypothesis Generation Agent", 
            description="Agent specializovanÃ½ na generovÃ¡nÃ­ novÃ½ch vÄ›deckÃ½ch hypotÃ©z"
        )
        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
        
    def analyze_research_gaps(self, papers: List[ResearchPaper]) -> List[str]:
        """AnalÃ½za vÃ½zkumnÃ½ch mezer"""
        abstracts = [paper.abstract for paper in papers]
        
        prompt = f"""
        Analyzuj nÃ¡sledujÃ­cÃ­ abstrakty vÄ›deckÃ½ch ÄlÃ¡nkÅ¯ a identifikuj 3-5 hlavnÃ­ch vÃ½zkumnÃ½ch mezer:
        
        {chr(10).join(abstracts[:5])}
        
        Pro kaÅ¾dou mezeru uveÄ:
        1. StruÄnÃ½ popis mezery
        2. ProÄ je dÅ¯leÅ¾itÃ¡
        3. MoÅ¾nÃ½ smÄ›r vÃ½zkumu
        
        FormÃ¡t: "Mezera: [popis] | DÅ¯leÅ¾itost: [dÅ¯vod] | SmÄ›r: [nÃ¡vrh]"
        """
        
        try:
            response = self.llm.predict(prompt)
            gaps = [gap.strip() for gap in response.split('\n') if gap.strip()]
            return gaps
        except Exception as e:
            logger.error(f"Chyba pÅ™i analÃ½ze vÃ½zkumnÃ½ch mezer: {e}")
            return []
    
    def generate_hypotheses(self, research_gaps: List[str], research_area: str) -> List[Hypothesis]:
        """GenerovÃ¡nÃ­ hypotÃ©z na zÃ¡kladÄ› vÃ½zkumnÃ½ch mezer"""
        hypotheses = []
        
        for i, gap in enumerate(research_gaps):
            prompt = f"""
            Na zÃ¡kladÄ› nÃ¡sledujÃ­cÃ­ vÃ½zkumnÃ© mezery vygeneruj konkrÃ©tnÃ­ testovatelnou hypotÃ©zu:
            
            VÃ½zkumnÃ¡ mezera: {gap}
            Oblast vÃ½zkumu: {research_area}
            
            HypotÃ©za musÃ­ bÃ½t:
            1. KonkrÃ©tnÃ­ a testovatelnÃ¡
            2. ZaloÅ¾enÃ¡ na existujÃ­cÃ­ch znalostech
            3. PotenciÃ¡lnÄ› pÅ™Ã­nosnÃ¡ pro obor
            
            FormÃ¡t odpovÄ›di:
            HypotÃ©za: [konkrÃ©tnÃ­ formulace]
            Confidence: [0.1-0.9]
            DÅ¯kazy: [seznam podporujÃ­cÃ­ch faktÅ¯]
            """
            
            try:
                response = self.llm.predict(prompt)
                
                # ParsovÃ¡nÃ­ odpovÄ›di
                lines = response.split('\n')
                hypothesis_text = ""
                confidence = 0.5
                evidence = []
                
                for line in lines:
                    if line.startswith("HypotÃ©za:"):
                        hypothesis_text = line.replace("HypotÃ©za:", "").strip()
                    elif line.startswith("Confidence:"):
                        try:
                            confidence = float(line.replace("Confidence:", "").strip())
                        except:
                            confidence = 0.5
                    elif line.startswith("DÅ¯kazy:"):
                        evidence.append(line.replace("DÅ¯kazy:", "").strip())
                
                if hypothesis_text:
                    hypothesis = Hypothesis(
                        id=f"hyp_{i}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                        description=hypothesis_text,
                        confidence=confidence,
                        supporting_evidence=evidence,
                        research_area=research_area,
                        generated_by=self.name,
                        timestamp=datetime.now()
                    )
                    hypotheses.append(hypothesis)
                    
            except Exception as e:
                logger.error(f"Chyba pÅ™i generovÃ¡nÃ­ hypotÃ©zy: {e}")
        
        return hypotheses
    
    async def process(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """ZpracovÃ¡nÃ­ Ãºkolu generovÃ¡nÃ­ hypotÃ©z"""
        papers = task.get('papers', [])
        research_area = task.get('research_area', 'General Science')
        
        research_gaps = self.analyze_research_gaps(papers)
        hypotheses = self.generate_hypotheses(research_gaps, research_area)
        
        return {
            "agent": self.name,
            "research_gaps": research_gaps,
            "hypotheses": hypotheses,
            "status": "completed"
        }

class PeerReviewAgent(BaseAgent):
    """Agent pro peer review proces"""
    
    def __init__(self):
        super().__init__(
            name="Peer Review Agent",
            description="Agent specializovanÃ½ na hodnocenÃ­ vÄ›deckÃ½ch hypotÃ©z a vÃ½zkumu"
        )
        
    def review_hypothesis(self, hypothesis: Hypothesis) -> Dict[str, Any]:
        """HodnocenÃ­ jednotlivÃ© hypotÃ©zy"""
        prompt = f"""
        Prove peer review nÃ¡sledujÃ­cÃ­ vÄ›deckÃ© hypotÃ©zy:
        
        HypotÃ©za: {hypothesis.description}
        Oblast: {hypothesis.research_area}
        Confidence: {hypothesis.confidence}
        PodporujÃ­cÃ­ dÅ¯kazy: {', '.join(hypothesis.supporting_evidence)}
        
        Hodno podle kritÃ©riÃ­:
        1. Testovatelnost (0-10)
        2. Originalita (0-10)
        3. VÄ›deckÃ¡ rigorÃ³znost (0-10)
        4. PotenciÃ¡lnÃ­ dopad (0-10)
        5. Realizovatelnost (0-10)
        
        Poskytni takÃ©:
        - CelkovÃ© hodnocenÃ­ (0-10)
        - HlavnÃ­ silnÃ© strÃ¡nky
        - HlavnÃ­ slabiny
        - DoporuÄenÃ­ pro zlepÅ¡enÃ­
        
        FormÃ¡t:
        Testovatelnost: [skÃ³re]
        Originalita: [skÃ³re]
        RigorÃ³znost: [skÃ³re]
        Dopad: [skÃ³re]
        Realizovatelnost: [skÃ³re]
        Celkem: [prÅ¯mÄ›r]
        SilnÃ© strÃ¡nky: [seznam]
        Slabiny: [seznam]
        DoporuÄenÃ­: [seznam]
        """
        
        try:
            response = self.llm.predict(prompt)
            
            # ParsovÃ¡nÃ­ hodnocenÃ­
            review = {
                "hypothesis_id": hypothesis.id,
                "reviewer": self.name,
                "timestamp": datetime.now(),
                "scores": {},
                "overall_score": 0.0,
                "strengths": [],
                "weaknesses": [],
                "recommendations": []
            }
            
            lines = response.split('\n')
            for line in lines:
                line = line.strip()
                if ':' in line:
                    key, value = line.split(':', 1)
                    key = key.strip().lower()
                    value = value.strip()
                    
                    if key in ['testovatelnost', 'originalita', 'rigorÃ³znost', 'dopad', 'realizovatelnost']:
                        try:
                            review["scores"][key] = float(value)
                        except:
                            pass
                    elif key == 'celkem':
                        try:
                            review["overall_score"] = float(value)
                        except:
                            pass
                    elif key == 'silnÃ© strÃ¡nky':
                        review["strengths"] = [s.strip() for s in value.split(',')]
                    elif key == 'slabiny':
                        review["weaknesses"] = [w.strip() for w in value.split(',')]
                    elif key == 'doporuÄenÃ­':
                        review["recommendations"] = [r.strip() for r in value.split(',')]
            
            return review
            
        except Exception as e:
            logger.error(f"Chyba pÅ™i peer review: {e}")
            return {}
    
    async def process(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """ZpracovÃ¡nÃ­ Ãºkolu peer review"""
        hypotheses = task.get('hypotheses', [])
        
        reviews = []
        for hypothesis in hypotheses:
            review = self.review_hypothesis(hypothesis)
            if review:
                reviews.append(review)
        
        return {
            "agent": self.name,
            "reviews": reviews,
            "status": "completed"
        }

class ResearchCoordinatorAgent(BaseAgent):
    """Agent pro koordinaci vÃ½zkumnÃ½ch aktivit"""
    
    def __init__(self):
        super().__init__(
            name="Research Coordinator Agent",
            description="HlavnÃ­ koordinÃ¡tor vÃ½zkumnÃ½ch aktivit a workflow"
        )
        self.literature_agent = LiteratureReviewAgent()
        self.hypothesis_agent = HypothesisGenerationAgent()
        self.review_agent = PeerReviewAgent()
        
    def prioritize_hypotheses(self, hypotheses: List[Hypothesis], reviews: List[Dict]) -> List[Dict]:
        """Prioritizace hypotÃ©z na zÃ¡kladÄ› hodnocenÃ­"""
        hypothesis_priorities = []
        
        for hypothesis in hypotheses:
            # Najdi odpovÃ­dajÃ­cÃ­ review
            review = next((r for r in reviews if r.get('hypothesis_id') == hypothesis.id), {})
            
            priority_score = hypothesis.confidence * 0.3
            if review:
                priority_score += review.get('overall_score', 0) * 0.7
            
            hypothesis_priorities.append({
                "hypothesis": hypothesis,
                "review": review,
                "priority_score": priority_score,
                "recommendation": self._get_recommendation(priority_score)
            })
        
        # SeÅ™azenÃ­ podle priority
        hypothesis_priorities.sort(key=lambda x: x['priority_score'], reverse=True)
        return hypothesis_priorities
    
    def _get_recommendation(self, score: float) -> str:
        """ZÃ­skÃ¡nÃ­ doporuÄenÃ­ na zÃ¡kladÄ› skÃ³re"""
        if score >= 7.0:
            return "VysokÃ¡ priorita - doporuÄeno k okamÅ¾itÃ© realizaci"
        elif score >= 5.0:
            return "StÅ™ednÃ­ priorita - potÅ™ebuje dalÅ¡Ã­ rozpracovÃ¡nÃ­"
        elif score >= 3.0:
            return "NÃ­zkÃ¡ priorita - vyÅ¾aduje vÃ½znamnÃ© Ãºpravy"
        else:
            return "NedoporuÄeno k realizaci v souÄasnÃ© formÄ›"
    
    async def coordinate_research(self, research_query: str, research_area: str) -> Dict[str, Any]:
        """HlavnÃ­ koordinaÄnÃ­ metoda pro vÃ½zkumnÃ½ proces"""
        try:
            logger.info(f"ZahajovÃ¡nÃ­ vÃ½zkumu pro dotaz: {research_query}")
            
            # Krok 1: Literature Review
            lit_task = {
                'query': research_query,
                'max_results': 20
            }
            lit_result = await self.literature_agent.process(lit_task)
            
            # Krok 2: GenerovÃ¡nÃ­ hypotÃ©z
            hyp_task = {
                'papers': lit_result.get('papers', []),
                'research_area': research_area
            }
            hyp_result = await self.hypothesis_agent.process(hyp_task)
            
            # Krok 3: Peer Review
            review_task = {
                'hypotheses': hyp_result.get('hypotheses', [])
            }
            review_result = await self.review_agent.process(review_task)
            
            # Krok 4: Prioritizace a koordinace
            priorities = self.prioritize_hypotheses(
                hyp_result.get('hypotheses', []),
                review_result.get('reviews', [])
            )
            
            # VytvoÅ™enÃ­ finÃ¡lnÃ­ho vÃ½zkumnÃ©ho plÃ¡nu
            research_plan = self._create_research_plan(priorities, research_area)
            
            return {
                "research_query": research_query,
                "research_area": research_area,
                "literature_review": lit_result,
                "hypothesis_generation": hyp_result,
                "peer_review": review_result,
                "prioritized_hypotheses": priorities,
                "research_plan": research_plan,
                "status": "completed",
                "timestamp": datetime.now()
            }
            
        except Exception as e:
            logger.error(f"Chyba pÅ™i koordinaci vÃ½zkumu: {e}")
            return {"status": "error", "error": str(e)}
    
    def _create_research_plan(self, priorities: List[Dict], research_area: str) -> Dict[str, Any]:
        """VytvoÅ™enÃ­ strukturovanÃ©ho vÃ½zkumnÃ©ho plÃ¡nu"""
        high_priority = [p for p in priorities if p['priority_score'] >= 7.0]
        medium_priority = [p for p in priorities if 5.0 <= p['priority_score'] < 7.0]
        
        plan = {
            "research_area": research_area,
            "immediate_actions": [
                {
                    "hypothesis": hp['hypothesis'].description,
                    "priority_score": hp['priority_score'],
                    "recommended_approach": hp['recommendation']
                }
                for hp in high_priority[:3]
            ],
            "future_research": [
                {
                    "hypothesis": mp['hypothesis'].description,
                    "priority_score": mp['priority_score'],
                    "development_needed": mp['recommendation']
                }
                for mp in medium_priority[:5]
            ],
            "resource_requirements": self._estimate_resources(high_priority + medium_priority),
            "timeline": self._create_timeline(high_priority, medium_priority)
        }
        
        return plan
    
    def _estimate_resources(self, priorities: List[Dict]) -> Dict[str, Any]:
        """Odhad potÅ™ebnÃ½ch zdrojÅ¯"""
        return {
            "estimated_researchers": len(priorities) * 2,
            "estimated_duration_months": len(priorities) * 3,
            "priority_areas": list(set([p['hypothesis'].research_area for p in priorities])),
            "recommended_collaborations": "MezioborovÃ¡ spoluprÃ¡ce doporuÄena"
        }
    
    def _create_timeline(self, high_priority: List[Dict], medium_priority: List[Dict]) -> Dict[str, List[str]]:
        """VytvoÅ™enÃ­ ÄasovÃ©ho plÃ¡nu"""
        return {
            "Phase 1 (0-6 months)": [hp['hypothesis'].description[:100] + "..." for hp in high_priority[:2]],
            "Phase 2 (6-12 months)": [hp['hypothesis'].description[:100] + "..." for hp in high_priority[2:]] + 
                                   [mp['hypothesis'].description[:100] + "..." for mp in medium_priority[:2]],
            "Phase 3 (12+ months)": [mp['hypothesis'].description[:100] + "..." for mp in medium_priority[2:]]
        }

# HlavnÃ­ aplikaÄnÃ­ rozhranÃ­
class ResearchAssistantNetwork:
    """HlavnÃ­ tÅ™Ã­da pro sÃ­Å¥ vÃ½zkumnÃ½ch asistentÅ¯"""
    
    def __init__(self):
        self.coordinator = ResearchCoordinatorAgent()
        self.session_history = []
        
    async def conduct_research(self, query: str, research_area: str = "General Science") -> Dict[str, Any]:
        """ProvedenÃ­ komplexnÃ­ho vÃ½zkumu"""
        result = await self.coordinator.coordinate_research(query, research_area)
        self.session_history.append(result)
        return result
    
    def get_session_summary(self) -> Dict[str, Any]:
        """ZÃ­skÃ¡nÃ­ souhrnu aktuÃ¡lnÃ­ session"""
        if not self.session_history:
            return {"message": "Å½Ã¡dnÃ½ vÃ½zkum nebyl dosud proveden"}
        
        total_papers = sum(r.get('literature_review', {}).get('papers_found', 0) for r in self.session_history)
        total_hypotheses = sum(len(r.get('hypothesis_generation', {}).get('hypotheses', [])) for r in self.session_history)
        
        return {
            "total_research_sessions": len(self.session_history),
            "total_papers_analyzed": total_papers,
            "total_hypotheses_generated": total_hypotheses,
            "research_areas": list(set(r.get('research_area', '') for r in self.session_history)),
            "last_research": self.session_history[-1] if self.session_history else None
        }

# DemonstraÄnÃ­ script
async def demo_research_network():
    """Demonstrace funkcionalit sÃ­tÄ› vÃ½zkumnÃ½ch asistentÅ¯"""
    
    print("ğŸ”¬ Inicializace SÃ­tÄ› KolaborativnÃ­ch VÃ½zkumnÃ½ch AsistentÅ¯")
    print("=" * 60)
    
    network = ResearchAssistantNetwork()
    
    # Simulace vÃ½zkumu
    research_queries = [
        ("machine learning interpretability", "Computer Science"),
        ("climate change mitigation", "Environmental Science"),
        ("quantum computing algorithms", "Physics")
    ]
    
    for query, area in research_queries:
        print(f"\nğŸ” ZahajovÃ¡nÃ­ vÃ½zkumu: '{query}' v oblasti '{area}'")
        print("-" * 50)
        
        try:
            result = await network.conduct_research(query, area)
            
            if result.get('status') == 'completed':
                print(f"âœ… VÃ½zkum dokonÄen ÃºspÄ›Å¡nÄ›")
                print(f"ğŸ“š Nalezeno ÄlÃ¡nkÅ¯: {result.get('literature_review', {}).get('papers_found', 0)}")
                print(f"ğŸ’¡ GenerovÃ¡no hypotÃ©z: {len(result.get('hypothesis_generation', {}).get('hypotheses', []))}")
                print(f"ğŸ“ Provedeno hodnocenÃ­: {len(result.get('peer_review', {}).get('reviews', []))}")
                
                # ZobrazenÃ­ top hypotÃ©zy
                priorities = result.get('prioritized_hypotheses', [])
                if priorities:
                    top_hypothesis = priorities[0]
                    print(f"ğŸ† NejlepÅ¡Ã­ hypotÃ©za (skÃ³re: {top_hypothesis['priority_score']:.2f}):")
                    print(f"   {top_hypothesis['hypothesis'].description[:150]}...")
                    
            else:
                print(f"âŒ Chyba pÅ™i vÃ½zkumu: {result.get('error', 'NeznÃ¡mÃ¡ chyba')}")
                
        except Exception as e:
            print(f"âŒ VÃ½jimka bÄ›hem vÃ½zkumu: {e}")
    
    # Souhrn session
    print(f"\nğŸ“Š Souhrn vÃ½zkumnÃ© session:")
    print("=" * 40)
    summary = network.get_session_summary()
    for key, value in summary.items():
        if key != 'last_research':
            print(f"{key}: {value}")

if __name__ == "__main__":
    # SpuÅ¡tÄ›nÃ­ demonstrace
    import asyncio
    asyncio.run(demo_research_network())
````

### KonfiguraÄnÃ­ soubor

````python
import os
from typing import Dict, Any

class Config:
    """KonfiguraÄnÃ­ tÅ™Ã­da pro sÃ­Å¥ vÃ½zkumnÃ½ch asistentÅ¯"""
    
    # API klÃ­Äe
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
    
    # NastavenÃ­ modelÅ¯
    LLM_MODEL = "gpt-4"
    EMBEDDING_MODEL = "text-embedding-ada-002"
    SENTENCE_TRANSFORMER_MODEL = "all-MiniLM-L6-v2"
    
    # NastavenÃ­ vektorovÃ©ho ÃºloÅ¾iÅ¡tÄ›
    CHROMA_PERSIST_DIRECTORY = "./chroma_db"
    VECTOR_COLLECTION_NAME = "research_papers"
    
    # Parametry vyhledÃ¡vÃ¡nÃ­
    DEFAULT_MAX_PAPERS = 20
    DEFAULT_TEMPERATURE = 0.7
    
    # NastavenÃ­ peer review
    REVIEW_CRITERIA = [
        "testovatelnost",
        "originalita", 
        "rigorÃ³znost",
        "dopad",
        "realizovatelnost"
    ]
    
    # PrahovÃ© hodnoty pro prioritizaci
    HIGH_PRIORITY_THRESHOLD = 7.0
    MEDIUM_PRIORITY_THRESHOLD = 5.0
    LOW_PRIORITY_THRESHOLD = 3.0
    
    @classmethod
    def get_llm_config(cls) -> Dict[str, Any]:
        """Konfigurace pro LLM"""
        return {
            "model": cls.LLM_MODEL,
            "temperature": cls.DEFAULT_TEMPERATURE,
            "api_key": cls.OPENAI_API_KEY
        }
    
    @classmethod
    def get_embeddings_config(cls) -> Dict[str, Any]:
        """Konfigurace pro embeddings"""
        return {
            "model": cls.EMBEDDING_MODEL,
            "api_key": cls.OPENAI_API_KEY
        }
    
    @classmethod
    def validate_config(cls) -> bool:
        """Validace konfigurace"""
        required_vars = ["OPENAI_API_KEY"]
        missing_vars = [var for var in required_vars if not getattr(cls, var)]
        
        if missing_vars:
            raise ValueError(f"ChybÃ­ povinnÃ© promÄ›nnÃ© prostÅ™edÃ­: {missing_vars}")
        
        return True
````

### Streamlit aplikace pro demonstraci

````python
import streamlit as st
import asyncio
import json
from datetime import datetime
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go

from research_agents import ResearchAssistantNetwork

# NastavenÃ­ strÃ¡nky
st.set_page_config(
    page_title="SÃ­Å¥ KolaborativnÃ­ch VÃ½zkumnÃ½ch AsistentÅ¯",
    page_icon="ğŸ”¬",
    layout="wide"
)

st.title("ğŸ”¬ SÃ­Å¥ KolaborativnÃ­ch VÃ½zkumnÃ½ch AsistentÅ¯")
st.markdown("PokroÄilÃ½ multi-agent systÃ©m pro akademickÃ½ vÃ½zkum")

# Inicializace session state
if 'network' not in st.session_state:
    st.session_state.network = ResearchAssistantNetwork()
if 'research_history' not in st.session_state:
    st.session_state.research_history = []

# HlavnÃ­ interface
col1, col2 = st.columns([2, 1])

with col1:
    st.header("ğŸ” NovÃ½ vÃ½zkum")
    
    research_query = st.text_input(
        "VÃ½zkumnÃ½ dotaz:",
        placeholder="NapÅ™. 'machine learning interpretability'"
    )
    
    research_area = st.selectbox(
        "Oblast vÃ½zkumu:",
        ["Computer Science", "Physics", "Biology", "Chemistry", 
         "Environmental Science", "Medicine", "Psychology", "Economics"]
    )
    
    if st.button("ğŸš€ ZahÃ¡jit vÃ½zkum", type="primary"):
        if research_query:
            with st.spinner("ProvÃ¡dÃ­m komplexnÃ­ vÃ½zkum..."):
                try:
                    # SpuÅ¡tÄ›nÃ­ asynchronnÃ­ho vÃ½zkumu
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    result = loop.run_until_complete(
                        st.session_state.network.conduct_research(research_query, research_area)
                    )
                    loop.close()
                    
                    st.session_state.research_history.append(result)
                    st.success("âœ… VÃ½zkum dokonÄen ÃºspÄ›Å¡nÄ›!")
                    
                except Exception as e:
                    st.error(f"âŒ Chyba pÅ™i vÃ½zkumu: {e}")
        else:
            st.warning("ProsÃ­m zadejte vÃ½zkumnÃ½ dotaz")

with col2:
    st.header("ğŸ“Š Statistiky session")
    summary = st.session_state.network.get_session_summary()
    
    if summary.get('total_research_sessions', 0) > 0:
        st.metric("VÃ½zkumnÃ© session", summary['total_research_sessions'])
        st.metric("AnalyzovanÃ© ÄlÃ¡nky", summary['total_papers_analyzed'])
        st.metric("GenerovanÃ© hypotÃ©zy", summary['total_hypotheses_generated'])
    else:
        st.info("ZatÃ­m nebyl proveden Å¾Ã¡dnÃ½ vÃ½zkum")

# ZobrazenÃ­ vÃ½sledkÅ¯ poslednÃ­ho vÃ½zkumu
if st.session_state.research_history:
    st.header("ğŸ“‹ VÃ½sledky poslednÃ­ho vÃ½zkumu")
    
    latest_result = st.session_state.research_history[-1]
    
    if latest_result.get('status') == 'completed':
        # Tabs pro rÅ¯znÃ© sekce
        tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“š Literatura", "ğŸ’¡ HypotÃ©zy", "ğŸ“ HodnocenÃ­", "ğŸ“‹ PlÃ¡n"])
        
        with tab1:
            lit_review = latest_result.get('literature_review', {})
            st.subheader(f"Nalezeno {lit_review.get('papers_found', 0)} ÄlÃ¡nkÅ¯")
            
            papers = lit_review.get('papers', [])
            if papers:
                papers_data = []
                for paper in papers[:5]:  # Top 5 ÄlÃ¡nkÅ¯
                    papers_data.append({
                        "NÃ¡zev": paper.title[:100] + "...",
                        "AutoÅ™i": ", ".join(paper.authors[:3]),
                        "Datum": paper.published_date.strftime("%Y-%m-%d"),
                        "KlÃ­ÄovÃ¡ slova": ", ".join(paper.keywords[:5])
                    })
                
                df_papers = pd.DataFrame(papers_data)
                st.dataframe(df_papers, use_container_width=True)
        
        with tab2:
            hyp_gen = latest_result.get('hypothesis_generation', {})
            hypotheses = hyp_gen.get('hypotheses', [])
            
            st.subheader(f"GenerovÃ¡no {len(hypotheses)} hypotÃ©z")
            
            for i, hyp in enumerate(hypotheses):
                with st.expander(f"HypotÃ©za {i+1} (Confidence: {hyp.confidence:.2f})"):
                    st.write(hyp.description)
                    if hyp.supporting_evidence:
                        st.write("**PodporujÃ­cÃ­ dÅ¯kazy:**")
                        for evidence in hyp.supporting_evidence:
                            st.write(f"â€¢ {evidence}")
        
        with tab3:
            reviews = latest_result.get('peer_review', {}).get('reviews', [])
            
            if reviews:
                st.subheader("HodnocenÃ­ hypotÃ©z")
                
                # Graf hodnocenÃ­
                review_data = []
                for review in reviews:
                    scores = review.get('scores', {})
                    review_data.append({
                        'HypotÃ©za': f"Hyp {len(review_data) + 1}",
                        'CelkovÃ© skÃ³re': review.get('overall_score', 0),
                        **scores
                    })
                
                if review_data:
                    df_reviews = pd.DataFrame(review_data)
                    
                    # Radar chart pro prvnÃ­ hypotÃ©zu
                    if len(df_reviews) > 0:
                        first_review = df_reviews.iloc[0]
                        categories = ['testovatelnost', 'originalita', 'rigorÃ³znost', 'dopad', 'realizovatelnost']
                        values = [first_review.get(cat, 0) for cat in categories]
                        
                        fig = go.Figure()
                        fig.add_trace(go.Scatterpolar(
                            r=values,
                            theta=categories,
                            fill='toself',
                            name='HodnocenÃ­ prvnÃ­ hypotÃ©zy'
                        ))
                        
                        fig.update_layout(
                            polar=dict(
                                radialaxis=dict(
                                    visible=True,
                                    range=[0, 10]
                                )),
                            showlegend=True
                        )
                        
                        st.plotly_chart(fig, use_container_width=True)
        
        with tab4:
            research_plan = latest_result.get('research_plan', {})
            
            st.subheader("VÃ½zkumnÃ½ plÃ¡n")
            
            # OkamÅ¾itÃ© akce
            immediate = research_plan.get('immediate_actions', [])
            if immediate:
                st.write("**ğŸ”¥ VysokÃ¡ priorita:**")
                for action in immediate:
                    st.write(f"â€¢ {action['hypothesis']} (SkÃ³re: {action['priority_score']:.2f})")
            
            # BudoucÃ­ vÃ½zkum
            future = research_plan.get('future_research', [])
            if future:
                st.write("**â³ BudoucÃ­ vÃ½zkum:**")
                for future_item in future:
                    st.write(f"â€¢ {future_item['hypothesis']} (SkÃ³re: {future_item['priority_score']:.2f})")
            
            # Zdroje a timeline
            resources = research_plan.get('resource_requirements', {})
            if resources:
                st.write("**ğŸ“Š Odhad zdrojÅ¯:**")
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("VÃ½zkumnÃ­ci", resources.get('estimated_researchers', 0))
                with col2:
                    st.metric("MÄ›sÃ­ce", resources.get('estimated_duration_months', 0))

# Historie vÃ½zkumÅ¯
if len(st.session_state.research_history) > 1:
    st.header("ğŸ“š Historie vÃ½zkumÅ¯")
    
    history_data = []
    for i, result in enumerate(st.session_state.research_history):
        if result.get('status') == 'completed':
            history_data.append({
                'Index': i + 1,
                'Dotaz': result.get('research_query', '')[:50] + "...",
                'Oblast': result.get('research_area', ''),
                'ÄŒlÃ¡nky': result.get('literature_review', {}).get('papers_found', 0),
                'HypotÃ©zy': len(result.get('hypothesis_generation', {}).get('hypotheses', [])),
                'ÄŒas': result.get('timestamp', datetime.now()).strftime("%H:%M:%S")
            })
    
    if history_data:
        df_history = pd.DataFrame(history_data)
        st.dataframe(df_history, use_container_width=True)
        
        # Graf trendÅ¯
        fig = px.line(df_history, x='Index', y=['ÄŒlÃ¡nky', 'HypotÃ©zy'], 
                     title="Trendy vÃ½zkumnÃ½ch vÃ½sledkÅ¯")
        st.plotly_chart(fig, use_container_width=True)

# Sidebar s informacemi
st.sidebar.header("â„¹ï¸ O systÃ©mu")
st.sidebar.markdown("""
**SÃ­Å¥ KolaborativnÃ­ch VÃ½zkumnÃ½ch AsistentÅ¯** je pokroÄilÃ½ multi-agent systÃ©m pro:

- ğŸ“š **Literature Review**: AutomatickÃ© vyhledÃ¡vÃ¡nÃ­ a analÃ½za vÄ›deckÃ© literatury
- ğŸ’¡ **GenerovÃ¡nÃ­ hypotÃ©z**: Tvorba novÃ½ch testovatelnÃ½ch hypotÃ©z  
- ğŸ“ **Peer Review**: HodnocenÃ­ kvality vÃ½zkumu
- ğŸ“‹ **Koordinace**: Å˜Ã­zenÃ­ vÃ½zkumnÃ©ho procesu

**PouÅ¾itÃ© technologie:**
- LangChain pro LLM orchestraci
- OpenAI GPT-4 pro inteligentnÃ­ analÃ½zu
- ChromaDB pro vektorovÃ© uklÃ¡dÃ¡nÃ­
- ArXiv API pro vÄ›deckÃ© ÄlÃ¡nky
""")
````

## Souhrn Projektu

SÃ­Å¥ KolaborativnÃ­ch VÃ½zkumnÃ½ch AsistentÅ¯ pÅ™edstavuje inovativnÃ­ Å™eÅ¡enÃ­ pro automatizaci a zlepÅ¡enÃ­ akademickÃ©ho vÃ½zkumu. SystÃ©m kombinuje sÃ­lu modernÃ­ch LLM modelÅ¯ s pokroÄilÃ½mi technikami multi-agent architektury.

### KlÃ­ÄovÃ© Hodnoty

**ğŸš€ Efektivita**: DramatickÃ© zkrÃ¡cenÃ­ Äasu potÅ™ebnÃ©ho pro literature review a generovÃ¡nÃ­ hypotÃ©z

**ğŸ¯ Kvalita**: SystematickÃ© peer review proces zajiÅ¡Å¥uje vysokou kvalitu vÃ½zkumnÃ½ch vÃ½stupÅ¯

**ğŸ”— Kolaborace**: UmoÅ¾Åˆuje efektivnÃ­ spoluprÃ¡ci mezi vÃ½zkumnÃ­ky a institucemi

**ğŸ“ˆ Å kÃ¡lovatelnost**: Architektura podporuje rozÅ¡Ã­Å™enÃ­ o dalÅ¡Ã­ specializovanÃ© agenty

### PotenciÃ¡lnÃ­ Dopady

- **AkademickÃ© instituce**: ZrychlenÃ­ vÃ½zkumnÃ½ch cyklÅ¯ a zlepÅ¡enÃ­ kvality publikacÃ­
- **PrÅ¯myslovÃ½ vÃ½zkum**: RychlejÅ¡Ã­ identifikace novÃ½ch pÅ™Ã­leÅ¾itostÃ­ a trendÅ¯  
- **MezioborovÃ¡ spoluprÃ¡ce**: UsnadnÄ›nÃ­ propojenÃ­ rÅ¯znÃ½ch vÄ›deckÃ½ch disciplÃ­n
- **MladÃ­ vÃ½zkumnÃ­ci**: PoskytnutÃ­ pokroÄilÃ½ch nÃ¡strojÅ¯ pro podporu jejich kariÃ©ry

Tento projekt demonstruje, jak mÅ¯Å¾e AI transformovat tradiÄnÃ­ vÄ›deckÃ© procesy a otevÅ™Ã­t novÃ© moÅ¾nosti pro objevovÃ¡nÃ­ znalostÃ­ v digitÃ¡lnÃ­m vÄ›ku.