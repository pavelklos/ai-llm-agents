<small>Claude Sonnet 4 **(Voice-Controlled Home Automation Agent)**</small>
# Voice-Controlled Home Automation Agent

## Key Concepts Explanation

### Speech-to-Text Processing
Advanced audio processing system that captures voice commands through microphones, converts speech signals to digital text using deep learning models, handles noise reduction and voice recognition, and supports multiple languages and accents for natural voice interaction.

### Natural Language Understanding (NLU)
Intelligent command interpretation system that parses voice commands, extracts intent and entities, understands context and ambiguity, and maps natural language requests to specific home automation actions with high accuracy and flexibility.

### IoT Device Integration
Comprehensive device management framework that connects to smart home devices through various protocols (WiFi, Zigbee, Z-Wave, Bluetooth), provides unified device control APIs, handles device discovery and pairing, and manages device states and capabilities.

### API Control and Orchestration
Centralized control system that interfaces with multiple smart home platforms (Philips Hue, Nest, SmartThings, Alexa), manages device authentication and security, coordinates complex automation scenarios, and provides real-time device monitoring and control.

### Context-Aware Automation
Intelligent automation engine that learns user preferences and patterns, considers environmental factors and time-based rules, manages multi-device scenarios and dependencies, and adapts automation behavior based on usage history and context.

## Comprehensive Project Explanation

### Objectives
The Voice-Controlled Home Automation Agent creates an intelligent voice interface for controlling smart home devices, enabling natural language interaction with IoT systems, and providing seamless automation experiences through advanced speech recognition and device integration.

### Key Features
- **Multi-Modal Voice Control**: Support for various languages and speaking styles
- **Device Discovery**: Automatic detection and integration of smart home devices
- **Scene Management**: Create and control complex automation scenarios
- **Learning Capabilities**: Adapt to user preferences and usage patterns
- **Security Management**: Secure device authentication and access control

### Challenges
- **Voice Recognition Accuracy**: Handling diverse accents, background noise, and speech patterns
- **Device Compatibility**: Supporting multiple IoT protocols and manufacturer APIs
- **Real-time Processing**: Low-latency voice-to-action execution for responsive control
- **Context Understanding**: Interpreting ambiguous commands and maintaining conversation state

### Potential Impact
This system can revolutionize home automation accessibility, reduce dependency on mobile apps and physical switches, improve accessibility for disabled users, enable hands-free control in various scenarios, and create more intuitive smart home experiences for mainstream adoption.

## Comprehensive Project Example with Python Implementation

### Dependencies and Setup

````python
# requirements.txt
streamlit==1.29.0
langchain==0.1.0
langchain-openai==0.0.5
speech_recognition==3.10.0
pydub==0.25.1
pyaudio==0.2.11
gtts==2.4.0
pygame==2.5.2
requests==2.31.0
paho-mqtt==1.6.1
zeroconf==0.112.0
bleak==0.21.1
flask==3.0.0
websockets==12.0
asyncio
pandas==2.1.4
numpy==1.24.3
plotly==5.17.0
pydantic==2.5.0
datetime
logging
typing
dataclasses
enum
json
re
uuid
threading
time
````

### Core Implementation

````python
import speech_recognition as sr
import pydub
import pygame
import json
import logging
import asyncio
import threading
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Union, Callable
from dataclasses import dataclass, field
from enum import Enum
import uuid
import requests
import re

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go

# LangChain components
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import BaseMessage

# IoT and networking
import paho.mqtt.client as mqtt
from zeroconf import ServiceBrowser, Zeroconf
import websockets

# Text-to-speech
from gtts import gTTS
import io

# Async support
import asyncio
from concurrent.futures import ThreadPoolExecutor

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DeviceType(Enum):
    LIGHT = "light"
    THERMOSTAT = "thermostat"
    LOCK = "lock"
    CAMERA = "camera"
    SENSOR = "sensor"
    SPEAKER = "speaker"
    SWITCH = "switch"
    FAN = "fan"
    BLINDS = "blinds"
    GARAGE = "garage"

class DeviceState(Enum):
    ON = "on"
    OFF = "off"
    UNKNOWN = "unknown"
    ERROR = "error"

class CommandIntent(Enum):
    TURN_ON = "turn_on"
    TURN_OFF = "turn_off"
    ADJUST = "adjust"
    SET_SCENE = "set_scene"
    GET_STATUS = "get_status"
    SCHEDULE = "schedule"
    UNKNOWN = "unknown"

class Protocol(Enum):
    WIFI = "wifi"
    ZIGBEE = "zigbee"
    ZWAVE = "zwave"
    BLUETOOTH = "bluetooth"
    MQTT = "mqtt"

@dataclass
class Device:
    device_id: str
    name: str
    device_type: DeviceType
    state: DeviceState = DeviceState.UNKNOWN
    protocol: Protocol = Protocol.WIFI
    ip_address: Optional[str] = None
    room: str = "Unknown"
    manufacturer: str = "Generic"
    model: str = "Unknown"
    capabilities: List[str] = field(default_factory=list)
    properties: Dict[str, Any] = field(default_factory=dict)
    last_seen: datetime = field(default_factory=datetime.now)
    is_online: bool = False

@dataclass
class VoiceCommand:
    command_id: str
    raw_text: str
    intent: CommandIntent
    entities: Dict[str, Any] = field(default_factory=dict)
    confidence: float = 0.0
    timestamp: datetime = field(default_factory=datetime.now)
    processed: bool = False

@dataclass
class AutomationScene:
    scene_id: str
    name: str
    description: str
    actions: List[Dict[str, Any]] = field(default_factory=list)
    conditions: List[Dict[str, Any]] = field(default_factory=list)
    is_active: bool = True
    created_at: datetime = field(default_factory=datetime.now)

@dataclass
class AutomationRule:
    rule_id: str
    name: str
    trigger: Dict[str, Any]
    conditions: List[Dict[str, Any]] = field(default_factory=list)
    actions: List[Dict[str, Any]] = field(default_factory=list)
    is_enabled: bool = True
    last_triggered: Optional[datetime] = None

class SpeechRecognitionEngine:
    """Advanced speech recognition with multiple engines."""
    
    def __init__(self):
        self.recognizer = sr.Recognizer()
        self.microphone = sr.Microphone()
        self.is_listening = False
        self.background_listener = None
        
        # Initialize pygame for audio playback
        pygame.mixer.init()
        
        # Calibrate microphone
        self._calibrate_microphone()
    
    def _calibrate_microphone(self):
        """Calibrate microphone for ambient noise."""
        try:
            with self.microphone as source:
                logger.info("Calibrating microphone for ambient noise...")
                self.recognizer.adjust_for_ambient_noise(source, duration=2)
                logger.info("Microphone calibration completed")
        except Exception as e:
            logger.error(f"Microphone calibration failed: {e}")
    
    def start_continuous_listening(self, callback: Callable[[str], None]):
        """Start continuous background listening."""
        if self.is_listening:
            return
        
        def listen_callback(recognizer, audio):
            try:
                # Try to recognize speech
                text = recognizer.recognize_google(audio, language='en-US')
                logger.info(f"Recognized speech: {text}")
                callback(text)
            except sr.UnknownValueError:
                logger.debug("Could not understand audio")
            except sr.RequestError as e:
                logger.error(f"Speech recognition error: {e}")
        
        try:
            self.background_listener = self.recognizer.listen_in_background(
                self.microphone, listen_callback, phrase_time_limit=5
            )
            self.is_listening = True
            logger.info("Continuous listening started")
        except Exception as e:
            logger.error(f"Failed to start continuous listening: {e}")
    
    def stop_listening(self):
        """Stop continuous listening."""
        if self.background_listener:
            self.background_listener(wait_for_stop=False)
            self.is_listening = False
            logger.info("Continuous listening stopped")
    
    def recognize_once(self, timeout: int = 5) -> Optional[str]:
        """Recognize speech from a single recording."""
        try:
            with self.microphone as source:
                logger.info("Listening for speech...")
                audio = self.recognizer.listen(source, timeout=timeout, phrase_time_limit=5)
            
            # Recognize speech
            text = self.recognizer.recognize_google(audio, language='en-US')
            logger.info(f"Recognized: {text}")
            return text
        
        except sr.WaitTimeoutError:
            logger.warning("Listening timed out")
            return None
        except sr.UnknownValueError:
            logger.warning("Could not understand the audio")
            return None
        except sr.RequestError as e:
            logger.error(f"Speech recognition service error: {e}")
            return None
        except Exception as e:
            logger.error(f"Speech recognition error: {e}")
            return None

class TextToSpeechEngine:
    """Text-to-speech for voice responses."""
    
    def __init__(self):
        self.language = 'en'
        self.temp_audio_files = []
    
    def speak(self, text: str) -> bool:
        """Convert text to speech and play it."""
        try:
            # Generate speech
            tts = gTTS(text=text, lang=self.language, slow=False)
            
            # Save to temporary file
            audio_file = f"temp_audio_{uuid.uuid4().hex[:8]}.mp3"
            tts.save(audio_file)
            self.temp_audio_files.append(audio_file)
            
            # Play audio
            pygame.mixer.music.load(audio_file)
            pygame.mixer.music.play()
            
            # Wait for playback to complete
            while pygame.mixer.music.get_busy():
                time.sleep(0.1)
            
            return True
        
        except Exception as e:
            logger.error(f"Text-to-speech error: {e}")
            return False
    
    def cleanup_temp_files(self):
        """Clean up temporary audio files."""
        import os
        for file_path in self.temp_audio_files:
            try:
                if os.path.exists(file_path):
                    os.remove(file_path)
            except Exception as e:
                logger.error(f"Error removing temp file {file_path}: {e}")
        self.temp_audio_files.clear()

class CommandProcessor:
    """Process and understand voice commands."""
    
    def __init__(self, openai_api_key: Optional[str] = None):
        self.llm = None
        if openai_api_key:
            self.llm = ChatOpenAI(
                temperature=0.1,
                model_name="gpt-4",
                openai_api_key=openai_api_key
            )
        
        self._initialize_prompts()
        self._initialize_patterns()
    
    def _initialize_prompts(self):
        """Initialize NLU prompts."""
        self.command_analysis_prompt = ChatPromptTemplate.from_template("""
        Analyze this voice command for home automation and extract the intent and entities.
        
        Command: "{command}"
        
        Available device types: light, thermostat, lock, camera, sensor, speaker, switch, fan, blinds, garage
        Available intents: turn_on, turn_off, adjust, set_scene, get_status, schedule
        
        Extract:
        1. Intent (one of the available intents)
        2. Device name or type mentioned
        3. Room location (if mentioned)
        4. Value/setting (if adjusting something)
        5. Time/schedule (if scheduling)
        
        Respond in JSON format:
        {{
            "intent": "intent_name",
            "device": "device_name_or_type",
            "room": "room_name",
            "value": "adjustment_value",
            "schedule": "time_specification",
            "confidence": 0.95
        }}
        """)
    
    def _initialize_patterns(self):
        """Initialize regex patterns for command processing."""
        self.patterns = {
            'turn_on': [
                r'turn on|switch on|activate|start',
                r'lights? on|lamp on'
            ],
            'turn_off': [
                r'turn off|switch off|deactivate|stop|shut off',
                r'lights? off|lamp off'
            ],
            'adjust': [
                r'set .+ to \d+|adjust .+ to \d+|dim .+ to \d+',
                r'increase|decrease|raise|lower|brighten|dim'
            ],
            'devices': {
                'light': r'light|lamp|bulb|lighting',
                'thermostat': r'thermostat|temperature|heating|cooling|ac|air',
                'lock': r'lock|door lock|front door|back door',
                'fan': r'fan|ceiling fan',
                'blinds': r'blind|curtain|shade',
                'garage': r'garage|garage door'
            },
            'rooms': [
                r'living room|bedroom|kitchen|bathroom|office|garage|basement',
                r'upstairs|downstairs|main floor'
            ]
        }
    
    def process_command(self, command_text: str) -> VoiceCommand:
        """Process voice command and extract intent/entities."""
        command = VoiceCommand(
            command_id=str(uuid.uuid4()),
            raw_text=command_text,
            intent=CommandIntent.UNKNOWN
        )
        
        try:
            if self.llm:
                # Use LLM for advanced processing
                response = self.llm.invoke(
                    self.command_analysis_prompt.format(command=command_text)
                )
                
                # Parse JSON response
                try:
                    result = json.loads(response.content)
                    command.intent = CommandIntent(result.get('intent', 'unknown'))
                    command.entities = {
                        'device': result.get('device'),
                        'room': result.get('room'),
                        'value': result.get('value'),
                        'schedule': result.get('schedule')
                    }
                    command.confidence = result.get('confidence', 0.0)
                except (json.JSONDecodeError, ValueError):
                    logger.error("Failed to parse LLM response")
                    command = self._pattern_based_processing(command_text)
            else:
                # Fallback to pattern-based processing
                command = self._pattern_based_processing(command_text)
            
            command.processed = True
            return command
        
        except Exception as e:
            logger.error(f"Command processing error: {e}")
            return command
    
    def _pattern_based_processing(self, command_text: str) -> VoiceCommand:
        """Pattern-based command processing fallback."""
        command = VoiceCommand(
            command_id=str(uuid.uuid4()),
            raw_text=command_text,
            intent=CommandIntent.UNKNOWN
        )
        
        text_lower = command_text.lower()
        
        # Detect intent
        for intent_name, patterns in self.patterns.items():
            if intent_name in ['turn_on', 'turn_off', 'adjust']:
                for pattern in patterns:
                    if re.search(pattern, text_lower):
                        command.intent = CommandIntent(intent_name)
                        break
        
        # Detect device type
        for device_type, pattern in self.patterns['devices'].items():
            if re.search(pattern, text_lower):
                command.entities['device'] = device_type
                break
        
        # Detect room
        for room_pattern in self.patterns['rooms']:
            room_match = re.search(room_pattern, text_lower)
            if room_match:
                command.entities['room'] = room_match.group(0)
                break
        
        # Extract numerical values
        numbers = re.findall(r'\d+', command_text)
        if numbers:
            command.entities['value'] = numbers[0]
        
        command.confidence = 0.7 if command.intent != CommandIntent.UNKNOWN else 0.3
        command.processed = True
        
        return command

class DeviceManager:
    """Manage IoT devices and their states."""
    
    def __init__(self):
        self.devices: Dict[str, Device] = {}
        self.device_apis = {}
        self.mqtt_client = None
        self.executor = ThreadPoolExecutor(max_workers=10)
        
        # Initialize mock devices for demo
        self._initialize_mock_devices()
    
    def _initialize_mock_devices(self):
        """Initialize mock smart home devices for demonstration."""
        mock_devices = [
            Device("light_001", "Living Room Lights", DeviceType.LIGHT, 
                   state=DeviceState.OFF, room="Living Room", 
                   capabilities=["brightness", "color"], 
                   properties={"brightness": 0, "color": "white"}),
            
            Device("light_002", "Bedroom Lights", DeviceType.LIGHT,
                   state=DeviceState.OFF, room="Bedroom",
                   capabilities=["brightness"],
                   properties={"brightness": 0}),
            
            Device("thermostat_001", "Main Thermostat", DeviceType.THERMOSTAT,
                   state=DeviceState.ON, room="Living Room",
                   capabilities=["temperature", "mode"],
                   properties={"temperature": 72, "mode": "auto"}),
            
            Device("lock_001", "Front Door Lock", DeviceType.LOCK,
                   state=DeviceState.OFF, room="Entrance",
                   capabilities=["lock", "unlock"],
                   properties={"locked": False}),
            
            Device("fan_001", "Ceiling Fan", DeviceType.FAN,
                   state=DeviceState.OFF, room="Bedroom",
                   capabilities=["speed"],
                   properties={"speed": 0})
        ]
        
        for device in mock_devices:
            self.devices[device.device_id] = device
            device.is_online = True
    
    def discover_devices(self) -> List[Device]:
        """Discover available IoT devices on the network."""
        try:
            # In a real implementation, this would use network discovery
            # For demo, we'll simulate discovery
            logger.info("Discovering IoT devices...")
            
            # Simulate device discovery delay
            time.sleep(2)
            
            discovered_devices = list(self.devices.values())
            logger.info(f"Discovered {len(discovered_devices)} devices")
            
            return discovered_devices
        
        except Exception as e:
            logger.error(f"Device discovery error: {e}")
            return []
    
    def get_device_by_name(self, name: str, room: str = None) -> Optional[Device]:
        """Find device by name and optionally room."""
        name_lower = name.lower()
        
        for device in self.devices.values():
            device_name_match = name_lower in device.name.lower()
            device_type_match = name_lower in device.device_type.value.lower()
            room_match = not room or room.lower() in device.room.lower()
            
            if (device_name_match or device_type_match) and room_match:
                return device
        
        return None
    
    def control_device(self, device_id: str, action: str, parameters: Dict[str, Any] = None) -> bool:
        """Control a specific device."""
        try:
            device = self.devices.get(device_id)
            if not device:
                logger.error(f"Device {device_id} not found")
                return False
            
            if not device.is_online:
                logger.error(f"Device {device.name} is offline")
                return False
            
            parameters = parameters or {}
            
            # Simulate device control
            if action == "turn_on":
                device.state = DeviceState.ON
                if device.device_type == DeviceType.LIGHT:
                    device.properties["brightness"] = parameters.get("brightness", 100)
                elif device.device_type == DeviceType.FAN:
                    device.properties["speed"] = parameters.get("speed", 3)
                
            elif action == "turn_off":
                device.state = DeviceState.OFF
                if device.device_type == DeviceType.LIGHT:
                    device.properties["brightness"] = 0
                elif device.device_type == DeviceType.FAN:
                    device.properties["speed"] = 0
                
            elif action == "adjust":
                if device.device_type == DeviceType.THERMOSTAT:
                    if "temperature" in parameters:
                        device.properties["temperature"] = parameters["temperature"]
                elif device.device_type == DeviceType.LIGHT:
                    if "brightness" in parameters:
                        device.properties["brightness"] = parameters["brightness"]
                        device.state = DeviceState.ON if parameters["brightness"] > 0 else DeviceState.OFF
            
            device.last_seen = datetime.now()
            logger.info(f"Controlled device {device.name}: {action} with {parameters}")
            return True
        
        except Exception as e:
            logger.error(f"Device control error: {e}")
            return False
    
    def get_device_status(self, device_id: str) -> Dict[str, Any]:
        """Get current status of a device."""
        device = self.devices.get(device_id)
        if not device:
            return {"error": "Device not found"}
        
        return {
            "name": device.name,
            "type": device.device_type.value,
            "state": device.state.value,
            "room": device.room,
            "properties": device.properties,
            "is_online": device.is_online,
            "last_seen": device.last_seen.isoformat()
        }

class SceneManager:
    """Manage automation scenes and rules."""
    
    def __init__(self, device_manager: DeviceManager):
        self.device_manager = device_manager
        self.scenes: Dict[str, AutomationScene] = {}
        self.rules: Dict[str, AutomationRule] = {}
        
        # Initialize default scenes
        self._initialize_default_scenes()
    
    def _initialize_default_scenes(self):
        """Initialize default automation scenes."""
        # Good Morning scene
        morning_scene = AutomationScene(
            scene_id="scene_morning",
            name="Good Morning",
            description="Turn on lights and adjust thermostat for morning",
            actions=[
                {"device_type": "light", "action": "turn_on", "room": "bedroom", "brightness": 70},
                {"device_type": "thermostat", "action": "adjust", "temperature": 72},
                {"device_type": "blinds", "action": "open"}
            ]
        )
        
        # Good Night scene
        night_scene = AutomationScene(
            scene_id="scene_night",
            name="Good Night",
            description="Turn off lights and lock doors",
            actions=[
                {"device_type": "light", "action": "turn_off", "room": "all"},
                {"device_type": "lock", "action": "lock"},
                {"device_type": "thermostat", "action": "adjust", "temperature": 68}
            ]
        )
        
        # Movie Time scene
        movie_scene = AutomationScene(
            scene_id="scene_movie",
            name="Movie Time",
            description="Dim lights and adjust environment for movies",
            actions=[
                {"device_type": "light", "action": "adjust", "room": "living room", "brightness": 20},
                {"device_type": "fan", "action": "turn_on", "speed": 1}
            ]
        )
        
        self.scenes[morning_scene.scene_id] = morning_scene
        self.scenes[night_scene.scene_id] = night_scene
        self.scenes[movie_scene.scene_id] = movie_scene
    
    def execute_scene(self, scene_name: str) -> bool:
        """Execute an automation scene."""
        try:
            # Find scene by name
            scene = None
            for s in self.scenes.values():
                if scene_name.lower() in s.name.lower():
                    scene = s
                    break
            
            if not scene:
                logger.error(f"Scene '{scene_name}' not found")
                return False
            
            logger.info(f"Executing scene: {scene.name}")
            
            # Execute each action in the scene
            for action in scene.actions:
                device_type = action.get("device_type")
                room = action.get("room")
                action_type = action.get("action")
                
                # Find matching devices
                matching_devices = []
                for device in self.device_manager.devices.values():
                    type_match = device.device_type.value == device_type
                    room_match = not room or room == "all" or room.lower() in device.room.lower()
                    
                    if type_match and room_match:
                        matching_devices.append(device)
                
                # Execute action on matching devices
                for device in matching_devices:
                    parameters = {k: v for k, v in action.items() 
                                if k not in ["device_type", "room", "action"]}
                    
                    self.device_manager.control_device(
                        device.device_id, 
                        action_type, 
                        parameters
                    )
            
            logger.info(f"Scene '{scene.name}' executed successfully")
            return True
        
        except Exception as e:
            logger.error(f"Scene execution error: {e}")
            return False
    
    def create_custom_scene(self, name: str, actions: List[Dict[str, Any]]) -> str:
        """Create a custom automation scene."""
        scene_id = f"scene_{uuid.uuid4().hex[:8]}"
        
        scene = AutomationScene(
            scene_id=scene_id,
            name=name,
            description=f"Custom scene: {name}",
            actions=actions
        )
        
        self.scenes[scene_id] = scene
        logger.info(f"Created custom scene: {name}")
        
        return scene_id

class VoiceHomeAutomationAgent:
    """Main voice-controlled home automation agent."""
    
    def __init__(self, openai_api_key: Optional[str] = None):
        self.speech_engine = SpeechRecognitionEngine()
        self.tts_engine = TextToSpeechEngine()
        self.command_processor = CommandProcessor(openai_api_key)
        self.device_manager = DeviceManager()
        self.scene_manager = SceneManager(self.device_manager)
        
        self.is_active = False
        self.command_history: List[VoiceCommand] = []
        self.response_queue = []
        
        # Wake word detection (simplified)
        self.wake_words = ["hey home", "ok home", "home assistant"]
        self.last_wake_time = None
        self.wake_timeout = 10  # seconds
    
    def start_listening(self):
        """Start the voice automation system."""
        try:
            self.is_active = True
            logger.info("Voice home automation agent started")
            
            # Start continuous listening
            self.speech_engine.start_continuous_listening(self._handle_speech)
            
            # Announce readiness
            self.tts_engine.speak("Home automation system is ready. Say 'Hey Home' to activate.")
            
        except Exception as e:
            logger.error(f"Failed to start voice agent: {e}")
            self.is_active = False
    
    def stop_listening(self):
        """Stop the voice automation system."""
        self.is_active = False
        self.speech_engine.stop_listening()
        self.tts_engine.cleanup_temp_files()
        logger.info("Voice home automation agent stopped")
    
    def _handle_speech(self, speech_text: str):
        """Handle incoming speech from continuous listening."""
        try:
            speech_lower = speech_text.lower()
            
            # Check for wake word
            if any(wake_word in speech_lower for wake_word in self.wake_words):
                self.last_wake_time = datetime.now()
                self.tts_engine.speak("Yes, how can I help?")
                logger.info("Wake word detected, listening for command")
                return
            
            # Process command if within wake timeout
            if self.last_wake_time and \
               (datetime.now() - self.last_wake_time).seconds <= self.wake_timeout:
                
                # Process the command
                self._process_voice_command(speech_text)
                self.last_wake_time = None  # Reset wake state
        
        except Exception as e:
            logger.error(f"Speech handling error: {e}")
    
    def _process_voice_command(self, command_text: str):
        """Process a voice command and execute actions."""
        try:
            # Process command
            command = self.command_processor.process_command(command_text)
            self.command_history.append(command)
            
            logger.info(f"Processing command: {command.raw_text}")
            logger.info(f"Intent: {command.intent.value}, Entities: {command.entities}")
            
            # Execute command based on intent
            if command.intent == CommandIntent.TURN_ON:
                success = self._handle_turn_on_command(command)
            elif command.intent == CommandIntent.TURN_OFF:
                success = self._handle_turn_off_command(command)
            elif command.intent == CommandIntent.ADJUST:
                success = self._handle_adjust_command(command)
            elif command.intent == CommandIntent.SET_SCENE:
                success = self._handle_scene_command(command)
            elif command.intent == CommandIntent.GET_STATUS:
                success = self._handle_status_command(command)
            else:
                success = False
                self.tts_engine.speak("I'm sorry, I didn't understand that command.")
            
            if success:
                # Generate appropriate response
                response = self._generate_response(command)
                self.tts_engine.speak(response)
            
        except Exception as e:
            logger.error(f"Command processing error: {e}")
            self.tts_engine.speak("Sorry, there was an error processing your command.")
    
    def _handle_turn_on_command(self, command: VoiceCommand) -> bool:
        """Handle turn on commands."""
        device_name = command.entities.get('device')
        room = command.entities.get('room')
        
        if not device_name:
            self.tts_engine.speak("Which device would you like me to turn on?")
            return False
        
        device = self.device_manager.get_device_by_name(device_name, room)
        if not device:
            self.tts_engine.speak(f"I couldn't find a {device_name} device.")
            return False
        
        # Extract parameters
        parameters = {}
        if command.entities.get('value'):
            try:
                value = int(command.entities['value'])
                if device.device_type == DeviceType.LIGHT:
                    parameters['brightness'] = min(100, max(0, value))
                elif device.device_type == DeviceType.FAN:
                    parameters['speed'] = min(5, max(1, value))
            except ValueError:
                pass
        
        return self.device_manager.control_device(device.device_id, "turn_on", parameters)
    
    def _handle_turn_off_command(self, command: VoiceCommand) -> bool:
        """Handle turn off commands."""
        device_name = command.entities.get('device')
        room = command.entities.get('room')
        
        if not device_name:
            self.tts_engine.speak("Which device would you like me to turn off?")
            return False
        
        device = self.device_manager.get_device_by_name(device_name, room)
        if not device:
            self.tts_engine.speak(f"I couldn't find a {device_name} device.")
            return False
        
        return self.device_manager.control_device(device.device_id, "turn_off")
    
    def _handle_adjust_command(self, command: VoiceCommand) -> bool:
        """Handle adjustment commands."""
        device_name = command.entities.get('device')
        room = command.entities.get('room')
        value = command.entities.get('value')
        
        if not device_name:
            self.tts_engine.speak("Which device would you like me to adjust?")
            return False
        
        device = self.device_manager.get_device_by_name(device_name, room)
        if not device:
            self.tts_engine.speak(f"I couldn't find a {device_name} device.")
            return False
        
        # Parse adjustment value
        parameters = {}
        if value:
            try:
                numeric_value = int(value)
                
                if device.device_type == DeviceType.THERMOSTAT:
                    parameters['temperature'] = numeric_value
                elif device.device_type == DeviceType.LIGHT:
                    parameters['brightness'] = min(100, max(0, numeric_value))
                elif device.device_type == DeviceType.FAN:
                    parameters['speed'] = min(5, max(0, numeric_value))
                
            except ValueError:
                self.tts_engine.speak("I couldn't understand the value you specified.")
                return False
        
        return self.device_manager.control_device(device.device_id, "adjust", parameters)
    
    def _handle_scene_command(self, command: VoiceCommand) -> bool:
        """Handle scene activation commands."""
        scene_text = command.raw_text.lower()
        
        # Extract scene name from common patterns
        scene_patterns = {
            'good morning': 'Good Morning',
            'morning': 'Good Morning',
            'good night': 'Good Night',
            'night': 'Good Night',
            'bedtime': 'Good Night',
            'movie': 'Movie Time',
            'movie time': 'Movie Time'
        }
        
        scene_name = None
        for pattern, name in scene_patterns.items():
            if pattern in scene_text:
                scene_name = name
                break
        
        if not scene_name:
            self.tts_engine.speak("Which scene would you like me to activate?")
            return False
        
        return self.scene_manager.execute_scene(scene_name)
    
    def _handle_status_command(self, command: VoiceCommand) -> bool:
        """Handle status inquiry commands."""
        device_name = command.entities.get('device')
        room = command.entities.get('room')
        
        if device_name:
            device = self.device_manager.get_device_by_name(device_name, room)
            if device:
                status = self.device_manager.get_device_status(device.device_id)
                response = f"The {device.name} is currently {device.state.value}"
                
                if device.device_type == DeviceType.THERMOSTAT:
                    temp = device.properties.get('temperature', 'unknown')
                    response += f" and set to {temp} degrees"
                elif device.device_type == DeviceType.LIGHT and device.state == DeviceState.ON:
                    brightness = device.properties.get('brightness', 0)
                    response += f" at {brightness}% brightness"
                
                self.tts_engine.speak(response)
                return True
        
        # General status
        online_devices = sum(1 for d in self.device_manager.devices.values() if d.is_online)
        active_devices = sum(1 for d in self.device_manager.devices.values() if d.state == DeviceState.ON)
        
        response = f"You have {online_devices} devices online and {active_devices} devices currently active."
        self.tts_engine.speak(response)
        return True
    
    def _generate_response(self, command: VoiceCommand) -> str:
        """Generate appropriate response for executed command."""
        responses = {
            CommandIntent.TURN_ON: ["Done", "Turned on", "Activated", "Sure thing"],
            CommandIntent.TURN_OFF: ["Done", "Turned off", "Deactivated", "All set"],
            CommandIntent.ADJUST: ["Adjusted", "Changed", "Set", "Updated"],
            CommandIntent.SET_SCENE: ["Scene activated", "Done", "All set"]
        }
        
        import random
        response_list = responses.get(command.intent, ["Done"])
        return random.choice(response_list)
    
    def get_system_status(self) -> Dict[str, Any]:
        """Get comprehensive system status."""
        try:
            total_devices = len(self.device_manager.devices)
            online_devices = sum(1 for d in self.device_manager.devices.values() if d.is_online)
            active_devices = sum(1 for d in self.device_manager.devices.values() if d.state == DeviceState.ON)
            
            # Device breakdown by type
            device_types = {}
            for device in self.device_manager.devices.values():
                device_type = device.device_type.value
                device_types[device_type] = device_types.get(device_type, 0) + 1
            
            # Recent commands
            recent_commands = len([c for c in self.command_history 
                                 if (datetime.now() - c.timestamp).seconds < 3600])
            
            return {
                "is_active": self.is_active,
                "total_devices": total_devices,
                "online_devices": online_devices,
                "active_devices": active_devices,
                "device_types": device_types,
                "total_scenes": len(self.scene_manager.scenes),
                "recent_commands": recent_commands,
                "last_command": self.command_history[-1].raw_text if self.command_history else None
            }
        
        except Exception as e:
            logger.error(f"Status error: {e}")
            return {"error": str(e)}

def main():
    """Main Streamlit application."""
    st.set_page_config(
        page_title="Voice-Controlled Home Automation Agent",
        page_icon="ðŸ ",
        layout="wide"
    )
    
    st.title("ðŸ  Voice-Controlled Home Automation Agent")
    st.markdown("**Intelligent voice control for smart home devices with speech-to-text and IoT integration**")
    
    # Initialize session state
    if 'agent' not in st.session_state:
        st.session_state['agent'] = None
    if 'is_listening' not in st.session_state:
        st.session_state['is_listening'] = False
    if 'system_status' not in st.session_state:
        st.session_state['system_status'] = None
    
    # Sidebar configuration
    with st.sidebar:
        st.header("ðŸ”§ Configuration")
        
        openai_key = st.text_input("OpenAI API Key (Optional)", type="password")
        
        if st.button("Initialize Agent") or st.session_state['agent'] is None:
            with st.spinner("Initializing Voice Home Automation..."):
                st.session_state['agent'] = VoiceHomeAutomationAgent(openai_key)
                st.success("Agent initialized!")
        
        st.header("ðŸŽ¤ Voice Control")
        
        if st.session_state['agent']:
            if not st.session_state['is_listening']:
                if st.button("ðŸŽ™ï¸ Start Voice Control"):
                    try:
                        st.session_state['agent'].start_listening()
                        st.session_state['is_listening'] = True
                        st.success("Voice control activated!")
                        st.info("Say 'Hey Home' to activate commands")
                    except Exception as e:
                        st.error(f"Failed to start voice control: {e}")
            else:
                if st.button("ðŸ”‡ Stop Voice Control"):
                    st.session_state['agent'].stop_listening()
                    st.session_state['is_listening'] = False
                    st.info("Voice control stopped")
        
        st.header("ðŸ’¬ Quick Commands")
        
        if st.session_state['agent']:
            st.write("**Available Commands:**")
            st.write("- 'Turn on living room lights'")
            st.write("- 'Set thermostat to 72 degrees'")
            st.write("- 'Activate good morning scene'")
            st.write("- 'Turn off all lights'")
            st.write("- 'What's the status of bedroom lights?'")
            
            # Manual command input
            manual_command = st.text_input("Manual Command Input:")
            if st.button("Execute Command") and manual_command:
                with st.spinner("Processing command..."):
                    st.session_state['agent']._process_voice_command(manual_command)
                    st.success("Command processed!")
    
    if not st.session_state['agent']:
        st.info("ðŸ‘ˆ Please initialize the Voice Home Automation Agent")
        return
    
    agent = st.session_state['agent']
    
    # Main interface tabs
    tab1, tab2, tab3, tab4, tab5 = st.tabs(["ðŸ  Dashboard", "ðŸ“± Devices", "ðŸŽ¬ Scenes", "ðŸ“Š Analytics", "âš™ï¸ Settings"])
    
    with tab1:
        st.header("ðŸ  Home Automation Dashboard")
        
        # Get system status
        if st.button("ðŸ”„ Refresh Status"):
            st.session_state['system_status'] = agent.get_system_status()
        
        if 'system_status' in st.session_state and st.session_state['system_status']:
            status = st.session_state['system_status']
            
            if 'error' not in status:
                # Key metrics
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric("Total Devices", status.get('total_devices', 0))
                with col2:
                    st.metric("Online Devices", status.get('online_devices', 0))
                with col3:
                    st.metric("Active Devices", status.get('active_devices', 0))
                with col4:
                    listening_status = "ðŸŽ™ï¸ Active" if st.session_state['is_listening'] else "ðŸ”‡ Inactive"
                    st.metric("Voice Control", listening_status)
                
                # Device types distribution
                if status.get('device_types'):
                    st.subheader("ðŸ“Š Device Types")
                    
                    device_data = status['device_types']
                    
                    fig = px.pie(
                        values=list(device_data.values()),
                        names=[name.title() for name in device_data.keys()],
                        title="Device Types Distribution"
                    )
                    st.plotly_chart(fig, use_container_width=True)
                
                # Recent activity
                col1, col2 = st.columns(2)
                
                with col1:
                    st.metric("Available Scenes", status.get('total_scenes', 0))
                with col2:
                    st.metric("Commands (Last Hour)", status.get('recent_commands', 0))
                
                if status.get('last_command'):
                    st.info(f"**Last Command:** {status['last_command']}")
            else:
                st.error(f"Status error: {status['error']}")
        
        # Quick controls
        st.subheader("âš¡ Quick Controls")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("ðŸ’¡ All Lights On"):
                for device in agent.device_manager.devices.values():
                    if device.device_type == DeviceType.LIGHT:
                        agent.device_manager.control_device(device.device_id, "turn_on")
                st.success("All lights turned on!")
        
        with col2:
            if st.button("ðŸŒ™ All Lights Off"):
                for device in agent.device_manager.devices.values():
                    if device.device_type == DeviceType.LIGHT:
                        agent.device_manager.control_device(device.device_id, "turn_off")
                st.success("All lights turned off!")
        
        with col3:
            if st.button("ðŸŒ… Good Morning Scene"):
                agent.scene_manager.execute_scene("Good Morning")
                st.success("Good Morning scene activated!")
    
    with tab2:
        st.header("ðŸ“± Device Management")
        
        # Discover devices
        if st.button("ðŸ” Discover Devices"):
            with st.spinner("Discovering devices..."):
                discovered = agent.device_manager.discover_devices()
                st.success(f"Discovered {len(discovered)} devices")
        
        # Display devices
        if agent.device_manager.devices:
            devices_data = []
            
            for device in agent.device_manager.devices.values():
                devices_data.append({
                    'Name': device.name,
                    'Type': device.device_type.value.title(),
                    'Room': device.room,
                    'State': device.state.value.title(),
                    'Online': 'âœ…' if device.is_online else 'âŒ',
                    'Last Seen': device.last_seen.strftime('%H:%M:%S')
                })
            
            devices_df = pd.DataFrame(devices_data)
            st.dataframe(devices_df, use_container_width=True)
            
            # Device details
            st.subheader("ðŸ”§ Device Control")
            
            device_names = [d.name for d in agent.device_manager.devices.values()]
            selected_device_name = st.selectbox("Select Device", device_names)
            
            if selected_device_name:
                selected_device = None
                for device in agent.device_manager.devices.values():
                    if device.name == selected_device_name:
                        selected_device = device
                        break
                
                if selected_device:
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.write(f"**Type:** {selected_device.device_type.value.title()}")
                        st.write(f"**Room:** {selected_device.room}")
                        st.write(f"**State:** {selected_device.state.value.title()}")
                        st.write(f"**Online:** {'Yes' if selected_device.is_online else 'No'}")
                    
                    with col2:
                        if selected_device.properties:
                            st.write("**Properties:**")
                            for key, value in selected_device.properties.items():
                                st.write(f"- {key.title()}: {value}")
                    
                    # Control buttons
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        if st.button("Turn On"):
                            agent.device_manager.control_device(selected_device.device_id, "turn_on")
                            st.success(f"Turned on {selected_device.name}")
                    
                    with col2:
                        if st.button("Turn Off"):
                            agent.device_manager.control_device(selected_device.device_id, "turn_off")
                            st.success(f"Turned off {selected_device.name}")
                    
                    with col3:
                        if st.button("Get Status"):
                            status = agent.device_manager.get_device_status(selected_device.device_id)
                            st.json(status)
        else:
            st.info("No devices found. Click 'Discover Devices' to scan for available devices.")
    
    with tab3:
        st.header("ðŸŽ¬ Scene Management")
        
        # Display available scenes
        st.subheader("ðŸ“‹ Available Scenes")
        
        for scene in agent.scene_manager.scenes.values():
            with st.expander(f"ðŸŽ¬ {scene.name}"):
                st.write(f"**Description:** {scene.description}")
                st.write(f"**Actions:** {len(scene.actions)}")
                
                if scene.actions:
                    st.write("**Scene Actions:**")
                    for i, action in enumerate(scene.actions, 1):
                        action_desc = f"{i}. {action.get('action', 'unknown').title()}"
                        if action.get('device_type'):
                            action_desc += f" {action['device_type']}"
                        if action.get('room'):
                            action_desc += f" in {action['room']}"
                        
                        st.write(action_desc)
                
                if st.button(f"â–¶ï¸ Execute {scene.name}", key=f"exec_{scene.scene_id}"):
                    success = agent.scene_manager.execute_scene(scene.name)
                    if success:
                        st.success(f"Scene '{scene.name}' executed successfully!")
                    else:
                        st.error(f"Failed to execute scene '{scene.name}'")
        
        # Create custom scene
        st.subheader("âž• Create Custom Scene")
        
        with st.form("create_scene"):
            scene_name = st.text_input("Scene Name")
            
            st.write("**Scene Actions:**")
            
            # Action configuration
            action_device_type = st.selectbox(
                "Device Type",
                [dtype.value for dtype in DeviceType]
            )
            
            action_type = st.selectbox(
                "Action",
                ["turn_on", "turn_off", "adjust"]
            )
            
            action_room = st.text_input("Room (optional)")
            
            # Additional parameters based on action type
            additional_params = {}
            if action_type == "adjust":
                if action_device_type == "light":
                    brightness = st.slider("Brightness", 0, 100, 50)
                    additional_params['brightness'] = brightness
                elif action_device_type == "thermostat":
                    temperature = st.number_input("Temperature", 60, 85, 72)
                    additional_params['temperature'] = temperature
                elif action_device_type == "fan":
                    speed = st.slider("Speed", 0, 5, 3)
                    additional_params['speed'] = speed
            
            if st.form_submit_button("Create Scene"):
                if scene_name:
                    # Create action
                    action = {
                        "device_type": action_device_type,
                        "action": action_type
                    }
                    
                    if action_room:
                        action["room"] = action_room
                    
                    action.update(additional_params)
                    
                    # Create scene
                    scene_id = agent.scene_manager.create_custom_scene(
                        scene_name, 
                        [action]
                    )
                    
                    st.success(f"Created scene '{scene_name}' with ID: {scene_id}")
                    st.rerun()
                else:
                    st.error("Please provide a scene name")
    
    with tab4:
        st.header("ðŸ“Š Analytics & Insights")
        
        # Command history
        if agent.command_history:
            st.subheader("ðŸ“ˆ Command History")
            
            # Recent commands
            recent_commands = agent.command_history[-10:]  # Last 10 commands
            
            command_data = []
            for cmd in recent_commands:
                command_data.append({
                    'Time': cmd.timestamp.strftime('%H:%M:%S'),
                    'Command': cmd.raw_text,
                    'Intent': cmd.intent.value.replace('_', ' ').title(),
                    'Confidence': f"{cmd.confidence:.2f}" if cmd.confidence else "N/A",
                    'Processed': 'âœ…' if cmd.processed else 'âŒ'
                })
            
            commands_df = pd.DataFrame(command_data)
            st.dataframe(commands_df, use_container_width=True)
            
            # Intent distribution
            if len(agent.command_history) > 5:
                st.subheader("ðŸŽ¯ Intent Distribution")
                
                intent_counts = {}
                for cmd in agent.command_history:
                    intent = cmd.intent.value
                    intent_counts[intent] = intent_counts.get(intent, 0) + 1
                
                fig = px.bar(
                    x=list(intent_counts.keys()),
                    y=list(intent_counts.values()),
                    title="Command Intent Distribution",
                    labels={'x': 'Intent', 'y': 'Count'}
                )
                st.plotly_chart(fig, use_container_width=True)
            
            # Usage patterns
            st.subheader("â° Usage Patterns")
            
            if len(agent.command_history) > 10:
                # Commands by hour
                hourly_usage = {}
                for cmd in agent.command_history:
                    hour = cmd.timestamp.hour
                    hourly_usage[hour] = hourly_usage.get(hour, 0) + 1
                
                fig = px.line(
                    x=list(hourly_usage.keys()),
                    y=list(hourly_usage.values()),
                    title="Commands by Hour of Day",
                    labels={'x': 'Hour', 'y': 'Number of Commands'}
                )
                st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("No command history available yet. Start using voice commands to see analytics.")
        
        # Device usage
        st.subheader("ðŸ“± Device Activity")
        
        device_activity = {}
        for device in agent.device_manager.devices.values():
            if device.state == DeviceState.ON:
                device_activity[device.name] = "Active"
            else:
                device_activity[device.name] = "Inactive"
        
        if device_activity:
            activity_df = pd.DataFrame(
                list(device_activity.items()),
                columns=['Device', 'Status']
            )
            
            fig = px.pie(
                activity_df,
                names='Device',
                title="Current Device Activity"
            )
            st.plotly_chart(fig, use_container_width=True)
    
    with tab5:
        st.header("âš™ï¸ Settings & Configuration")
        
        st.subheader("ðŸŽ¤ Voice Settings")
        
        # Voice recognition settings
        col1, col2 = st.columns(2)
        
        with col1:
            wake_timeout = st.slider("Wake Word Timeout (seconds)", 5, 30, 10)
            if st.session_state['agent']:
                st.session_state['agent'].wake_timeout = wake_timeout
        
        with col2:
            language = st.selectbox("Recognition Language", ["en-US", "en-GB", "es-ES", "fr-FR"])
        
        # Wake words
        st.subheader("ðŸ—£ï¸ Wake Words")
        current_wake_words = ", ".join(agent.wake_words) if agent else ""
        new_wake_words = st.text_input("Wake Words (comma-separated)", value=current_wake_words)
        
        if st.button("Update Wake Words") and agent:
            agent.wake_words = [word.strip() for word in new_wake_words.split(',')]
            st.success("Wake words updated!")
        
        # Device settings
        st.subheader("ðŸ“± Device Settings")
        
        auto_discovery = st.checkbox("Auto-discover new devices", value=True)
        device_timeout = st.slider("Device timeout (minutes)", 1, 60, 5)
        
        # Scene settings
        st.subheader("ðŸŽ¬ Scene Settings")
        
        scene_confirmation = st.checkbox("Confirm before executing scenes", value=False)
        voice_feedback = st.checkbox("Voice feedback for all actions", value=True)
        
        # Advanced settings
        st.subheader("ðŸ”§ Advanced Settings")
        
        log_level = st.selectbox("Log Level", ["INFO", "DEBUG", "WARNING", "ERROR"])
        max_history = st.slider("Max command history", 10, 1000, 100)
        
        if st.button("Apply Settings"):
            st.success("Settings applied successfully!")
            st.info("Some settings may require restarting the agent to take effect.")
        
        # System information
        st.subheader("â„¹ï¸ System Information")
        
        if agent:
            st.write(f"**Agent Status:** {'Active' if agent.is_active else 'Inactive'}")
            st.write(f"**Voice Control:** {'Listening' if st.session_state['is_listening'] else 'Stopped'}")
            st.write(f"**Devices Managed:** {len(agent.device_manager.devices)}")
            st.write(f"**Scenes Available:** {len(agent.scene_manager.scenes)}")
            st.write(f"**Commands Processed:** {len(agent.command_history)}")

if __name__ == "__main__":
    main()
````

## Project Summary

The Voice-Controlled Home Automation Agent delivers seamless smart home control through advanced speech recognition, natural language understanding, and comprehensive IoT device integration, creating an intuitive voice interface that transforms how users interact with their connected home environments.

### Key Value Propositions:
- **Natural Voice Interaction**: Multi-language speech recognition with context-aware command processing
- **Universal Device Control**: Support for multiple IoT protocols and smart home platforms with unified management
- **Intelligent Scene Management**: Complex automation scenarios with learning capabilities and user preference adaptation
- **Real-time Responsiveness**: Low-latency voice-to-action execution with immediate feedback and confirmation

### Technical Architecture:
The system integrates speech recognition engines for audio processing, LangChain for natural language understanding, multiple IoT protocol handlers for device communication, and real-time orchestration engines, creating a scalable voice automation platform that bridges the gap between natural human communication and technical device control through intelligent command interpretation and reliable execution.