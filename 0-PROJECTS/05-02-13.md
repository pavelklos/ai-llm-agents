<small>Claude Sonnet 4 **(News Aggregator & Summarizer Agent)**</small>
# News Aggregator & Summarizer Agent

## Key Concepts Explanation

### Web Browsing and Content Extraction
Advanced web scraping system that intelligently navigates news websites, extracts relevant content from HTML structures, handles dynamic content loading, and respects robots.txt protocols while gathering high-quality news articles from multiple sources.

### Automated Summarization
Multi-level summarization framework using extractive and abstractive techniques to create concise, coherent summaries that preserve key information, context, and factual accuracy while reducing content length by 70-90%.

### Retrieval-Augmented Generation (RAG)
Knowledge retrieval system that stores processed articles in vector databases, enables semantic search across historical news data, and provides context-aware information retrieval for trend analysis and related article discovery.

### Content Deduplication and Clustering
Intelligent system that identifies duplicate stories across sources, clusters related articles by topic and sentiment, and prevents information redundancy while preserving diverse perspectives on the same events.

### Real-time News Monitoring
Continuous monitoring system that tracks RSS feeds, news APIs, and website updates to provide real-time news aggregation with customizable update frequencies and source prioritization.

## Comprehensive Project Explanation

### Objectives
The News Aggregator & Summarizer Agent creates an intelligent news processing system that automatically collects, processes, summarizes, and organizes news content from multiple sources to provide users with comprehensive, up-to-date information in digestible formats.

### Key Features
- **Multi-Source Aggregation**: Collects news from RSS feeds, APIs, and web scraping
- **Intelligent Summarization**: Creates multi-level summaries with key point extraction
- **Topic Clustering**: Groups related stories and identifies trending topics
- **Sentiment Analysis**: Analyzes news sentiment and bias detection
- **Personalization**: Customizable news feeds based on user interests

### Challenges
- **Content Quality**: Ensuring accuracy and filtering out low-quality sources
- **Real-time Processing**: Handling high-volume news feeds efficiently
- **Bias Detection**: Identifying and balancing news source bias
- **Duplicate Detection**: Managing same stories from multiple sources

### Potential Impact
This system can democratize access to comprehensive news analysis, reduce information overload, provide unbiased news summaries, enable trend identification, and create personalized news experiences for millions of users globally.

## Comprehensive Project Example with Python Implementation

### Dependencies and Setup

````python
# requirements.txt
streamlit==1.29.0
langchain==0.1.0
langchain-openai==0.0.5
langchain-community==0.0.10
chromadb==0.4.15
sentence-transformers==2.2.2
feedparser==6.0.10
requests==2.31.0
beautifulsoup4==4.12.2
newspaper3k==0.2.8
pandas==2.1.4
numpy==1.24.3
plotly==5.17.0
textblob==0.17.1
scikit-learn==1.3.2
transformers==4.35.0
sumy==0.11.0
pydantic==2.5.0
datetime
logging
typing
dataclasses
enum
json
re
uuid
hashlib
threading
time
````

### Core Implementation

````python
import pandas as pd
import numpy as np
import json
import re
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from enum import Enum
import uuid
import hashlib
import threading
import time
from urllib.parse import urlparse, urljoin
import requests
from bs4 import BeautifulSoup
import feedparser

import streamlit as st
import plotly.express as px
import plotly.graph_objects as go

# NLP and ML libraries
from textblob import TextBlob
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
import nltk
from sumy.parsers.html import HtmlParser
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer
from sumy.summarizers.luhn import LuhnSummarizer

# LangChain components
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import Document

# Vector database
import chromadb
from sentence_transformers import SentenceTransformer

# Newspaper library for article extraction
try:
    from newspaper import Article, Config
    NEWSPAPER_AVAILABLE = True
except ImportError:
    NEWSPAPER_AVAILABLE = False
    logging.warning("newspaper3k not available, using basic scraping")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class NewsSource(Enum):
    RSS_FEED = "rss_feed"
    WEB_SCRAPING = "web_scraping"
    NEWS_API = "news_api"

class ArticleCategory(Enum):
    POLITICS = "politics"
    TECHNOLOGY = "technology"
    BUSINESS = "business"
    SCIENCE = "science"
    HEALTH = "health"
    SPORTS = "sports"
    ENTERTAINMENT = "entertainment"
    WORLD = "world"
    OTHER = "other"

class SentimentType(Enum):
    POSITIVE = "positive"
    NEGATIVE = "negative"
    NEUTRAL = "neutral"

@dataclass
class NewsArticle:
    article_id: str
    title: str
    content: str
    summary: str = ""
    url: str = ""
    source: str = ""
    published_date: datetime = field(default_factory=datetime.now)
    category: ArticleCategory = ArticleCategory.OTHER
    sentiment: SentimentType = SentimentType.NEUTRAL
    sentiment_score: float = 0.0
    keywords: List[str] = field(default_factory=list)
    image_url: Optional[str] = None
    author: Optional[str] = None

@dataclass
class NewsCluster:
    cluster_id: str
    topic: str
    articles: List[NewsArticle]
    summary: str
    created_at: datetime = field(default_factory=datetime.now)

@dataclass
class NewsSource:
    source_id: str
    name: str
    url: str
    source_type: NewsSource
    rss_url: Optional[str] = None
    last_updated: datetime = field(default_factory=datetime.now)
    is_active: bool = True
    reliability_score: float = 0.8

class WebScraper:
    """Advanced web scraping for news content."""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # Download NLTK data if needed
        try:
            nltk.data.find('tokenizers/punkt')
        except LookupError:
            nltk.download('punkt')
    
    def extract_article_content(self, url: str) -> Optional[NewsArticle]:
        """Extract article content from URL."""
        try:
            if NEWSPAPER_AVAILABLE:
                return self._extract_with_newspaper(url)
            else:
                return self._extract_with_beautifulsoup(url)
        
        except Exception as e:
            logger.error(f"Error extracting article from {url}: {e}")
            return None
    
    def _extract_with_newspaper(self, url: str) -> Optional[NewsArticle]:
        """Extract using newspaper3k library."""
        try:
            config = Config()
            config.browser_user_agent = self.session.headers['User-Agent']
            
            article = Article(url, config=config)
            article.download()
            article.parse()
            
            # Generate article ID
            article_id = hashlib.md5(url.encode()).hexdigest()
            
            return NewsArticle(
                article_id=article_id,
                title=article.title or "No Title",
                content=article.text or "",
                url=url,
                published_date=article.publish_date or datetime.now(),
                image_url=article.top_image,
                author=", ".join(article.authors) if article.authors else None
            )
        
        except Exception as e:
            logger.error(f"Newspaper extraction failed for {url}: {e}")
            return self._extract_with_beautifulsoup(url)
    
    def _extract_with_beautifulsoup(self, url: str) -> Optional[NewsArticle]:
        """Fallback extraction using BeautifulSoup."""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extract title
            title_selectors = ['h1', 'title', '.article-title', '.post-title']
            title = ""
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    title = title_elem.get_text().strip()
                    break
            
            # Extract content
            content_selectors = [
                '.article-content', '.post-content', '.entry-content',
                'article', '.story-body', 'main'
            ]
            content = ""
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # Remove scripts and style elements
                    for script in content_elem(["script", "style"]):
                        script.decompose()
                    content = content_elem.get_text().strip()
                    break
            
            # If no content found, try paragraphs
            if not content:
                paragraphs = soup.find_all('p')
                content = " ".join([p.get_text().strip() for p in paragraphs[:10]])
            
            # Extract image
            image_url = None
            img_selectors = ['meta[property="og:image"]', '.article-image img', 'img']
            for selector in img_selectors:
                img_elem = soup.select_one(selector)
                if img_elem:
                    image_url = img_elem.get('content') or img_elem.get('src')
                    if image_url:
                        image_url = urljoin(url, image_url)
                        break
            
            article_id = hashlib.md5(url.encode()).hexdigest()
            
            return NewsArticle(
                article_id=article_id,
                title=title or "No Title",
                content=content,
                url=url,
                image_url=image_url
            )
        
        except Exception as e:
            logger.error(f"BeautifulSoup extraction failed for {url}: {e}")
            return None

class RSSFeedProcessor:
    """Process RSS feeds for news aggregation."""
    
    def __init__(self):
        self.processed_articles = set()
    
    def process_rss_feed(self, rss_url: str, source_name: str) -> List[NewsArticle]:
        """Process RSS feed and extract articles."""
        try:
            feed = feedparser.parse(rss_url)
            articles = []
            
            for entry in feed.entries[:20]:  # Limit to 20 recent articles
                try:
                    # Generate article ID
                    article_id = hashlib.md5(entry.link.encode()).hexdigest()
                    
                    # Skip if already processed
                    if article_id in self.processed_articles:
                        continue
                    
                    # Parse published date
                    published_date = datetime.now()
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        published_date = datetime(*entry.published_parsed[:6])
                    
                    # Extract content
                    content = ""
                    if hasattr(entry, 'summary'):
                        content = entry.summary
                    elif hasattr(entry, 'description'):
                        content = entry.description
                    
                    # Clean content
                    if content:
                        soup = BeautifulSoup(content, 'html.parser')
                        content = soup.get_text().strip()
                    
                    article = NewsArticle(
                        article_id=article_id,
                        title=entry.title,
                        content=content,
                        url=entry.link,
                        source=source_name,
                        published_date=published_date
                    )
                    
                    articles.append(article)
                    self.processed_articles.add(article_id)
                
                except Exception as e:
                    logger.error(f"Error processing RSS entry: {e}")
                    continue
            
            return articles
        
        except Exception as e:
            logger.error(f"Error processing RSS feed {rss_url}: {e}")
            return []

class NewsSummarizer:
    """Multi-level news summarization."""
    
    def __init__(self, openai_api_key: Optional[str] = None):
        self.llm = None
        if openai_api_key:
            self.llm = ChatOpenAI(
                temperature=0.3,
                model_name="gpt-4",
                openai_api_key=openai_api_key
            )
        
        # Initialize extractive summarizers
        self.lsa_summarizer = LsaSummarizer()
        self.luhn_summarizer = LuhnSummarizer()
        
        self._initialize_prompts()
    
    def _initialize_prompts(self):
        """Initialize summarization prompts."""
        self.summary_prompt = ChatPromptTemplate.from_template("""
        Summarize the following news article in a clear, concise manner.
        
        Article Title: {title}
        Article Content: {content}
        
        Requirements:
        1. Create a summary that is 2-3 sentences long
        2. Preserve the most important facts and key points
        3. Maintain objectivity and factual accuracy
        4. Use clear, professional language
        
        Summary:
        """)
        
        self.cluster_summary_prompt = ChatPromptTemplate.from_template("""
        Create a comprehensive summary for this group of related news articles about the same topic.
        
        Topic: {topic}
        
        Articles:
        {articles}
        
        Requirements:
        1. Synthesize information from all articles
        2. Highlight key developments and facts
        3. Note any conflicting information or different perspectives
        4. Keep summary to 3-4 sentences
        5. Maintain objectivity
        
        Cluster Summary:
        """)
    
    def summarize_article(self, article: NewsArticle) -> str:
        """Generate summary for single article."""
        try:
            if self.llm and len(article.content) > 100:
                # Use LLM for high-quality summary
                response = self.llm.invoke(
                    self.summary_prompt.format(
                        title=article.title,
                        content=article.content[:2000]  # Limit content length
                    )
                )
                return response.content.strip()
            else:
                # Use extractive summarization
                return self._extractive_summary(article.content)
        
        except Exception as e:
            logger.error(f"Error summarizing article: {e}")
            return self._extractive_summary(article.content)
    
    def _extractive_summary(self, content: str) -> str:
        """Generate extractive summary."""
        try:
            if len(content) < 100:
                return content
            
            # Use Sumy for extractive summarization
            parser = PlaintextParser.from_string(content, Tokenizer("english"))
            
            # Try LSA summarizer first
            sentences = self.lsa_summarizer(parser.document, 2)
            
            if sentences:
                summary = " ".join([str(sentence) for sentence in sentences])
                return summary
            else:
                # Fallback to first few sentences
                sentences = content.split('.')[:2]
                return '. '.join(sentences) + '.'
        
        except Exception as e:
            logger.error(f"Extractive summarization error: {e}")
            # Ultimate fallback
            return content[:200] + "..." if len(content) > 200 else content
    
    def summarize_cluster(self, cluster: NewsCluster) -> str:
        """Generate summary for article cluster."""
        try:
            if self.llm and len(cluster.articles) > 1:
                # Prepare articles text
                articles_text = "\n".join([
                    f"- {article.title}: {article.summary or article.content[:200]}"
                    for article in cluster.articles[:5]  # Limit to 5 articles
                ])
                
                response = self.llm.invoke(
                    self.cluster_summary_prompt.format(
                        topic=cluster.topic,
                        articles=articles_text
                    )
                )
                return response.content.strip()
            else:
                # Fallback to combining summaries
                summaries = [article.summary for article in cluster.articles if article.summary]
                if summaries:
                    return " ".join(summaries[:3])
                else:
                    return f"Multiple articles about {cluster.topic}"
        
        except Exception as e:
            logger.error(f"Error summarizing cluster: {e}")
            return f"Cluster of {len(cluster.articles)} articles about {cluster.topic}"

class SentimentAnalyzer:
    """Analyze sentiment and bias in news articles."""
    
    def __init__(self):
        pass
    
    def analyze_sentiment(self, text: str) -> Tuple[SentimentType, float]:
        """Analyze sentiment of text."""
        try:
            blob = TextBlob(text)
            polarity = blob.sentiment.polarity
            
            # Determine sentiment type
            if polarity > 0.1:
                sentiment_type = SentimentType.POSITIVE
            elif polarity < -0.1:
                sentiment_type = SentimentType.NEGATIVE
            else:
                sentiment_type = SentimentType.NEUTRAL
            
            return sentiment_type, polarity
        
        except Exception as e:
            logger.error(f"Sentiment analysis error: {e}")
            return SentimentType.NEUTRAL, 0.0
    
    def extract_keywords(self, text: str, max_keywords: int = 10) -> List[str]:
        """Extract keywords from text."""
        try:
            blob = TextBlob(text)
            
            # Get noun phrases
            noun_phrases = list(blob.noun_phrases)
            
            # Simple keyword extraction based on frequency
            words = [word.lower() for word in blob.words if len(word) > 3]
            word_freq = {}
            
            for word in words:
                word_freq[word] = word_freq.get(word, 0) + 1
            
            # Sort by frequency and get top keywords
            keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
            
            # Combine noun phrases and single words
            all_keywords = noun_phrases + [kw[0] for kw in keywords[:max_keywords]]
            
            return list(set(all_keywords))[:max_keywords]
        
        except Exception as e:
            logger.error(f"Keyword extraction error: {e}")
            return []

class NewsClusterer:
    """Cluster related news articles."""
    
    def __init__(self):
        self.vectorizer = TfidfVectorizer(
            max_features=1000,
            stop_words='english',
            ngram_range=(1, 2)
        )
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
    
    def cluster_articles(self, articles: List[NewsArticle], num_clusters: int = None) -> List[NewsCluster]:
        """Cluster articles by topic similarity."""
        try:
            if len(articles) < 2:
                return []
            
            # Prepare text for clustering
            texts = [f"{article.title} {article.content}" for article in articles]
            
            # Generate embeddings
            embeddings = self.embedding_model.encode(texts)
            
            # Determine number of clusters
            if num_clusters is None:
                num_clusters = min(max(2, len(articles) // 3), 10)
            
            # Perform clustering
            kmeans = KMeans(n_clusters=num_clusters, random_state=42)
            cluster_labels = kmeans.fit_predict(embeddings)
            
            # Group articles by cluster
            clusters = {}
            for i, label in enumerate(cluster_labels):
                if label not in clusters:
                    clusters[label] = []
                clusters[label].append(articles[i])
            
            # Create NewsCluster objects
            news_clusters = []
            for cluster_id, cluster_articles in clusters.items():
                if len(cluster_articles) > 1:  # Only include clusters with multiple articles
                    # Generate topic from most common keywords
                    all_keywords = []
                    for article in cluster_articles:
                        all_keywords.extend(article.keywords)
                    
                    topic = self._generate_cluster_topic(cluster_articles)
                    
                    cluster = NewsCluster(
                        cluster_id=str(uuid.uuid4()),
                        topic=topic,
                        articles=cluster_articles,
                        summary=""  # Will be filled by summarizer
                    )
                    
                    news_clusters.append(cluster)
            
            return news_clusters
        
        except Exception as e:
            logger.error(f"Clustering error: {e}")
            return []
    
    def _generate_cluster_topic(self, articles: List[NewsArticle]) -> str:
        """Generate topic name for cluster."""
        try:
            # Extract common keywords
            all_keywords = []
            for article in articles:
                all_keywords.extend(article.keywords)
            
            # Count keyword frequency
            keyword_freq = {}
            for keyword in all_keywords:
                keyword_freq[keyword] = keyword_freq.get(keyword, 0) + 1
            
            # Get most common keywords
            if keyword_freq:
                top_keywords = sorted(keyword_freq.items(), key=lambda x: x[1], reverse=True)
                return " ".join([kw[0] for kw in top_keywords[:2]]).title()
            else:
                # Fallback to first article title
                return articles[0].title[:50] + "..."
        
        except Exception as e:
            logger.error(f"Topic generation error: {e}")
            return "News Cluster"

class NewsDatabase:
    """Vector database for news storage and retrieval."""
    
    def __init__(self):
        # Initialize ChromaDB
        self.chroma_client = chromadb.Client()
        
        try:
            self.collection = self.chroma_client.create_collection(
                name="news_articles",
                metadata={"description": "News articles vector database"}
            )
        except:
            self.collection = self.chroma_client.get_collection("news_articles")
        
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        
        # In-memory storage for structured data
        self.articles = {}
        self.clusters = {}
    
    def store_article(self, article: NewsArticle) -> bool:
        """Store article in database."""
        try:
            # Generate embedding
            text_content = f"{article.title} {article.content}"
            embedding = self.embedding_model.encode(text_content).tolist()
            
            # Store in vector database
            self.collection.add(
                documents=[text_content],
                embeddings=[embedding],
                metadatas=[{
                    "article_id": article.article_id,
                    "title": article.title,
                    "source": article.source,
                    "category": article.category.value,
                    "sentiment": article.sentiment.value,
                    "published_date": article.published_date.isoformat(),
                    "url": article.url
                }],
                ids=[article.article_id]
            )
            
            # Store in memory
            self.articles[article.article_id] = article
            
            return True
        
        except Exception as e:
            logger.error(f"Error storing article: {e}")
            return False
    
    def search_articles(self, query: str, limit: int = 10) -> List[NewsArticle]:
        """Search articles by semantic similarity."""
        try:
            # Generate query embedding
            query_embedding = self.embedding_model.encode(query).tolist()
            
            # Search vector database
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=limit
            )
            
            articles = []
            if results['metadatas']:
                for metadata in results['metadatas'][0]:
                    article_id = metadata['article_id']
                    if article_id in self.articles:
                        articles.append(self.articles[article_id])
            
            return articles
        
        except Exception as e:
            logger.error(f"Search error: {e}")
            return []
    
    def get_articles_by_category(self, category: ArticleCategory) -> List[NewsArticle]:
        """Get articles by category."""
        return [article for article in self.articles.values() 
                if article.category == category]
    
    def get_recent_articles(self, hours: int = 24) -> List[NewsArticle]:
        """Get articles from last N hours."""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        return [article for article in self.articles.values() 
                if article.published_date >= cutoff_time]

class NewsAggregatorAgent:
    """Main news aggregator and summarizer agent."""
    
    def __init__(self, openai_api_key: Optional[str] = None):
        self.web_scraper = WebScraper()
        self.rss_processor = RSSFeedProcessor()
        self.summarizer = NewsSummarizer(openai_api_key)
        self.sentiment_analyzer = SentimentAnalyzer()
        self.clusterer = NewsClusterer()
        self.database = NewsDatabase()
        
        # News sources
        self.news_sources = self._initialize_news_sources()
        
        # Processing status
        self.is_processing = False
        self.last_update = None
    
    def _initialize_news_sources(self) -> List[NewsSource]:
        """Initialize default news sources."""
        return [
            NewsSource(
                source_id="bbc_rss",
                name="BBC News",
                url="https://feeds.bbci.co.uk/news/rss.xml",
                source_type=NewsSource.RSS_FEED,
                rss_url="https://feeds.bbci.co.uk/news/rss.xml"
            ),
            NewsSource(
                source_id="reuters_rss",
                name="Reuters",
                url="http://feeds.reuters.com/reuters/topNews",
                source_type=NewsSource.RSS_FEED,
                rss_url="http://feeds.reuters.com/reuters/topNews"
            ),
            NewsSource(
                source_id="techcrunch_rss",
                name="TechCrunch",
                url="https://techcrunch.com/feed/",
                source_type=NewsSource.RSS_FEED,
                rss_url="https://techcrunch.com/feed/"
            )
        ]
    
    def process_news_sources(self, max_articles_per_source: int = 10) -> Dict[str, Any]:
        """Process all configured news sources."""
        try:
            self.is_processing = True
            results = {
                "total_articles": 0,
                "new_articles": 0,
                "clusters_created": 0,
                "sources_processed": 0,
                "errors": []
            }
            
            all_new_articles = []
            
            # Process each news source
            for source in self.news_sources:
                if not source.is_active:
                    continue
                
                try:
                    logger.info(f"Processing source: {source.name}")
                    
                    if source.source_type == NewsSource.RSS_FEED and source.rss_url:
                        articles = self.rss_processor.process_rss_feed(
                            source.rss_url, source.name
                        )
                    else:
                        # Could add other source types here
                        continue
                    
                    # Limit articles per source
                    articles = articles[:max_articles_per_source]
                    
                    # Process each article
                    for article in articles:
                        # Enhance article with additional processing
                        enhanced_article = self._enhance_article(article)
                        
                        # Store in database
                        if self.database.store_article(enhanced_article):
                            all_new_articles.append(enhanced_article)
                            results["new_articles"] += 1
                    
                    results["total_articles"] += len(articles)
                    results["sources_processed"] += 1
                    source.last_updated = datetime.now()
                
                except Exception as e:
                    error_msg = f"Error processing source {source.name}: {e}"
                    logger.error(error_msg)
                    results["errors"].append(error_msg)
            
            # Cluster new articles
            if all_new_articles:
                clusters = self.clusterer.cluster_articles(all_new_articles)
                
                # Generate cluster summaries
                for cluster in clusters:
                    cluster.summary = self.summarizer.summarize_cluster(cluster)
                    self.database.clusters[cluster.cluster_id] = cluster
                
                results["clusters_created"] = len(clusters)
            
            self.last_update = datetime.now()
            self.is_processing = False
            
            return results
        
        except Exception as e:
            self.is_processing = False
            logger.error(f"Error in news processing: {e}")
            return {"error": str(e)}
    
    def _enhance_article(self, article: NewsArticle) -> NewsArticle:
        """Enhance article with additional processing."""
        try:
            # Generate summary if not present
            if not article.summary and article.content:
                article.summary = self.summarizer.summarize_article(article)
            
            # Analyze sentiment
            if article.content:
                sentiment_type, sentiment_score = self.sentiment_analyzer.analyze_sentiment(
                    article.content
                )
                article.sentiment = sentiment_type
                article.sentiment_score = sentiment_score
            
            # Extract keywords
            text_for_keywords = f"{article.title} {article.content}"
            article.keywords = self.sentiment_analyzer.extract_keywords(text_for_keywords)
            
            # Categorize article (simple keyword-based categorization)
            article.category = self._categorize_article(article)
            
            return article
        
        except Exception as e:
            logger.error(f"Error enhancing article: {e}")
            return article
    
    def _categorize_article(self, article: NewsArticle) -> ArticleCategory:
        """Categorize article based on content."""
        text = f"{article.title} {article.content}".lower()
        
        category_keywords = {
            ArticleCategory.TECHNOLOGY: ["technology", "tech", "ai", "artificial intelligence", "software", "computer", "digital"],
            ArticleCategory.POLITICS: ["politics", "election", "government", "president", "senator", "congress", "vote"],
            ArticleCategory.BUSINESS: ["business", "economy", "market", "stock", "financial", "company", "earnings"],
            ArticleCategory.SCIENCE: ["science", "research", "study", "scientist", "discovery", "experiment"],
            ArticleCategory.HEALTH: ["health", "medical", "hospital", "doctor", "disease", "medicine", "treatment"],
            ArticleCategory.SPORTS: ["sports", "game", "team", "player", "football", "basketball", "soccer"],
            ArticleCategory.ENTERTAINMENT: ["entertainment", "movie", "music", "celebrity", "film", "actor", "actress"],
            ArticleCategory.WORLD: ["world", "international", "global", "country", "nation", "foreign"]
        }
        
        # Score each category
        category_scores = {}
        for category, keywords in category_keywords.items():
            score = sum(1 for keyword in keywords if keyword in text)
            category_scores[category] = score
        
        # Return category with highest score
        if max(category_scores.values()) > 0:
            return max(category_scores.items(), key=lambda x: x[1])[0]
        else:
            return ArticleCategory.OTHER
    
    def search_news(self, query: str, limit: int = 10) -> List[NewsArticle]:
        """Search news articles."""
        return self.database.search_articles(query, limit)
    
    def get_trending_topics(self, hours: int = 24) -> List[NewsCluster]:
        """Get trending topics from recent articles."""
        recent_articles = self.database.get_recent_articles(hours)
        
        if len(recent_articles) > 5:
            clusters = self.clusterer.cluster_articles(recent_articles)
            # Sort by number of articles in cluster
            return sorted(clusters, key=lambda x: len(x.articles), reverse=True)
        else:
            return []
    
    def get_category_summary(self, category: ArticleCategory, hours: int = 24) -> Dict[str, Any]:
        """Get summary for specific category."""
        articles = self.database.get_articles_by_category(category)
        
        # Filter to recent articles
        cutoff_time = datetime.now() - timedelta(hours=hours)
        recent_articles = [a for a in articles if a.published_date >= cutoff_time]
        
        if not recent_articles:
            return {"category": category.value, "articles": [], "summary": "No recent articles"}
        
        # Calculate average sentiment
        sentiments = [a.sentiment_score for a in recent_articles if a.sentiment_score is not None]
        avg_sentiment = np.mean(sentiments) if sentiments else 0.0
        
        return {
            "category": category.value,
            "article_count": len(recent_articles),
            "articles": recent_articles[:10],  # Latest 10
            "average_sentiment": avg_sentiment,
            "summary": f"{len(recent_articles)} recent articles in {category.value}"
        }
    
    def get_analytics(self) -> Dict[str, Any]:
        """Get news analytics and statistics."""
        try:
            all_articles = list(self.database.articles.values())
            
            if not all_articles:
                return {"total_articles": 0, "message": "No articles available"}
            
            # Basic statistics
            total_articles = len(all_articles)
            recent_articles = self.database.get_recent_articles(24)
            
            # Category distribution
            category_counts = {}
            for article in all_articles:
                cat = article.category.value
                category_counts[cat] = category_counts.get(cat, 0) + 1
            
            # Sentiment distribution
            sentiment_counts = {}
            for article in all_articles:
                sent = article.sentiment.value
                sentiment_counts[sent] = sentiment_counts.get(sent, 0) + 1
            
            # Source distribution
            source_counts = {}
            for article in all_articles:
                source = article.source or "Unknown"
                source_counts[source] = source_counts.get(source, 0) + 1
            
            return {
                "total_articles": total_articles,
                "recent_articles_24h": len(recent_articles),
                "category_distribution": category_counts,
                "sentiment_distribution": sentiment_counts,
                "source_distribution": source_counts,
                "total_clusters": len(self.database.clusters),
                "last_update": self.last_update.isoformat() if self.last_update else None
            }
        
        except Exception as e:
            logger.error(f"Analytics error: {e}")
            return {"error": str(e)}

def main():
    """Main Streamlit application."""
    st.set_page_config(
        page_title="News Aggregator & Summarizer Agent",
        page_icon="üì∞",
        layout="wide"
    )
    
    st.title("üì∞ News Aggregator & Summarizer Agent")
    st.markdown("**Intelligent news aggregation with web browsing, summarization, and RAG**")
    
    # Initialize session state
    if 'agent' not in st.session_state:
        st.session_state['agent'] = None
    if 'processing_results' not in st.session_state:
        st.session_state['processing_results'] = None
    
    # Sidebar configuration
    with st.sidebar:
        st.header("üîß Configuration")
        
        openai_key = st.text_input("OpenAI API Key (Optional)", type="password")
        
        if st.button("Initialize Agent") or st.session_state['agent'] is None:
            with st.spinner("Initializing News Agent..."):
                st.session_state['agent'] = NewsAggregatorAgent(openai_key)
                st.success("News Agent ready!")
        
        st.header("‚öôÔ∏è Processing Settings")
        
        max_articles = st.slider("Max Articles per Source", 5, 50, 15)
        auto_refresh = st.checkbox("Auto-refresh (every 30 min)")
        
        if st.button("üîÑ Process News Sources"):
            if st.session_state['agent']:
                with st.spinner("Processing news sources..."):
                    results = st.session_state['agent'].process_news_sources(max_articles)
                    st.session_state['processing_results'] = results
                    st.success("News processing completed!")
            else:
                st.error("Please initialize the agent first")
        
        # Show processing status
        if st.session_state['processing_results']:
            results = st.session_state['processing_results']
            st.subheader("üìä Last Processing Results")
            
            if 'error' not in results:
                st.metric("New Articles", results.get('new_articles', 0))
                st.metric("Sources Processed", results.get('sources_processed', 0))
                st.metric("Clusters Created", results.get('clusters_created', 0))
                
                if results.get('errors'):
                    st.error(f"Errors: {len(results['errors'])}")
            else:
                st.error(f"Processing error: {results['error']}")
    
    if not st.session_state['agent']:
        st.info("üëà Please initialize the News Agent")
        return
    
    agent = st.session_state['agent']
    
    # Main interface tabs
    tab1, tab2, tab3, tab4, tab5 = st.tabs(["üì∞ News Feed", "üîç Search", "üìà Trending", "üìä Analytics", "‚öôÔ∏è Sources"])
    
    with tab1:
        st.header("üì∞ Latest News")
        
        # Category filter
        category_filter = st.selectbox(
            "Filter by Category",
            ["All"] + [cat.value.title() for cat in ArticleCategory]
        )
        
        # Time filter
        time_filter = st.selectbox(
            "Time Range",
            ["Last 24 hours", "Last 7 days", "All time"]
        )
        
        # Get articles based on filters
        if category_filter == "All":
            if time_filter == "Last 24 hours":
                articles = agent.database.get_recent_articles(24)
            elif time_filter == "Last 7 days":
                articles = agent.database.get_recent_articles(168)
            else:
                articles = list(agent.database.articles.values())
        else:
            category = ArticleCategory(category_filter.lower())
            articles = agent.database.get_articles_by_category(category)
            
            # Apply time filter
            if time_filter == "Last 24 hours":
                cutoff = datetime.now() - timedelta(hours=24)
                articles = [a for a in articles if a.published_date >= cutoff]
            elif time_filter == "Last 7 days":
                cutoff = datetime.now() - timedelta(days=7)
                articles = [a for a in articles if a.published_date >= cutoff]
        
        # Sort by publication date
        articles = sorted(articles, key=lambda x: x.published_date, reverse=True)
        
        # Display articles
        if articles:
            for article in articles[:20]:  # Show top 20
                with st.container():
                    col1, col2 = st.columns([3, 1])
                    
                    with col1:
                        st.subheader(article.title)
                        
                        # Article metadata
                        col_a, col_b, col_c = st.columns(3)
                        with col_a:
                            st.write(f"**Source:** {article.source}")
                        with col_b:
                            st.write(f"**Category:** {article.category.value.title()}")
                        with col_c:
                            sentiment_color = {
                                SentimentType.POSITIVE: "üü¢",
                                SentimentType.NEGATIVE: "üî¥",
                                SentimentType.NEUTRAL: "‚ö™"
                            }
                            st.write(f"**Sentiment:** {sentiment_color[article.sentiment]} {article.sentiment.value.title()}")
                        
                        # Summary
                        if article.summary:
                            st.write(article.summary)
                        else:
                            st.write(article.content[:200] + "..." if len(article.content) > 200 else article.content)
                        
                        # Keywords
                        if article.keywords:
                            st.write(f"**Keywords:** {', '.join(article.keywords[:5])}")
                        
                        # Link
                        if article.url:
                            st.markdown(f"[Read full article]({article.url})")
                    
                    with col2:
                        if article.image_url:
                            try:
                                st.image(article.image_url, width=200)
                            except:
                                pass
                        
                        st.write(f"üìÖ {article.published_date.strftime('%Y-%m-%d %H:%M')}")
                    
                    st.divider()
        else:
            st.info("No articles found matching the selected criteria")
    
    with tab2:
        st.header("üîç Search News")
        
        search_query = st.text_input("Enter search terms...")
        
        if search_query:
            with st.spinner("Searching..."):
                search_results = agent.search_news(search_query, limit=15)
                
                if search_results:
                    st.write(f"Found {len(search_results)} relevant articles:")
                    
                    for article in search_results:
                        with st.expander(f"{article.title} - {article.source}"):
                            st.write(f"**Published:** {article.published_date.strftime('%Y-%m-%d %H:%M')}")
                            st.write(f"**Category:** {article.category.value.title()}")
                            
                            if article.summary:
                                st.write(f"**Summary:** {article.summary}")
                            
                            if article.keywords:
                                st.write(f"**Keywords:** {', '.join(article.keywords[:5])}")
                            
                            if article.url:
                                st.markdown(f"[Read full article]({article.url})")
                else:
                    st.info("No articles found for your search query")
    
    with tab3:
        st.header("üìà Trending Topics")
        
        time_range = st.selectbox("Trending in last:", ["24 hours", "7 days"], key="trending_time")
        hours = 24 if time_range == "24 hours" else 168
        
        if st.button("Get Trending Topics"):
            with st.spinner("Analyzing trending topics..."):
                trending_clusters = agent.get_trending_topics(hours)
                st.session_state['trending_clusters'] = trending_clusters
        
        if 'trending_clusters' in st.session_state:
            clusters = st.session_state['trending_clusters']
            
            if clusters:
                st.write(f"Found {len(clusters)} trending topics:")
                
                for i, cluster in enumerate(clusters[:10], 1):
                    with st.expander(f"#{i} {cluster.topic} ({len(cluster.articles)} articles)"):
                        if cluster.summary:
                            st.write(f"**Summary:** {cluster.summary}")
                        
                        st.write("**Related Articles:**")
                        for article in cluster.articles[:5]:
                            st.write(f"- [{article.title}]({article.url}) ({article.source})")
            else:
                st.info("No trending topics found for the selected time range")
    
    with tab4:
        st.header("üìä News Analytics")
        
        if st.button("Generate Analytics"):
            analytics = agent.get_analytics()
            st.session_state['analytics'] = analytics
        
        if 'analytics' in st.session_state:
            analytics = st.session_state['analytics']
            
            if 'error' not in analytics:
                # Key metrics
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric("Total Articles", analytics.get('total_articles', 0))
                with col2:
                    st.metric("Last 24h", analytics.get('recent_articles_24h', 0))
                with col3:
                    st.metric("Total Clusters", analytics.get('total_clusters', 0))
                with col4:
                    if analytics.get('last_update'):
                        update_time = datetime.fromisoformat(analytics['last_update'])
                        st.metric("Last Update", update_time.strftime('%H:%M'))
                
                # Category distribution
                if analytics.get('category_distribution'):
                    st.subheader("üìä Articles by Category")
                    
                    cat_data = analytics['category_distribution']
                    
                    fig_cat = px.pie(
                        values=list(cat_data.values()),
                        names=[name.title() for name in cat_data.keys()],
                        title="Category Distribution"
                    )
                    st.plotly_chart(fig_cat, use_container_width=True)
                
                # Sentiment distribution
                if analytics.get('sentiment_distribution'):
                    st.subheader("üòä Sentiment Analysis")
                    
                    sent_data = analytics['sentiment_distribution']
                    
                    colors = {'positive': 'green', 'negative': 'red', 'neutral': 'gray'}
                    
                    fig_sent = px.bar(
                        x=list(sent_data.keys()),
                        y=list(sent_data.values()),
                        title="Sentiment Distribution",
                        color=list(sent_data.keys()),
                        color_discrete_map=colors
                    )
                    st.plotly_chart(fig_sent, use_container_width=True)
                
                # Source distribution
                if analytics.get('source_distribution'):
                    st.subheader("üì∞ Articles by Source")
                    
                    source_data = analytics['source_distribution']
                    
                    fig_source = px.bar(
                        x=list(source_data.values()),
                        y=list(source_data.keys()),
                        orientation='h',
                        title="Source Distribution"
                    )
                    st.plotly_chart(fig_source, use_container_width=True)
            else:
                st.error(f"Analytics error: {analytics['error']}")
    
    with tab5:
        st.header("‚öôÔ∏è News Sources Management")
        
        # Display current sources
        st.subheader("üì° Configured Sources")
        
        sources_data = []
        for source in agent.news_sources:
            sources_data.append({
                'Name': source.name,
                'Type': source.source_type.value,
                'URL': source.url,
                'Active': source.is_active,
                'Last Updated': source.last_updated.strftime('%Y-%m-%d %H:%M') if source.last_updated else 'Never'
            })
        
        sources_df = pd.DataFrame(sources_data)
        st.dataframe(sources_df, use_container_width=True)
        
        # Add new source
        st.subheader("‚ûï Add New Source")
        
        with st.form("add_source"):
            source_name = st.text_input("Source Name")
            source_url = st.text_input("RSS Feed URL")
            
            if st.form_submit_button("Add Source"):
                if source_name and source_url:
                    new_source = NewsSource(
                        source_id=f"custom_{len(agent.news_sources)}",
                        name=source_name,
                        url=source_url,
                        source_type=NewsSource.RSS_FEED,
                        rss_url=source_url
                    )
                    
                    agent.news_sources.append(new_source)
                    st.success(f"Added source: {source_name}")
                    st.rerun()
                else:
                    st.error("Please provide both name and URL")

if __name__ == "__main__":
    main()
````

## Project Summary

The News Aggregator & Summarizer Agent provides comprehensive automated news processing through intelligent web browsing, multi-level summarization, and RAG-powered knowledge management to deliver personalized, up-to-date news consumption experiences while filtering information overload and identifying trending topics.

### Key Value Propositions:
- **Multi-Source Aggregation**: Automated collection from RSS feeds, web scraping, and news APIs with intelligent deduplication
- **Advanced Summarization**: Multi-level summaries using both extractive and abstractive techniques for optimal comprehension
- **Intelligent Clustering**: Topic-based article grouping and trend identification for comprehensive news understanding
- **RAG-Powered Search**: Semantic search across historical news data for contextual information retrieval

### Technical Architecture:
The system integrates LangChain for summarization, ChromaDB for vector storage, newspaper3k for content extraction, advanced NLP models for sentiment analysis and clustering, creating a scalable news intelligence platform that transforms information overload into actionable insights through automated processing, intelligent organization, and personalized delivery mechanisms.