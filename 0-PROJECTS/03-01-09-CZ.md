<small>Claude Sonnet 4 **(Platforma pro Objevování Vědeckých Výzkumných Článků)**</small>
# Scientific Research Paper Discovery Platform

## Klíčové Koncepty

### RAG (Retrieval-Augmented Generation)
RAG je architektura kombinující vyhledávání relevantních dokumentů s generováním odpovědí pomocí LLM. Umožňuje AI systémům přistupovat k aktuálním informacím z externí databáze znalostí.

### ArXiv Papers
ArXiv je přední repozitář vědeckých preprint článků pokrývající fyziku, matematiku, informatiku, biologii a další obory. Obsahuje miliony vědeckých publikací.

### Research Abstracts
Abstrakty výzkumných prací poskytují stručné shrnutí cílů, metod a výsledků studie. Jsou klíčové pro rychlé posouzení relevance článku.

### Citation Networks
Citační sítě reprezentují vztahy mezi vědeckými publikacemi prostřednictvím citací. Umožňují analýzu vlivu a souvislostí mezi výzkumy.

### SciBERT
SciBERT je specializovaný BERT model trénovaný na vědeckých textech. Lépe rozumí vědecké terminologii a konceptům než standardní jazykové modely.

### Neo4j
Neo4j je grafová databáze optimalizovaná pro ukládání a dotazování složitých vztahů mezi entitami, ideální pro citační sítě.

### Graph RAG
Graph RAG využívá grafové struktury pro reprezentaci znalostí, což umožňuje sofistikovanější vyhledávání založené na vztazích mezi entitami.

### Collaborative Filtering
Techniky kolaborativního filtrování doporučují relevantní obsah na základě podobností mezi uživateli nebo položkami.

## Komplexní Vysvětlení Projektu

### Cíle Projektu
Platforma si klade za cíl revolucionizovat způsob, jakým vědci a výzkumníci objevují relevantní vědecké publikace. Kombinuje pokročilé technologie AI pro inteligentní vyhledávání, doporučování a analýzu vědeckých článků.

### Hlavní Výzvy
- **Obrovský objem dat**: ArXiv obsahuje miliony článků napříč obory
- **Sémantické porozumění**: Nutnost porozumět vědecké terminologii
- **Citační analýza**: Komplexní vztahy mezi publikacemi
- **Personalizace**: Přizpůsobení doporučení individuálním potřebám
- **Aktuálnost**: Integrace nových publikací v reálném čase

### Potenciální Dopad
- Urychlení vědeckého objevování
- Zlepšení kvality výzkumu prostřednictvím lepší orientace v literatuře
- Podpora interdisciplinární spolupráce
- Efektivnější využití vědeckých zdrojů

## Komplexní Příklad s Python Implementací

### Závislosti a Instalace

````python
# requirements.txt
langchain==0.1.0
chromadb==0.4.15
sentence-transformers==2.2.2
neo4j==5.15.0
openai==1.6.1
arxiv==1.4.8
scikit-learn==1.3.0
pandas==2.1.4
numpy==1.24.3
fastapi==0.104.1
uvicorn==0.24.0
pydantic==2.5.0
beautifulsoup4==4.12.2
requests==2.31.0
transformers==4.36.2
torch==2.1.0
networkx==3.2.1
````

### Hlavní Implementace

````python
import asyncio
import logging
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import json
import re

import arxiv
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
import chromadb
from neo4j import GraphDatabase
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import PCA
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.document_loaders import ArxivLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import networkx as nx

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ResearchPaper:
    """Reprezentace vědeckého článku"""
    arxiv_id: str
    title: str
    authors: List[str]
    abstract: str
    categories: List[str]
    published: datetime
    pdf_url: str
    citations: List[str] = None
    embedding: np.ndarray = None

class ArxivDataCollector:
    """Sběr dat z ArXiv API"""
    
    def __init__(self):
        self.client = arxiv.Client()
    
    async def search_papers(self, query: str, max_results: int = 1000) -> List[ResearchPaper]:
        """Vyhledání článků podle dotazu"""
        try:
            search = arxiv.Search(
                query=query,
                max_results=max_results,
                sort_by=arxiv.SortCriterion.Relevance,
                sort_order=arxiv.SortOrder.Descending
            )
            
            papers = []
            for result in self.client.results(search):
                paper = ResearchPaper(
                    arxiv_id=result.entry_id.split('/')[-1],
                    title=result.title,
                    authors=[str(author) for author in result.authors],
                    abstract=result.summary,
                    categories=[str(cat) for cat in result.categories],
                    published=result.published,
                    pdf_url=result.pdf_url
                )
                papers.append(paper)
            
            logger.info(f"Nalezeno {len(papers)} článků")
            return papers
            
        except Exception as e:
            logger.error(f"Chyba při vyhledávání: {e}")
            return []

class ScientificEmbeddingService:
    """Služba pro vytváření embeddings vědeckých textů"""
    
    def __init__(self, model_name: str = "allenai/scibert_scivocab_uncased"):
        self.model = SentenceTransformer('all-MiniLM-L6-v2')  # Fallback model
        self.dimension = 384
    
    def create_embeddings(self, texts: List[str]) -> np.ndarray:
        """Vytvoření embeddings pro texty"""
        try:
            embeddings = self.model.encode(texts, show_progress_bar=True)
            return embeddings
        except Exception as e:
            logger.error(f"Chyba při vytváření embeddings: {e}")
            return np.zeros((len(texts), self.dimension))
    
    def create_paper_embedding(self, paper: ResearchPaper) -> np.ndarray:
        """Vytvoření embedding pro článek"""
        combined_text = f"{paper.title} {paper.abstract}"
        return self.create_embeddings([combined_text])[0]

class CitationNetworkBuilder:
    """Stavitel citačních sítí"""
    
    def __init__(self, neo4j_uri: str, username: str, password: str):
        self.driver = GraphDatabase.driver(neo4j_uri, auth=(username, password))
    
    def create_paper_node(self, paper: ResearchPaper):
        """Vytvoření uzlu pro článek"""
        with self.driver.session() as session:
            session.run("""
                MERGE (p:Paper {arxiv_id: $arxiv_id})
                SET p.title = $title,
                    p.authors = $authors,
                    p.abstract = $abstract,
                    p.categories = $categories,
                    p.published = $published
            """, 
            arxiv_id=paper.arxiv_id,
            title=paper.title,
            authors=paper.authors,
            abstract=paper.abstract,
            categories=paper.categories,
            published=paper.published.isoformat()
            )
    
    def create_citation_relationship(self, citing_id: str, cited_id: str):
        """Vytvoření citačního vztahu"""
        with self.driver.session() as session:
            session.run("""
                MATCH (citing:Paper {arxiv_id: $citing_id})
                MATCH (cited:Paper {arxiv_id: $cited_id})
                MERGE (citing)-[:CITES]->(cited)
            """, citing_id=citing_id, cited_id=cited_id)
    
    def get_similar_papers(self, arxiv_id: str, limit: int = 10) -> List[Dict]:
        """Nalezení podobných článků pomocí citační sítě"""
        with self.driver.session() as session:
            result = session.run("""
                MATCH (p:Paper {arxiv_id: $arxiv_id})
                MATCH (p)-[:CITES]->(cited)<-[:CITES]-(similar)
                WHERE similar <> p
                WITH similar, count(cited) as common_citations
                ORDER BY common_citations DESC
                LIMIT $limit
                RETURN similar.arxiv_id as arxiv_id, 
                       similar.title as title,
                       common_citations
            """, arxiv_id=arxiv_id, limit=limit)
            
            return [dict(record) for record in result]
    
    def close(self):
        self.driver.close()

class GraphRAGRetriever:
    """Graph RAG retriever pro komplexní vyhledávání"""
    
    def __init__(self, vector_store, citation_network: CitationNetworkBuilder):
        self.vector_store = vector_store
        self.citation_network = citation_network
    
    def hybrid_search(self, query: str, k: int = 10) -> List[Dict]:
        """Hybridní vyhledávání kombinující sémantickou a citační podobnost"""
        # Sémantické vyhledávání
        semantic_results = self.vector_store.similarity_search(query, k=k*2)
        
        # Rozšíření o citačně podobné články
        enhanced_results = []
        for doc in semantic_results[:k]:
            arxiv_id = doc.metadata.get('arxiv_id')
            if arxiv_id:
                # Přidání původního článku
                enhanced_results.append({
                    'content': doc.page_content,
                    'metadata': doc.metadata,
                    'score': 1.0,
                    'type': 'semantic'
                })
                
                # Přidání citačně podobných
                similar_papers = self.citation_network.get_similar_papers(arxiv_id, 3)
                for paper in similar_papers:
                    enhanced_results.append({
                        'arxiv_id': paper['arxiv_id'],
                        'title': paper['title'],
                        'score': paper['common_citations'] / 10.0,
                        'type': 'citation'
                    })
        
        return enhanced_results[:k]

class CollaborativeRecommender:
    """Systém kolaborativního doporučování"""
    
    def __init__(self):
        self.user_interactions = {}
        self.paper_features = {}
    
    def record_interaction(self, user_id: str, paper_id: str, interaction_type: str, rating: float = 1.0):
        """Zaznamenání interakce uživatele s článkem"""
        if user_id not in self.user_interactions:
            self.user_interactions[user_id] = {}
        
        self.user_interactions[user_id][paper_id] = {
            'type': interaction_type,
            'rating': rating,
            'timestamp': datetime.now()
        }
    
    def get_user_similarity(self, user1: str, user2: str) -> float:
        """Výpočet podobnosti mezi uživateli"""
        papers1 = set(self.user_interactions.get(user1, {}).keys())
        papers2 = set(self.user_interactions.get(user2, {}).keys())
        
        if not papers1 or not papers2:
            return 0.0
        
        intersection = papers1.intersection(papers2)
        union = papers1.union(papers2)
        
        return len(intersection) / len(union) if union else 0.0
    
    def recommend_papers(self, user_id: str, all_papers: List[str], k: int = 10) -> List[str]:
        """Doporučení článků na základě kolaborativního filtrování"""
        if user_id not in self.user_interactions:
            return all_papers[:k]  # Pro nové uživatele
        
        user_papers = set(self.user_interactions[user_id].keys())
        candidates = [p for p in all_papers if p not in user_papers]
        
        # Nalezení podobných uživatelů
        similar_users = []
        for other_user in self.user_interactions:
            if other_user != user_id:
                similarity = self.get_user_similarity(user_id, other_user)
                if similarity > 0.1:
                    similar_users.append((other_user, similarity))
        
        # Skórování kandidátů
        paper_scores = {}
        for paper in candidates:
            score = 0.0
            for other_user, similarity in similar_users:
                if paper in self.user_interactions[other_user]:
                    rating = self.user_interactions[other_user][paper]['rating']
                    score += similarity * rating
            paper_scores[paper] = score
        
        # Seřazení a vrácení top-k
        recommended = sorted(paper_scores.items(), key=lambda x: x[1], reverse=True)
        return [paper for paper, score in recommended[:k]]

class ScientificDiscoveryPlatform:
    """Hlavní platforma pro objevování vědeckých článků"""
    
    def __init__(self, openai_api_key: str, neo4j_config: Dict):
        self.openai_api_key = openai_api_key
        self.neo4j_config = neo4j_config
        
        # Inicializace komponent
        self.data_collector = ArxivDataCollector()
        self.embedding_service = ScientificEmbeddingService()
        self.citation_network = CitationNetworkBuilder(**neo4j_config)
        self.recommender = CollaborativeRecommender()
        
        # Inicializace vector store
        self.chroma_client = chromadb.Client()
        self.collection = self.chroma_client.create_collection(
            name="scientific_papers",
            metadata={"hnsw:space": "cosine"}
        )
        
        # LangChain komponenty
        self.llm = OpenAI(api_key=openai_api_key, temperature=0.1)
        self.embeddings = HuggingFaceEmbeddings(
            model_name="all-MiniLM-L6-v2"
        )
    
    async def index_papers(self, query: str, max_results: int = 1000):
        """Indexování článků do platformy"""
        logger.info("Zahajuji sběr dat z ArXiv...")
        papers = await self.data_collector.search_papers(query, max_results)
        
        logger.info("Vytváření embeddings...")
        documents = []
        metadatas = []
        ids = []
        
        for paper in papers:
            # Vytvoření embedding
            embedding = self.embedding_service.create_paper_embedding(paper)
            paper.embedding = embedding
            
            # Příprava pro Chroma
            combined_text = f"{paper.title}\n\n{paper.abstract}"
            documents.append(combined_text)
            metadatas.append({
                'arxiv_id': paper.arxiv_id,
                'title': paper.title,
                'authors': json.dumps(paper.authors),
                'categories': json.dumps(paper.categories),
                'published': paper.published.isoformat()
            })
            ids.append(paper.arxiv_id)
            
            # Vytvoření uzlu v Neo4j
            self.citation_network.create_paper_node(paper)
        
        # Přidání do Chroma
        self.collection.add(
            documents=documents,
            metadatas=metadatas,
            ids=ids
        )
        
        logger.info(f"Indexováno {len(papers)} článků")
        return papers
    
    def semantic_search(self, query: str, k: int = 10) -> List[Dict]:
        """Sémantické vyhledávání článků"""
        try:
            results = self.collection.query(
                query_texts=[query],
                n_results=k
            )
            
            search_results = []
            for i, doc_id in enumerate(results['ids'][0]):
                search_results.append({
                    'arxiv_id': doc_id,
                    'content': results['documents'][0][i],
                    'metadata': results['metadatas'][0][i],
                    'distance': results['distances'][0][i] if 'distances' in results else 0.0
                })
            
            return search_results
            
        except Exception as e:
            logger.error(f"Chyba při sémantickém vyhledávání: {e}")
            return []
    
    def generate_research_summary(self, papers: List[Dict], research_question: str) -> str:
        """Generování souhrnu výzkumu pomocí LLM"""
        papers_text = "\n\n".join([
            f"Název: {paper['metadata']['title']}\nAbstrakt: {paper['content'][:500]}..."
            for paper in papers[:5]
        ])
        
        prompt = PromptTemplate(
            input_variables=["research_question", "papers"],
            template="""
            Na základě následujících vědeckých článků odpověz na výzkumnou otázku:
            
            Výzkumná otázka: {research_question}
            
            Relevantní články:
            {papers}
            
            Poskytni strukturovaný souhrn zahrnující:
            1. Klíčové poznatky
            2. Hlavní trendy
            3. Identifikované mezery ve výzkumu
            4. Doporučení pro další výzkum
            
            Odpověď:
            """
        )
        
        try:
            response = self.llm(prompt.format(
                research_question=research_question,
                papers=papers_text
            ))
            return response
        except Exception as e:
            logger.error(f"Chyba při generování souhrnu: {e}")
            return "Nepodařilo se vygenerovat souhrn."
    
    def get_personalized_recommendations(self, user_id: str, interests: List[str], k: int = 10) -> List[Dict]:
        """Personalizovaná doporučení pro uživatele"""
        # Kombinace sémantického vyhledávání podle zájmů
        all_recommendations = []
        
        for interest in interests:
            semantic_results = self.semantic_search(interest, k=5)
            all_recommendations.extend(semantic_results)
        
        # Kolaborativní filtrování
        all_paper_ids = [r['arxiv_id'] for r in all_recommendations]
        collaborative_recommendations = self.recommender.recommend_papers(
            user_id, all_paper_ids, k
        )
        
        # Kombinace a deduplikace
        final_recommendations = []
        seen_ids = set()
        
        for paper_id in collaborative_recommendations:
            if paper_id not in seen_ids:
                paper_data = next(
                    (r for r in all_recommendations if r['arxiv_id'] == paper_id),
                    None
                )
                if paper_data:
                    final_recommendations.append(paper_data)
                    seen_ids.add(paper_id)
        
        return final_recommendations[:k]
    
    def close(self):
        """Uzavření připojení"""
        self.citation_network.close()

# Příklad použití
async def main():
    """Demonstrace použití platformy"""
    
    # Konfigurace
    config = {
        'openai_api_key': 'your-openai-api-key',
        'neo4j_config': {
            'neo4j_uri': 'bolt://localhost:7687',
            'username': 'neo4j',
            'password': 'password'
        }
    }
    
    # Vytvoření platformy
    platform = ScientificDiscoveryPlatform(
        config['openai_api_key'],
        config['neo4j_config']
    )
    
    try:
        # Indexování článků o machine learning
        print("Indexování článků o strojovém učení...")
        await platform.index_papers("machine learning natural language processing", 100)
        
        # Sémantické vyhledávání
        print("\nSémantické vyhledávání...")
        results = platform.semantic_search("transformer neural networks attention", k=5)
        for result in results:
            print(f"- {result['metadata']['title']}")
        
        # Generování souhrnu výzkumu
        print("\nGenerování souhrnu výzkumu...")
        summary = platform.generate_research_summary(
            results,
            "Jaké jsou nejnovější trendy v architektuře transformerů?"
        )
        print(summary)
        
        # Personalizovaná doporučení
        print("\nPersonalizovaná doporučení...")
        user_interests = ["deep learning", "computer vision", "nlp"]
        recommendations = platform.get_personalized_recommendations(
            "user_123", user_interests, k=5
        )
        for rec in recommendations:
            print(f"- {rec['metadata']['title']}")
        
        # Simulace uživatelských interakcí
        for i, rec in enumerate(recommendations):
            platform.recommender.record_interaction(
                "user_123", rec['arxiv_id'], "view", rating=0.8 + i*0.1
            )
        
    except Exception as e:
        logger.error(f"Chyba v hlavní smyčce: {e}")
    finally:
        platform.close()

if __name__ == "__main__":
    asyncio.run(main())
````

### API Server

````python
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel
from typing import List, Optional
import asyncio
from scientific_discovery_platform import ScientificDiscoveryPlatform

app = FastAPI(title="Scientific Discovery Platform API")

# Globální instance platformy
platform = None

class SearchRequest(BaseModel):
    query: str
    k: int = 10

class RecommendationRequest(BaseModel):
    user_id: str
    interests: List[str]
    k: int = 10

class ResearchSummaryRequest(BaseModel):
    research_question: str
    papers: List[dict]

@app.on_event("startup")
async def startup_event():
    global platform
    config = {
        'openai_api_key': 'your-openai-api-key',
        'neo4j_config': {
            'neo4j_uri': 'bolt://localhost:7687',
            'username': 'neo4j',
            'password': 'password'
        }
    }
    platform = ScientificDiscoveryPlatform(
        config['openai_api_key'],
        config['neo4j_config']
    )

@app.post("/search")
async def search_papers(request: SearchRequest):
    """Sémantické vyhledávání článků"""
    try:
        results = platform.semantic_search(request.query, request.k)
        return {"results": results}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/recommendations")
async def get_recommendations(request: RecommendationRequest):
    """Personalizovaná doporučení"""
    try:
        recommendations = platform.get_personalized_recommendations(
            request.user_id, request.interests, request.k
        )
        return {"recommendations": recommendations}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/research-summary")
async def generate_summary(request: ResearchSummaryRequest):
    """Generování souhrnu výzkumu"""
    try:
        summary = platform.generate_research_summary(
            request.papers, request.research_question
        )
        return {"summary": summary}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/index")
async def index_papers(query: str, max_results: int = 100):
    """Indexování nových článků"""
    try:
        papers = await platform.index_papers(query, max_results)
        return {"indexed_count": len(papers)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
````

## Shrnutí Projektu

### Hodnota Projektu
Platforma pro Objevování Vědeckých Výzkumných Článků představuje významný pokrok v oblasti vědeckého výzkumu. Kombinuje nejmodernější technologie AI pro vytvoření inteligentního systému, který:

- **Urychluje objevování relevantních výzkumů** prostřednictvím pokročilého sémantického vyhledávání
- **Odhaluje skryté souvislosti** mezi publikacemi pomocí grafové analýzy citací  
- **Personalizuje vědecké objevování** na základě individuálních preferencí výzkumníků
- **Automatizuje syntézu znalostí** generováním inteligentních souhrnů výzkumu

### Klíčové Vlastnosti
- **Škálovatelná architektura** podporující miliony vědeckých publikací
- **Hybridní vyhledávání** kombinující sémantiku a citační vztahy
- **Kolaborativní doporučování** na základě chování vědecké komunity
- **Grafová reprezentace znalostí** pro odhalení komplexních vztahů
- **RESTful API** pro snadnou integraci s existujícími systémy

### Technologické Inovace
Projekt využívá nejmodernější technologie včetně SciBERT pro vědecké texty, Neo4j pro citační sítě, a Graph RAG pro komplexní vyhledávání. Tato kombinace vytváří synergii, která přesahuje možnosti jednotlivých technologií.

Platforma má potenciál transformovat způsob, jakým vědecká komunita přistupuje k objevování a syntéze znalostí, což může vést k akceleraci vědeckého pokroku napříč všemi obory.