<small>Claude Sonnet 4 **(Personal Knowledge Manager s MCP (Model Context Protocol))**</small>
# Personal Knowledge Manager

## Kl√≠ƒçov√© koncepty

### Model Context Protocol (MCP)
**MCP** je protokol pro komunikaci mezi AI asistenty a extern√≠mi zdroji dat. Umo≈æ≈àuje AI model≈Øm bezpeƒçnƒõ p≈ôistupovat k strukturovan√Ωm i nestrukturovan√Ωm dat≈Øm z r≈Øzn√Ωch zdroj≈Ø jako jsou datab√°ze, API, soubory nebo webov√© slu≈æby.

### Vector Databases (Vektorov√© datab√°ze)
**Vektorov√© datab√°ze** ukl√°daj√≠ data jako vysokodimenzion√°ln√≠ vektory, kter√© reprezentuj√≠ s√©mantick√Ω v√Ωznam textu. Umo≈æ≈àuj√≠ rychl√© vyhled√°v√°n√≠ podobn√©ho obsahu na z√°kladƒõ s√©mantick√© bl√≠zkosti m√≠sto p≈ôesn√© shody kl√≠ƒçov√Ωch slov.

### RAG (Retrieval-Augmented Generation)
**RAG** je technika, kter√° kombinuje vyhled√°v√°n√≠ relevantn√≠ch informac√≠ z datab√°ze znalost√≠ s generativn√≠mi schopnostmi AI model≈Ø. Umo≈æ≈àuje AI poskytovat p≈ôesnƒõj≈°√≠ a aktu√°lnƒõj≈°√≠ odpovƒõdi zalo≈æen√© na konkr√©tn√≠ch datech.

### Document Indexing (Indexov√°n√≠ dokument≈Ø)
**Indexov√°n√≠ dokument≈Ø** je proces p≈ôevodu textov√Ωch dokument≈Ø na prohled√°vatelnou formu. Zahrnuje extrakci textu, rozdƒõlen√≠ na ƒç√°sti, vytvo≈ôen√≠ vektorov√Ωch reprezentac√≠ a ulo≈æen√≠ do datab√°ze.

### Semantic Search (S√©mantick√© vyhled√°v√°n√≠)
**S√©mantick√© vyhled√°v√°n√≠** hled√° informace na z√°kladƒõ v√Ωznamu dotazu, nikoli pouze kl√≠ƒçov√Ωch slov. Vyu≈æ√≠v√° embeddingy k porozumƒõn√≠ kontextu a souvislostem.

## Komplexn√≠ vysvƒõtlen√≠ projektu

### C√≠le projektu
Personal Knowledge Manager je inteligentn√≠ syst√©m pro spr√°vu osobn√≠ch znalost√≠, kter√Ω vyu≈æ√≠v√° nejmodernƒõj≈°√≠ AI technologie k organizaci, indexov√°n√≠ a vyhled√°v√°n√≠ informac√≠. Projekt si klade za c√≠l:

1. **Automatizovan√© zpracov√°n√≠ dokument≈Ø** - Automatick√© extraktov√°n√≠ a indexov√°n√≠ obsahu z r≈Øzn√Ωch form√°t≈Ø
2. **Inteligentn√≠ vyhled√°v√°n√≠** - S√©mantick√© vyhled√°v√°n√≠ zalo≈æen√© na v√Ωznamu, nikoli pouze kl√≠ƒçov√Ωch slovech
3. **Personalizace** - Adaptace na u≈æivatelsk√© vzorce a preference
4. **Integrace** - Propojen√≠ s popul√°rn√≠mi n√°stroji jako Obsidian nebo Notion

### V√Ωzvy a ≈ôe≈°en√≠
**Technick√© v√Ωzvy:**
- ≈†k√°lov√°n√≠ na velk√© objemy dat
- Zachov√°n√≠ kvality embeddings
- Optimalizace rychlosti vyhled√°v√°n√≠
- Spr√°va r≈Øzn√Ωch form√°t≈Ø dokument≈Ø

**Architektonick√° ≈ôe≈°en√≠:**
- Mikroservisov√° architektura s MCP protokolem
- Distribuovan√© vektorov√© datab√°ze
- Asynchr√°ln√≠ zpracov√°n√≠
- Inteligentn√≠ cachov√°n√≠

### Potenci√°ln√≠ dopad
Syst√©m m≈Ø≈æe revolucionizovat zp≈Øsob, jak√Ωm lid√© pracuj√≠ s informacemi, zv√Ω≈°it produktivitu v√Ωzkumn√≠k≈Ø, student≈Ø a knowledge worker≈Ø a umo≈ænit vytv√°≈ôen√≠ pokroƒçil√Ωch AI asistent≈Ø.

## Komplexn√≠ p≈ô√≠klad implementace v Pythonu

### Instalace z√°vislost√≠

````bash
pip install fastapi uvicorn chromadb langchain openai sentence-transformers markdown beautifulsoup4 python-multipart aiofiles pydantic
````

### Hlavn√≠ implementace

````python
import asyncio
import os
import json
import hashlib
from typing import List, Dict, Any, Optional
from datetime import datetime
from pathlib import Path

import chromadb
from chromadb.config import Settings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document
from sentence_transformers import SentenceTransformer
import markdown
from bs4 import BeautifulSoup
from fastapi import FastAPI, HTTPException, UploadFile, File
from pydantic import BaseModel
import aiofiles

class MCPRequest(BaseModel):
    method: str
    params: Dict[str, Any]
    id: str

class MCPResponse(BaseModel):
    result: Optional[Dict[str, Any]] = None
    error: Optional[Dict[str, Any]] = None
    id: str

class DocumentMetadata(BaseModel):
    title: str
    source: str
    created_at: datetime
    file_type: str
    word_count: int

class SearchResult(BaseModel):
    content: str
    metadata: Dict[str, Any]
    score: float

class PersonalKnowledgeManager:
    def __init__(self, db_path: str = "./chroma_db"):
        """Inicializace Personal Knowledge Manageru"""
        self.db_path = Path(db_path)
        self.db_path.mkdir(exist_ok=True)
        
        # Inicializace Chroma DB
        self.chroma_client = chromadb.PersistentClient(
            path=str(self.db_path),
            settings=Settings(anonymized_telemetry=False)
        )
        
        # Kolekce pro dokumenty
        self.collection = self.chroma_client.get_or_create_collection(
            name="personal_knowledge",
            metadata={"description": "Personal knowledge base"}
        )
        
        # Embedding model
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        )
        
        # Text splitter
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        
        print("‚úÖ Personal Knowledge Manager inicializov√°n")

    async def add_document(self, content: str, metadata: DocumentMetadata) -> str:
        """P≈ôid√°n√≠ dokumentu do znalostn√≠ b√°ze"""
        try:
            # Rozdƒõlen√≠ textu na chunky
            chunks = self.text_splitter.split_text(content)
            
            # Vytvo≈ôen√≠ unik√°tn√≠ho ID pro dokument
            doc_id = hashlib.md5(f"{metadata.source}{metadata.title}".encode()).hexdigest()
            
            # P≈ô√≠prava dat pro vlo≈æen√≠
            documents = []
            metadatas = []
            ids = []
            
            for i, chunk in enumerate(chunks):
                chunk_id = f"{doc_id}_chunk_{i}"
                chunk_metadata = {
                    "document_id": doc_id,
                    "chunk_index": i,
                    "title": metadata.title,
                    "source": metadata.source,
                    "created_at": metadata.created_at.isoformat(),
                    "file_type": metadata.file_type,
                    "word_count": len(chunk.split())
                }
                
                documents.append(chunk)
                metadatas.append(chunk_metadata)
                ids.append(chunk_id)
            
            # Vlo≈æen√≠ do Chroma DB
            self.collection.add(
                documents=documents,
                metadatas=metadatas,
                ids=ids
            )
            
            print(f"‚úÖ Dokument '{metadata.title}' p≈ôid√°n ({len(chunks)} chunks)")
            return doc_id
            
        except Exception as e:
            print(f"‚ùå Chyba p≈ôi p≈ôid√°v√°n√≠ dokumentu: {e}")
            raise

    async def search_documents(self, query: str, n_results: int = 5) -> List[SearchResult]:
        """S√©mantick√© vyhled√°v√°n√≠ v dokumentech"""
        try:
            # Vyhled√°n√≠ podobn√Ωch dokument≈Ø
            results = self.collection.query(
                query_texts=[query],
                n_results=n_results,
                include=['documents', 'metadatas', 'distances']
            )
            
            search_results = []
            for i in range(len(results['documents'][0])):
                result = SearchResult(
                    content=results['documents'][0][i],
                    metadata=results['metadatas'][0][i],
                    score=1 - results['distances'][0][i]  # P≈ôevod distance na score
                )
                search_results.append(result)
            
            return search_results
            
        except Exception as e:
            print(f"‚ùå Chyba p≈ôi vyhled√°v√°n√≠: {e}")
            raise

    async def get_document_summary(self, doc_id: str) -> Dict[str, Any]:
        """Z√≠sk√°n√≠ shrnut√≠ dokumentu"""
        try:
            # Vyhled√°n√≠ v≈°ech chunks dokumentu
            results = self.collection.get(
                where={"document_id": doc_id},
                include=['documents', 'metadatas']
            )
            
            if not results['documents']:
                return {"error": "Dokument nenalezen"}
            
            # Sestaven√≠ shrnut√≠
            total_chunks = len(results['documents'])
            total_words = sum(meta['word_count'] for meta in results['metadatas'])
            
            metadata = results['metadatas'][0]
            summary = {
                "document_id": doc_id,
                "title": metadata['title'],
                "source": metadata['source'],
                "created_at": metadata['created_at'],
                "file_type": metadata['file_type'],
                "total_chunks": total_chunks,
                "total_words": total_words,
                "preview": results['documents'][0][:200] + "..."
            }
            
            return summary
            
        except Exception as e:
            print(f"‚ùå Chyba p≈ôi z√≠sk√°v√°n√≠ shrnut√≠: {e}")
            raise

    async def delete_document(self, doc_id: str) -> bool:
        """Smaz√°n√≠ dokumentu"""
        try:
            # Smaz√°n√≠ v≈°ech chunks dokumentu
            results = self.collection.get(
                where={"document_id": doc_id}
            )
            
            if not results['ids']:
                return False
            
            self.collection.delete(ids=results['ids'])
            print(f"‚úÖ Dokument {doc_id} smaz√°n")
            return True
            
        except Exception as e:
            print(f"‚ùå Chyba p≈ôi maz√°n√≠ dokumentu: {e}")
            return False

class MCPServer:
    def __init__(self):
        """Inicializace MCP serveru"""
        self.km = PersonalKnowledgeManager()
        self.app = FastAPI(title="Personal Knowledge Manager MCP Server")
        
        # Registrace MCP endpoint≈Ø
        self.setup_routes()
    
    def setup_routes(self):
        """Nastaven√≠ API routes"""
        
        @self.app.post("/mcp", response_model=MCPResponse)
        async def handle_mcp_request(request: MCPRequest):
            """Hlavn√≠ MCP endpoint"""
            try:
                if request.method == "search":
                    query = request.params.get("query", "")
                    n_results = request.params.get("n_results", 5)
                    
                    results = await self.km.search_documents(query, n_results)
                    return MCPResponse(
                        result={"documents": [r.dict() for r in results]},
                        id=request.id
                    )
                
                elif request.method == "add_document":
                    content = request.params.get("content", "")
                    metadata_dict = request.params.get("metadata", {})
                    
                    metadata = DocumentMetadata(
                        title=metadata_dict.get("title", "Bez n√°zvu"),
                        source=metadata_dict.get("source", ""),
                        created_at=datetime.now(),
                        file_type=metadata_dict.get("file_type", "text"),
                        word_count=len(content.split())
                    )
                    
                    doc_id = await self.km.add_document(content, metadata)
                    return MCPResponse(
                        result={"document_id": doc_id},
                        id=request.id
                    )
                
                elif request.method == "get_summary":
                    doc_id = request.params.get("document_id", "")
                    summary = await self.km.get_document_summary(doc_id)
                    
                    return MCPResponse(
                        result=summary,
                        id=request.id
                    )
                
                elif request.method == "delete_document":
                    doc_id = request.params.get("document_id", "")
                    success = await self.km.delete_document(doc_id)
                    
                    return MCPResponse(
                        result={"success": success},
                        id=request.id
                    )
                
                else:
                    return MCPResponse(
                        error={"code": -32601, "message": "Nezn√°m√° metoda"},
                        id=request.id
                    )
                    
            except Exception as e:
                return MCPResponse(
                    error={"code": -32603, "message": str(e)},
                    id=request.id
                )
        
        @self.app.post("/upload")
        async def upload_file(file: UploadFile = File(...)):
            """Upload souboru"""
            try:
                content = await file.read()
                
                # Zpracov√°n√≠ podle typu souboru
                if file.filename.endswith('.md'):
                    text_content = content.decode('utf-8')
                    html = markdown.markdown(text_content)
                    soup = BeautifulSoup(html, 'html.parser')
                    clean_text = soup.get_text()
                elif file.filename.endswith('.txt'):
                    clean_text = content.decode('utf-8')
                else:
                    clean_text = content.decode('utf-8', errors='ignore')
                
                metadata = DocumentMetadata(
                    title=file.filename,
                    source=f"upload/{file.filename}",
                    created_at=datetime.now(),
                    file_type=file.content_type or "text/plain",
                    word_count=len(clean_text.split())
                )
                
                doc_id = await self.km.add_document(clean_text, metadata)
                
                return {"document_id": doc_id, "message": "Soubor √∫spƒõ≈°nƒõ nahr√°n"}
                
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))

async def create_sample_data(km: PersonalKnowledgeManager):
    """Vytvo≈ôen√≠ uk√°zkov√Ωch dat"""
    sample_documents = [
        {
            "content": """
            # √övod do umƒõl√© inteligence
            
            Umƒõl√° inteligence (AI) je oblast informatiky, kter√° se zab√Ωv√° vytv√°≈ôen√≠m inteligentn√≠ch syst√©m≈Ø 
            schopn√Ωch prov√°dƒõt √∫koly, kter√© tradiƒçnƒõ vy≈æaduj√≠ lidskou inteligenci. Zahrnuje oblasti jako 
            strojov√© uƒçen√≠, zpracov√°n√≠ p≈ôirozen√©ho jazyka, poƒç√≠taƒçov√© vidƒõn√≠ a robotiku.
            
            ## Hlavn√≠ odvƒõtv√≠ AI
            
            1. **Strojov√© uƒçen√≠** - Algoritmy, kter√© se uƒç√≠ z dat
            2. **Hlubok√© uƒçen√≠** - Neuronov√© s√≠tƒõ s mnoha vrstvami
            3. **Zpracov√°n√≠ p≈ôirozen√©ho jazyka** - Porozumƒõn√≠ a generov√°n√≠ lidsk√©ho jazyka
            4. **Poƒç√≠taƒçov√© vidƒõn√≠** - Anal√Ωza a interpretace vizu√°ln√≠ch dat
            
            AI m√° potenci√°l transformovat mnoho odvƒõtv√≠ vƒçetnƒõ zdravotnictv√≠, dopravy, 
            financ√≠ a vzdƒõl√°v√°n√≠.
            """,
            "metadata": {
                "title": "√övod do umƒõl√© inteligence",
                "source": "ai_course/lesson_1.md",
                "file_type": "markdown"
            }
        },
        {
            "content": """
            # Python pro data science
            
            Python je jeden z nejpopul√°rnƒõj≈°√≠ch programovac√≠ch jazyk≈Ø pro data science d√≠ky 
            sv√© jednoduchosti a bohat√© ekosyst√©mu knihoven.
            
            ## Kl√≠ƒçov√© knihovny
            
            - **NumPy**: Numerick√© v√Ωpoƒçty a pr√°ce s poli
            - **Pandas**: Manipulace a anal√Ωza dat
            - **Matplotlib/Seaborn**: Vizualizace dat
            - **Scikit-learn**: Strojov√© uƒçen√≠
            - **TensorFlow/PyTorch**: Hlubok√© uƒçen√≠
            
            ## Typick√Ω workflow
            
            1. Naƒçten√≠ a ƒçi≈°tƒõn√≠ dat
            2. Exploraƒçn√≠ anal√Ωza dat (EDA)
            3. Feature engineering
            4. Tr√©nov√°n√≠ modelu
            5. Evaluace a validace
            6. Deployment
            
            Python poskytuje n√°stroje pro cel√Ω ≈æivotn√≠ cyklus data science projekt≈Ø.
            """,
            "metadata": {
                "title": "Python pro data science",
                "source": "python_course/data_science.md",
                "file_type": "markdown"
            }
        },
        {
            "content": """
            # Vektorov√© datab√°ze a embeddingy
            
            Vektorov√© datab√°ze jsou specializovan√© datab√°zov√© syst√©my optimalizovan√© pro 
            ukl√°d√°n√≠ a vyhled√°v√°n√≠ vysokodimenzion√°ln√≠ch vektor≈Ø.
            
            ## Co jsou embeddingy?
            
            Embeddingy jsou numerick√© reprezentace objekt≈Ø (text, obr√°zky, audio) ve 
            vysokodimenzion√°ln√≠m prostoru, kde podobn√© objekty maj√≠ podobn√© vektory.
            
            ## Popul√°rn√≠ vektorov√© datab√°ze
            
            - **Chroma**: Open-source, jednoduch√© na pou≈æit√≠
            - **Pinecone**: Cloudov√° slu≈æba, vysok√Ω v√Ωkon
            - **Weaviate**: GraphQL API, multimod√°ln√≠
            - **Qdrant**: Rychl√°, Rust-based
            - **FAISS**: Facebook AI, vysok√Ω v√Ωkon
            
            ## Aplikace
            
            - S√©mantick√© vyhled√°v√°n√≠
            - Recommendation syst√©my
            - Detekce duplik√°t≈Ø
            - Clustering podobn√©ho obsahu
            - RAG (Retrieval-Augmented Generation)
            """,
            "metadata": {
                "title": "Vektorov√© datab√°ze a embeddingy",
                "source": "database_course/vector_db.md",
                "file_type": "markdown"
            }
        }
    ]
    
    print("üìÅ Vytv√°≈ôen√≠ uk√°zkov√Ωch dat...")
    
    for doc_data in sample_documents:
        metadata = DocumentMetadata(
            title=doc_data["metadata"]["title"],
            source=doc_data["metadata"]["source"],
            created_at=datetime.now(),
            file_type=doc_data["metadata"]["file_type"],
            word_count=len(doc_data["content"].split())
        )
        
        await km.add_document(doc_data["content"], metadata)
    
    print("‚úÖ Uk√°zkov√° data vytvo≈ôena")

async def demo_search(km: PersonalKnowledgeManager):
    """Demonstrace vyhled√°v√°n√≠"""
    print("\nüîç Demonstrace vyhled√°v√°n√≠:")
    
    queries = [
        "Co je strojov√© uƒçen√≠?",
        "Jak√© jsou knihovny pro Python?",
        "Vektorov√© datab√°ze aplikace"
    ]
    
    for query in queries:
        print(f"\nüí≠ Dotaz: '{query}'")
        results = await km.search_documents(query, n_results=2)
        
        for i, result in enumerate(results, 1):
            print(f"\n  üìÑ V√Ωsledek {i} (sk√≥re: {result.score:.3f}):")
            print(f"     N√°zev: {result.metadata['title']}")
            print(f"     Zdroj: {result.metadata['source']}")
            print(f"     Obsah: {result.content[:150]}...")

if __name__ == "__main__":
    import uvicorn
    
    # Vytvo≈ôen√≠ MCP serveru
    mcp_server = MCPServer()
    
    async def startup():
        """Startup procedura"""
        print("üöÄ Spou≈°tƒõn√≠ Personal Knowledge Manager MCP serveru...")
        await create_sample_data(mcp_server.km)
        await demo_search(mcp_server.km)
        print("\n‚úÖ Server je p≈ôipraven na portu 8000")
        print("üìñ API dokumentace: http://localhost:8000/docs")
    
    # Spu≈°tƒõn√≠ serveru
    asyncio.create_task(startup())
    uvicorn.run(mcp_server.app, host="0.0.0.0", port=8000)
````

### MCP klient pro testov√°n√≠

````python
import asyncio
import aiohttp
import json
from typing import Dict, Any

class MCPClient:
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.session = None
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def call_mcp_method(self, method: str, params: Dict[str, Any]) -> Dict[str, Any]:
        """Vol√°n√≠ MCP metody"""
        request_data = {
            "method": method,
            "params": params,
            "id": f"req_{method}_{asyncio.get_event_loop().time()}"
        }
        
        async with self.session.post(
            f"{self.base_url}/mcp",
            json=request_data
        ) as response:
            result = await response.json()
            return result
    
    async def search(self, query: str, n_results: int = 5):
        """Vyhled√°v√°n√≠ dokument≈Ø"""
        return await self.call_mcp_method("search", {
            "query": query,
            "n_results": n_results
        })
    
    async def add_document(self, content: str, metadata: Dict[str, Any]):
        """P≈ôid√°n√≠ dokumentu"""
        return await self.call_mcp_method("add_document", {
            "content": content,
            "metadata": metadata
        })

async def test_mcp_client():
    """Test MCP klienta"""
    async with MCPClient() as client:
        print("üß™ Testov√°n√≠ MCP klienta...")
        
        # Test vyhled√°v√°n√≠
        search_result = await client.search("machine learning Python")
        print(f"üîç Vyhled√°v√°n√≠: {len(search_result['result']['documents'])} v√Ωsledk≈Ø")
        
        # Test p≈ôid√°n√≠ dokumentu
        add_result = await client.add_document(
            "Toto je testovac√≠ dokument o MCP protokolu.",
            {
                "title": "Test MCP",
                "source": "test_client.py",
                "file_type": "text"
            }
        )
        print(f"‚ûï P≈ôid√°n dokument: {add_result['result']['document_id']}")

if __name__ == "__main__":
    asyncio.run(test_mcp_client())
````

## Shrnut√≠ projektu

Personal Knowledge Manager s MCP p≈ôedstavuje modern√≠ ≈ôe≈°en√≠ pro spr√°vu osobn√≠ch znalost√≠ kombinuj√≠c√≠:

### Kl√≠ƒçov√© v√Ωhody
- **S√©mantick√© vyhled√°v√°n√≠** - Inteligentn√≠ hled√°n√≠ na z√°kladƒõ v√Ωznamu
- **Automatizace** - Minim√°ln√≠ manu√°ln√≠ pr√°ce p≈ôi indexov√°n√≠
- **≈†k√°lovatelnost** - Schopnost r≈Øst s objemem dat
- **Integrace** - Snadn√© propojen√≠ s existuj√≠c√≠mi n√°stroji

### Technologick√° hodnota
- Vyu≈æit√≠ nejmodernƒõj≈°√≠ch AI technologi√≠
- Modul√°rn√≠ architektura umo≈æ≈àuj√≠c√≠ roz≈°√≠≈ôen√≠
- Standardizovan√Ω MCP protokol pro interoperabilitu
- Vysok√Ω v√Ωkon d√≠ky optimalizovan√Ωm vektorov√Ωm datab√°z√≠m

### Praktick√© vyu≈æit√≠
Syst√©m je ide√°ln√≠ pro v√Ωzkumn√≠ky, studenty, konzultanty a v≈°echny knowledge workery, kte≈ô√≠ pot≈ôebuj√≠ efektivnƒõ organizovat a vyhled√°vat ve velk√Ωch objemech informac√≠. M≈Ø≈æe slou≈æit jako z√°klad pro pokroƒçil√© AI asistenty nebo integrovan√° ≈ôe≈°en√≠ pro organizace.