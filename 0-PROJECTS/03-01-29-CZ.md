<small>Claude Sonnet 4 **(Podcast Content Discovery and Analysis)**</small>
# Podcast Content Discovery and Analysis

## 1. Projekt N√°zev

**AI-LLM RAG Syst√©m pro Objevov√°n√≠ a Anal√Ωzu Podcastov√©ho Obsahu**

Inteligentn√≠ platforma vyu≈æ√≠vaj√≠c√≠ Retrieval-Augmented Generation pro pokroƒçilou anal√Ωzu podcastov√Ωch episod, automatickou kategorizaci t√©mat, doporuƒçovac√≠ syst√©m a komplexn√≠ spr√°vu metadat podcast≈Ø.

## 2. Kl√≠ƒçov√© Koncepty

### RAG (Retrieval-Augmented Generation)
Hybridn√≠ p≈ô√≠stup kombinuj√≠c√≠ vektorov√© vyhled√°v√°n√≠ s generativn√≠ AI. Umo≈æ≈àuje AI modelu p≈ôistupovat k extern√≠m znalostn√≠m datab√°z√≠m a generovat odpovƒõdi na z√°kladƒõ aktu√°ln√≠ch a relevantn√≠ch informac√≠.

### Episode Transcripts (Transkripty Episod)
Textov√© p≈ôevody audio obsahu podcast≈Ø z√≠skan√© pomoc√≠ speech-to-text technologi√≠. Slou≈æ√≠ jako hlavn√≠ zdroj dat pro anal√Ωzu obsahu a vyhled√°v√°n√≠.

### Host Information (Informace o Moder√°torech)
Metadata o moder√°torech podcast≈Ø vƒçetnƒõ jejich odbornosti, historie, stylu moderov√°n√≠ a dal≈°√≠ch charakteristik relevantn√≠ch pro doporuƒçovac√≠ algoritmy.

### Topic Categorization (Kategorizace T√©mat)
Automatick√© t≈ô√≠dƒõn√≠ podcastov√Ωch episod do tematick√Ωch kategori√≠ pomoc√≠ NLP technik a strojov√©ho uƒçen√≠ pro lep≈°√≠ organizaci a vyhled√°v√°n√≠ obsahu.

### Listener Reviews (Recenze Posluchaƒç≈Ø)
Anal√Ωza zpƒõtn√© vazby od posluchaƒç≈Ø pro zlep≈°en√≠ doporuƒçovac√≠ch algoritm≈Ø a pochopen√≠ preferenc√≠ u≈æivatel≈Ø.

### Spotify Podcast API
Rozhran√≠ pro p≈ô√≠stup k podcastov√Ωm dat≈Øm ze Spotify platformy, vƒçetnƒõ metadat, popularity a dal≈°√≠ch u≈æiteƒçn√Ωch informac√≠.

### Audio Processing (Zpracov√°n√≠ Audia)
Techniky pro anal√Ωzu audio sign√°lu, extrakci p≈ô√≠znak≈Ø a konverzi zvuku na text pomoc√≠ pokroƒçil√Ωch algoritm≈Ø.

### Recommendation Engine (Doporuƒçovac√≠ Syst√©m)
Algoritmus pro personalizovan√© doporuƒçov√°n√≠ podcast≈Ø na z√°kladƒõ u≈æivatelsk√Ωch preferenc√≠, historie poslechu a obsahov√© anal√Ωzy.

## 3. Komplexn√≠ Vysvƒõtlen√≠ Projektu

### C√≠le Projektu

Tento projekt vytv√°≈ô√≠ pokroƒçilou platformu pro anal√Ωzu a objevov√°n√≠ podcastov√©ho obsahu pomoc√≠ modern√≠ch AI technologi√≠. Hlavn√≠mi c√≠li jsou:

- **Automatick√° kategorizace**: Inteligentn√≠ t≈ô√≠dƒõn√≠ podcast≈Ø podle t√©mat a ≈æ√°nr≈Ø
- **S√©mantick√© vyhled√°v√°n√≠**: Pokroƒçil√© vyhled√°v√°n√≠ v transkriptech pomoc√≠ vektorov√Ωch datab√°z√≠
- **Personalizovan√© doporuƒçov√°n√≠**: AI-driven doporuƒçovac√≠ syst√©m na z√°kladƒõ u≈æivatelsk√Ωch preferenc√≠
- **Anal√Ωza sentimentu**: Hodnocen√≠ n√°lady a t√≥nu podcastov√Ωch episod
- **Extrakce kl√≠ƒçov√Ωch t√©mat**: Automatick√© identifikov√°n√≠ hlavn√≠ch t√©mat diskuse

### V√Ωzvy Projektu

- **≈†k√°lovatelnost**: Zpracov√°n√≠ velk√Ωch objem≈Ø audio dat a transkript≈Ø
- **P≈ôesnost transkripce**: Kvalitn√≠ p≈ôevod ≈ôeƒçi na text v r≈Øzn√Ωch jazyc√≠ch a dialektech
- **Real-time zpracov√°n√≠**: Rychl√° anal√Ωza novƒõ publikovan√Ωch episod
- **Multimod√°ln√≠ anal√Ωza**: Kombinace audio a textov√Ωch dat pro lep≈°√≠ porozumƒõn√≠
- **Personalizace**: Adaptace na individu√°ln√≠ preference u≈æivatel≈Ø

### Potenci√°ln√≠ Dopad

Projekt m≈Ø≈æe revolucionizovat zp≈Øsob, jak√Ωm lid√© objevuj√≠ a konzumuj√≠ podcastov√Ω obsah, poskytuje tv≈Ørc≈Øm lep≈°√≠ insights o sv√© audience a umo≈æ≈àuje efektivnƒõj≈°√≠ monetizaci obsahu.

## 4. Komplexn√≠ P≈ô√≠klad s Python Implementac√≠

### Instalace Z√°vislost√≠

````python
# requirements.txt
langchain==0.1.0
chromadb==0.4.18
openai==1.6.1
sentence-transformers==2.2.2
spotipy==2.22.1
whisper==1.1.10
pandas==2.0.3
numpy==1.24.3
scikit-learn==1.3.0
fastapi==0.104.1
uvicorn==0.24.0
python-multipart==0.0.6
pydantic==2.5.0
````

### Hlavn√≠ Konfigurace a Modely

````python
import os
from dataclasses import dataclass
from typing import List, Optional
import pandas as pd

@dataclass
class PodcastConfig:
    """Konfigurace pro podcast anal√Ωzu"""
    openai_api_key: str = os.getenv("OPENAI_API_KEY")
    spotify_client_id: str = os.getenv("SPOTIFY_CLIENT_ID")
    spotify_client_secret: str = os.getenv("SPOTIFY_CLIENT_SECRET")
    chroma_persist_directory: str = "./chroma_db"
    embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2"
    whisper_model: str = "base"
    max_transcript_length: int = 10000

@dataclass
class PodcastEpisode:
    """Model pro podcast epizodu"""
    id: str
    title: str
    description: str
    transcript: str
    host_name: str
    duration_ms: int
    release_date: str
    categories: List[str]
    sentiment_score: float
    audio_url: Optional[str] = None
    spotify_url: Optional[str] = None

@dataclass
class UserPreferences:
    """Model pro u≈æivatelsk√© preference"""
    user_id: str
    preferred_categories: List[str]
    preferred_hosts: List[str]
    listening_history: List[str]
    sentiment_preference: str  # "positive", "neutral", "negative"
````

### RAG Syst√©m pro Podcast Anal√Ωzu

````python
import chromadb
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
import openai
from typing import List, Dict, Any
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import logging

class PodcastRAGSystem:
    """RAG syst√©m pro anal√Ωzu podcast≈Ø"""
    
    def __init__(self, config: PodcastConfig):
        self.config = config
        self.setup_logging()
        self.setup_embeddings()
        self.setup_vector_store()
        self.setup_llm()
        self.setup_qa_chain()
        
    def setup_logging(self):
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
    def setup_embeddings(self):
        """Inicializace embedding modelu"""
        self.embeddings = HuggingFaceEmbeddings(
            model_name=self.config.embedding_model
        )
        
    def setup_vector_store(self):
        """Inicializace vektorov√© datab√°ze"""
        self.vector_store = Chroma(
            persist_directory=self.config.chroma_persist_directory,
            embedding_function=self.embeddings
        )
        
    def setup_llm(self):
        """Inicializace LLM"""
        openai.api_key = self.config.openai_api_key
        self.llm = OpenAI(
            temperature=0.7,
            max_tokens=500
        )
        
    def setup_qa_chain(self):
        """Inicializace QA ≈ôetƒõzce"""
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vector_store.as_retriever(search_kwargs={"k": 5})
        )
        
    def add_podcast_episode(self, episode: PodcastEpisode):
        """P≈ôid√°n√≠ podcast epizody do datab√°ze"""
        try:
            # Rozdƒõlen√≠ transkriptu na men≈°√≠ ƒç√°sti
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=100
            )
            
            chunks = text_splitter.split_text(episode.transcript)
            
            # Vytvo≈ôen√≠ dokument≈Ø s metadaty
            documents = []
            for i, chunk in enumerate(chunks):
                doc = Document(
                    page_content=chunk,
                    metadata={
                        "episode_id": episode.id,
                        "title": episode.title,
                        "host": episode.host_name,
                        "categories": ",".join(episode.categories),
                        "chunk_id": i,
                        "release_date": episode.release_date,
                        "sentiment_score": episode.sentiment_score
                    }
                )
                documents.append(doc)
            
            # P≈ôid√°n√≠ do vektorov√© datab√°ze
            self.vector_store.add_documents(documents)
            self.vector_store.persist()
            
            self.logger.info(f"P≈ôid√°na epizoda: {episode.title}")
            
        except Exception as e:
            self.logger.error(f"Chyba p≈ôi p≈ôid√°v√°n√≠ epizody: {str(e)}")
            raise
            
    def search_episodes(self, query: str, filters: Dict[str, Any] = None) -> List[Dict]:
        """S√©mantick√© vyhled√°v√°n√≠ v epizod√°ch"""
        try:
            # Vyhled√°n√≠ relevantn√≠ch dokument≈Ø
            docs = self.vector_store.similarity_search_with_score(
                query=query,
                k=10
            )
            
            results = []
            seen_episodes = set()
            
            for doc, score in docs:
                episode_id = doc.metadata.get("episode_id")
                
                # Filtrov√°n√≠ duplicitn√≠ch epizod
                if episode_id not in seen_episodes:
                    seen_episodes.add(episode_id)
                    
                    result = {
                        "episode_id": episode_id,
                        "title": doc.metadata.get("title"),
                        "host": doc.metadata.get("host"),
                        "categories": doc.metadata.get("categories", "").split(","),
                        "relevance_score": float(score),
                        "content_preview": doc.page_content[:200] + "..."
                    }
                    
                    # Aplikace filtr≈Ø
                    if filters:
                        if self._apply_filters(result, filters):
                            results.append(result)
                    else:
                        results.append(result)
            
            return sorted(results, key=lambda x: x["relevance_score"])
            
        except Exception as e:
            self.logger.error(f"Chyba p≈ôi vyhled√°v√°n√≠: {str(e)}")
            raise
            
    def _apply_filters(self, result: Dict, filters: Dict[str, Any]) -> bool:
        """Aplikace filtr≈Ø na v√Ωsledky vyhled√°v√°n√≠"""
        if "host" in filters and result["host"] not in filters["host"]:
            return False
            
        if "categories" in filters:
            if not any(cat in result["categories"] for cat in filters["categories"]):
                return False
                
        return True
        
    def get_episode_insights(self, episode_id: str) -> Dict[str, Any]:
        """Z√≠sk√°n√≠ detailn√≠ch insights o epizodƒõ"""
        try:
            query = f"episode_id:{episode_id}"
            docs = self.vector_store.similarity_search(
                query=query,
                k=20,
                filter={"episode_id": episode_id}
            )
            
            if not docs:
                return {"error": "Epizoda nenalezena"}
            
            # Extrakce cel√©ho transkriptu
            full_transcript = " ".join([doc.page_content for doc in docs])
            
            # Generov√°n√≠ insights pomoc√≠ LLM
            insights_prompt = f"""
            Analyzuj n√°sleduj√≠c√≠ transkript podcast epizody a poskytni strukturovan√© insights:
            
            Transkript: {full_transcript[:3000]}...
            
            Poskytni anal√Ωzu v n√°sleduj√≠c√≠m form√°tu:
            1. Hlavn√≠ t√©mata (3-5 bod≈Ø)
            2. Kl√≠ƒçov√© poznatky
            3. Sentiment cel√© epizody
            4. Doporuƒçen√° audience
            5. Hodnotn√© cit√°ty (2-3)
            """
            
            insights = self.llm(insights_prompt)
            
            return {
                "episode_id": episode_id,
                "title": docs[0].metadata.get("title"),
                "insights": insights,
                "transcript_length": len(full_transcript),
                "chunk_count": len(docs)
            }
            
        except Exception as e:
            self.logger.error(f"Chyba p≈ôi generov√°n√≠ insights: {str(e)}")
            return {"error": str(e)}
````

### Doporuƒçovac√≠ Syst√©m

````python
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from typing import List, Dict, Tuple
import pandas as pd
from collections import defaultdict

class PodcastRecommendationEngine:
    """Doporuƒçovac√≠ syst√©m pro podcasty"""
    
    def __init__(self, rag_system: PodcastRAGSystem):
        self.rag_system = rag_system
        self.user_profiles = {}
        self.episode_features = {}
        self.tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        
    def build_user_profile(self, user_preferences: UserPreferences) -> Dict[str, Any]:
        """Vytvo≈ôen√≠ u≈æivatelsk√©ho profilu"""
        try:
            profile = {
                "user_id": user_preferences.user_id,
                "category_weights": self._calculate_category_weights(
                    user_preferences.preferred_categories
                ),
                "host_preferences": user_preferences.preferred_hosts,
                "sentiment_preference": user_preferences.sentiment_preference,
                "listening_history": user_preferences.listening_history
            }
            
            # Anal√Ωza posluchov√Ωch n√°vyk≈Ø
            if user_preferences.listening_history:
                profile["content_embeddings"] = self._extract_content_preferences(
                    user_preferences.listening_history
                )
            
            self.user_profiles[user_preferences.user_id] = profile
            return profile
            
        except Exception as e:
            raise Exception(f"Chyba p≈ôi vytv√°≈ôen√≠ profilu: {str(e)}")
            
    def _calculate_category_weights(self, preferred_categories: List[str]) -> Dict[str, float]:
        """V√Ωpoƒçet vah pro kategorie"""
        if not preferred_categories:
            return {}
            
        weight = 1.0 / len(preferred_categories)
        return {category: weight for category in preferred_categories}
        
    def _extract_content_preferences(self, listening_history: List[str]) -> np.ndarray:
        """Extrakce preferenc√≠ z historie poslechu"""
        try:
            # Z√≠sk√°n√≠ transkript≈Ø z historie
            transcripts = []
            for episode_id in listening_history:
                docs = self.rag_system.vector_store.similarity_search(
                    query=f"episode_id:{episode_id}",
                    k=5
                )
                if docs:
                    transcript = " ".join([doc.page_content for doc in docs])
                    transcripts.append(transcript)
            
            if not transcripts:
                return np.array([])
                
            # TF-IDF vektorizace
            tfidf_matrix = self.tfidf_vectorizer.fit_transform(transcripts)
            
            # Pr≈Ømƒõrn√Ω vektor preferenc√≠
            avg_preferences = np.mean(tfidf_matrix.toarray(), axis=0)
            return avg_preferences
            
        except Exception as e:
            print(f"Chyba p≈ôi extrakci preferenc√≠: {str(e)}")
            return np.array([])
            
    def recommend_episodes(
        self, 
        user_id: str, 
        num_recommendations: int = 10,
        exclude_listened: bool = True
    ) -> List[Dict[str, Any]]:
        """Generov√°n√≠ doporuƒçen√≠ pro u≈æivatele"""
        try:
            if user_id not in self.user_profiles:
                raise ValueError("U≈æivatelsk√Ω profil nenalezen")
                
            user_profile = self.user_profiles[user_id]
            
            # Z√≠sk√°n√≠ v≈°ech dostupn√Ωch epizod
            all_episodes = self._get_all_episodes()
            
            # V√Ωpoƒçet sk√≥re pro ka≈ædou epizodu
            episode_scores = []
            for episode in all_episodes:
                if exclude_listened and episode["id"] in user_profile["listening_history"]:
                    continue
                    
                score = self._calculate_episode_score(episode, user_profile)
                episode_scores.append((episode, score))
            
            # Se≈ôazen√≠ podle sk√≥re
            episode_scores.sort(key=lambda x: x[1], reverse=True)
            
            # Vr√°cen√≠ top doporuƒçen√≠
            recommendations = []
            for episode, score in episode_scores[:num_recommendations]:
                recommendation = {
                    "episode_id": episode["id"],
                    "title": episode["title"],
                    "host": episode["host"],
                    "categories": episode["categories"],
                    "recommendation_score": float(score),
                    "reasons": self._generate_recommendation_reasons(episode, user_profile)
                }
                recommendations.append(recommendation)
                
            return recommendations
            
        except Exception as e:
            raise Exception(f"Chyba p≈ôi generov√°n√≠ doporuƒçen√≠: {str(e)}")
            
    def _get_all_episodes(self) -> List[Dict[str, Any]]:
        """Z√≠sk√°n√≠ v≈°ech epizod z datab√°ze"""
        # Simulace - v re√°ln√© aplikaci by se z√≠sk√°valy z datab√°ze
        return [
            {
                "id": "ep1",
                "title": "AI a budoucnost pr√°ce",
                "host": "Jan Nov√°k",
                "categories": ["technologie", "ai", "pr√°ce"],
                "sentiment_score": 0.7,
                "transcript": "Diskuse o dopadu AI na trh pr√°ce..."
            },
            {
                "id": "ep2", 
                "title": "Investov√°n√≠ pro zaƒç√°teƒçn√≠ky",
                "host": "Marie Svobodov√°",
                "categories": ["finance", "investice"],
                "sentiment_score": 0.5,
                "transcript": "Z√°klady investov√°n√≠ do akci√≠..."
            }
        ]
        
    def _calculate_episode_score(self, episode: Dict, user_profile: Dict) -> float:
        """V√Ωpoƒçet sk√≥re epizody pro u≈æivatele"""
        score = 0.0
        
        # Sk√≥re na z√°kladƒõ kategori√≠
        category_score = 0.0
        for category in episode["categories"]:
            if category in user_profile["category_weights"]:
                category_score += user_profile["category_weights"][category]
        score += category_score * 0.4
        
        # Sk√≥re na z√°kladƒõ hostitele
        if episode["host"] in user_profile["host_preferences"]:
            score += 0.3
            
        # Sk√≥re na z√°kladƒõ sentimentu
        sentiment_match = self._calculate_sentiment_match(
            episode["sentiment_score"],
            user_profile["sentiment_preference"]
        )
        score += sentiment_match * 0.2
        
        # Sk√≥re na z√°kladƒõ obsahu (pokud jsou dostupn√© embeddings)
        if "content_embeddings" in user_profile and len(user_profile["content_embeddings"]) > 0:
            content_score = self._calculate_content_similarity(episode, user_profile)
            score += content_score * 0.1
            
        return score
        
    def _calculate_sentiment_match(self, episode_sentiment: float, user_preference: str) -> float:
        """V√Ωpoƒçet shody sentimentu"""
        if user_preference == "positive" and episode_sentiment > 0.3:
            return 1.0
        elif user_preference == "neutral" and -0.3 <= episode_sentiment <= 0.3:
            return 1.0
        elif user_preference == "negative" and episode_sentiment < -0.3:
            return 1.0
        else:
            return 0.5
            
    def _calculate_content_similarity(self, episode: Dict, user_profile: Dict) -> float:
        """V√Ωpoƒçet podobnosti obsahu"""
        try:
            # Vektorizace epizody
            episode_vector = self.tfidf_vectorizer.transform([episode["transcript"]])
            
            # Porovn√°n√≠ s u≈æivatelsk√Ωmi preferencemi
            similarity = cosine_similarity(
                episode_vector.toarray(),
                user_profile["content_embeddings"].reshape(1, -1)
            )[0][0]
            
            return similarity
            
        except Exception:
            return 0.0
            
    def _generate_recommendation_reasons(self, episode: Dict, user_profile: Dict) -> List[str]:
        """Generov√°n√≠ d≈Øvod≈Ø doporuƒçen√≠"""
        reasons = []
        
        # Kontrola kategori√≠
        matching_categories = [
            cat for cat in episode["categories"] 
            if cat in user_profile["category_weights"]
        ]
        if matching_categories:
            reasons.append(f"Odpov√≠d√° va≈°im z√°jm≈Øm: {', '.join(matching_categories)}")
            
        # Kontrola hostitele
        if episode["host"] in user_profile["host_preferences"]:
            reasons.append(f"Obl√≠ben√Ω moder√°tor: {episode['host']}")
            
        # Sentiment
        if user_profile["sentiment_preference"] == "positive" and episode["sentiment_score"] > 0.3:
            reasons.append("Pozitivn√≠ a motivuj√≠c√≠ obsah")
            
        return reasons
````

### API Rozhran√≠

````python
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import asyncio
from config import PodcastConfig, PodcastEpisode, UserPreferences
from podcast_rag_system import PodcastRAGSystem
from recommendation_engine import PodcastRecommendationEngine

app = FastAPI(title="Podcast Discovery API", version="1.0.0")

# Inicializace syst√©m≈Ø
config = PodcastConfig()
rag_system = PodcastRAGSystem(config)
recommendation_engine = PodcastRecommendationEngine(rag_system)

# API Modely
class SearchRequest(BaseModel):
    query: str
    filters: Optional[Dict[str, Any]] = None

class RecommendationRequest(BaseModel):
    user_id: str
    num_recommendations: int = 10
    exclude_listened: bool = True

class EpisodeAnalysisRequest(BaseModel):
    episode_id: str

@app.post("/episodes/add")
async def add_episode(episode: PodcastEpisode):
    """P≈ôid√°n√≠ nov√© epizody"""
    try:
        rag_system.add_podcast_episode(episode)
        return {"message": "Epizoda √∫spƒõ≈°nƒõ p≈ôid√°na", "episode_id": episode.id}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/search")
async def search_episodes(request: SearchRequest):
    """Vyhled√°v√°n√≠ v epizod√°ch"""
    try:
        results = rag_system.search_episodes(request.query, request.filters)
        return {"results": results, "count": len(results)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/recommendations")
async def get_recommendations(request: RecommendationRequest):
    """Z√≠sk√°n√≠ doporuƒçen√≠ pro u≈æivatele"""
    try:
        recommendations = recommendation_engine.recommend_episodes(
            request.user_id,
            request.num_recommendations,
            request.exclude_listened
        )
        return {"recommendations": recommendations}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/users/profile")
async def create_user_profile(preferences: UserPreferences):
    """Vytvo≈ôen√≠ u≈æivatelsk√©ho profilu"""
    try:
        profile = recommendation_engine.build_user_profile(preferences)
        return {"message": "Profil vytvo≈ôen", "profile": profile}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/episodes/insights")
async def get_episode_insights(request: EpisodeAnalysisRequest):
    """Z√≠sk√°n√≠ insights o epizodƒõ"""
    try:
        insights = rag_system.get_episode_insights(request.episode_id)
        return insights
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy", "service": "Podcast Discovery API"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
````

### Testovac√≠ Script

````python
import asyncio
import requests
import json
from config import PodcastEpisode, UserPreferences

# Testovac√≠ data
sample_episodes = [
    PodcastEpisode(
        id="ep001",
        title="Budoucnost umƒõl√© inteligence",
        description="Diskuse o trendech v AI",
        transcript="V t√©to epizodƒõ diskutujeme o souƒçasn√©m stavu umƒõl√© inteligence a jej√≠ch budouc√≠ch mo≈ænostech. AI m√° potenci√°l zmƒõnit zp≈Øsob, jak√Ωm pracujeme, komunikujeme a ≈æijeme...",
        host_name="Dr. Pavel Nov√°k",
        duration_ms=3600000,
        release_date="2024-01-15",
        categories=["technologie", "ai", "vƒõda"],
        sentiment_score=0.8
    ),
    PodcastEpisode(
        id="ep002",
        title="Investov√°n√≠ v dobƒõ inflace",
        description="Strategie pro ochranu portfolia",
        transcript="Inflace v√Ωznamnƒõ ovliv≈àuje investiƒçn√≠ rozhodov√°n√≠. V t√©to epizodƒõ probereme strategie, jak chr√°nit sv√© √∫spory a investice p≈ôed znehodnocov√°n√≠m...",
        host_name="Ing. Marie Svobodov√°",
        duration_ms=2700000,
        release_date="2024-01-20",
        categories=["finance", "investice", "ekonomika"],
        sentiment_score=0.3
    )
]

sample_user = UserPreferences(
    user_id="user123",
    preferred_categories=["technologie", "ai"],
    preferred_hosts=["Dr. Pavel Nov√°k"],
    listening_history=["ep001"],
    sentiment_preference="positive"
)

def test_system():
    """Testov√°n√≠ cel√©ho syst√©mu"""
    base_url = "http://localhost:8000"
    
    print("üéß Testov√°n√≠ Podcast Discovery Syst√©mu")
    print("=" * 50)
    
    # 1. P≈ôid√°n√≠ epizod
    print("\n1. P≈ôid√°v√°n√≠ testovac√≠ch epizod...")
    for episode in sample_episodes:
        response = requests.post(
            f"{base_url}/episodes/add",
            json=episode.__dict__
        )
        print(f"   ‚úì Epizoda '{episode.title}': {response.status_code}")
    
    # 2. Vytvo≈ôen√≠ u≈æivatelsk√©ho profilu
    print("\n2. Vytv√°≈ôen√≠ u≈æivatelsk√©ho profilu...")
    response = requests.post(
        f"{base_url}/users/profile",
        json=sample_user.__dict__
    )
    print(f"   ‚úì Profil u≈æivatele: {response.status_code}")
    
    # 3. Testov√°n√≠ vyhled√°v√°n√≠
    print("\n3. Testov√°n√≠ vyhled√°v√°n√≠...")
    search_queries = [
        "umƒõl√° inteligence",
        "investov√°n√≠",
        "budoucnost technologi√≠"
    ]
    
    for query in search_queries:
        response = requests.post(
            f"{base_url}/search",
            json={"query": query}
        )
        if response.status_code == 200:
            results = response.json()
            print(f"   ‚úì '{query}': {results['count']} v√Ωsledk≈Ø")
        else:
            print(f"   ‚úó '{query}': Chyba {response.status_code}")
    
    # 4. Testov√°n√≠ doporuƒçen√≠
    print("\n4. Testov√°n√≠ doporuƒçovac√≠ho syst√©mu...")
    response = requests.post(
        f"{base_url}/recommendations",
        json={
            "user_id": "user123",
            "num_recommendations": 5
        }
    )
    
    if response.status_code == 200:
        recommendations = response.json()
        print(f"   ‚úì Generov√°no {len(recommendations['recommendations'])} doporuƒçen√≠")
        for rec in recommendations['recommendations']:
            print(f"      - {rec['title']} (sk√≥re: {rec['recommendation_score']:.2f})")
    else:
        print(f"   ‚úó Chyba p≈ôi generov√°n√≠ doporuƒçen√≠: {response.status_code}")
    
    # 5. Testov√°n√≠ insights
    print("\n5. Testov√°n√≠ anal√Ωzy epizod...")
    response = requests.post(
        f"{base_url}/episodes/insights",
        json={"episode_id": "ep001"}
    )
    
    if response.status_code == 200:
        insights = response.json()
        print(f"   ‚úì Insights pro epizodu: {insights.get('title', 'N/A')}")
    else:
        print(f"   ‚úó Chyba p≈ôi generov√°n√≠ insights: {response.status_code}")
    
    print("\n" + "=" * 50)
    print("‚úÖ Testov√°n√≠ dokonƒçeno!")

if __name__ == "__main__":
    test_system()
````

## 5. Shrnut√≠ Projektu

### Hodnota Projektu

Tento AI-LLM RAG syst√©m pro podcast anal√Ωzu poskytuje:

**üéØ Kl√≠ƒçov√© V√Ωhody:**
- **Inteligentn√≠ Objevov√°n√≠**: S√©mantick√© vyhled√°v√°n√≠ umo≈æ≈àuje naj√≠t relevantn√≠ obsah i bez p≈ôesn√Ωch kl√≠ƒçov√Ωch slov
- **Personalizovan√© Doporuƒçov√°n√≠**: AI-driven algoritmy se uƒç√≠ z u≈æivatelsk√©ho chov√°n√≠ a preferenc√≠
- **Automatick√° Kategorizace**: √öspora ƒçasu p≈ôi organizaci velk√Ωch objem≈Ø podcastov√©ho obsahu
- **Hlub≈°√≠ Insights**: Pokroƒçil√° anal√Ωza sentimentu a t√©mat poskytuje cenn√© informace tv≈Ørc≈Øm i posluchaƒç≈Øm

**üîß Technick√© Inovace:**
- Vyu≈æit√≠ modern√≠ch RAG technik pro kombinaci vyhled√°v√°n√≠ a generov√°n√≠
- ≈†k√°lovateln√° architektura s vektorov√Ωmi datab√°zemi
- Multimod√°ln√≠ p≈ô√≠stup kombinuj√≠c√≠ audio a textov√° data
- Real-time zpracov√°n√≠ nov√©ho obsahu

**üìà Obchodn√≠ Potenci√°l:**
- Zlep≈°en√≠ user experience na podcast platform√°ch
- Efektivnƒõj≈°√≠ monetizace obsahu pro tv≈Ørce
- Pokroƒçil√© analytics pro reklamn√≠ partnery
- Mo≈ænost integrace s existuj√≠c√≠mi platformami

**üöÄ Budouc√≠ Roz≈°√≠≈ôen√≠:**
- Podpora v√≠ce jazyk≈Ø a dialekt≈Ø
- Integrace s dal≈°√≠mi audio platformami
- Anal√Ωza emoc√≠ v hlasu moder√°tor≈Ø
- Predikce trend≈Ø v podcastov√©m obsahu

Projekt demonstruje s√≠lu modern√≠ch AI technologi√≠ v oblasti content discovery a personalizace, poskytuje praktick√© ≈ôe≈°en√≠ pro rostouc√≠ trh podcast≈Ø.