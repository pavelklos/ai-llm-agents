<small>Claude Sonnet 4 **(Multi-Document Research Agent s MCP)**</small>
# Multi-Document Research Agent

## Klíčové koncepty

### Model Context Protocol (MCP)
Protokol pro standardizaci komunikace mezi AI modely a externími nástroji. Umožňuje modelům bezpečně přistupovat k datům a funkcionalitám mimo jejich základní schopnosti.

### LangChain Agents
Framework pro vytváření inteligentních agentů, kteří mohou používat nástroje, pamatovat si kontext a provádět komplexní úkoly řetězením operací.

### Multi-Modal RAG (Retrieval-Augmented Generation)
Technika kombinující vyhledávání relevantních informací z různých typů dokumentů (text, obrázky, tabulky) s generováním odpovědí.

### Context Pruning
Proces optimalizace kontextu odstraněním irelevantních informací pro udržení efektivity a přesnosti modelu.

### Pinecone
Vektorová databáze optimalizovaná pro sémantické vyhledávání a ukládání embeddings s vysokou rychlostí.

### OpenAI Functions
Funkcionality umožňující AI modelům volat externí funkce a nástroje strukturovaným způsobem.

## Komplexní vysvětlení projektu

Multi-Document Research Agent je pokročilý systém umělé inteligence navržený pro analýzu a dotazování napříč různými typy dokumentů. Projekt řeší základní problém moderního informačního věku - schopnost efektivně zpracovat a analyzovat obrovské množství heterogenních dat z různých zdrojů.

### Hlavní cíle:
- **Univerzální analýza dokumentů**: Podpora PDF, DOCX, webových stránek, obrázků s textem
- **Perzistentní paměť**: Dlouhodobé uchovávání kontextu napříč sezeními
- **Inteligentní vyhledávání**: Sémantické porozumění obsahu bez závislosti na klíčových slovech
- **Adaptivní optimalizace**: Automatické řízení velikosti kontextu pro optimální výkon

### Technické výzvy:
- Zpracování různých formátů dokumentů
- Správa velkých objemů vektorových dat
- Optimalizace rychlosti vyhledávání
- Udržení relevantního kontextu
- Škálovatelnost systému

## Komplexní příklad s Python implementací

### Instalace závislostí

```bash
pip install langchain langchain-openai langchain-pinecone langchain-community
pip install pinecone-client openai pymupdf python-docx beautifulsoup4
pip install chromadb sentence-transformers tiktoken
```

### Hlavní implementace

````python
import os
import logging
from typing import List, Dict, Optional, Any
from dataclasses import dataclass
from pathlib import Path
import asyncio
import json

# Core imports
import openai
from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain.memory import ConversationBufferWindowMemory
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.tools import BaseTool
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder

# Document processing
import fitz  # PyMuPDF
from docx import Document as DocxDocument
import requests
from bs4 import BeautifulSoup

# Vector store
import pinecone
from pinecone import Pinecone, ServerlessSpec

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class DocumentMetadata:
    """Metadata pro dokumenty"""
    source: str
    doc_type: str
    chunk_id: int
    total_chunks: int
    upload_timestamp: str

class DocumentProcessor:
    """Zpracování různých typů dokumentů"""
    
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", " ", ""]
        )
    
    async def process_pdf(self, file_path: str) -> List[Document]:
        """Zpracování PDF dokumentu"""
        try:
            doc = fitz.open(file_path)
            text = ""
            for page in doc:
                text += page.get_text()
            doc.close()
            
            chunks = self.text_splitter.split_text(text)
            documents = []
            
            for i, chunk in enumerate(chunks):
                metadata = DocumentMetadata(
                    source=file_path,
                    doc_type="pdf",
                    chunk_id=i,
                    total_chunks=len(chunks),
                    upload_timestamp=str(asyncio.get_event_loop().time())
                ).__dict__
                
                documents.append(Document(page_content=chunk, metadata=metadata))
            
            return documents
            
        except Exception as e:
            logger.error(f"Chyba při zpracování PDF {file_path}: {e}")
            return []
    
    async def process_docx(self, file_path: str) -> List[Document]:
        """Zpracování DOCX dokumentu"""
        try:
            doc = DocxDocument(file_path)
            text = "\n".join([paragraph.text for paragraph in doc.paragraphs])
            
            chunks = self.text_splitter.split_text(text)
            documents = []
            
            for i, chunk in enumerate(chunks):
                metadata = DocumentMetadata(
                    source=file_path,
                    doc_type="docx",
                    chunk_id=i,
                    total_chunks=len(chunks),
                    upload_timestamp=str(asyncio.get_event_loop().time())
                ).__dict__
                
                documents.append(Document(page_content=chunk, metadata=metadata))
            
            return documents
            
        except Exception as e:
            logger.error(f"Chyba při zpracování DOCX {file_path}: {e}")
            return []
    
    async def process_web_page(self, url: str) -> List[Document]:
        """Zpracování webové stránky"""
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            # Odstranění scriptů a stylů
            for script in soup(["script", "style"]):
                script.decompose()
            
            text = soup.get_text()
            # Čištění textu
            lines = (line.strip() for line in text.splitlines())
            text = '\n'.join(line for line in lines if line)
            
            chunks = self.text_splitter.split_text(text)
            documents = []
            
            for i, chunk in enumerate(chunks):
                metadata = DocumentMetadata(
                    source=url,
                    doc_type="web",
                    chunk_id=i,
                    total_chunks=len(chunks),
                    upload_timestamp=str(asyncio.get_event_loop().time())
                ).__dict__
                
                documents.append(Document(page_content=chunk, metadata=metadata))
            
            return documents
            
        except Exception as e:
            logger.error(f"Chyba při zpracování URL {url}: {e}")
            return []

class ContextPruner:
    """Optimalizace kontextu"""
    
    def __init__(self, max_tokens: int = 4000):
        self.max_tokens = max_tokens
    
    def prune_context(self, documents: List[Document], query: str) -> List[Document]:
        """Ořezání kontextu podle relevance"""
        if not documents:
            return documents
        
        # Jednoduché řazení podle relevance (v praxi by se použily pokročilejší metriky)
        scored_docs = []
        query_words = set(query.lower().split())
        
        for doc in documents:
            content_words = set(doc.page_content.lower().split())
            relevance_score = len(query_words.intersection(content_words)) / len(query_words)
            scored_docs.append((relevance_score, doc))
        
        # Řazení podle skóre
        scored_docs.sort(key=lambda x: x[0], reverse=True)
        
        # Výběr nejrelevantnějších dokumentů
        selected_docs = []
        token_count = 0
        
        for score, doc in scored_docs:
            doc_tokens = len(doc.page_content.split())
            if token_count + doc_tokens <= self.max_tokens:
                selected_docs.append(doc)
                token_count += doc_tokens
            else:
                break
        
        return selected_docs

class DocumentSearchTool(BaseTool):
    """Nástroj pro vyhledávání v dokumentech"""
    
    name = "document_search"
    description = "Vyhledává relevantní informace v nahraných dokumentech"
    
    def __init__(self, vector_store: PineconeVectorStore, context_pruner: ContextPruner):
        super().__init__()
        self.vector_store = vector_store
        self.context_pruner = context_pruner
    
    def _run(self, query: str, k: int = 5) -> str:
        """Spuštění vyhledávání"""
        try:
            # Vyhledání podobných dokumentů
            docs = self.vector_store.similarity_search(query, k=k*2)  # Více dokumentů pro pruning
            
            # Ořezání kontextu
            pruned_docs = self.context_pruner.prune_context(docs, query)
            
            if not pruned_docs:
                return "Nenalezeny žádné relevantní dokumenty."
            
            # Formátování výsledků
            results = []
            for doc in pruned_docs:
                metadata = doc.metadata
                result = f"Zdroj: {metadata.get('source', 'Neznámý')}\n"
                result += f"Typ: {metadata.get('doc_type', 'Neznámý')}\n"
                result += f"Obsah: {doc.page_content[:500]}...\n"
                results.append(result)
            
            return "\n" + "="*50 + "\n".join(results)
            
        except Exception as e:
            logger.error(f"Chyba při vyhledávání: {e}")
            return f"Chyba při vyhledávání: {str(e)}"

class MultiDocumentResearchAgent:
    """Hlavní třída research agenta"""
    
    def __init__(self, openai_api_key: str, pinecone_api_key: str):
        self.openai_api_key = openai_api_key
        self.pinecone_api_key = pinecone_api_key
        
        # Inicializace komponent
        self.document_processor = DocumentProcessor()
        self.context_pruner = ContextPruner()
        
        # Setup OpenAI
        self.llm = ChatOpenAI(
            api_key=openai_api_key,
            model="gpt-4",
            temperature=0.7
        )
        
        self.embeddings = OpenAIEmbeddings(api_key=openai_api_key)
        
        # Setup Pinecone
        self.setup_pinecone()
        
        # Setup agent
        self.setup_agent()
    
    def setup_pinecone(self):
        """Nastavení Pinecone vektorové databáze"""
        try:
            pc = Pinecone(api_key=self.pinecone_api_key)
            
            index_name = "research-agent-index"
            
            # Vytvoření indexu pokud neexistuje
            if index_name not in pc.list_indexes().names():
                pc.create_index(
                    name=index_name,
                    dimension=1536,  # OpenAI embeddings dimenze
                    metric="cosine",
                    spec=ServerlessSpec(
                        cloud='aws',
                        region='us-east-1'
                    )
                )
            
            # Připojení k vector store
            self.vector_store = PineconeVectorStore(
                index_name=index_name,
                embedding=self.embeddings,
                pinecone_api_key=self.pinecone_api_key
            )
            
        except Exception as e:
            logger.error(f"Chyba při nastavení Pinecone: {e}")
            raise
    
    def setup_agent(self):
        """Nastavení LangChain agenta"""
        # Nástroje
        tools = [
            DocumentSearchTool(self.vector_store, self.context_pruner)
        ]
        
        # Paměť
        self.memory = ConversationBufferWindowMemory(
            memory_key="chat_history",
            return_messages=True,
            k=10  # Posledních 10 zpráv
        )
        
        # Prompt
        prompt = ChatPromptTemplate.from_messages([
            ("system", """Jsi pokročilý research agent specializovaný na analýzu dokumentů. 
            Umíš pracovat s různými typy dokumentů a poskytovat přesné, dobře podložené odpovědi.
            
            Klíčové schopnosti:
            - Vyhledávání v nahraných dokumentech
            - Analýza a syntéza informací z více zdrojů
            - Poskytování citací a odkazů na zdroje
            - Rozpoznání kdy nemáš dostatek informací
            
            Vždy cituj zdroje svých informací a buď transparentní ohledně omezení."""),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad")
        ])
        
        # Vytvoření agenta
        agent = create_openai_functions_agent(
            llm=self.llm,
            tools=tools,
            prompt=prompt
        )
        
        self.agent_executor = AgentExecutor(
            agent=agent,
            tools=tools,
            memory=self.memory,
            verbose=True,
            max_iterations=5
        )
    
    async def upload_document(self, file_path: str) -> bool:
        """Nahrání dokumentu do systému"""
        try:
            file_path = Path(file_path)
            
            if not file_path.exists():
                logger.error(f"Soubor neexistuje: {file_path}")
                return False
            
            # Zpracování podle typu souboru
            if file_path.suffix.lower() == '.pdf':
                documents = await self.document_processor.process_pdf(str(file_path))
            elif file_path.suffix.lower() == '.docx':
                documents = await self.document_processor.process_docx(str(file_path))
            else:
                logger.error(f"Nepodporovaný typ souboru: {file_path.suffix}")
                return False
            
            if not documents:
                logger.error("Nepodařilo se zpracovat dokument")
                return False
            
            # Uložení do vector store
            self.vector_store.add_documents(documents)
            logger.info(f"Úspěšně nahráno {len(documents)} chunků z {file_path}")
            return True
            
        except Exception as e:
            logger.error(f"Chyba při nahrávání dokumentu: {e}")
            return False
    
    async def upload_web_page(self, url: str) -> bool:
        """Nahrání webové stránky"""
        try:
            documents = await self.document_processor.process_web_page(url)
            
            if not documents:
                logger.error("Nepodařilo se zpracovat webovou stránku")
                return False
            
            # Uložení do vector store
            self.vector_store.add_documents(documents)
            logger.info(f"Úspěšně nahráno {len(documents)} chunků z {url}")
            return True
            
        except Exception as e:
            logger.error(f"Chyba při nahrávání webové stránky: {e}")
            return False
    
    async def query(self, question: str) -> str:
        """Dotaz na agenta"""
        try:
            response = await self.agent_executor.ainvoke({"input": question})
            return response["output"]
        except Exception as e:
            logger.error(f"Chyba při dotazu: {e}")
            return f"Chyba při zpracování dotazu: {str(e)}"
    
    def get_memory_summary(self) -> Dict[str, Any]:
        """Získání shrnutí paměti"""
        return {
            "conversation_length": len(self.memory.chat_memory.messages),
            "recent_messages": [msg.content for msg in self.memory.chat_memory.messages[-3:]]
        }

# Demo použití
async def main():
    """Demonstrace použití research agenta"""
    
    # Inicializace (vyžaduje API klíče)
    OPENAI_API_KEY = "your-openai-api-key"
    PINECONE_API_KEY = "your-pinecone-api-key"
    
    # Vytvoření ukázkových dat
    sample_pdf_content = """
    Umělá inteligence v roce 2024
    
    Umělá inteligence (AI) zažívá v roce 2024 bezprecedentní rozvoj. Velké jazykové modely 
    dosahují nových milníků v porozumění přirozenému jazyku a generování textu.
    
    Klíčové trendy:
    1. Multimodální AI systémy
    2. Specializované agenty pro konkrétní úkoly
    3. Lepší integrace s externími nástroji
    4. Zvýšená pozornost na bezpečnost AI
    
    Podle průzkumů očekává 78% firem významné investice do AI technologií v následujících
    dvou letech. Největší nárůst se očekává v oblastech automatizace podnikových procesů
    a analýzy dat.
    """
    
    # Vytvoření ukázkového PDF
    import io
    import fitz
    
    # Pouze pro demonstraci - v praxi byste použili skutečné soubory
    print("🚀 Inicializace Multi-Document Research Agenta...")
    
    try:
        agent = MultiDocumentResearchAgent(OPENAI_API_KEY, PINECONE_API_KEY)
        
        # Simulace nahrání dokumentu (v praxi použijte skutečné cesty)
        print("📄 Nahrávání dokumentů...")
        
        # Nahrání webové stránky (příklad)
        await agent.upload_web_page("https://cs.wikipedia.org/wiki/Um%C4%9Bl%C3%A1_inteligence")
        
        # Dotazy
        print("\n💬 Testování dotazů...")
        
        questions = [
            "Co je umělá inteligence?",
            "Jaké jsou hlavní trendy v AI pro rok 2024?",
            "Jak se AI využívá v podnikání?"
        ]
        
        for question in questions:
            print(f"\n❓ Otázka: {question}")
            answer = await agent.query(question)
            print(f"💡 Odpověď: {answer}")
        
        # Zobrazení stavu paměti
        memory_info = agent.get_memory_summary()
        print(f"\n🧠 Stav paměti: {memory_info}")
        
    except Exception as e:
        print(f"❌ Chyba při spuštění demo: {e}")
        print("💡 Pro spuštění je potřeba nastavit platné API klíče")

if __name__ == "__main__":
    asyncio.run(main())
````

### Konfigurační soubor

````python
import os
from typing import Dict, Any

class Config:
    """Konfigurace aplikace"""
    
    # API klíče
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
    
    # Pinecone nastavení
    PINECONE_INDEX_NAME = "research-agent-index"
    PINECONE_DIMENSION = 1536
    PINECONE_METRIC = "cosine"
    
    # Chunking nastavení
    CHUNK_SIZE = 1000
    CHUNK_OVERLAP = 200
    
    # Context nastavení
    MAX_CONTEXT_TOKENS = 4000
    MEMORY_WINDOW_SIZE = 10
    
    # Vyhledávání
    DEFAULT_SEARCH_K = 5
    
    # Podporované formáty
    SUPPORTED_EXTENSIONS = ['.pdf', '.docx', '.txt']
    
    @classmethod
    def validate(cls) -> bool:
        """Validace konfigurace"""
        required_keys = ['OPENAI_API_KEY', 'PINECONE_API_KEY']
        missing_keys = [key for key in required_keys if not getattr(cls, key)]
        
        if missing_keys:
            print(f"❌ Chybí povinné konfigurační klíče: {missing_keys}")
            return False
        
        return True
````

### Testovací soubor

````python
import pytest
import asyncio
from unittest.mock import Mock, patch
from research_agent import MultiDocumentResearchAgent, DocumentProcessor, ContextPruner

class TestDocumentProcessor:
    """Testy pro zpracování dokumentů"""
    
    @pytest.fixture
    def processor(self):
        return DocumentProcessor()
    
    def test_text_splitter_initialization(self, processor):
        """Test inicializace text splitteru"""
        assert processor.text_splitter.chunk_size == 1000
        assert processor.text_splitter.chunk_overlap == 200
    
    @pytest.mark.asyncio
    async def test_process_web_page_success(self, processor):
        """Test úspěšného zpracování webové stránky"""
        with patch('requests.get') as mock_get:
            mock_response = Mock()
            mock_response.content = b"<html><body><h1>Test</h1><p>Obsah stránky</p></body></html>"
            mock_response.raise_for_status = Mock()
            mock_get.return_value = mock_response
            
            documents = await processor.process_web_page("https://example.com")
            
            assert len(documents) > 0
            assert "Test" in documents[0].page_content
            assert documents[0].metadata['doc_type'] == 'web'

class TestContextPruner:
    """Testy pro ořezávání kontextu"""
    
    @pytest.fixture
    def pruner(self):
        return ContextPruner(max_tokens=100)
    
    def test_prune_context_relevance(self, pruner):
        """Test ořezávání podle relevance"""
        from langchain.schema import Document
        
        docs = [
            Document(page_content="Python programming language", metadata={}),
            Document(page_content="Java programming tutorial", metadata={}),
            Document(page_content="Cooking recipes for dinner", metadata={})
        ]
        
        query = "Python programming"
        pruned = pruner.prune_context(docs, query)
        
        assert len(pruned) > 0
        assert "Python" in pruned[0].page_content

def test_config_validation():
    """Test validace konfigurace"""
    from config import Config
    
    # Mock environment variables
    with patch.dict('os.environ', {
        'OPENAI_API_KEY': 'test-key',
        'PINECONE_API_KEY': 'test-key'
    }):
        # Reload config to pick up mocked env vars
        import importlib
        import config
        importlib.reload(config)
        
        assert config.Config.validate() == True

if __name__ == "__main__":
    pytest.main([__file__])
````

### Requirements soubor

````txt
langchain>=0.1.0
langchain-openai>=0.0.5
langchain-pinecone>=0.0.3
langchain-community>=0.0.10
openai>=1.12.0
pinecone-client>=3.0.0
pymupdf>=1.23.0
python-docx>=0.8.11
beautifulsoup4>=4.12.0
requests>=2.31.0
tiktoken>=0.5.0
pytest>=7.4.0
pytest-asyncio>=0.21.0
python-dotenv>=1.0.0
````

### Docker konfigurace

````dockerfile
FROM python:3.11-slim

WORKDIR /app

# Instalace systémových závislostí
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# Kopírování requirements
COPY requirements.txt .

# Instalace Python závislostí
RUN pip install --no-cache-dir -r requirements.txt

# Kopírování aplikace
COPY . .

# Nastavení environment
ENV PYTHONPATH=/app

# Spuštění
CMD ["python", "research_agent.py"]
````

## Shrnutí projektu

Multi-Document Research Agent představuje pokročilé řešení pro automatizovanou analýzu a dotazování dokumentů. Projekt kombinuje nejmodernější technologie AI pro vytvoření inteligentního systému schopného:

### Klíčové hodnoty:
- **Univerzálnost**: Podpora různých formátů dokumentů a zdrojů
- **Inteligence**: Sémantické porozumění obsahu bez závislosti na klíčových slovech  
- **Škálovatelnost**: Optimalizace pro práci s velkými objemy dat
- **Perzistence**: Dlouhodobá paměť a kontext napříč sezeními
- **Přesnost**: Pokročilé techniky pro relevantní vyhledávání a odpovědi

### Praktické využití:
- Akademický výzkum a analýza literatury
- Podnikové knowledge management systémy
- Právní analýza dokumentů a smluv
- Novinářské investigace a fact-checking
- Technická dokumentace a manuály

Projekt demonstruje sílu kombinace moderních AI frameworků pro vytvoření praktického, škálovatelného řešení s reálnou hodnotou pro uživatele pracující s velkými objemy informací.