<small>Claude Sonnet 4 **(Multi-Document Research Agent s MCP)**</small>
# Multi-Document Research Agent

## Kl√≠ƒçov√© koncepty

### Model Context Protocol (MCP)
Protokol pro standardizaci komunikace mezi AI modely a extern√≠mi n√°stroji. Umo≈æ≈àuje model≈Øm bezpeƒçnƒõ p≈ôistupovat k dat≈Øm a funkcionalit√°m mimo jejich z√°kladn√≠ schopnosti.

### LangChain Agents
Framework pro vytv√°≈ôen√≠ inteligentn√≠ch agent≈Ø, kte≈ô√≠ mohou pou≈æ√≠vat n√°stroje, pamatovat si kontext a prov√°dƒõt komplexn√≠ √∫koly ≈ôetƒõzen√≠m operac√≠.

### Multi-Modal RAG (Retrieval-Augmented Generation)
Technika kombinuj√≠c√≠ vyhled√°v√°n√≠ relevantn√≠ch informac√≠ z r≈Øzn√Ωch typ≈Ø dokument≈Ø (text, obr√°zky, tabulky) s generov√°n√≠m odpovƒõd√≠.

### Context Pruning
Proces optimalizace kontextu odstranƒõn√≠m irelevantn√≠ch informac√≠ pro udr≈æen√≠ efektivity a p≈ôesnosti modelu.

### Pinecone
Vektorov√° datab√°ze optimalizovan√° pro s√©mantick√© vyhled√°v√°n√≠ a ukl√°d√°n√≠ embeddings s vysokou rychlost√≠.

### OpenAI Functions
Funkcionality umo≈æ≈àuj√≠c√≠ AI model≈Øm volat extern√≠ funkce a n√°stroje strukturovan√Ωm zp≈Øsobem.

## Komplexn√≠ vysvƒõtlen√≠ projektu

Multi-Document Research Agent je pokroƒçil√Ω syst√©m umƒõl√© inteligence navr≈æen√Ω pro anal√Ωzu a dotazov√°n√≠ nap≈ô√≠ƒç r≈Øzn√Ωmi typy dokument≈Ø. Projekt ≈ôe≈°√≠ z√°kladn√≠ probl√©m modern√≠ho informaƒçn√≠ho vƒõku - schopnost efektivnƒõ zpracovat a analyzovat obrovsk√© mno≈æstv√≠ heterogenn√≠ch dat z r≈Øzn√Ωch zdroj≈Ø.

### Hlavn√≠ c√≠le:
- **Univerz√°ln√≠ anal√Ωza dokument≈Ø**: Podpora PDF, DOCX, webov√Ωch str√°nek, obr√°zk≈Ø s textem
- **Perzistentn√≠ pamƒõ≈•**: Dlouhodob√© uchov√°v√°n√≠ kontextu nap≈ô√≠ƒç sezen√≠mi
- **Inteligentn√≠ vyhled√°v√°n√≠**: S√©mantick√© porozumƒõn√≠ obsahu bez z√°vislosti na kl√≠ƒçov√Ωch slovech
- **Adaptivn√≠ optimalizace**: Automatick√© ≈ô√≠zen√≠ velikosti kontextu pro optim√°ln√≠ v√Ωkon

### Technick√© v√Ωzvy:
- Zpracov√°n√≠ r≈Øzn√Ωch form√°t≈Ø dokument≈Ø
- Spr√°va velk√Ωch objem≈Ø vektorov√Ωch dat
- Optimalizace rychlosti vyhled√°v√°n√≠
- Udr≈æen√≠ relevantn√≠ho kontextu
- ≈†k√°lovatelnost syst√©mu

## Komplexn√≠ p≈ô√≠klad s Python implementac√≠

### Instalace z√°vislost√≠

```bash
pip install langchain langchain-openai langchain-pinecone langchain-community
pip install pinecone-client openai pymupdf python-docx beautifulsoup4
pip install chromadb sentence-transformers tiktoken
```

### Hlavn√≠ implementace

````python
import os
import logging
from typing import List, Dict, Optional, Any
from dataclasses import dataclass
from pathlib import Path
import asyncio
import json

# Core imports
import openai
from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain.memory import ConversationBufferWindowMemory
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.tools import BaseTool
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder

# Document processing
import fitz  # PyMuPDF
from docx import Document as DocxDocument
import requests
from bs4 import BeautifulSoup

# Vector store
import pinecone
from pinecone import Pinecone, ServerlessSpec

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class DocumentMetadata:
    """Metadata pro dokumenty"""
    source: str
    doc_type: str
    chunk_id: int
    total_chunks: int
    upload_timestamp: str

class DocumentProcessor:
    """Zpracov√°n√≠ r≈Øzn√Ωch typ≈Ø dokument≈Ø"""
    
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", " ", ""]
        )
    
    async def process_pdf(self, file_path: str) -> List[Document]:
        """Zpracov√°n√≠ PDF dokumentu"""
        try:
            doc = fitz.open(file_path)
            text = ""
            for page in doc:
                text += page.get_text()
            doc.close()
            
            chunks = self.text_splitter.split_text(text)
            documents = []
            
            for i, chunk in enumerate(chunks):
                metadata = DocumentMetadata(
                    source=file_path,
                    doc_type="pdf",
                    chunk_id=i,
                    total_chunks=len(chunks),
                    upload_timestamp=str(asyncio.get_event_loop().time())
                ).__dict__
                
                documents.append(Document(page_content=chunk, metadata=metadata))
            
            return documents
            
        except Exception as e:
            logger.error(f"Chyba p≈ôi zpracov√°n√≠ PDF {file_path}: {e}")
            return []
    
    async def process_docx(self, file_path: str) -> List[Document]:
        """Zpracov√°n√≠ DOCX dokumentu"""
        try:
            doc = DocxDocument(file_path)
            text = "\n".join([paragraph.text for paragraph in doc.paragraphs])
            
            chunks = self.text_splitter.split_text(text)
            documents = []
            
            for i, chunk in enumerate(chunks):
                metadata = DocumentMetadata(
                    source=file_path,
                    doc_type="docx",
                    chunk_id=i,
                    total_chunks=len(chunks),
                    upload_timestamp=str(asyncio.get_event_loop().time())
                ).__dict__
                
                documents.append(Document(page_content=chunk, metadata=metadata))
            
            return documents
            
        except Exception as e:
            logger.error(f"Chyba p≈ôi zpracov√°n√≠ DOCX {file_path}: {e}")
            return []
    
    async def process_web_page(self, url: str) -> List[Document]:
        """Zpracov√°n√≠ webov√© str√°nky"""
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            # Odstranƒõn√≠ script≈Ø a styl≈Ø
            for script in soup(["script", "style"]):
                script.decompose()
            
            text = soup.get_text()
            # ƒåi≈°tƒõn√≠ textu
            lines = (line.strip() for line in text.splitlines())
            text = '\n'.join(line for line in lines if line)
            
            chunks = self.text_splitter.split_text(text)
            documents = []
            
            for i, chunk in enumerate(chunks):
                metadata = DocumentMetadata(
                    source=url,
                    doc_type="web",
                    chunk_id=i,
                    total_chunks=len(chunks),
                    upload_timestamp=str(asyncio.get_event_loop().time())
                ).__dict__
                
                documents.append(Document(page_content=chunk, metadata=metadata))
            
            return documents
            
        except Exception as e:
            logger.error(f"Chyba p≈ôi zpracov√°n√≠ URL {url}: {e}")
            return []

class ContextPruner:
    """Optimalizace kontextu"""
    
    def __init__(self, max_tokens: int = 4000):
        self.max_tokens = max_tokens
    
    def prune_context(self, documents: List[Document], query: str) -> List[Document]:
        """O≈ôez√°n√≠ kontextu podle relevance"""
        if not documents:
            return documents
        
        # Jednoduch√© ≈ôazen√≠ podle relevance (v praxi by se pou≈æily pokroƒçilej≈°√≠ metriky)
        scored_docs = []
        query_words = set(query.lower().split())
        
        for doc in documents:
            content_words = set(doc.page_content.lower().split())
            relevance_score = len(query_words.intersection(content_words)) / len(query_words)
            scored_docs.append((relevance_score, doc))
        
        # ≈òazen√≠ podle sk√≥re
        scored_docs.sort(key=lambda x: x[0], reverse=True)
        
        # V√Ωbƒõr nejrelevantnƒõj≈°√≠ch dokument≈Ø
        selected_docs = []
        token_count = 0
        
        for score, doc in scored_docs:
            doc_tokens = len(doc.page_content.split())
            if token_count + doc_tokens <= self.max_tokens:
                selected_docs.append(doc)
                token_count += doc_tokens
            else:
                break
        
        return selected_docs

class DocumentSearchTool(BaseTool):
    """N√°stroj pro vyhled√°v√°n√≠ v dokumentech"""
    
    name = "document_search"
    description = "Vyhled√°v√° relevantn√≠ informace v nahran√Ωch dokumentech"
    
    def __init__(self, vector_store: PineconeVectorStore, context_pruner: ContextPruner):
        super().__init__()
        self.vector_store = vector_store
        self.context_pruner = context_pruner
    
    def _run(self, query: str, k: int = 5) -> str:
        """Spu≈°tƒõn√≠ vyhled√°v√°n√≠"""
        try:
            # Vyhled√°n√≠ podobn√Ωch dokument≈Ø
            docs = self.vector_store.similarity_search(query, k=k*2)  # V√≠ce dokument≈Ø pro pruning
            
            # O≈ôez√°n√≠ kontextu
            pruned_docs = self.context_pruner.prune_context(docs, query)
            
            if not pruned_docs:
                return "Nenalezeny ≈æ√°dn√© relevantn√≠ dokumenty."
            
            # Form√°tov√°n√≠ v√Ωsledk≈Ø
            results = []
            for doc in pruned_docs:
                metadata = doc.metadata
                result = f"Zdroj: {metadata.get('source', 'Nezn√°m√Ω')}\n"
                result += f"Typ: {metadata.get('doc_type', 'Nezn√°m√Ω')}\n"
                result += f"Obsah: {doc.page_content[:500]}...\n"
                results.append(result)
            
            return "\n" + "="*50 + "\n".join(results)
            
        except Exception as e:
            logger.error(f"Chyba p≈ôi vyhled√°v√°n√≠: {e}")
            return f"Chyba p≈ôi vyhled√°v√°n√≠: {str(e)}"

class MultiDocumentResearchAgent:
    """Hlavn√≠ t≈ô√≠da research agenta"""
    
    def __init__(self, openai_api_key: str, pinecone_api_key: str):
        self.openai_api_key = openai_api_key
        self.pinecone_api_key = pinecone_api_key
        
        # Inicializace komponent
        self.document_processor = DocumentProcessor()
        self.context_pruner = ContextPruner()
        
        # Setup OpenAI
        self.llm = ChatOpenAI(
            api_key=openai_api_key,
            model="gpt-4",
            temperature=0.7
        )
        
        self.embeddings = OpenAIEmbeddings(api_key=openai_api_key)
        
        # Setup Pinecone
        self.setup_pinecone()
        
        # Setup agent
        self.setup_agent()
    
    def setup_pinecone(self):
        """Nastaven√≠ Pinecone vektorov√© datab√°ze"""
        try:
            pc = Pinecone(api_key=self.pinecone_api_key)
            
            index_name = "research-agent-index"
            
            # Vytvo≈ôen√≠ indexu pokud neexistuje
            if index_name not in pc.list_indexes().names():
                pc.create_index(
                    name=index_name,
                    dimension=1536,  # OpenAI embeddings dimenze
                    metric="cosine",
                    spec=ServerlessSpec(
                        cloud='aws',
                        region='us-east-1'
                    )
                )
            
            # P≈ôipojen√≠ k vector store
            self.vector_store = PineconeVectorStore(
                index_name=index_name,
                embedding=self.embeddings,
                pinecone_api_key=self.pinecone_api_key
            )
            
        except Exception as e:
            logger.error(f"Chyba p≈ôi nastaven√≠ Pinecone: {e}")
            raise
    
    def setup_agent(self):
        """Nastaven√≠ LangChain agenta"""
        # N√°stroje
        tools = [
            DocumentSearchTool(self.vector_store, self.context_pruner)
        ]
        
        # Pamƒõ≈•
        self.memory = ConversationBufferWindowMemory(
            memory_key="chat_history",
            return_messages=True,
            k=10  # Posledn√≠ch 10 zpr√°v
        )
        
        # Prompt
        prompt = ChatPromptTemplate.from_messages([
            ("system", """Jsi pokroƒçil√Ω research agent specializovan√Ω na anal√Ωzu dokument≈Ø. 
            Um√≠≈° pracovat s r≈Øzn√Ωmi typy dokument≈Ø a poskytovat p≈ôesn√©, dob≈ôe podlo≈æen√© odpovƒõdi.
            
            Kl√≠ƒçov√© schopnosti:
            - Vyhled√°v√°n√≠ v nahran√Ωch dokumentech
            - Anal√Ωza a synt√©za informac√≠ z v√≠ce zdroj≈Ø
            - Poskytov√°n√≠ citac√≠ a odkaz≈Ø na zdroje
            - Rozpozn√°n√≠ kdy nem√°≈° dostatek informac√≠
            
            V≈ædy cituj zdroje sv√Ωch informac√≠ a buƒè transparentn√≠ ohlednƒõ omezen√≠."""),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad")
        ])
        
        # Vytvo≈ôen√≠ agenta
        agent = create_openai_functions_agent(
            llm=self.llm,
            tools=tools,
            prompt=prompt
        )
        
        self.agent_executor = AgentExecutor(
            agent=agent,
            tools=tools,
            memory=self.memory,
            verbose=True,
            max_iterations=5
        )
    
    async def upload_document(self, file_path: str) -> bool:
        """Nahr√°n√≠ dokumentu do syst√©mu"""
        try:
            file_path = Path(file_path)
            
            if not file_path.exists():
                logger.error(f"Soubor neexistuje: {file_path}")
                return False
            
            # Zpracov√°n√≠ podle typu souboru
            if file_path.suffix.lower() == '.pdf':
                documents = await self.document_processor.process_pdf(str(file_path))
            elif file_path.suffix.lower() == '.docx':
                documents = await self.document_processor.process_docx(str(file_path))
            else:
                logger.error(f"Nepodporovan√Ω typ souboru: {file_path.suffix}")
                return False
            
            if not documents:
                logger.error("Nepoda≈ôilo se zpracovat dokument")
                return False
            
            # Ulo≈æen√≠ do vector store
            self.vector_store.add_documents(documents)
            logger.info(f"√öspƒõ≈°nƒõ nahr√°no {len(documents)} chunk≈Ø z {file_path}")
            return True
            
        except Exception as e:
            logger.error(f"Chyba p≈ôi nahr√°v√°n√≠ dokumentu: {e}")
            return False
    
    async def upload_web_page(self, url: str) -> bool:
        """Nahr√°n√≠ webov√© str√°nky"""
        try:
            documents = await self.document_processor.process_web_page(url)
            
            if not documents:
                logger.error("Nepoda≈ôilo se zpracovat webovou str√°nku")
                return False
            
            # Ulo≈æen√≠ do vector store
            self.vector_store.add_documents(documents)
            logger.info(f"√öspƒõ≈°nƒõ nahr√°no {len(documents)} chunk≈Ø z {url}")
            return True
            
        except Exception as e:
            logger.error(f"Chyba p≈ôi nahr√°v√°n√≠ webov√© str√°nky: {e}")
            return False
    
    async def query(self, question: str) -> str:
        """Dotaz na agenta"""
        try:
            response = await self.agent_executor.ainvoke({"input": question})
            return response["output"]
        except Exception as e:
            logger.error(f"Chyba p≈ôi dotazu: {e}")
            return f"Chyba p≈ôi zpracov√°n√≠ dotazu: {str(e)}"
    
    def get_memory_summary(self) -> Dict[str, Any]:
        """Z√≠sk√°n√≠ shrnut√≠ pamƒõti"""
        return {
            "conversation_length": len(self.memory.chat_memory.messages),
            "recent_messages": [msg.content for msg in self.memory.chat_memory.messages[-3:]]
        }

# Demo pou≈æit√≠
async def main():
    """Demonstrace pou≈æit√≠ research agenta"""
    
    # Inicializace (vy≈æaduje API kl√≠ƒçe)
    OPENAI_API_KEY = "your-openai-api-key"
    PINECONE_API_KEY = "your-pinecone-api-key"
    
    # Vytvo≈ôen√≠ uk√°zkov√Ωch dat
    sample_pdf_content = """
    Umƒõl√° inteligence v roce 2024
    
    Umƒõl√° inteligence (AI) za≈æ√≠v√° v roce 2024 bezprecedentn√≠ rozvoj. Velk√© jazykov√© modely 
    dosahuj√≠ nov√Ωch miln√≠k≈Ø v porozumƒõn√≠ p≈ôirozen√©mu jazyku a generov√°n√≠ textu.
    
    Kl√≠ƒçov√© trendy:
    1. Multimod√°ln√≠ AI syst√©my
    2. Specializovan√© agenty pro konkr√©tn√≠ √∫koly
    3. Lep≈°√≠ integrace s extern√≠mi n√°stroji
    4. Zv√Ω≈°en√° pozornost na bezpeƒçnost AI
    
    Podle pr≈Øzkum≈Ø oƒçek√°v√° 78% firem v√Ωznamn√© investice do AI technologi√≠ v n√°sleduj√≠c√≠ch
    dvou letech. Nejvƒõt≈°√≠ n√°r≈Øst se oƒçek√°v√° v oblastech automatizace podnikov√Ωch proces≈Ø
    a anal√Ωzy dat.
    """
    
    # Vytvo≈ôen√≠ uk√°zkov√©ho PDF
    import io
    import fitz
    
    # Pouze pro demonstraci - v praxi byste pou≈æili skuteƒçn√© soubory
    print("üöÄ Inicializace Multi-Document Research Agenta...")
    
    try:
        agent = MultiDocumentResearchAgent(OPENAI_API_KEY, PINECONE_API_KEY)
        
        # Simulace nahr√°n√≠ dokumentu (v praxi pou≈æijte skuteƒçn√© cesty)
        print("üìÑ Nahr√°v√°n√≠ dokument≈Ø...")
        
        # Nahr√°n√≠ webov√© str√°nky (p≈ô√≠klad)
        await agent.upload_web_page("https://cs.wikipedia.org/wiki/Um%C4%9Bl%C3%A1_inteligence")
        
        # Dotazy
        print("\nüí¨ Testov√°n√≠ dotaz≈Ø...")
        
        questions = [
            "Co je umƒõl√° inteligence?",
            "Jak√© jsou hlavn√≠ trendy v AI pro rok 2024?",
            "Jak se AI vyu≈æ√≠v√° v podnik√°n√≠?"
        ]
        
        for question in questions:
            print(f"\n‚ùì Ot√°zka: {question}")
            answer = await agent.query(question)
            print(f"üí° Odpovƒõƒè: {answer}")
        
        # Zobrazen√≠ stavu pamƒõti
        memory_info = agent.get_memory_summary()
        print(f"\nüß† Stav pamƒõti: {memory_info}")
        
    except Exception as e:
        print(f"‚ùå Chyba p≈ôi spu≈°tƒõn√≠ demo: {e}")
        print("üí° Pro spu≈°tƒõn√≠ je pot≈ôeba nastavit platn√© API kl√≠ƒçe")

if __name__ == "__main__":
    asyncio.run(main())
````

### Konfiguraƒçn√≠ soubor

````python
import os
from typing import Dict, Any

class Config:
    """Konfigurace aplikace"""
    
    # API kl√≠ƒçe
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
    
    # Pinecone nastaven√≠
    PINECONE_INDEX_NAME = "research-agent-index"
    PINECONE_DIMENSION = 1536
    PINECONE_METRIC = "cosine"
    
    # Chunking nastaven√≠
    CHUNK_SIZE = 1000
    CHUNK_OVERLAP = 200
    
    # Context nastaven√≠
    MAX_CONTEXT_TOKENS = 4000
    MEMORY_WINDOW_SIZE = 10
    
    # Vyhled√°v√°n√≠
    DEFAULT_SEARCH_K = 5
    
    # Podporovan√© form√°ty
    SUPPORTED_EXTENSIONS = ['.pdf', '.docx', '.txt']
    
    @classmethod
    def validate(cls) -> bool:
        """Validace konfigurace"""
        required_keys = ['OPENAI_API_KEY', 'PINECONE_API_KEY']
        missing_keys = [key for key in required_keys if not getattr(cls, key)]
        
        if missing_keys:
            print(f"‚ùå Chyb√≠ povinn√© konfiguraƒçn√≠ kl√≠ƒçe: {missing_keys}")
            return False
        
        return True
````

### Testovac√≠ soubor

````python
import pytest
import asyncio
from unittest.mock import Mock, patch
from research_agent import MultiDocumentResearchAgent, DocumentProcessor, ContextPruner

class TestDocumentProcessor:
    """Testy pro zpracov√°n√≠ dokument≈Ø"""
    
    @pytest.fixture
    def processor(self):
        return DocumentProcessor()
    
    def test_text_splitter_initialization(self, processor):
        """Test inicializace text splitteru"""
        assert processor.text_splitter.chunk_size == 1000
        assert processor.text_splitter.chunk_overlap == 200
    
    @pytest.mark.asyncio
    async def test_process_web_page_success(self, processor):
        """Test √∫spƒõ≈°n√©ho zpracov√°n√≠ webov√© str√°nky"""
        with patch('requests.get') as mock_get:
            mock_response = Mock()
            mock_response.content = b"<html><body><h1>Test</h1><p>Obsah str√°nky</p></body></html>"
            mock_response.raise_for_status = Mock()
            mock_get.return_value = mock_response
            
            documents = await processor.process_web_page("https://example.com")
            
            assert len(documents) > 0
            assert "Test" in documents[0].page_content
            assert documents[0].metadata['doc_type'] == 'web'

class TestContextPruner:
    """Testy pro o≈ôez√°v√°n√≠ kontextu"""
    
    @pytest.fixture
    def pruner(self):
        return ContextPruner(max_tokens=100)
    
    def test_prune_context_relevance(self, pruner):
        """Test o≈ôez√°v√°n√≠ podle relevance"""
        from langchain.schema import Document
        
        docs = [
            Document(page_content="Python programming language", metadata={}),
            Document(page_content="Java programming tutorial", metadata={}),
            Document(page_content="Cooking recipes for dinner", metadata={})
        ]
        
        query = "Python programming"
        pruned = pruner.prune_context(docs, query)
        
        assert len(pruned) > 0
        assert "Python" in pruned[0].page_content

def test_config_validation():
    """Test validace konfigurace"""
    from config import Config
    
    # Mock environment variables
    with patch.dict('os.environ', {
        'OPENAI_API_KEY': 'test-key',
        'PINECONE_API_KEY': 'test-key'
    }):
        # Reload config to pick up mocked env vars
        import importlib
        import config
        importlib.reload(config)
        
        assert config.Config.validate() == True

if __name__ == "__main__":
    pytest.main([__file__])
````

### Requirements soubor

````txt
langchain>=0.1.0
langchain-openai>=0.0.5
langchain-pinecone>=0.0.3
langchain-community>=0.0.10
openai>=1.12.0
pinecone-client>=3.0.0
pymupdf>=1.23.0
python-docx>=0.8.11
beautifulsoup4>=4.12.0
requests>=2.31.0
tiktoken>=0.5.0
pytest>=7.4.0
pytest-asyncio>=0.21.0
python-dotenv>=1.0.0
````

### Docker konfigurace

````dockerfile
FROM python:3.11-slim

WORKDIR /app

# Instalace syst√©mov√Ωch z√°vislost√≠
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# Kop√≠rov√°n√≠ requirements
COPY requirements.txt .

# Instalace Python z√°vislost√≠
RUN pip install --no-cache-dir -r requirements.txt

# Kop√≠rov√°n√≠ aplikace
COPY . .

# Nastaven√≠ environment
ENV PYTHONPATH=/app

# Spu≈°tƒõn√≠
CMD ["python", "research_agent.py"]
````

## Shrnut√≠ projektu

Multi-Document Research Agent p≈ôedstavuje pokroƒçil√© ≈ôe≈°en√≠ pro automatizovanou anal√Ωzu a dotazov√°n√≠ dokument≈Ø. Projekt kombinuje nejmodernƒõj≈°√≠ technologie AI pro vytvo≈ôen√≠ inteligentn√≠ho syst√©mu schopn√©ho:

### Kl√≠ƒçov√© hodnoty:
- **Univerz√°lnost**: Podpora r≈Øzn√Ωch form√°t≈Ø dokument≈Ø a zdroj≈Ø
- **Inteligence**: S√©mantick√© porozumƒõn√≠ obsahu bez z√°vislosti na kl√≠ƒçov√Ωch slovech  
- **≈†k√°lovatelnost**: Optimalizace pro pr√°ci s velk√Ωmi objemy dat
- **Perzistence**: Dlouhodob√° pamƒõ≈• a kontext nap≈ô√≠ƒç sezen√≠mi
- **P≈ôesnost**: Pokroƒçil√© techniky pro relevantn√≠ vyhled√°v√°n√≠ a odpovƒõdi

### Praktick√© vyu≈æit√≠:
- Akademick√Ω v√Ωzkum a anal√Ωza literatury
- Podnikov√© knowledge management syst√©my
- Pr√°vn√≠ anal√Ωza dokument≈Ø a smluv
- Novin√°≈ôsk√© investigace a fact-checking
- Technick√° dokumentace a manu√°ly

Projekt demonstruje s√≠lu kombinace modern√≠ch AI framework≈Ø pro vytvo≈ôen√≠ praktick√©ho, ≈°k√°lovateln√©ho ≈ôe≈°en√≠ s re√°lnou hodnotou pro u≈æivatele pracuj√≠c√≠ s velk√Ωmi objemy informac√≠.