<small>Claude Sonnet 4 **(Academic Plagiarism Detector with Explanation)**</small>
# Academic Plagiarism Detector with Explanation

## Key Concepts Explanation

### Text Similarity Analysis
Advanced computational method that compares textual content using various algorithms including semantic similarity, syntactic matching, and statistical analysis. Employs techniques like cosine similarity on TF-IDF vectors, semantic embeddings from transformer models, and n-gram matching to identify potential plagiarism at different levels of text manipulation including paraphrasing and structural reorganization.

### AI-Generated Content Detection
Sophisticated machine learning system that identifies text produced by large language models versus human-authored content. Utilizes statistical patterns, linguistic markers, perplexity analysis, and behavioral signatures unique to AI systems to distinguish between human creativity and artificial generation, addressing the growing challenge of AI-assisted academic dishonesty.

### Cross-Document Search Engine
Comprehensive search system that efficiently queries large academic databases, institutional repositories, and web sources to find potential source materials. Implements vector-based similarity search, fuzzy matching algorithms, and distributed search capabilities to identify matching or similar content across millions of documents with real-time performance.

### Intelligent Explanation Generation
AI-powered system that provides detailed, educational explanations of detected similarities and potential plagiarism instances. Generates clear descriptions of why content was flagged, explains similarity metrics, provides context about academic integrity standards, and offers constructive feedback to help students understand and avoid plagiarism.

### Semantic Fingerprinting
Advanced technique that creates unique digital signatures for textual content based on meaning rather than exact wording. Uses deep learning models to extract semantic features that remain stable across paraphrasing and translation, enabling detection of plagiarism even when text has been significantly modified or disguised.

### Citation Verification System
Automated system that validates citations, checks reference accuracy, and identifies missing attributions. Analyzes citation formats, verifies source existence, and cross-references quoted or paraphrased content with original sources to ensure proper academic attribution and identify potential citation-based plagiarism.

## Comprehensive Project Explanation

### Objectives
The Academic Plagiarism Detector aims to provide comprehensive, intelligent plagiarism detection that goes beyond simple text matching to identify sophisticated forms of academic dishonesty while providing educational feedback that helps students understand academic integrity principles and improve their research and writing practices.

### Key Features
- **Multi-Modal Detection**: Combines similarity analysis, AI detection, and semantic analysis for comprehensive coverage
- **Real-Time Analysis**: Instant feedback on document submissions with detailed similarity reports
- **Educational Explanations**: Clear, constructive feedback explaining detected issues and improvement suggestions
- **Cross-Platform Search**: Searches academic databases, web sources, and institutional repositories
- **Citation Analysis**: Automated citation verification and missing attribution detection
- **Granular Reporting**: Detailed analysis at sentence, paragraph, and document levels

### Challenges
- **False Positive Management**: Distinguishing legitimate similarities from actual plagiarism in academic contexts
- **AI Detection Accuracy**: Reliably identifying AI-generated content as detection methods evolve
- **Scalability Requirements**: Processing large volumes of documents with acceptable performance
- **Privacy Concerns**: Protecting sensitive academic content while enabling effective detection
- **Evolving Techniques**: Adapting to increasingly sophisticated plagiarism and AI detection evasion methods
- **Cross-Language Detection**: Identifying plagiarism across different languages and translation-based cheating

### Potential Impact
This system can significantly improve academic integrity enforcement, provide valuable educational feedback to students, reduce administrative burden on educators, enable institution-wide plagiarism monitoring, and contribute to maintaining the quality and credibility of academic research and education.

## Comprehensive Project Example with Python Implementation

### Dependencies and Setup

````python
# requirements.txt
openai==1.6.1
langchain==0.1.0
langchain-openai==0.0.5
langchain-community==0.0.10
streamlit==1.29.0
chromadb==0.4.18
sentence-transformers==2.2.2
pandas==2.1.4
numpy==1.24.3
pydantic==2.5.0
python-dotenv==1.0.0
scikit-learn==1.3.2
nltk==3.8.1
spacy==3.7.2
transformers==4.36.0
torch==2.1.0
faiss-cpu==1.7.4
beautifulsoup4==4.12.2
requests==2.31.0
plotly==5.17.0
textstat==0.7.3
wordcloud==1.9.2
matplotlib==3.8.2
seaborn==0.12.2
hashlib
re
json
datetime
uuid
logging
typing
dataclasses
enum
asyncio
pathlib
zipfile
````

### Core Implementation

````python
import os
import re
import json
import hashlib
import uuid
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from enum import Enum
import asyncio
from pathlib import Path

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

# NLP and ML libraries
import nltk
import spacy
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
import torch
from transformers import pipeline, AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer

# LangChain components
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

# Vector databases
import chromadb
import faiss

# Web scraping
import requests
from bs4 import BeautifulSoup

# Text analysis
import textstat

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Download required NLTK data
try:
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
    nltk.download('wordnet', quiet=True)
except:
    pass

class SimilarityType(Enum):
    EXACT_MATCH = "exact_match"
    NEAR_DUPLICATE = "near_duplicate"
    PARAPHRASE = "paraphrase"
    SEMANTIC_SIMILARITY = "semantic_similarity"
    STRUCTURAL_SIMILARITY = "structural_similarity"

class DetectionLevel(Enum):
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    SUSPICIOUS = "suspicious"

class ContentType(Enum):
    HUMAN_WRITTEN = "human_written"
    AI_GENERATED = "ai_generated"
    MIXED = "mixed"
    UNCERTAIN = "uncertain"

@dataclass
class SimilarityMatch:
    match_id: str
    source_text: str
    matched_text: str
    similarity_score: float
    similarity_type: SimilarityType
    source_info: Dict[str, Any]
    start_position: int
    end_position: int
    explanation: str = ""

@dataclass
class AIDetectionResult:
    text_segment: str
    ai_probability: float
    confidence_level: DetectionLevel
    content_type: ContentType
    indicators: List[str]
    explanation: str

@dataclass
class CitationIssue:
    issue_id: str
    text_segment: str
    issue_type: str  # missing_citation, incorrect_format, invalid_source
    suggested_citation: str
    severity: DetectionLevel
    explanation: str

@dataclass
class PlagiarismReport:
    document_id: str
    filename: str
    total_similarity_score: float
    ai_detection_score: float
    similarity_matches: List[SimilarityMatch]
    ai_detection_results: List[AIDetectionResult]
    citation_issues: List[CitationIssue]
    overall_assessment: DetectionLevel
    recommendations: List[str]
    detailed_explanation: str
    timestamp: datetime = field(default_factory=datetime.now)

class TextPreprocessor:
    """Handles text preprocessing and normalization."""
    
    def __init__(self):
        try:
            self.nlp = spacy.load("en_core_web_sm")
        except OSError:
            logger.warning("Spacy model not found. Install with: python -m spacy download en_core_web_sm")
            self.nlp = None
        
        self.stop_words = set(nltk.corpus.stopwords.words('english'))
    
    def clean_text(self, text: str) -> str:
        """Clean and normalize text."""
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text)
        
        # Remove special characters but keep punctuation
        text = re.sub(r'[^\w\s\.\,\;\:\!\?\'\"]', ' ', text)
        
        # Normalize quotes
        text = re.sub(r'[""]', '"', text)
        text = re.sub(r'['']', "'", text)
        
        return text.strip()
    
    def extract_sentences(self, text: str) -> List[str]:
        """Extract sentences from text."""
        if self.nlp:
            doc = self.nlp(text)
            return [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 10]
        else:
            # Fallback to NLTK
            return nltk.sent_tokenize(text)
    
    def extract_paragraphs(self, text: str) -> List[str]:
        """Extract paragraphs from text."""
        paragraphs = text.split('\n\n')
        return [p.strip() for p in paragraphs if len(p.strip()) > 50]
    
    def extract_keywords(self, text: str, num_keywords: int = 20) -> List[str]:
        """Extract keywords from text."""
        if self.nlp:
            doc = self.nlp(text)
            keywords = []
            
            for token in doc:
                if (token.is_alpha and 
                    not token.is_stop and 
                    len(token.text) > 2 and
                    token.pos_ in ['NOUN', 'ADJ', 'VERB']):
                    keywords.append(token.lemma_.lower())
            
            # Count frequency and return top keywords
            from collections import Counter
            keyword_counts = Counter(keywords)
            return [kw for kw, count in keyword_counts.most_common(num_keywords)]
        else:
            # Fallback to simple keyword extraction
            words = re.findall(r'\b\w+\b', text.lower())
            words = [w for w in words if w not in self.stop_words and len(w) > 2]
            from collections import Counter
            word_counts = Counter(words)
            return [w for w, count in word_counts.most_common(num_keywords)]

class SimilarityDetector:
    """Detects text similarity using multiple approaches."""
    
    def __init__(self):
        self.embeddings_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=5000,
            stop_words='english',
            ngram_range=(1, 3)
        )
        self.preprocessor = TextPreprocessor()
    
    def calculate_similarity_matrix(self, documents: List[str]) -> np.ndarray:
        """Calculate similarity matrix for multiple documents."""
        # TF-IDF similarity
        tfidf_matrix = self.tfidf_vectorizer.fit_transform(documents)
        tfidf_similarity = cosine_similarity(tfidf_matrix)
        
        # Semantic similarity
        embeddings = self.embeddings_model.encode(documents)
        semantic_similarity = cosine_similarity(embeddings)
        
        # Combined similarity (weighted average)
        combined_similarity = 0.6 * semantic_similarity + 0.4 * tfidf_similarity
        
        return combined_similarity
    
    def find_exact_matches(self, text1: str, text2: str, min_length: int = 20) -> List[Dict]:
        """Find exact text matches between two documents."""
        matches = []
        
        # Clean texts
        clean_text1 = self.preprocessor.clean_text(text1)
        clean_text2 = self.preprocessor.clean_text(text2)
        
        # Find exact substring matches
        words1 = clean_text1.split()
        words2 = clean_text2.split()
        
        for i in range(len(words1)):
            for j in range(len(words2)):
                # Find longest common substring starting at positions i, j
                match_length = 0
                while (i + match_length < len(words1) and 
                       j + match_length < len(words2) and
                       words1[i + match_length].lower() == words2[j + match_length].lower()):
                    match_length += 1
                
                if match_length >= min_length // 5:  # Approximate word count
                    matched_text = ' '.join(words1[i:i + match_length])
                    if len(matched_text) >= min_length:
                        matches.append({
                            'text': matched_text,
                            'start_pos1': i,
                            'end_pos1': i + match_length,
                            'start_pos2': j,
                            'end_pos2': j + match_length,
                            'length': len(matched_text)
                        })
        
        # Remove overlapping matches (keep longest)
        matches = self._remove_overlapping_matches(matches)
        
        return matches
    
    def find_paraphrase_matches(self, text1: str, text2: str, threshold: float = 0.8) -> List[Dict]:
        """Find paraphrased content between documents."""
        sentences1 = self.preprocessor.extract_sentences(text1)
        sentences2 = self.preprocessor.extract_sentences(text2)
        
        matches = []
        
        if len(sentences1) > 0 and len(sentences2) > 0:
            # Calculate semantic similarity between all sentence pairs
            all_sentences = sentences1 + sentences2
            embeddings = self.embeddings_model.encode(all_sentences)
            
            for i, sent1 in enumerate(sentences1):
                for j, sent2 in enumerate(sentences2):
                    # Calculate similarity
                    emb1 = embeddings[i]
                    emb2 = embeddings[len(sentences1) + j]
                    similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
                    
                    if similarity >= threshold:
                        matches.append({
                            'sentence1': sent1,
                            'sentence2': sent2,
                            'similarity': float(similarity),
                            'type': 'paraphrase'
                        })
        
        return matches
    
    def _remove_overlapping_matches(self, matches: List[Dict]) -> List[Dict]:
        """Remove overlapping matches, keeping the longest ones."""
        if not matches:
            return []
        
        # Sort by length (longest first)
        matches = sorted(matches, key=lambda x: x['length'], reverse=True)
        
        filtered_matches = []
        used_positions = set()
        
        for match in matches:
            # Check if this match overlaps with any already selected match
            overlap = False
            for pos in range(match['start_pos1'], match['end_pos1']):
                if pos in used_positions:
                    overlap = True
                    break
            
            if not overlap:
                filtered_matches.append(match)
                # Mark positions as used
                for pos in range(match['start_pos1'], match['end_pos1']):
                    used_positions.add(pos)
        
        return filtered_matches

class AIContentDetector:
    """Detects AI-generated content using multiple methods."""
    
    def __init__(self, openai_api_key: str = None):
        self.preprocessor = TextPreprocessor()
        
        # Initialize AI detection models
        try:
            self.perplexity_model = pipeline(
                "text-classification",
                model="roberta-base-openai-detector",
                tokenizer="roberta-base-openai-detector"
            )
        except:
            logger.warning("AI detection model not available")
            self.perplexity_model = None
        
        # LLM for analysis
        if openai_api_key:
            self.llm = ChatOpenAI(
                temperature=0.1,
                model_name="gpt-4",
                openai_api_key=openai_api_key
            )
        else:
            self.llm = None
    
    def detect_ai_content(self, text: str) -> AIDetectionResult:
        """Detect if text is AI-generated."""
        indicators = []
        ai_probability = 0.0
        
        # Statistical analysis
        stats = self._analyze_text_statistics(text)
        statistical_score = self._calculate_statistical_ai_score(stats)
        
        if statistical_score > 0.6:
            indicators.append(f"Statistical patterns suggest AI origin (score: {statistical_score:.2f})")
        
        # Linguistic analysis
        linguistic_score = self._analyze_linguistic_patterns(text)
        
        if linguistic_score > 0.7:
            indicators.append(f"Linguistic patterns consistent with AI (score: {linguistic_score:.2f})")
        
        # Perplexity analysis (if model available)
        perplexity_score = 0.5  # Default neutral score
        if self.perplexity_model:
            try:
                result = self.perplexity_model(text[:512])  # Limit text length
                if result and len(result) > 0:
                    # Assuming the model returns probability of being AI-generated
                    perplexity_score = result[0].get('score', 0.5)
                    if perplexity_score > 0.7:
                        indicators.append(f"Perplexity analysis indicates AI generation (score: {perplexity_score:.2f})")
            except:
                pass
        
        # Combined score
        ai_probability = (statistical_score * 0.3 + linguistic_score * 0.4 + perplexity_score * 0.3)
        
        # Determine confidence level and content type
        if ai_probability > 0.8:
            confidence_level = DetectionLevel.HIGH
            content_type = ContentType.AI_GENERATED
        elif ai_probability > 0.6:
            confidence_level = DetectionLevel.MEDIUM
            content_type = ContentType.AI_GENERATED
        elif ai_probability > 0.4:
            confidence_level = DetectionLevel.MEDIUM
            content_type = ContentType.MIXED
        else:
            confidence_level = DetectionLevel.LOW
            content_type = ContentType.HUMAN_WRITTEN
        
        # Generate explanation
        explanation = self._generate_ai_detection_explanation(ai_probability, indicators)
        
        return AIDetectionResult(
            text_segment=text[:200] + "..." if len(text) > 200 else text,
            ai_probability=ai_probability,
            confidence_level=confidence_level,
            content_type=content_type,
            indicators=indicators,
            explanation=explanation
        )
    
    def _analyze_text_statistics(self, text: str) -> Dict[str, float]:
        """Analyze statistical properties of text."""
        stats = {}
        
        # Basic statistics
        stats['length'] = len(text)
        stats['word_count'] = len(text.split())
        stats['sentence_count'] = len(self.preprocessor.extract_sentences(text))
        stats['avg_word_length'] = np.mean([len(word) for word in text.split()])
        stats['avg_sentence_length'] = stats['word_count'] / max(stats['sentence_count'], 1)
        
        # Readability scores
        stats['flesch_reading_ease'] = textstat.flesch_reading_ease(text)
        stats['flesch_kincaid_grade'] = textstat.flesch_kincaid_grade(text)
        
        # Vocabulary diversity
        words = text.lower().split()
        unique_words = set(words)
        stats['vocabulary_diversity'] = len(unique_words) / max(len(words), 1)
        
        return stats
    
    def _calculate_statistical_ai_score(self, stats: Dict[str, float]) -> float:
        """Calculate AI probability based on statistical features."""
        score = 0.0
        
        # AI text often has very consistent sentence lengths
        if 15 <= stats['avg_sentence_length'] <= 25:
            score += 0.2
        
        # AI text often has moderate vocabulary diversity
        if 0.4 <= stats['vocabulary_diversity'] <= 0.7:
            score += 0.2
        
        # AI text often has moderate readability scores
        if 30 <= stats['flesch_reading_ease'] <= 70:
            score += 0.1
        
        # AI text often has consistent word lengths
        if 4 <= stats['avg_word_length'] <= 6:
            score += 0.1
        
        return min(score, 1.0)
    
    def _analyze_linguistic_patterns(self, text: str) -> float:
        """Analyze linguistic patterns that may indicate AI generation."""
        score = 0.0
        
        # Check for common AI patterns
        ai_phrases = [
            "it is important to note", "furthermore", "moreover", "in conclusion",
            "it should be noted", "one must consider", "it is worth mentioning"
        ]
        
        text_lower = text.lower()
        ai_phrase_count = sum(1 for phrase in ai_phrases if phrase in text_lower)
        
        if ai_phrase_count > 0:
            score += min(ai_phrase_count * 0.1, 0.3)
        
        # Check for repetitive patterns
        sentences = self.preprocessor.extract_sentences(text)
        if len(sentences) > 3:
            # Calculate sentence similarity
            sentence_similarities = []
            for i in range(len(sentences) - 1):
                for j in range(i + 1, len(sentences)):
                    # Simple word overlap similarity
                    words1 = set(sentences[i].lower().split())
                    words2 = set(sentences[j].lower().split())
                    if len(words1) > 0 and len(words2) > 0:
                        similarity = len(words1 & words2) / len(words1 | words2)
                        sentence_similarities.append(similarity)
            
            if sentence_similarities:
                avg_similarity = np.mean(sentence_similarities)
                if avg_similarity > 0.3:  # High similarity between sentences
                    score += 0.2
        
        return min(score, 1.0)
    
    def _generate_ai_detection_explanation(self, probability: float, indicators: List[str]) -> str:
        """Generate explanation for AI detection result."""
        if probability > 0.8:
            explanation = "This text shows strong indicators of AI generation. "
        elif probability > 0.6:
            explanation = "This text shows moderate indicators of AI generation. "
        elif probability > 0.4:
            explanation = "This text shows some indicators that may suggest mixed human/AI authorship. "
        else:
            explanation = "This text appears to be primarily human-written. "
        
        if indicators:
            explanation += "Key indicators include: " + "; ".join(indicators[:3])
        
        return explanation

class CrossDocumentSearcher:
    """Searches for similar content across document databases."""
    
    def __init__(self):
        self.embeddings_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.preprocessor = TextPreprocessor()
        
        # Initialize vector database
        self.chroma_client = chromadb.Client()
        self.collection_name = "academic_documents"
        
        try:
            self.collection = self.chroma_client.get_collection(self.collection_name)
        except:
            self.collection = self.chroma_client.create_collection(self.collection_name)
        
        # Sample academic database
        self._initialize_sample_database()
    
    def _initialize_sample_database(self):
        """Initialize with sample academic content."""
        sample_documents = [
            {
                'title': 'Introduction to Machine Learning',
                'content': '''Machine learning is a subset of artificial intelligence that enables 
                computers to learn and make decisions without being explicitly programmed. The field 
                encompasses various algorithms and statistical models that allow systems to improve 
                their performance on specific tasks through experience. Supervised learning uses 
                labeled data to train models, while unsupervised learning finds patterns in data 
                without labeled examples. Deep learning, a subset of machine learning, uses neural 
                networks with multiple layers to model complex patterns in data.''',
                'author': 'Dr. Sarah Johnson',
                'source': 'Journal of AI Research',
                'year': '2023'
            },
            {
                'title': 'Climate Change and Global Warming',
                'content': '''Climate change refers to long-term shifts in global temperatures and 
                weather patterns. While climate variations are natural, scientific evidence shows 
                that human activities have been the main driver of climate change since the 1800s. 
                The burning of fossil fuels generates greenhouse gas emissions that trap heat in 
                Earth's atmosphere. The consequences include rising sea levels, extreme weather 
                events, and disruptions to ecosystems worldwide.''',
                'author': 'Prof. Michael Chen',
                'source': 'Environmental Science Quarterly',
                'year': '2023'
            },
            {
                'title': 'Quantum Computing Fundamentals',
                'content': '''Quantum computing harnesses quantum mechanical phenomena to process 
                information in fundamentally different ways than classical computers. Unlike 
                classical bits that exist in states of 0 or 1, quantum bits (qubits) can exist 
                in superposition of both states simultaneously. This property, along with quantum 
                entanglement, enables quantum computers to solve certain problems exponentially 
                faster than classical computers.''',
                'author': 'Dr. Lisa Wang',
                'source': 'Physics Today',
                'year': '2023'
            }
        ]
        
        for doc in sample_documents:
            self.add_document_to_database(
                doc['content'], 
                doc['title'], 
                doc['author'], 
                doc['source'], 
                doc['year']
            )
    
    def add_document_to_database(self, content: str, title: str, author: str, 
                               source: str, year: str):
        """Add document to searchable database."""
        try:
            document_id = str(uuid.uuid4())
            
            # Split content into chunks for better search
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=500,
                chunk_overlap=100
            )
            
            chunks = text_splitter.split_text(content)
            
            for i, chunk in enumerate(chunks):
                chunk_id = f"{document_id}_chunk_{i}"
                
                self.collection.add(
                    documents=[chunk],
                    metadatas=[{
                        'document_id': document_id,
                        'title': title,
                        'author': author,
                        'source': source,
                        'year': year,
                        'chunk_index': i
                    }],
                    ids=[chunk_id]
                )
            
            logger.info(f"Added document to database: {title}")
            
        except Exception as e:
            logger.error(f"Error adding document to database: {e}")
    
    def search_similar_content(self, query_text: str, top_k: int = 10) -> List[Dict]:
        """Search for similar content in the database."""
        try:
            results = self.collection.query(
                query_texts=[query_text],
                n_results=top_k
            )
            
            matches = []
            if results['documents'] and results['documents'][0]:
                for doc, metadata, distance in zip(
                    results['documents'][0],
                    results['metadatas'][0],
                    results['distances'][0]
                ):
                    similarity_score = 1 - distance  # Convert distance to similarity
                    
                    matches.append({
                        'matched_text': doc,
                        'similarity_score': similarity_score,
                        'source_info': {
                            'title': metadata.get('title', 'Unknown'),
                            'author': metadata.get('author', 'Unknown'),
                            'source': metadata.get('source', 'Unknown'),
                            'year': metadata.get('year', 'Unknown')
                        },
                        'metadata': metadata
                    })
            
            # Sort by similarity score
            matches = sorted(matches, key=lambda x: x['similarity_score'], reverse=True)
            
            return matches
            
        except Exception as e:
            logger.error(f"Error searching database: {e}")
            return []

class CitationAnalyzer:
    """Analyzes citations and references in academic text."""
    
    def __init__(self):
        self.citation_patterns = {
            'apa': r'\([A-Za-z]+,?\s+\d{4}\)',
            'mla': r'\([A-Za-z]+\s+\d+\)',
            'chicago': r'\([A-Za-z]+\s+\d{4},?\s+\d+\)',
            'ieee': r'\[\d+\]'
        }
        
        self.common_citation_indicators = [
            'according to', 'as stated by', 'research shows', 'studies indicate',
            'it has been found', 'previous work', 'literature suggests'
        ]
    
    def analyze_citations(self, text: str) -> List[CitationIssue]:
        """Analyze text for citation issues."""
        issues = []
        
        # Find potential citation contexts
        citation_contexts = self._find_citation_contexts(text)
        
        for context in citation_contexts:
            # Check if proper citation exists
            has_citation = self._has_proper_citation(context['text'])
            
            if not has_citation:
                issue = CitationIssue(
                    issue_id=str(uuid.uuid4()),
                    text_segment=context['text'],
                    issue_type='missing_citation',
                    suggested_citation='[Author, Year]',
                    severity=DetectionLevel.MEDIUM,
                    explanation=f"This text appears to reference external work but lacks proper citation."
                )
                issues.append(issue)
        
        return issues
    
    def _find_citation_contexts(self, text: str) -> List[Dict]:
        """Find text segments that likely need citations."""
        contexts = []
        sentences = self.preprocessor.extract_sentences(text)
        
        for sentence in sentences:
            sentence_lower = sentence.lower()
            
            # Check for citation indicators
            for indicator in self.common_citation_indicators:
                if indicator in sentence_lower:
                    contexts.append({
                        'text': sentence,
                        'indicator': indicator,
                        'needs_citation': True
                    })
                    break
        
        return contexts
    
    def _has_proper_citation(self, text: str) -> bool:
        """Check if text contains proper citation."""
        for pattern in self.citation_patterns.values():
            if re.search(pattern, text):
                return True
        return False

class ExplanationGenerator:
    """Generates detailed explanations for plagiarism detection results."""
    
    def __init__(self, openai_api_key: str = None):
        if openai_api_key:
            self.llm = ChatOpenAI(
                temperature=0.3,
                model_name="gpt-4",
                openai_api_key=openai_api_key
            )
        else:
            self.llm = None
        
        self.explanation_prompt = ChatPromptTemplate.from_messages([
            SystemMessagePromptTemplate.from_template("""
            You are an expert academic integrity advisor. Your role is to provide clear, 
            educational explanations about plagiarism detection results.
            
            Guidelines:
            1. Be constructive and educational, not punitive
            2. Explain what was detected and why it's problematic
            3. Provide specific suggestions for improvement
            4. Reference academic integrity standards
            5. Encourage proper citation and original thinking
            
            Tone: Professional, educational, supportive
            """),
            ("human", """
            Plagiarism Detection Results:
            - Overall Similarity Score: {similarity_score}%
            - AI Content Probability: {ai_probability}%
            - Number of Similarity Matches: {num_matches}
            - Citation Issues Found: {citation_issues}
            
            Key Findings:
            {key_findings}
            
            Please provide a comprehensive explanation that:
            1. Summarizes the main concerns
            2. Explains the significance of each finding
            3. Provides specific recommendations for improvement
            4. Includes guidance on academic integrity best practices
            
            Response (300-500 words):
            """)
        ])
    
    def generate_detailed_explanation(self, report: PlagiarismReport) -> str:
        """Generate detailed explanation for plagiarism report."""
        if not self.llm:
            return self._generate_basic_explanation(report)
        
        try:
            # Prepare key findings
            key_findings = []
            
            if report.similarity_matches:
                key_findings.append(f"Found {len(report.similarity_matches)} potential similarity matches")
            
            if report.ai_detection_results:
                high_ai_segments = [r for r in report.ai_detection_results if r.ai_probability > 0.7]
                if high_ai_segments:
                    key_findings.append(f"Detected {len(high_ai_segments)} segments with high AI probability")
            
            if report.citation_issues:
                key_findings.append(f"Identified {len(report.citation_issues)} citation-related issues")
            
            # Generate explanation
            response = self.llm.invoke(self.explanation_prompt.format(
                similarity_score=f"{report.total_similarity_score:.1f}",
                ai_probability=f"{report.ai_detection_score:.1f}",
                num_matches=len(report.similarity_matches),
                citation_issues=len(report.citation_issues),
                key_findings="\n".join([f"- {finding}" for finding in key_findings])
            ))
            
            return response.content
            
        except Exception as e:
            logger.error(f"Error generating explanation: {e}")
            return self._generate_basic_explanation(report)
    
    def _generate_basic_explanation(self, report: PlagiarismReport) -> str:
        """Generate basic explanation without LLM."""
        explanation = f"Plagiarism Analysis Summary for {report.filename}:\n\n"
        
        if report.overall_assessment == DetectionLevel.HIGH:
            explanation += "‚ö†Ô∏è HIGH CONCERN: This document shows significant indicators of potential plagiarism.\n\n"
        elif report.overall_assessment == DetectionLevel.MEDIUM:
            explanation += "‚ö†Ô∏è MODERATE CONCERN: This document shows some indicators that require attention.\n\n"
        else:
            explanation += "‚úÖ LOW CONCERN: This document shows minimal indicators of plagiarism.\n\n"
        
        if report.similarity_matches:
            explanation += f"Similarity Analysis: Found {len(report.similarity_matches)} potential matches "
            explanation += f"with an overall similarity score of {report.total_similarity_score:.1f}%.\n\n"
        
        if report.ai_detection_score > 50:
            explanation += f"AI Detection: {report.ai_detection_score:.1f}% probability of AI-generated content detected.\n\n"
        
        if report.citation_issues:
            explanation += f"Citation Issues: {len(report.citation_issues)} potential citation problems identified.\n\n"
        
        explanation += "Recommendations:\n"
        for rec in report.recommendations:
            explanation += f"‚Ä¢ {rec}\n"
        
        return explanation

class PlagiarismDetectionEngine:
    """Main plagiarism detection engine combining all components."""
    
    def __init__(self, openai_api_key: str = None):
        self.similarity_detector = SimilarityDetector()
        self.ai_detector = AIContentDetector(openai_api_key)
        self.cross_search = CrossDocumentSearcher()
        self.citation_analyzer = CitationAnalyzer()
        self.explanation_generator = ExplanationGenerator(openai_api_key)
        self.preprocessor = TextPreprocessor()
    
    def analyze_document(self, text: str, filename: str = "document.txt") -> PlagiarismReport:
        """Perform comprehensive plagiarism analysis on a document."""
        try:
            document_id = str(uuid.uuid4())
            logger.info(f"Starting analysis for document: {filename}")
            
            # Clean and preprocess text
            cleaned_text = self.preprocessor.clean_text(text)
            
            # Initialize results containers
            similarity_matches = []
            ai_detection_results = []
            citation_issues = []
            
            # 1. Cross-document similarity search
            logger.info("Performing cross-document search...")
            search_results = self.cross_search.search_similar_content(cleaned_text, top_k=10)
            
            for result in search_results:
                if result['similarity_score'] > 0.7:  # High similarity threshold
                    match = SimilarityMatch(
                        match_id=str(uuid.uuid4()),
                        source_text=cleaned_text[:200] + "...",
                        matched_text=result['matched_text'],
                        similarity_score=result['similarity_score'],
                        similarity_type=SimilarityType.SEMANTIC_SIMILARITY,
                        source_info=result['source_info'],
                        start_position=0,
                        end_position=len(result['matched_text']),
                        explanation=f"High semantic similarity ({result['similarity_score']:.2f}) with content from {result['source_info']['title']}"
                    )
                    similarity_matches.append(match)
            
            # 2. AI content detection
            logger.info("Performing AI content detection...")
            
            # Analyze document in chunks
            paragraphs = self.preprocessor.extract_paragraphs(cleaned_text)
            
            for paragraph in paragraphs:
                if len(paragraph) > 100:  # Only analyze substantial paragraphs
                    ai_result = self.ai_detector.detect_ai_content(paragraph)
                    if ai_result.ai_probability > 0.5:  # Medium threshold
                        ai_detection_results.append(ai_result)
            
            # 3. Citation analysis
            logger.info("Analyzing citations...")
            citation_issues = self.citation_analyzer.analyze_citations(cleaned_text)
            
            # 4. Calculate overall scores
            total_similarity_score = self._calculate_overall_similarity_score(similarity_matches)
            ai_detection_score = self._calculate_overall_ai_score(ai_detection_results)
            
            # 5. Determine overall assessment
            overall_assessment = self._determine_overall_assessment(
                total_similarity_score, ai_detection_score, len(citation_issues)
            )
            
            # 6. Generate recommendations
            recommendations = self._generate_recommendations(
                similarity_matches, ai_detection_results, citation_issues
            )
            
            # 7. Create report
            report = PlagiarismReport(
                document_id=document_id,
                filename=filename,
                total_similarity_score=total_similarity_score,
                ai_detection_score=ai_detection_score,
                similarity_matches=similarity_matches,
                ai_detection_results=ai_detection_results,
                citation_issues=citation_issues,
                overall_assessment=overall_assessment,
                recommendations=recommendations,
                detailed_explanation=""  # Will be filled next
            )
            
            # 8. Generate detailed explanation
            logger.info("Generating detailed explanation...")
            report.detailed_explanation = self.explanation_generator.generate_detailed_explanation(report)
            
            logger.info(f"Analysis completed for {filename}")
            return report
            
        except Exception as e:
            logger.error(f"Error analyzing document: {e}")
            
            # Return minimal report with error
            return PlagiarismReport(
                document_id=str(uuid.uuid4()),
                filename=filename,
                total_similarity_score=0.0,
                ai_detection_score=0.0,
                similarity_matches=[],
                ai_detection_results=[],
                citation_issues=[],
                overall_assessment=DetectionLevel.LOW,
                recommendations=["Analysis failed - please try again"],
                detailed_explanation=f"Error occurred during analysis: {str(e)}"
            )
    
    def _calculate_overall_similarity_score(self, matches: List[SimilarityMatch]) -> float:
        """Calculate overall similarity score from matches."""
        if not matches:
            return 0.0
        
        # Weight by similarity score and number of matches
        total_score = sum(match.similarity_score for match in matches)
        avg_score = total_score / len(matches)
        
        # Apply scaling based on number of matches
        scale_factor = min(len(matches) / 5.0, 1.0)  # More matches = higher concern
        
        return min(avg_score * 100 * scale_factor, 100.0)
    
    def _calculate_overall_ai_score(self, ai_results: List[AIDetectionResult]) -> float:
        """Calculate overall AI detection score."""
        if not ai_results:
            return 0.0
        
        # Weight by probability and confidence
        weighted_scores = []
        for result in ai_results:
            weight = 1.0
            if result.confidence_level == DetectionLevel.HIGH:
                weight = 1.5
            elif result.confidence_level == DetectionLevel.LOW:
                weight = 0.5
            
            weighted_scores.append(result.ai_probability * weight)
        
        return min(np.mean(weighted_scores) * 100, 100.0)
    
    def _determine_overall_assessment(self, similarity_score: float, 
                                    ai_score: float, citation_issues: int) -> DetectionLevel:
        """Determine overall assessment level."""
        total_score = similarity_score * 0.5 + ai_score * 0.3 + min(citation_issues * 10, 20)
        
        if total_score > 70:
            return DetectionLevel.HIGH
        elif total_score > 40:
            return DetectionLevel.MEDIUM
        else:
            return DetectionLevel.LOW
    
    def _generate_recommendations(self, similarity_matches: List[SimilarityMatch],
                                ai_results: List[AIDetectionResult],
                                citation_issues: List[CitationIssue]) -> List[str]:
        """Generate recommendations based on analysis results."""
        recommendations = []
        
        if similarity_matches:
            recommendations.append("Review highlighted similar content and ensure proper attribution")
            recommendations.append("Paraphrase content more extensively and add proper citations")
        
        if ai_results:
            high_ai_count = len([r for r in ai_results if r.ai_probability > 0.7])
            if high_ai_count > 0:
                recommendations.append("Review sections flagged as potentially AI-generated")
                recommendations.append("Ensure all content represents original thought and analysis")
        
        if citation_issues:
            recommendations.append("Add missing citations for referenced work")
            recommendations.append("Follow proper citation format for your academic style")
        
        # General recommendations
        recommendations.extend([
            "Review academic integrity policies",
            "Consult with instructor if clarification is needed",
            "Use plagiarism detection as a learning tool for improvement"
        ])
        
        return recommendations

def main():
    """Main Streamlit application."""
    st.set_page_config(
        page_title="Academic Plagiarism Detector",
        page_icon="üìö",
        layout="wide"
    )
    
    st.title("üìö Academic Plagiarism Detector with AI Analysis")
    st.markdown("Comprehensive plagiarism detection with educational feedback and explanations")
    
    # Sidebar
    with st.sidebar:
        st.header("‚öôÔ∏è Configuration")
        openai_api_key = st.text_input("OpenAI API Key (Optional)", type="password",
                                     help="For advanced AI detection and explanations")
        
        st.header("üîç Detection Settings")
        similarity_threshold = st.slider("Similarity Threshold", 0.5, 1.0, 0.7, 0.05)
        ai_threshold = st.slider("AI Detection Threshold", 0.3, 1.0, 0.6, 0.05)
        
        st.header("üìä Analysis Options")
        include_ai_detection = st.checkbox("Include AI Content Detection", value=True)
        include_citation_check = st.checkbox("Include Citation Analysis", value=True)
        detailed_explanations = st.checkbox("Generate Detailed Explanations", value=True)
    
    # Initialize detection engine
    if 'detector' not in st.session_state:
        with st.spinner("Initializing plagiarism detection engine..."):
            st.session_state['detector'] = PlagiarismDetectionEngine(openai_api_key)
            st.success("Detection engine ready!")
    
    detector = st.session_state['detector']
    
    # Main tabs
    tab1, tab2, tab3, tab4 = st.tabs([
        "üìù Document Analysis",
        "üìä Analysis Results",
        "üìö Database Management",
        "üìà Analytics"
    ])
    
    with tab1:
        st.header("üìù Document Analysis")
        
        # Document input options
        input_method = st.radio("Choose input method:", 
                              ["Text Input", "File Upload", "Sample Text"])
        
        document_text = ""
        filename = "document.txt"
        
        if input_method == "Text Input":
            document_text = st.text_area(
                "Paste your document text here:",
                height=300,
                placeholder="Enter or paste the text you want to analyze for plagiarism..."
            )
            filename = st.text_input("Document name (optional):", value="my_document.txt")
        
        elif input_method == "File Upload":
            uploaded_file = st.file_uploader(
                "Choose a text file",
                type=['txt', 'docx', 'pdf'],
                help="Upload a document for plagiarism analysis"
            )
            
            if uploaded_file:
                filename = uploaded_file.name
                
                if uploaded_file.type == "text/plain":
                    document_text = str(uploaded_file.read(), "utf-8")
                else:
                    st.warning("Currently only .txt files are supported in this demo")
        
        elif input_method == "Sample Text":
            sample_texts = {
                "Academic Essay Sample": """
                Machine learning has revolutionized the way we approach data analysis and 
                pattern recognition. According to recent studies, deep learning algorithms 
                have shown remarkable performance in various domains including computer vision, 
                natural language processing, and speech recognition. The field encompasses 
                various algorithms and statistical models that allow systems to improve 
                their performance on specific tasks through experience. Supervised learning 
                uses labeled data to train models, while unsupervised learning finds patterns 
                in data without labeled examples.
                """,
                "Research Paper Excerpt": """
                Climate change refers to long-term shifts in global temperatures and weather 
                patterns. While climate variations are natural, scientific evidence shows that 
                human activities have been the main driver of climate change since the 1800s. 
                The burning of fossil fuels generates greenhouse gas emissions that trap heat 
                in Earth's atmosphere. The consequences include rising sea levels, extreme 
                weather events, and disruptions to ecosystems worldwide. Research indicates 
                that immediate action is required to mitigate these effects.
                """
            }
            
            selected_sample = st.selectbox("Select sample text:", list(sample_texts.keys()))
            document_text = sample_texts[selected_sample]
            filename = f"{selected_sample.lower().replace(' ', '_')}.txt"
            
            st.text_area("Selected sample text:", value=document_text, height=200, disabled=True)
        
        # Analysis button
        if st.button("üîç Analyze Document", type="primary", disabled=not document_text.strip()):
            if document_text.strip():
                with st.spinner("Analyzing document for plagiarism..."):
                    
                    # Perform analysis
                    analysis_report = detector.analyze_document(document_text.strip(), filename)
                    
                    # Store report in session state
                    st.session_state['current_report'] = analysis_report
                    
                    # Display quick summary
                    st.success("Analysis completed!")
                    
                    col1, col2, col3, col4 = st.columns(4)
                    
                    with col1:
                        st.metric("Overall Risk", analysis_report.overall_assessment.value.upper())
                    
                    with col2:
                        st.metric("Similarity Score", f"{analysis_report.total_similarity_score:.1f}%")
                    
                    with col3:
                        st.metric("AI Detection", f"{analysis_report.ai_detection_score:.1f}%")
                    
                    with col4:
                        st.metric("Issues Found", 
                                len(analysis_report.similarity_matches) + 
                                len(analysis_report.ai_detection_results) + 
                                len(analysis_report.citation_issues))
                    
                    st.info("üìä View detailed results in the 'Analysis Results' tab")
            else:
                st.warning("Please enter text to analyze")
    
    with tab2:
        st.header("üìä Analysis Results")
        
        if 'current_report' in st.session_state:
            report = st.session_state['current_report']
            
            # Overall assessment
            st.subheader("üéØ Overall Assessment")
            
            if report.overall_assessment == DetectionLevel.HIGH:
                st.error(f"‚ö†Ô∏è HIGH RISK: Significant plagiarism indicators detected")
            elif report.overall_assessment == DetectionLevel.MEDIUM:
                st.warning(f"‚ö†Ô∏è MEDIUM RISK: Some plagiarism indicators detected")
            else:
                st.success(f"‚úÖ LOW RISK: Minimal plagiarism indicators")
            
            # Detailed explanation
            if report.detailed_explanation:
                st.subheader("üìù Detailed Explanation")
                st.write(report.detailed_explanation)
            
            # Similarity matches
            if report.similarity_matches:
                st.subheader(f"üîç Similarity Matches ({len(report.similarity_matches)})")
                
                for i, match in enumerate(report.similarity_matches):
                    with st.expander(f"Match {i+1}: {match.similarity_score:.2f} similarity"):
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            st.write("**Your Text:**")
                            st.write(match.source_text)
                        
                        with col2:
                            st.write("**Similar Content Found:**")
                            st.write(match.matched_text)
                        
                        st.write(f"**Source:** {match.source_info['title']} by {match.source_info['author']}")
                        st.write(f"**Similarity Score:** {match.similarity_score:.2f}")
                        st.write(f"**Type:** {match.similarity_type.value.replace('_', ' ').title()}")
                        
                        if match.explanation:
                            st.write(f"**Explanation:** {match.explanation}")
            
            # AI detection results
            if report.ai_detection_results:
                st.subheader(f"ü§ñ AI Content Detection ({len(report.ai_detection_results)})")
                
                for i, ai_result in enumerate(report.ai_detection_results):
                    with st.expander(f"Segment {i+1}: {ai_result.ai_probability:.1%} AI probability"):
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            st.write("**Text Segment:**")
                            st.write(ai_result.text_segment)
                        
                        with col2:
                            st.write(f"**AI Probability:** {ai_result.ai_probability:.1%}")
                            st.write(f"**Confidence:** {ai_result.confidence_level.value.title()}")
                            st.write(f"**Assessment:** {ai_result.content_type.value.replace('_', ' ').title()}")
                        
                        if ai_result.indicators:
                            st.write("**Key Indicators:**")
                            for indicator in ai_result.indicators:
                                st.write(f"‚Ä¢ {indicator}")
                        
                        st.write(f"**Explanation:** {ai_result.explanation}")
            
            # Citation issues
            if report.citation_issues:
                st.subheader(f"üìã Citation Issues ({len(report.citation_issues)})")
                
                for i, issue in enumerate(report.citation_issues):
                    with st.expander(f"Issue {i+1}: {issue.issue_type.replace('_', ' ').title()}"):
                        st.write("**Text Segment:**")
                        st.write(issue.text_segment)
                        
                        st.write(f"**Issue Type:** {issue.issue_type.replace('_', ' ').title()}")
                        st.write(f"**Severity:** {issue.severity.value.title()}")
                        st.write(f"**Suggested Citation:** {issue.suggested_citation}")
                        st.write(f"**Explanation:** {issue.explanation}")
            
            # Recommendations
            if report.recommendations:
                st.subheader("üí° Recommendations")
                
                for rec in report.recommendations:
                    st.write(f"‚Ä¢ {rec}")
            
            # Report summary visualization
            st.subheader("üìà Analysis Summary")
            
            # Create visualization
            fig = go.Figure()
            
            # Add similarity score
            fig.add_trace(go.Bar(
                name='Similarity Score',
                x=['Similarity'],
                y=[report.total_similarity_score],
                marker_color='red'
            ))
            
            # Add AI detection score
            fig.add_trace(go.Bar(
                name='AI Detection Score',
                x=['AI Detection'],
                y=[report.ai_detection_score],
                marker_color='blue'
            ))
            
            fig.update_layout(
                title='Plagiarism Analysis Scores',
                yaxis_title='Score (%)',
                yaxis=dict(range=[0, 100])
            )
            
            st.plotly_chart(fig, use_container_width=True)
            
        else:
            st.info("No analysis results available. Please analyze a document first.")
    
    with tab3:
        st.header("üìö Database Management")
        
        # Database statistics
        st.subheader("üìä Database Statistics")
        
        # Simulated stats (in real implementation, would query actual database)
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("Total Documents", "1,247")
        
        with col2:
            st.metric("Academic Papers", "892")
        
        with col3:
            st.metric("Last Updated", "Today")
        
        # Add new document to database
        st.subheader("‚ûï Add Document to Database")
        
        with st.form("add_document"):
            new_title = st.text_input("Document Title")
            new_author = st.text_input("Author")
            new_source = st.text_input("Source/Journal")
            new_year = st.text_input("Year")
            new_content = st.text_area("Document Content", height=200)
            
            if st.form_submit_button("Add Document"):
                if all([new_title, new_author, new_content]):
                    detector.cross_search.add_document_to_database(
                        new_content, new_title, new_author, 
                        new_source or "Unknown", new_year or "Unknown"
                    )
                    st.success(f"Added '{new_title}' to database!")
                else:
                    st.error("Please fill in title, author, and content fields.")
        
        # Search database
        st.subheader("üîç Search Database")
        
        search_query = st.text_input("Search for similar content:")
        
        if search_query:
            search_results = detector.cross_search.search_similar_content(search_query, top_k=5)
            
            if search_results:
                st.write(f"Found {len(search_results)} similar documents:")
                
                for i, result in enumerate(search_results):
                    with st.expander(f"Result {i+1}: {result['source_info']['title']} ({result['similarity_score']:.2f})"):
                        st.write(f"**Author:** {result['source_info']['author']}")
                        st.write(f"**Source:** {result['source_info']['source']}")
                        st.write(f"**Year:** {result['source_info']['year']}")
                        st.write(f"**Similarity:** {result['similarity_score']:.2f}")
                        st.write("**Content:**")
                        st.write(result['matched_text'])
            else:
                st.info("No similar content found.")
    
    with tab4:
        st.header("üìà Analytics Dashboard")
        
        # Simulated analytics data
        st.subheader("üìä Detection Statistics")
        
        # Generate sample data
        detection_data = {
            'Date': pd.date_range('2024-01-01', periods=30, freq='D'),
            'Documents_Analyzed': np.random.randint(10, 50, 30),
            'High_Risk_Detected': np.random.randint(0, 10, 30),
            'Medium_Risk_Detected': np.random.randint(5, 20, 30),
            'AI_Content_Detected': np.random.randint(2, 15, 30)
        }
        
        df = pd.DataFrame(detection_data)
        
        # Line chart for trends
        fig = px.line(
            df, 
            x='Date', 
            y=['Documents_Analyzed', 'High_Risk_Detected', 'Medium_Risk_Detected', 'AI_Content_Detected'],
            title='Plagiarism Detection Trends'
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Risk distribution
        st.subheader("üéØ Risk Level Distribution")
        
        risk_data = pd.DataFrame({
            'Risk_Level': ['Low', 'Medium', 'High'],
            'Count': [45, 30, 25]
        })
        
        fig = px.pie(risk_data, values='Count', names='Risk_Level', 
                    title='Distribution of Risk Levels')
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Detection metrics
        st.subheader("üìè Detection Metrics")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Accuracy Rate", "94.2%", "2.1%")
        
        with col2:
            st.metric("False Positive Rate", "5.8%", "-1.2%")
        
        with col3:
            st.metric("Processing Speed", "2.3s/doc", "-0.4s")
        
        with col4:
            st.metric("Database Size", "15.2K docs", "127")
        
        # Recent activity
        st.subheader("üïí Recent Activity")
        
        recent_activity = pd.DataFrame({
            'Timestamp': [
                '2024-01-15 14:30:22',
                '2024-01-15 14:28:15',
                '2024-01-15 14:25:03',
                '2024-01-15 14:22:41',
                '2024-01-15 14:20:18'
            ],
            'Document': [
                'research_paper.txt',
                'essay_draft.txt',
                'thesis_chapter.txt',
                'assignment.txt',
                'literature_review.txt'
            ],
            'Risk_Level': ['Medium', 'Low', 'High', 'Low', 'Medium'],
            'Similarity_Score': [45.2, 12.8, 78.5, 8.3, 52.1]
        })
        
        st.dataframe(recent_activity, use_container_width=True)

if __name__ == "__main__":
    main()
````

## Project Summary

The Academic Plagiarism Detector with Explanation represents a comprehensive, AI-powered solution for detecting various forms of academic dishonesty while providing educational feedback that helps students understand and improve their academic writing practices.

### Key Value Propositions:
- **Multi-Modal Detection System**: Combines semantic similarity analysis, AI-generated content detection, and citation verification for comprehensive plagiarism identification
- **Cross-Document Search Engine**: Efficient vector-based search across academic databases and repositories using sentence transformers and ChromaDB
- **Intelligent Explanation Generation**: LLM-powered system providing constructive, educational feedback that explains detection results and offers improvement guidance
- **Real-Time Analysis Capabilities**: Instant document processing with detailed similarity reports and granular analysis at sentence and paragraph levels
- **Educational Focus**: Emphasizes learning and improvement rather than punishment, helping students develop proper academic integrity practices

### Technical Highlights:
- Sentence-Transformers for semantic similarity analysis with cosine similarity calculations and TF-IDF weighting
- Advanced AI detection using statistical analysis, linguistic pattern recognition, and perplexity scoring
- ChromaDB vector database for efficient similarity search with metadata filtering and relevance ranking
- LangChain integration for intelligent explanation generation with structured prompting and educational guidance
- Comprehensive citation analysis with pattern recognition for multiple academic formats (APA, MLA, Chicago, IEEE)
- Streamlit interface providing interactive analysis tools, visualization dashboards, and database management capabilities

This system demonstrates how AI can enhance academic integrity enforcement while maintaining an educational focus, providing valuable feedback that helps students understand plagiarism concepts and develop better research and writing practices for long-term academic success.