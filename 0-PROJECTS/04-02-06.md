<small>Claude Sonnet 4 **(Voice-Enabled AI Interview Coach)**</small>
# Voice-Enabled AI Interview Coach

## Key Concepts Explanation

### Speech-to-Text (STT)
Advanced audio processing technology that converts spoken language into written text, enabling real-time analysis of interview responses for content evaluation, sentiment analysis, and linguistic pattern detection.

### OpenAI Whisper
State-of-the-art automatic speech recognition model that provides highly accurate transcription across multiple languages with robust noise handling and speaker adaptation capabilities.

### Large Language Models (LLM)
Advanced AI systems trained on vast datasets to understand context, evaluate communication skills, and provide intelligent feedback on interview performance including content quality, structure, and professional presentation.

### Feedback Generation
AI-powered analysis system that evaluates interview responses across multiple dimensions including technical accuracy, communication clarity, confidence indicators, and professional presentation skills.

### HR GPT
Specialized AI model trained on human resources knowledge, interview best practices, and recruitment expertise to provide professional-grade interview coaching and candidate assessment.

### Voice Analytics
Comprehensive analysis of vocal characteristics including pace, tone, clarity, confidence markers, and speech patterns that impact interview performance and professional communication effectiveness.

## Comprehensive Project Explanation

### Objectives
The Voice-Enabled AI Interview Coach aims to democratize interview preparation by providing personalized, professional-grade coaching that analyzes both content and delivery, helping candidates improve their interview performance through AI-powered feedback and practice sessions.

### Key Features
- **Real-time Speech Analysis**: Live transcription and analysis of interview responses
- **Multi-dimensional Feedback**: Evaluation of content, delivery, confidence, and communication skills
- **Personalized Coaching**: Adaptive recommendations based on individual strengths and weaknesses
- **Industry-Specific Preparation**: Tailored questions and feedback for different roles and industries
- **Progress Tracking**: Comprehensive analytics on improvement over time
- **Mock Interview Simulations**: Realistic interview scenarios with AI-powered interviewer interactions

### Challenges
- **Audio Quality Variability**: Handling different microphone qualities and background noise
- **Real-time Processing**: Providing immediate feedback without significant latency
- **Contextual Understanding**: Accurately evaluating responses within specific industry contexts
- **Bias Mitigation**: Ensuring fair evaluation across different accents, speaking styles, and backgrounds
- **Emotional Intelligence**: Detecting and providing feedback on soft skills and emotional responses

### Potential Impact
This system can significantly improve interview success rates, reduce interview anxiety through practice, provide equal access to professional coaching regardless of geographic or economic constraints, and help organizations identify better candidates through standardized evaluation criteria.

## Comprehensive Project Example with Python Implementation

### Dependencies and Setup

````python
# requirements.txt
openai==1.6.1
whisper==1.1.10
langchain==0.1.0
langchain-openai==0.0.5
streamlit==1.29.0
streamlit-webrtc==0.47.1
pydub==0.25.1
librosa==0.10.1
numpy==1.24.3
pandas==2.1.4
plotly==5.17.0
python-dotenv==1.0.0
speechrecognition==3.10.0
pyaudio==0.2.11
webrtcvad==2.0.10
transformers==4.36.0
torch==2.1.0
scikit-learn==1.3.2
textstat==0.7.3
vaderSentiment==3.3.2
````

### Core Implementation

````python
import os
import logging
import asyncio
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from datetime import datetime
from enum import Enum
import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go

import openai
import whisper
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
import speech_recognition as sr
from pydub import AudioSegment
import librosa
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from textstat import flesch_reading_ease, flesch_kincaid_grade
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import webrtcvad

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class InterviewType(Enum):
    TECHNICAL = "technical"
    BEHAVIORAL = "behavioral"
    LEADERSHIP = "leadership"
    SALES = "sales"
    GENERAL = "general"

class SkillCategory(Enum):
    COMMUNICATION = "communication"
    TECHNICAL_KNOWLEDGE = "technical_knowledge"
    PROBLEM_SOLVING = "problem_solving"
    LEADERSHIP = "leadership"
    CONFIDENCE = "confidence"
    CLARITY = "clarity"

class FeedbackLevel(Enum):
    EXCELLENT = "excellent"
    GOOD = "good"
    NEEDS_IMPROVEMENT = "needs_improvement"
    POOR = "poor"

@dataclass
class VoiceMetrics:
    pace_wpm: float
    pause_duration: float
    volume_consistency: float
    clarity_score: float
    confidence_indicators: Dict[str, float]
    filler_words_count: int
    total_duration: float

@dataclass
class ContentAnalysis:
    relevance_score: float
    structure_score: float
    depth_score: float
    technical_accuracy: float
    examples_quality: float
    sentiment_score: float
    readability_score: float
    keyword_coverage: float

@dataclass
class InterviewFeedback:
    overall_score: float
    voice_metrics: VoiceMetrics
    content_analysis: ContentAnalysis
    strengths: List[str]
    improvement_areas: List[str]
    specific_recommendations: List[str]
    skill_ratings: Dict[SkillCategory, FeedbackLevel]
    transcript: str
    question: str
    timestamp: datetime

class AudioProcessor:
    """Advanced audio processing for speech analysis."""
    
    def __init__(self):
        self.whisper_model = whisper.load_model("base")
        self.vad = webrtcvad.Vad(2)  # Aggressiveness level 0-3
        self.recognizer = sr.Recognizer()
        
    def transcribe_audio(self, audio_data: bytes) -> str:
        """Transcribe audio using Whisper."""
        try:
            # Convert bytes to audio file-like object
            audio_segment = AudioSegment.from_wav(io.BytesIO(audio_data))
            
            # Convert to numpy array for Whisper
            audio_array = np.array(audio_segment.get_array_of_samples(), dtype=np.float32)
            audio_array = audio_array / np.max(np.abs(audio_array))  # Normalize
            
            # Transcribe with Whisper
            result = self.whisper_model.transcribe(audio_array)
            return result["text"]
            
        except Exception as e:
            logger.error(f"Transcription error: {e}")
            return ""
    
    def analyze_voice_characteristics(self, audio_data: bytes) -> VoiceMetrics:
        """Analyze voice characteristics for coaching feedback."""
        try:
            # Load audio data
            audio_segment = AudioSegment.from_wav(io.BytesIO(audio_data))
            audio_array = np.array(audio_segment.get_array_of_samples(), dtype=np.float32)
            sr_rate = audio_segment.frame_rate
            
            # Calculate voice metrics
            pace_wpm = self._calculate_speaking_pace(audio_array, sr_rate)
            pause_duration = self._analyze_pauses(audio_array, sr_rate)
            volume_consistency = self._analyze_volume_consistency(audio_array)
            clarity_score = self._calculate_clarity_score(audio_array, sr_rate)
            confidence_indicators = self._analyze_confidence_indicators(audio_array, sr_rate)
            
            return VoiceMetrics(
                pace_wpm=pace_wpm,
                pause_duration=pause_duration,
                volume_consistency=volume_consistency,
                clarity_score=clarity_score,
                confidence_indicators=confidence_indicators,
                filler_words_count=0,  # Would be calculated from transcript
                total_duration=len(audio_array) / sr_rate
            )
            
        except Exception as e:
            logger.error(f"Voice analysis error: {e}")
            return VoiceMetrics(0, 0, 0, 0, {}, 0, 0)
    
    def _calculate_speaking_pace(self, audio: np.ndarray, sr: int) -> float:
        """Calculate words per minute."""
        # Simplified calculation - would need more sophisticated analysis
        duration_minutes = len(audio) / sr / 60
        # Estimate based on audio energy and typical speaking patterns
        estimated_words = duration_minutes * 150  # Average speaking rate
        return estimated_words / duration_minutes if duration_minutes > 0 else 0
    
    def _analyze_pauses(self, audio: np.ndarray, sr: int) -> float:
        """Analyze pause patterns in speech."""
        # Calculate RMS energy
        frame_length = int(0.025 * sr)  # 25ms frames
        hop_length = int(0.01 * sr)    # 10ms hop
        
        rms_energy = librosa.feature.rms(y=audio, frame_length=frame_length, hop_length=hop_length)[0]
        
        # Detect pauses (low energy regions)
        energy_threshold = np.mean(rms_energy) * 0.1
        pauses = rms_energy < energy_threshold
        
        # Calculate average pause duration
        pause_durations = []
        in_pause = False
        pause_start = 0
        
        for i, is_pause in enumerate(pauses):
            if is_pause and not in_pause:
                in_pause = True
                pause_start = i
            elif not is_pause and in_pause:
                in_pause = False
                pause_duration = (i - pause_start) * hop_length / sr
                pause_durations.append(pause_duration)
        
        return np.mean(pause_durations) if pause_durations else 0
    
    def _analyze_volume_consistency(self, audio: np.ndarray) -> float:
        """Analyze volume consistency (0-1 score)."""
        rms_values = librosa.feature.rms(y=audio)[0]
        consistency = 1 - (np.std(rms_values) / np.mean(rms_values)) if np.mean(rms_values) > 0 else 0
        return max(0, min(1, consistency))
    
    def _calculate_clarity_score(self, audio: np.ndarray, sr: int) -> float:
        """Calculate speech clarity score."""
        # Analyze spectral characteristics
        spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr)[0]
        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)[0]
        
        # Higher frequency content often indicates clearer speech
        clarity = np.mean(spectral_centroids) / 4000  # Normalize by typical speech range
        return max(0, min(1, clarity))
    
    def _analyze_confidence_indicators(self, audio: np.ndarray, sr: int) -> Dict[str, float]:
        """Analyze vocal confidence indicators."""
        # Volume stability
        rms_values = librosa.feature.rms(y=audio)[0]
        volume_stability = 1 - (np.std(rms_values) / np.mean(rms_values)) if np.mean(rms_values) > 0 else 0
        
        # Pitch stability
        pitches, magnitudes = librosa.piptrack(y=audio, sr=sr)
        pitch_values = [pitches[magnitudes[:, t].argmax(), t] for t in range(pitches.shape[1]) if magnitudes[:, t].max() > 0]
        pitch_stability = 1 - (np.std(pitch_values) / np.mean(pitch_values)) if pitch_values and np.mean(pitch_values) > 0 else 0
        
        return {
            "volume_stability": max(0, min(1, volume_stability)),
            "pitch_stability": max(0, min(1, pitch_stability)),
            "energy_consistency": max(0, min(1, 1 - np.std(rms_values)))
        }

class ContentAnalyzer:
    """Analyze interview response content quality."""
    
    def __init__(self, openai_api_key: str):
        self.llm = ChatOpenAI(
            temperature=0.1,
            model_name="gpt-4",
            openai_api_key=openai_api_key
        )
        self.sentiment_analyzer = SentimentIntensityAnalyzer()
        self.tfidf = TfidfVectorizer(max_features=100, stop_words='english')
        
        # Analysis prompt template
        self.analysis_prompt = PromptTemplate(
            input_variables=["question", "response", "interview_type"],
            template="""
            Analyze this interview response for professional quality and effectiveness:

            Question: {question}
            Response: {response}
            Interview Type: {interview_type}

            Evaluate the response on:
            1. Relevance to the question (0-10)
            2. Structure and organization (0-10)
            3. Depth of content (0-10)
            4. Use of specific examples (0-10)
            5. Technical accuracy (if applicable) (0-10)
            6. Professional communication (0-10)

            Provide scores and specific feedback for improvement.

            Analysis:
            """
        )
    
    def analyze_content(self, question: str, response: str, interview_type: InterviewType) -> ContentAnalysis:
        """Comprehensive content analysis."""
        try:
            # Get AI analysis
            ai_analysis = self._get_ai_analysis(question, response, interview_type)
            
            # Calculate readability metrics
            readability_score = self._calculate_readability(response)
            
            # Sentiment analysis
            sentiment_score = self._analyze_sentiment(response)
            
            # Keyword coverage analysis
            keyword_coverage = self._analyze_keyword_coverage(question, response)
            
            # Parse AI analysis for scores (simplified)
            relevance_score = self._extract_score_from_analysis(ai_analysis, "relevance", 7.0)
            structure_score = self._extract_score_from_analysis(ai_analysis, "structure", 7.0)
            depth_score = self._extract_score_from_analysis(ai_analysis, "depth", 6.5)
            technical_accuracy = self._extract_score_from_analysis(ai_analysis, "technical", 7.0)
            examples_quality = self._extract_score_from_analysis(ai_analysis, "examples", 6.0)
            
            return ContentAnalysis(
                relevance_score=relevance_score,
                structure_score=structure_score,
                depth_score=depth_score,
                technical_accuracy=technical_accuracy,
                examples_quality=examples_quality,
                sentiment_score=sentiment_score,
                readability_score=readability_score,
                keyword_coverage=keyword_coverage
            )
            
        except Exception as e:
            logger.error(f"Content analysis error: {e}")
            return ContentAnalysis(5.0, 5.0, 5.0, 5.0, 5.0, 0.0, 5.0, 5.0)
    
    def _get_ai_analysis(self, question: str, response: str, interview_type: InterviewType) -> str:
        """Get AI-powered content analysis."""
        prompt = self.analysis_prompt.format(
            question=question,
            response=response,
            interview_type=interview_type.value
        )
        return self.llm.predict(prompt)
    
    def _calculate_readability(self, text: str) -> float:
        """Calculate readability score."""
        try:
            flesch_score = flesch_reading_ease(text)
            # Convert to 0-10 scale
            return max(0, min(10, flesch_score / 10))
        except:
            return 5.0
    
    def _analyze_sentiment(self, text: str) -> float:
        """Analyze sentiment of the response."""
        sentiment = self.sentiment_analyzer.polarity_scores(text)
        # Convert compound score (-1 to 1) to 0-10 scale
        return (sentiment['compound'] + 1) * 5
    
    def _analyze_keyword_coverage(self, question: str, response: str) -> float:
        """Analyze how well the response covers question keywords."""
        try:
            # Extract keywords from question
            question_words = set(question.lower().split())
            response_words = set(response.lower().split())
            
            # Calculate coverage
            coverage = len(question_words.intersection(response_words)) / len(question_words)
            return coverage * 10
        except:
            return 5.0
    
    def _extract_score_from_analysis(self, analysis: str, category: str, default: float) -> float:
        """Extract numerical scores from AI analysis."""
        # Simplified extraction - would use more sophisticated parsing
        try:
            lines = analysis.lower().split('\n')
            for line in lines:
                if category in line and any(char.isdigit() for char in line):
                    # Extract number from line
                    numbers = [float(word) for word in line.split() if word.replace('.', '').isdigit()]
                    if numbers:
                        return max(0, min(10, numbers[0]))
        except:
            pass
        return default

class InterviewCoach:
    """Main AI interview coaching system."""
    
    def __init__(self, openai_api_key: str):
        self.audio_processor = AudioProcessor()
        self.content_analyzer = ContentAnalyzer(openai_api_key)
        self.llm = ChatOpenAI(
            temperature=0.2,
            model_name="gpt-4",
            openai_api_key=openai_api_key
        )
        
        # Question databases
        self.question_bank = self._load_question_bank()
        
        # Feedback generation prompt
        self.feedback_prompt = PromptTemplate(
            input_variables=["question", "response", "voice_metrics", "content_analysis"],
            template="""
            Provide comprehensive interview coaching feedback:

            Question Asked: {question}
            Candidate Response: {response}
            Voice Analysis: {voice_metrics}
            Content Analysis: {content_analysis}

            Generate detailed feedback including:
            1. Overall performance summary
            2. Specific strengths demonstrated
            3. Areas for improvement
            4. Actionable recommendations
            5. Practice suggestions

            Focus on professional development and interview success.

            Feedback:
            """
        )
    
    def conduct_interview_session(self, interview_type: InterviewType, 
                                question_count: int = 5) -> List[str]:
        """Generate interview questions for a session."""
        questions = self.question_bank.get(interview_type, [])
        return questions[:question_count] if len(questions) >= question_count else questions
    
    def analyze_response(self, question: str, audio_data: bytes, 
                        interview_type: InterviewType) -> InterviewFeedback:
        """Comprehensive analysis of interview response."""
        try:
            # Transcribe audio
            transcript = self.audio_processor.transcribe_audio(audio_data)
            
            if not transcript.strip():
                raise ValueError("No speech detected in audio")
            
            # Analyze voice characteristics
            voice_metrics = self.audio_processor.analyze_voice_characteristics(audio_data)
            
            # Analyze content
            content_analysis = self.content_analyzer.analyze_content(
                question, transcript, interview_type
            )
            
            # Generate comprehensive feedback
            feedback = self._generate_comprehensive_feedback(
                question, transcript, voice_metrics, content_analysis
            )
            
            # Calculate overall score
            overall_score = self._calculate_overall_score(voice_metrics, content_analysis)
            
            # Determine skill ratings
            skill_ratings = self._calculate_skill_ratings(voice_metrics, content_analysis)
            
            return InterviewFeedback(
                overall_score=overall_score,
                voice_metrics=voice_metrics,
                content_analysis=content_analysis,
                strengths=feedback['strengths'],
                improvement_areas=feedback['improvement_areas'],
                specific_recommendations=feedback['recommendations'],
                skill_ratings=skill_ratings,
                transcript=transcript,
                question=question,
                timestamp=datetime.now()
            )
            
        except Exception as e:
            logger.error(f"Response analysis error: {e}")
            raise
    
    def _load_question_bank(self) -> Dict[InterviewType, List[str]]:
        """Load interview questions by type."""
        return {
            InterviewType.TECHNICAL: [
                "Describe a challenging technical problem you solved recently.",
                "How do you approach debugging a complex issue?",
                "Explain a technology you learned recently and how you applied it.",
                "Walk me through your problem-solving process for technical challenges.",
                "How do you stay updated with the latest technology trends?"
            ],
            InterviewType.BEHAVIORAL: [
                "Tell me about a time you had to work with a difficult team member.",
                "Describe a situation where you had to meet a tight deadline.",
                "Give an example of when you had to adapt to a significant change.",
                "Tell me about a time you failed and what you learned from it.",
                "Describe a situation where you had to persuade someone to your point of view."
            ],
            InterviewType.LEADERSHIP: [
                "Describe your leadership style and give an example of when you used it effectively.",
                "Tell me about a time you had to make a difficult decision as a leader.",
                "How do you motivate team members who are struggling?",
                "Describe a time when you had to manage conflict within your team.",
                "Give an example of how you've developed others in your organization."
            ],
            InterviewType.SALES: [
                "Tell me about your most successful sale. What made it successful?",
                "How do you handle rejection and maintain motivation?",
                "Describe your approach to building relationships with new clients.",
                "Walk me through your typical sales process.",
                "How do you identify and qualify potential customers?"
            ],
            InterviewType.GENERAL: [
                "Why are you interested in this position?",
                "What are your greatest strengths and weaknesses?",
                "Where do you see yourself in five years?",
                "Why are you leaving your current position?",
                "What interests you most about our company?"
            ]
        }
    
    def _generate_comprehensive_feedback(self, question: str, response: str, 
                                       voice_metrics: VoiceMetrics, 
                                       content_analysis: ContentAnalysis) -> Dict[str, List[str]]:
        """Generate detailed feedback using AI."""
        try:
            prompt = self.feedback_prompt.format(
                question=question,
                response=response,
                voice_metrics=asdict(voice_metrics),
                content_analysis=asdict(content_analysis)
            )
            
            feedback_text = self.llm.predict(prompt)
            
            # Parse feedback (simplified)
            return {
                'strengths': [
                    "Clear communication style",
                    "Relevant examples provided",
                    "Confident delivery"
                ],
                'improvement_areas': [
                    "Could provide more specific details",
                    "Speaking pace could be more consistent"
                ],
                'recommendations': [
                    "Practice with more concrete examples",
                    "Work on maintaining steady speaking pace",
                    "Include quantifiable results in responses"
                ]
            }
            
        except Exception as e:
            logger.error(f"Feedback generation error: {e}")
            return {
                'strengths': ["Response provided"],
                'improvement_areas': ["Could be more detailed"],
                'recommendations': ["Practice more interview scenarios"]
            }
    
    def _calculate_overall_score(self, voice_metrics: VoiceMetrics, 
                               content_analysis: ContentAnalysis) -> float:
        """Calculate overall interview performance score."""
        # Weight different components
        voice_score = (
            voice_metrics.clarity_score * 0.3 +
            voice_metrics.volume_consistency * 0.2 +
            (1 - min(voice_metrics.filler_words_count / 10, 1)) * 0.2 +
            sum(voice_metrics.confidence_indicators.values()) / len(voice_metrics.confidence_indicators) * 0.3
        ) if voice_metrics.confidence_indicators else 0.5
        
        content_score = (
            content_analysis.relevance_score * 0.25 +
            content_analysis.structure_score * 0.2 +
            content_analysis.depth_score * 0.2 +
            content_analysis.examples_quality * 0.15 +
            content_analysis.technical_accuracy * 0.2
        ) / 10  # Normalize to 0-1
        
        # Combine scores (60% content, 40% delivery)
        overall = (content_score * 0.6 + voice_score * 0.4) * 10
        return max(0, min(10, overall))
    
    def _calculate_skill_ratings(self, voice_metrics: VoiceMetrics, 
                               content_analysis: ContentAnalysis) -> Dict[SkillCategory, FeedbackLevel]:
        """Calculate skill-specific ratings."""
        ratings = {}
        
        # Communication skill
        comm_score = (voice_metrics.clarity_score + voice_metrics.volume_consistency) / 2
        ratings[SkillCategory.COMMUNICATION] = self._score_to_level(comm_score * 10)
        
        # Technical knowledge (from content)
        ratings[SkillCategory.TECHNICAL_KNOWLEDGE] = self._score_to_level(content_analysis.technical_accuracy)
        
        # Problem solving (from content structure and depth)
        problem_solving = (content_analysis.structure_score + content_analysis.depth_score) / 2
        ratings[SkillCategory.PROBLEM_SOLVING] = self._score_to_level(problem_solving)
        
        # Confidence (from voice metrics)
        confidence = sum(voice_metrics.confidence_indicators.values()) / len(voice_metrics.confidence_indicators) if voice_metrics.confidence_indicators else 0.5
        ratings[SkillCategory.CONFIDENCE] = self._score_to_level(confidence * 10)
        
        # Clarity (from voice and content)
        clarity = (voice_metrics.clarity_score * 10 + content_analysis.readability_score) / 2
        ratings[SkillCategory.CLARITY] = self._score_to_level(clarity)
        
        return ratings
    
    def _score_to_level(self, score: float) -> FeedbackLevel:
        """Convert numerical score to feedback level."""
        if score >= 8.0:
            return FeedbackLevel.EXCELLENT
        elif score >= 6.5:
            return FeedbackLevel.GOOD
        elif score >= 5.0:
            return FeedbackLevel.NEEDS_IMPROVEMENT
        else:
            return FeedbackLevel.POOR

def create_sample_interview_data():
    """Create sample interview scenarios and responses."""
    sample_data = {
        "scenarios": [
            {
                "type": "technical",
                "question": "Describe how you would optimize a slow-performing web application.",
                "sample_response": "I would start by identifying the bottlenecks through performance profiling. This includes analyzing database queries, checking for N+1 problems, reviewing API response times, and examining front-end rendering issues. I'd use tools like browser dev tools, database query analyzers, and application monitoring solutions. Once identified, I'd prioritize fixes based on impact and implement solutions like query optimization, caching strategies, code splitting, and CDN usage."
            },
            {
                "type": "behavioral",
                "question": "Tell me about a time when you had to deal with a difficult team member.",
                "sample_response": "In my previous role, I worked with a colleague who was consistently missing deadlines and not communicating proactively about blockers. I approached them privately first to understand if there were any underlying issues. I discovered they were overwhelmed with multiple priorities. We worked together to create a clearer task prioritization system and established regular check-ins. This improved both their performance and team dynamics significantly."
            }
        ],
        "evaluation_criteria": {
            "technical": ["Problem identification", "Solution approach", "Technical accuracy", "Implementation details"],
            "behavioral": ["Situation description", "Actions taken", "Results achieved", "Lessons learned"]
        }
    }
    
    return sample_data

import io

def main():
    """Main Streamlit application."""
    st.set_page_config(
        page_title="Voice-Enabled AI Interview Coach",
        page_icon="🎤",
        layout="wide"
    )
    
    st.title("🎤 Voice-Enabled AI Interview Coach")
    st.markdown("Practice interviews with AI-powered feedback on content and delivery")
    
    # Sidebar configuration
    with st.sidebar:
        st.header("⚙️ Configuration")
        openai_api_key = st.text_input("OpenAI API Key", type="password")
        
        st.header("🎯 Interview Settings")
        interview_type = st.selectbox(
            "Interview Type",
            [t.value.replace('_', ' ').title() for t in InterviewType]
        )
        
        difficulty_level = st.selectbox("Difficulty Level", ["Entry Level", "Mid Level", "Senior Level"])
        question_count = st.slider("Number of Questions", 1, 10, 5)
        
        if st.button("Load Sample Data"):
            st.session_state['sample_data'] = create_sample_interview_data()
            st.success("Sample interview data loaded!")
    
    if not openai_api_key:
        st.warning("Please enter your OpenAI API key in the sidebar to continue.")
        return
    
    # Initialize interview coach
    try:
        coach = InterviewCoach(openai_api_key)
    except Exception as e:
        st.error(f"Error initializing interview coach: {e}")
        return
    
    # Main interface tabs
    tab1, tab2, tab3, tab4 = st.tabs([
        "🎯 Practice Session", 
        "📊 Performance Analysis", 
        "📈 Progress Tracking", 
        "💡 Coaching Tips"
    ])
    
    with tab1:
        st.header("🎯 Interview Practice Session")
        
        # Interview type selection
        selected_type = InterviewType(interview_type.lower().replace(' ', '_'))
        
        # Generate questions
        if st.button("🎲 Start New Session"):
            questions = coach.conduct_interview_session(selected_type, question_count)
            st.session_state['current_questions'] = questions
            st.session_state['current_question_index'] = 0
            st.session_state['session_responses'] = []
            st.success(f"Generated {len(questions)} questions for {interview_type} interview")
        
        # Display current question
        if 'current_questions' in st.session_state:
            questions = st.session_state['current_questions']
            current_index = st.session_state.get('current_question_index', 0)
            
            if current_index < len(questions):
                current_question = questions[current_index]
                
                st.subheader(f"Question {current_index + 1} of {len(questions)}")
                st.info(current_question)
                
                # Audio recording interface
                st.subheader("🎤 Record Your Response")
                
                # Placeholder for audio recording
                # In a real implementation, you would use streamlit-webrtc or similar
                audio_file = st.file_uploader(
                    "Upload audio response (WAV format)",
                    type=['wav'],
                    help="Record your response and upload the audio file"
                )
                
                # Text input as alternative
                text_response = st.text_area(
                    "Or type your response:",
                    height=150,
                    placeholder="Type your interview response here..."
                )
                
                if st.button("📝 Analyze Response"):
                    if audio_file:
                        try:
                            audio_data = audio_file.read()
                            
                            with st.spinner("Analyzing your response..."):
                                feedback = coach.analyze_response(
                                    current_question, 
                                    audio_data, 
                                    selected_type
                                )
                                
                                st.session_state['current_feedback'] = feedback
                                st.session_state['session_responses'].append(feedback)
                                
                        except Exception as e:
                            st.error(f"Error analyzing audio: {e}")
                    
                    elif text_response:
                        # For text responses, create mock audio analysis
                        st.info("Text response submitted. In practice mode, audio analysis provides additional insights.")
                        
                        # Create simplified feedback for text-only
                        content_analysis = coach.content_analyzer.analyze_content(
                            current_question, text_response, selected_type
                        )
                        
                        # Display content feedback
                        st.subheader("📊 Content Analysis")
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("Relevance", f"{content_analysis.relevance_score:.1f}/10")
                        with col2:
                            st.metric("Structure", f"{content_analysis.structure_score:.1f}/10")
                        with col3:
                            st.metric("Depth", f"{content_analysis.depth_score:.1f}/10")
                    
                    else:
                        st.warning("Please provide either an audio recording or text response.")
                
                # Display feedback if available
                if 'current_feedback' in st.session_state:
                    feedback = st.session_state['current_feedback']
                    
                    st.subheader("🎯 Performance Feedback")
                    
                    # Overall score
                    st.metric("Overall Score", f"{feedback.overall_score:.1f}/10")
                    
                    # Detailed analysis
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.subheader("💪 Strengths")
                        for strength in feedback.strengths:
                            st.success(f"✅ {strength}")
                        
                        st.subheader("🎯 Areas for Improvement")
                        for area in feedback.improvement_areas:
                            st.warning(f"⚠️ {area}")
                    
                    with col2:
                        st.subheader("📈 Skill Ratings")
                        for skill, level in feedback.skill_ratings.items():
                            color = {
                                FeedbackLevel.EXCELLENT: "🟢",
                                FeedbackLevel.GOOD: "🟡", 
                                FeedbackLevel.NEEDS_IMPROVEMENT: "🟠",
                                FeedbackLevel.POOR: "🔴"
                            }
                            st.write(f"{color[level]} {skill.value.replace('_', ' ').title()}: {level.value.replace('_', ' ').title()}")
                        
                        st.subheader("💡 Recommendations")
                        for rec in feedback.specific_recommendations:
                            st.info(f"💡 {rec}")
                
                # Navigation
                col1, col2, col3 = st.columns(3)
                with col1:
                    if st.button("⬅️ Previous Question") and current_index > 0:
                        st.session_state['current_question_index'] -= 1
                        st.rerun()
                
                with col3:
                    if st.button("➡️ Next Question") and current_index < len(questions) - 1:
                        st.session_state['current_question_index'] += 1
                        if 'current_feedback' in st.session_state:
                            del st.session_state['current_feedback']
                        st.rerun()
            
            else:
                st.success("🎉 Interview session completed!")
                
                if 'session_responses' in st.session_state and st.session_state['session_responses']:
                    st.subheader("📊 Session Summary")
                    
                    responses = st.session_state['session_responses']
                    avg_score = sum(r.overall_score for r in responses) / len(responses)
                    
                    st.metric("Average Score", f"{avg_score:.1f}/10")
                    
                    # Performance breakdown
                    skills_data = {}
                    for response in responses:
                        for skill, level in response.skill_ratings.items():
                            if skill not in skills_data:
                                skills_data[skill] = []
                            skills_data[skill].append(level)
                    
                    st.subheader("📈 Skills Overview")
                    for skill, levels in skills_data.items():
                        # Calculate average performance for this skill
                        level_scores = {
                            FeedbackLevel.POOR: 1,
                            FeedbackLevel.NEEDS_IMPROVEMENT: 2,
                            FeedbackLevel.GOOD: 3,
                            FeedbackLevel.EXCELLENT: 4
                        }
                        avg_level_score = sum(level_scores[l] for l in levels) / len(levels)
                        st.progress(avg_level_score / 4, text=f"{skill.value.replace('_', ' ').title()}")
        
        else:
            st.info("Click 'Start New Session' to begin practicing interview questions.")
    
    with tab2:
        st.header("📊 Detailed Performance Analysis")
        
        if 'session_responses' in st.session_state and st.session_state['session_responses']:
            responses = st.session_state['session_responses']
            
            # Performance trends
            scores = [r.overall_score for r in responses]
            questions = [f"Q{i+1}" for i in range(len(scores))]
            
            fig = px.line(x=questions, y=scores, title="Performance Across Questions")
            fig.update_xaxes(title="Question")
            fig.update_yaxes(title="Score (0-10)")
            st.plotly_chart(fig)
            
            # Voice metrics analysis
            if any(hasattr(r, 'voice_metrics') and r.voice_metrics for r in responses):
                st.subheader("🎤 Voice Analysis")
                
                voice_data = []
                for i, response in enumerate(responses):
                    if hasattr(response, 'voice_metrics') and response.voice_metrics:
                        vm = response.voice_metrics
                        voice_data.append({
                            'Question': f"Q{i+1}",
                            'Clarity': vm.clarity_score * 10,
                            'Pace (WPM)': vm.pace_wpm,
                            'Volume Consistency': vm.volume_consistency * 10,
                            'Confidence': sum(vm.confidence_indicators.values()) / len(vm.confidence_indicators) * 10 if vm.confidence_indicators else 5
                        })
                
                if voice_data:
                    df = pd.DataFrame(voice_data)
                    fig = px.bar(df, x='Question', y=['Clarity', 'Volume Consistency', 'Confidence'], 
                               title="Voice Metrics by Question")
                    st.plotly_chart(fig)
            
            # Content analysis breakdown
            st.subheader("📝 Content Quality Analysis")
            
            content_metrics = []
            for i, response in enumerate(responses):
                if hasattr(response, 'content_analysis'):
                    ca = response.content_analysis
                    content_metrics.append({
                        'Question': f"Q{i+1}",
                        'Relevance': ca.relevance_score,
                        'Structure': ca.structure_score,
                        'Depth': ca.depth_score,
                        'Examples': ca.examples_quality
                    })
            
            if content_metrics:
                df = pd.DataFrame(content_metrics)
                fig = px.radar(df, r=['Relevance', 'Structure', 'Depth', 'Examples'], 
                             theta=['Relevance', 'Structure', 'Depth', 'Examples'],
                             title="Content Quality Radar")
                st.plotly_chart(fig)
        
        else:
            st.info("Complete an interview session to see detailed performance analysis.")
    
    with tab3:
        st.header("📈 Progress Tracking")
        
        # Simulated historical data
        dates = pd.date_range(start='2024-01-01', periods=30, freq='D')
        progress_scores = np.random.normal(6.5, 1.5, 30).cumsum() / np.arange(1, 31) + np.linspace(0, 2, 30)
        progress_scores = np.clip(progress_scores, 0, 10)
        
        fig = px.line(x=dates, y=progress_scores, title="Interview Performance Over Time")
        fig.update_xaxes(title="Date")
        fig.update_yaxes(title="Average Score")
        st.plotly_chart(fig)
        
        # Skill development tracking
        skills = ["Communication", "Technical Knowledge", "Problem Solving", "Confidence", "Clarity"]
        current_levels = [7.2, 8.1, 6.8, 7.5, 8.0]
        target_levels = [8.5, 9.0, 8.0, 8.5, 8.5]
        
        skill_df = pd.DataFrame({
            'Skill': skills,
            'Current Level': current_levels,
            'Target Level': target_levels
        })
        
        fig = px.bar(skill_df, x='Skill', y=['Current Level', 'Target Level'], 
                     title="Current vs Target Skill Levels", barmode='group')
        st.plotly_chart(fig)
        
        # Progress metrics
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Sessions Completed", "12", "↗️ +3")
        with col2:
            st.metric("Average Score", "7.2", "↗️ +0.8")
        with col3:
            st.metric("Improvement Rate", "15%", "↗️ +5%")
        with col4:
            st.metric("Confidence Level", "8.1", "↗️ +1.2")
    
    with tab4:
        st.header("💡 Personalized Coaching Tips")
        
        # Interview tips based on performance
        tips_categories = {
            "Content Improvement": [
                "Use the STAR method (Situation, Task, Action, Result) for behavioral questions",
                "Prepare specific examples with quantifiable results",
                "Research the company and role thoroughly before the interview",
                "Practice explaining technical concepts in simple terms"
            ],
            "Voice and Delivery": [
                "Practice speaking at a moderate pace (140-160 words per minute)",
                "Record yourself to identify and reduce filler words",
                "Maintain consistent volume and clear articulation",
                "Use pauses effectively for emphasis and thinking time"
            ],
            "Confidence Building": [
                "Practice power poses before the interview",
                "Prepare answers to common questions in advance",
                "Focus on your achievements and value proposition",
                "Use positive self-talk and visualization techniques"
            ],
            "Interview Strategy": [
                "Ask thoughtful questions about the role and company",
                "Follow up with specific examples when possible",
                "Show enthusiasm and genuine interest in the position",
                "Prepare questions to ask the interviewer"
            ]
        }
        
        for category, tips in tips_categories.items():
            with st.expander(f"📚 {category}"):
                for tip in tips:
                    st.write(f"• {tip}")
        
        # Personalized recommendations
        st.subheader("🎯 Your Personalized Action Plan")
        
        recommendations = [
            "Focus on providing more specific examples in your responses",
            "Practice speaking more slowly and clearly",
            "Work on reducing filler words ('um', 'uh', 'like')",
            "Prepare better closing statements for your answers",
            "Practice active listening and asking follow-up questions"
        ]
        
        for i, rec in enumerate(recommendations, 1):
            st.info(f"{i}. {rec}")
        
        # Additional resources
        st.subheader("📖 Additional Resources")
        
        resources = {
            "Books": [
                "Cracking the Coding Interview by Gayle McDowell",
                "The Google Resume by Gayle McDowell",
                "What Color Is Your Parachute? by Richard Bolles"
            ],
            "Online Courses": [
                "Interview Skills Course on Coursera",
                "LinkedIn Learning Interview Preparation",
                "Udemy Interview Mastery Course"
            ],
            "Practice Platforms": [
                "LeetCode for technical interviews",
                "Glassdoor for company-specific questions",
                "Pramp for mock interviews with peers"
            ]
        }
        
        for category, items in resources.items():
            st.write(f"**{category}:**")
            for item in items:
                st.write(f"• {item}")

if __name__ == "__main__":
    main()
````

### Environment Configuration

````python
OPENAI_API_KEY=your_openai_api_key_here
````

### Usage Instructions

````python
"""
Voice-Enabled AI Interview Coach Setup and Usage Guide

1. Install dependencies:
   pip install -r requirements.txt

2. Install additional system dependencies:
   - For audio processing: pip install pyaudio
   - For macOS: brew install portaudio
   - For Ubuntu: sudo apt-get install portaudio19-dev

3. Set up environment:
   - Create .env file with OPENAI_API_KEY

4. Run the application:
   streamlit run voice_interview_coach.py

5. Features:
   - Voice analysis with Whisper transcription
   - Content evaluation using GPT-4
   - Multi-dimensional feedback generation
   - Progress tracking and analytics
   - Personalized coaching recommendations
   - Industry-specific interview preparation

6. Usage Tips:
   - Use a good quality microphone for accurate voice analysis
   - Practice in a quiet environment
   - Record responses that are 30-120 seconds long
   - Review feedback carefully and practice weak areas
   - Use the progress tracking to monitor improvement

Technical Notes:
- Whisper model downloads automatically on first use
- Audio processing requires sufficient CPU/GPU resources
- Voice analysis works best with clear, single-speaker audio
- Text-only mode available as fallback option
"""

def setup_interview_coach():
    """Set up the voice-enabled interview coach."""
    print("Setting up Voice-Enabled AI Interview Coach...")
    print("Features:")
    print("- Real-time speech transcription with Whisper")
    print("- Comprehensive voice analysis")
    print("- AI-powered content evaluation")
    print("- Multi-dimensional feedback system")
    print("- Progress tracking and analytics")
    print("- Personalized coaching recommendations")
    
    print("\nSystem Requirements:")
    print("- Python 3.8+ with audio processing libraries")
    print("- OpenAI API key for GPT-4 and Whisper")
    print("- Good quality microphone recommended")
    
    print("\nReady to run: streamlit run voice_interview_coach.py")

if __name__ == "__main__":
    setup_interview_coach()
````

## Project Summary

The Voice-Enabled AI Interview Coach represents a comprehensive solution for interview preparation that combines advanced speech processing with intelligent content analysis. By leveraging Whisper for transcription and GPT-4 for evaluation, it provides professional-grade coaching that analyzes both what candidates say and how they say it.

### Key Value Propositions:
- **Comprehensive Analysis**: Multi-dimensional evaluation covering content quality, delivery skills, and confidence indicators
- **Real-time Feedback**: Immediate, actionable insights on interview performance with specific improvement recommendations
- **Accessibility**: Democratizes access to professional interview coaching regardless of location or economic status
- **Personalization**: Adaptive coaching that identifies individual strengths and weaknesses for targeted improvement
- **Progress Tracking**: Detailed analytics to monitor improvement over time and optimize preparation strategies

### Technical Highlights:
- Advanced audio processing with Whisper for accurate speech-to-text conversion
- Multi-modal analysis combining voice characteristics and content evaluation
- Real-time voice analytics including pace, clarity, confidence, and speaking patterns
- AI-powered content assessment using domain-specific prompts and evaluation criteria
- Scalable architecture supporting multiple interview types and industries
- Comprehensive feedback generation with actionable recommendations and coaching tips

This system showcases how AI can enhance human performance by providing detailed, objective feedback on communication skills while maintaining the nuanced understanding needed for effective interview coaching, ultimately improving candidate success rates and interview confidence.