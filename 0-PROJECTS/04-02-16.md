<small>Claude Sonnet 4 **(LLM-Powered Agent for Research Paper Insights)**</small>
# LLM-Powered Agent for Research Paper Insights

## Key Concepts Explanation

### PDF Document Ingestion
Automated process of extracting and preprocessing textual content from academic PDF papers, handling complex layouts, mathematical equations, figures, and references while preserving semantic structure and metadata for optimal downstream processing.

### Vector Embeddings & Semantic Search
Advanced text representation technique that converts document chunks into high-dimensional vectors capturing semantic meaning, enabling similarity-based retrieval of relevant content through cosine similarity calculations and approximate nearest neighbor search algorithms.

### Retrieval-Augmented Generation (RAG)
Hybrid AI architecture combining information retrieval with generative language models, where relevant document chunks are first retrieved based on query similarity, then provided as context to LLMs for generating accurate, grounded responses with source attribution.

### Citation Extraction & Entity Recognition
Intelligent parsing system that identifies and extracts bibliographic references, author names, publication venues, and citation relationships from academic papers using named entity recognition and pattern matching techniques for comprehensive literature mapping.

### Multi-Modal Document Understanding
Advanced document processing capability that handles not only text but also tables, figures, equations, and other non-textual elements through specialized parsers and OCR technologies for complete document comprehension.

### Hierarchical Text Chunking
Strategic document segmentation approach that respects academic paper structure (abstract, sections, paragraphs) while maintaining optimal chunk sizes for both embedding generation and context window limitations in language models.

## Comprehensive Project Explanation

### Objectives
The LLM-Powered Research Paper Insights Agent aims to revolutionize academic research by providing intelligent document analysis, automated literature review capabilities, and interactive question-answering systems that help researchers quickly extract insights, identify connections, and accelerate discovery processes.

### Key Features
- **Intelligent PDF Processing**: Automated extraction of text, tables, figures, and metadata from complex academic documents
- **Semantic Literature Search**: Vector-based similarity search across large corpora of research papers
- **Interactive Q&A Interface**: Natural language querying with context-aware responses and source citations
- **Citation Network Analysis**: Automated extraction and visualization of citation relationships and academic lineage
- **Multi-Paper Synthesis**: Cross-document analysis for identifying trends, contradictions, and research gaps
- **Research Trend Discovery**: Temporal analysis of research topics and emerging themes across literature

### Challenges
- **Document Complexity**: Academic papers contain complex layouts, mathematical notation, and specialized formatting
- **Scale and Performance**: Processing thousands of papers while maintaining fast query response times
- **Accuracy and Hallucination**: Ensuring generated insights are grounded in source documents without fabrication
- **Citation Integrity**: Maintaining accurate attribution and preventing misrepresentation of research findings
- **Domain Specialization**: Handling diverse academic fields with varying terminologies and conventions
- **Version Control**: Managing updates to papers and maintaining consistency across document versions

### Potential Impact
This system can dramatically accelerate research processes, democratize access to literature analysis, reduce time spent on manual paper review, enable discovery of hidden connections between research areas, and support evidence-based decision making in academic and industrial research contexts.

## Comprehensive Project Example with Python Implementation

### Dependencies and Setup

````python
# requirements.txt
langchain==0.1.0
langchain-openai==0.0.5
langchain-community==0.0.10
openai==1.6.1
streamlit==1.29.0
PyPDF2==3.0.1
pdfplumber==0.9.0
pymupdf==1.23.14
chromadb==0.4.18
faiss-cpu==1.7.4
sentence-transformers==2.2.2
tiktoken==0.5.2
pandas==2.1.4
numpy==1.24.3
plotly==5.17.0
networkx==3.2.1
spacy==3.7.2
scikit-learn==1.3.2
python-dotenv==1.0.0
pydantic==2.5.0
requests==2.31.0
beautifulsoup4==4.12.2
arxiv==1.4.8
scholarly==1.7.11
regex==2023.10.3
textstat==0.7.3
wordcloud==1.9.2
matplotlib==3.8.2
seaborn==0.12.2
````

### Core Implementation

````python
import os
import re
import json
import logging
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import networkx as nx

# Document processing
import PyPDF2
import pdfplumber
import fitz  # PyMuPDF
from io import BytesIO

# LLM and embeddings
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from langchain.prompts import ChatPromptTemplate
from langchain.schema import BaseRetriever
from langchain.vectorstores import Chroma, FAISS
from langchain.chains import RetrievalQA
from langchain.memory import ConversationBufferMemory

# Vector databases
import chromadb
from chromadb.config import Settings
import faiss
from sentence_transformers import SentenceTransformer

# Text processing
import spacy
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import tiktoken
import textstat

# External APIs
import arxiv
import requests
from bs4 import BeautifulSoup

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class Citation:
    title: str
    authors: List[str]
    venue: str
    year: int
    doi: Optional[str] = None
    url: Optional[str] = None
    citation_text: str = ""

@dataclass
class ResearchPaper:
    paper_id: str
    title: str
    authors: List[str]
    abstract: str
    content: str
    citations: List[Citation]
    metadata: Dict[str, Any]
    file_path: Optional[str] = None
    processed_at: datetime = field(default_factory=datetime.now)

@dataclass
class DocumentChunk:
    chunk_id: str
    paper_id: str
    content: str
    section: str
    page_number: int
    embedding: Optional[np.ndarray] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

class PDFProcessor:
    """Extract and process content from PDF research papers."""
    
    def __init__(self):
        self.nlp = spacy.load("en_core_web_sm")
        self.citation_patterns = [
            r'\[(\d+(?:,\s*\d+)*)\]',  # [1], [1,2,3]
            r'\(([^)]+,\s*\d{4})\)',   # (Author, 2023)
            r'([A-Z][a-z]+ et al\.,? \d{4})',  # Smith et al., 2023
        ]
    
    def extract_text_from_pdf(self, pdf_path: str) -> Dict[str, Any]:
        """Extract text and metadata from PDF."""
        try:
            extracted_data = {
                'title': '',
                'authors': [],
                'abstract': '',
                'content': '',
                'sections': {},
                'citations': [],
                'metadata': {}
            }
            
            # Try pdfplumber first for better layout preservation
            try:
                with pdfplumber.open(pdf_path) as pdf:
                    full_text = ""
                    for page_num, page in enumerate(pdf.pages, 1):
                        page_text = page.extract_text()
                        if page_text:
                            full_text += f"\n--- Page {page_num} ---\n{page_text}"
                    
                    extracted_data['content'] = full_text
                    
            except Exception as e:
                logger.warning(f"pdfplumber failed, trying PyMuPDF: {e}")
                
                # Fallback to PyMuPDF
                doc = fitz.open(pdf_path)
                full_text = ""
                for page_num in range(doc.page_count):
                    page = doc[page_num]
                    page_text = page.get_text()
                    full_text += f"\n--- Page {page_num + 1} ---\n{page_text}"
                
                extracted_data['content'] = full_text
                doc.close()
            
            # Extract structured information
            extracted_data.update(self._extract_paper_structure(extracted_data['content']))
            extracted_data['citations'] = self._extract_citations(extracted_data['content'])
            
            return extracted_data
            
        except Exception as e:
            logger.error(f"PDF extraction error for {pdf_path}: {e}")
            raise
    
    def _extract_paper_structure(self, text: str) -> Dict[str, Any]:
        """Extract title, abstract, and sections from paper text."""
        structure = {
            'title': '',
            'authors': [],
            'abstract': '',
            'sections': {}
        }
        
        lines = text.split('\n')
        
        # Simple heuristics for title extraction
        for i, line in enumerate(lines[:20]):  # Check first 20 lines
            line = line.strip()
            if len(line) > 10 and len(line) < 200 and not line.lower().startswith('page'):
                if any(char.isupper() for char in line) and not line.isupper():
                    structure['title'] = line
                    break
        
        # Extract abstract
        abstract_pattern = r'(?i)(abstract|summary)\s*:?\s*(.*?)(?=\n\s*(?:1\.|introduction|keywords))'
        abstract_match = re.search(abstract_pattern, text, re.DOTALL)
        if abstract_match:
            structure['abstract'] = abstract_match.group(2).strip()
        
        # Extract sections
        section_pattern = r'\n\s*(\d+\.?\s+[A-Z][A-Za-z\s]+)\n'
        sections = re.findall(section_pattern, text)
        
        current_section = "Introduction"
        section_content = ""
        
        for line in lines:
            line = line.strip()
            if any(line.startswith(section) for section in sections):
                if section_content:
                    structure['sections'][current_section] = section_content
                current_section = line
                section_content = ""
            else:
                section_content += line + "\n"
        
        if section_content:
            structure['sections'][current_section] = section_content
        
        return structure
    
    def _extract_citations(self, text: str) -> List[Citation]:
        """Extract citations from paper text."""
        citations = []
        
        # Look for references section
        ref_pattern = r'(?i)(references|bibliography)\s*\n(.*?)(?=\n\s*(?:appendix|$))'
        ref_match = re.search(ref_pattern, text, re.DOTALL)
        
        if ref_match:
            references_text = ref_match.group(2)
            
            # Split by numbered references
            ref_entries = re.split(r'\n\s*\[\d+\]|\n\s*\d+\.', references_text)
            
            for entry in ref_entries:
                citation = self._parse_citation_entry(entry.strip())
                if citation:
                    citations.append(citation)
        
        return citations
    
    def _parse_citation_entry(self, entry: str) -> Optional[Citation]:
        """Parse individual citation entry."""
        if len(entry) < 20:  # Too short to be a citation
            return None
        
        try:
            # Simple parsing - can be enhanced with more sophisticated NLP
            title_match = re.search(r'"([^"]+)"', entry)
            title = title_match.group(1) if title_match else entry.split('.')[0]
            
            # Extract year
            year_match = re.search(r'\b(19|20)\d{2}\b', entry)
            year = int(year_match.group()) if year_match else 0
            
            # Extract authors (simplified)
            authors_part = entry.split('.')[0] if '.' in entry else entry
            authors = [authors_part.strip()]
            
            # Extract venue (journal/conference)
            venue_patterns = [
                r'(?:In\s+)?([A-Z][^,.\n]+(?:Journal|Conference|Proceedings|Review))',
                r'(?:In\s+)?([A-Z][^,.\n]+(?:Science|Nature|IEEE|ACM))',
            ]
            
            venue = ""
            for pattern in venue_patterns:
                venue_match = re.search(pattern, entry)
                if venue_match:
                    venue = venue_match.group(1)
                    break
            
            return Citation(
                title=title,
                authors=authors,
                venue=venue,
                year=year,
                citation_text=entry
            )
            
        except Exception as e:
            logger.warning(f"Citation parsing error: {e}")
            return None

class DocumentChunker:
    """Intelligent chunking of research papers."""
    
    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", ". ", "! ", "? ", " ", ""]
        )
        
        # Token counter for OpenAI models
        self.encoding = tiktoken.get_encoding("cl100k_base")
    
    def chunk_paper(self, paper: ResearchPaper) -> List[DocumentChunk]:
        """Create semantic chunks from research paper."""
        chunks = []
        
        # Chunk abstract separately
        if paper.abstract:
            abstract_chunks = self._create_chunks(
                paper.abstract, paper.paper_id, "abstract", 1
            )
            chunks.extend(abstract_chunks)
        
        # Chunk main content
        if paper.content:
            content_chunks = self._create_chunks(
                paper.content, paper.paper_id, "content", 1
            )
            chunks.extend(content_chunks)
        
        return chunks
    
    def _create_chunks(self, text: str, paper_id: str, 
                      section: str, start_page: int) -> List[DocumentChunk]:
        """Create chunks from text with metadata."""
        chunks = []
        
        # Split text into chunks
        text_chunks = self.text_splitter.split_text(text)
        
        for i, chunk_text in enumerate(text_chunks):
            # Calculate token count
            token_count = len(self.encoding.encode(chunk_text))
            
            chunk = DocumentChunk(
                chunk_id=f"{paper_id}_chunk_{i}",
                paper_id=paper_id,
                content=chunk_text,
                section=section,
                page_number=start_page + (i // 3),  # Rough page estimation
                metadata={
                    'token_count': token_count,
                    'chunk_index': i,
                    'readability_score': textstat.flesch_reading_ease(chunk_text)
                }
            )
            chunks.append(chunk)
        
        return chunks
    
    def optimize_chunk_size(self, text: str, target_token_limit: int = 8000) -> int:
        """Optimize chunk size based on content characteristics."""
        total_tokens = len(self.encoding.encode(text))
        
        if total_tokens <= target_token_limit:
            return len(text)
        
        # Calculate optimal chunk size
        num_chunks = max(1, total_tokens // target_token_limit)
        optimal_chunk_size = len(text) // num_chunks
        
        return min(optimal_chunk_size, self.chunk_size)

class VectorStoreManager:
    """Manage vector storage and similarity search."""
    
    def __init__(self, embedding_model: str = "text-embedding-ada-002"):
        self.embeddings = OpenAIEmbeddings(model=embedding_model)
        self.sentence_transformer = SentenceTransformer('all-MiniLM-L6-v2')
        
        # Initialize vector stores
        self.chroma_client = chromadb.Client(Settings(anonymized_telemetry=False))
        self.collection_name = "research_papers"
        
        try:
            self.collection = self.chroma_client.get_collection(self.collection_name)
        except:
            self.collection = self.chroma_client.create_collection(self.collection_name)
        
        # FAISS index for fast similarity search
        self.faiss_index = None
        self.chunk_metadata = {}
    
    def add_paper_chunks(self, chunks: List[DocumentChunk]) -> bool:
        """Add paper chunks to vector store."""
        try:
            # Prepare documents for ChromaDB
            documents = []
            metadatas = []
            ids = []
            
            for chunk in chunks:
                documents.append(chunk.content)
                metadatas.append({
                    'paper_id': chunk.paper_id,
                    'section': chunk.section,
                    'page_number': chunk.page_number,
                    **chunk.metadata
                })
                ids.append(chunk.chunk_id)
            
            # Add to ChromaDB
            self.collection.add(
                documents=documents,
                metadatas=metadatas,
                ids=ids
            )
            
            # Generate embeddings for FAISS
            embeddings = self.sentence_transformer.encode(documents)
            
            # Initialize or update FAISS index
            if self.faiss_index is None:
                dimension = embeddings.shape[1]
                self.faiss_index = faiss.IndexFlatIP(dimension)
            
            # Normalize embeddings for cosine similarity
            faiss.normalize_L2(embeddings)
            self.faiss_index.add(embeddings.astype(np.float32))
            
            # Store metadata for FAISS results
            for i, chunk in enumerate(chunks):
                self.chunk_metadata[len(self.chunk_metadata)] = chunk
            
            logger.info(f"Added {len(chunks)} chunks to vector store")
            return True
            
        except Exception as e:
            logger.error(f"Error adding chunks to vector store: {e}")
            return False
    
    def similarity_search(self, query: str, k: int = 5, 
                         filter_metadata: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """Perform similarity search across all papers."""
        try:
            # Use ChromaDB for filtered search
            if filter_metadata:
                results = self.collection.query(
                    query_texts=[query],
                    n_results=k,
                    where=filter_metadata
                )
            else:
                results = self.collection.query(
                    query_texts=[query],
                    n_results=k
                )
            
            # Format results
            search_results = []
            if results['documents'] and results['documents'][0]:
                for i, (doc, metadata, distance) in enumerate(zip(
                    results['documents'][0],
                    results['metadatas'][0],
                    results['distances'][0]
                )):
                    search_results.append({
                        'content': doc,
                        'metadata': metadata,
                        'similarity_score': 1 - distance,  # Convert distance to similarity
                        'rank': i + 1
                    })
            
            return search_results
            
        except Exception as e:
            logger.error(f"Similarity search error: {e}")
            return []
    
    def get_paper_statistics(self) -> Dict[str, Any]:
        """Get statistics about stored papers."""
        try:
            # Get collection info
            collection_count = self.collection.count()
            
            # Get all metadata to analyze
            all_results = self.collection.get()
            
            if not all_results['metadatas']:
                return {'total_chunks': 0}
            
            # Analyze metadata
            papers = set()
            sections = {}
            
            for metadata in all_results['metadatas']:
                papers.add(metadata.get('paper_id', 'unknown'))
                section = metadata.get('section', 'unknown')
                sections[section] = sections.get(section, 0) + 1
            
            return {
                'total_chunks': collection_count,
                'unique_papers': len(papers),
                'sections_distribution': sections,
                'average_chunks_per_paper': collection_count / len(papers) if papers else 0
            }
            
        except Exception as e:
            logger.error(f"Statistics error: {e}")
            return {'error': str(e)}

class CitationAnalyzer:
    """Analyze citation networks and relationships."""
    
    def __init__(self):
        self.citation_graph = nx.DiGraph()
        self.papers_db = {}
    
    def add_paper_citations(self, paper: ResearchPaper):
        """Add paper and its citations to the graph."""
        paper_node = paper.paper_id
        
        # Add paper to graph
        self.citation_graph.add_node(paper_node, 
                                   title=paper.title,
                                   authors=paper.authors,
                                   year=paper.metadata.get('year', 0))
        
        self.papers_db[paper_node] = paper
        
        # Add citation edges
        for citation in paper.citations:
            citation_id = self._generate_citation_id(citation)
            
            # Add citation node if not exists
            if not self.citation_graph.has_node(citation_id):
                self.citation_graph.add_node(citation_id,
                                           title=citation.title,
                                           authors=citation.authors,
                                           year=citation.year)
            
            # Add edge from paper to citation
            self.citation_graph.add_edge(paper_node, citation_id)
    
    def _generate_citation_id(self, citation: Citation) -> str:
        """Generate unique ID for citation."""
        # Use title and year to create ID
        title_clean = re.sub(r'[^a-zA-Z0-9]', '_', citation.title[:50])
        return f"cite_{title_clean}_{citation.year}"
    
    def find_influential_papers(self, top_k: int = 10) -> List[Dict[str, Any]]:
        """Find most influential papers by citation count."""
        try:
            # Calculate in-degree (times cited)
            in_degrees = dict(self.citation_graph.in_degree())
            
            # Sort by citation count
            sorted_papers = sorted(in_degrees.items(), 
                                 key=lambda x: x[1], reverse=True)
            
            influential_papers = []
            for paper_id, citation_count in sorted_papers[:top_k]:
                if paper_id in self.papers_db:
                    paper = self.papers_db[paper_id]
                    influential_papers.append({
                        'paper_id': paper_id,
                        'title': paper.title,
                        'authors': paper.authors,
                        'citation_count': citation_count,
                        'year': paper.metadata.get('year', 0)
                    })
            
            return influential_papers
            
        except Exception as e:
            logger.error(f"Influential papers analysis error: {e}")
            return []
    
    def find_citation_clusters(self, min_cluster_size: int = 3) -> List[List[str]]:
        """Find clusters of related papers."""
        try:
            # Use community detection algorithm
            undirected_graph = self.citation_graph.to_undirected()
            
            # Simple clustering based on connected components
            components = list(nx.connected_components(undirected_graph))
            
            # Filter by minimum size
            clusters = [list(component) for component in components 
                       if len(component) >= min_cluster_size]
            
            return clusters
            
        except Exception as e:
            logger.error(f"Citation clustering error: {e}")
            return []
    
    def analyze_temporal_trends(self) -> Dict[str, Any]:
        """Analyze citation trends over time."""
        try:
            year_citations = {}
            
            for node, data in self.citation_graph.nodes(data=True):
                year = data.get('year', 0)
                if year > 1900:  # Valid year
                    year_citations[year] = year_citations.get(year, 0) + 1
            
            # Calculate trends
            years = sorted(year_citations.keys())
            citations = [year_citations[year] for year in years]
            
            return {
                'years': years,
                'citation_counts': citations,
                'total_papers': len(self.papers_db),
                'year_range': (min(years), max(years)) if years else (0, 0)
            }
            
        except Exception as e:
            logger.error(f"Temporal analysis error: {e}")
            return {}

class ResearchQASystem:
    """Question-answering system for research papers."""
    
    def __init__(self, llm: ChatOpenAI, vector_store: VectorStoreManager):
        self.llm = llm
        self.vector_store = vector_store
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True
        )
        
        # QA prompts
        self.qa_prompt = ChatPromptTemplate.from_template("""
        You are a research assistant helping analyze academic papers. 
        Use the provided research paper excerpts to answer the question accurately.
        
        Context from research papers:
        {context}
        
        Question: {question}
        
        Instructions:
        1. Answer based solely on the provided context
        2. Include specific citations and page references when available
        3. If the context doesn't contain enough information, say so clearly
        4. Highlight any conflicting information between papers
        5. Provide direct quotes when relevant
        
        Answer:
        """)
        
        self.synthesis_prompt = ChatPromptTemplate.from_template("""
        Synthesize information from multiple research papers to answer this question:
        
        Question: {question}
        
        Research paper excerpts:
        {context}
        
        Provide a comprehensive synthesis that:
        1. Combines insights from multiple sources
        2. Identifies patterns and trends
        3. Notes any contradictions or disagreements
        4. Suggests areas for future research
        5. Cites specific papers and sections
        
        Synthesis:
        """)
    
    def answer_question(self, question: str, paper_filter: Dict[str, Any] = None,
                       synthesis_mode: bool = False) -> Dict[str, Any]:
        """Answer question using retrieved paper content."""
        try:
            # Retrieve relevant chunks
            search_results = self.vector_store.similarity_search(
                question, k=10, filter_metadata=paper_filter
            )
            
            if not search_results:
                return {
                    'answer': "I couldn't find relevant information in the available papers to answer your question.",
                    'sources': [],
                    'confidence': 0.0
                }
            
            # Prepare context
            context_parts = []
            sources = []
            
            for result in search_results:
                context_parts.append(f"""
                Paper ID: {result['metadata'].get('paper_id', 'Unknown')}
                Section: {result['metadata'].get('section', 'Unknown')}
                Content: {result['content']}
                Similarity: {result['similarity_score']:.3f}
                """)
                
                sources.append({
                    'paper_id': result['metadata'].get('paper_id'),
                    'section': result['metadata'].get('section'),
                    'page': result['metadata'].get('page_number'),
                    'similarity': result['similarity_score']
                })
            
            context = "\n---\n".join(context_parts)
            
            # Generate answer
            if synthesis_mode:
                prompt = self.synthesis_prompt
            else:
                prompt = self.qa_prompt
            
            response = self.llm.invoke(prompt.format(
                question=question,
                context=context
            ))
            
            # Calculate confidence based on source quality
            confidence = self._calculate_confidence(search_results)
            
            return {
                'answer': response.content,
                'sources': sources,
                'confidence': confidence,
                'context_used': len(search_results)
            }
            
        except Exception as e:
            logger.error(f"QA system error: {e}")
            return {
                'answer': f"Error processing question: {str(e)}",
                'sources': [],
                'confidence': 0.0
            }
    
    def _calculate_confidence(self, search_results: List[Dict[str, Any]]) -> float:
        """Calculate confidence score for answer."""
        if not search_results:
            return 0.0
        
        # Average similarity score
        similarities = [result['similarity_score'] for result in search_results]
        avg_similarity = np.mean(similarities)
        
        # Bonus for multiple sources
        source_bonus = min(0.2, len(search_results) * 0.02)
        
        confidence = min(1.0, avg_similarity + source_bonus)
        return confidence
    
    def generate_research_summary(self, papers: List[str]) -> str:
        """Generate comprehensive summary of multiple papers."""
        try:
            # Get content from specified papers
            all_content = []
            
            for paper_id in papers:
                search_results = self.vector_store.similarity_search(
                    "", k=5, filter_metadata={'paper_id': paper_id}
                )
                
                for result in search_results:
                    all_content.append(result['content'])
            
            if not all_content:
                return "No content found for the specified papers."
            
            # Combine content
            combined_content = "\n\n".join(all_content[:10])  # Limit to avoid token limits
            
            summary_prompt = f"""
            Provide a comprehensive research summary based on the following academic paper content:
            
            {combined_content}
            
            The summary should include:
            1. Main research questions and objectives
            2. Key methodologies used
            3. Primary findings and contributions
            4. Implications and future work suggestions
            5. Common themes across papers
            
            Summary:
            """
            
            response = self.llm.invoke(summary_prompt)
            return response.content
            
        except Exception as e:
            logger.error(f"Summary generation error: {e}")
            return f"Error generating summary: {str(e)}"

class ResearchPaperAgent:
    """Main agent orchestrating all research paper analysis capabilities."""
    
    def __init__(self, openai_api_key: str):
        self.llm = ChatOpenAI(
            temperature=0.1,
            model_name="gpt-4",
            openai_api_key=openai_api_key
        )
        
        # Initialize components
        self.pdf_processor = PDFProcessor()
        self.chunker = DocumentChunker()
        self.vector_store = VectorStoreManager()
        self.citation_analyzer = CitationAnalyzer()
        self.qa_system = ResearchQASystem(self.llm, self.vector_store)
        
        # Paper storage
        self.papers = {}
    
    def ingest_paper(self, pdf_path: str, paper_id: str = None) -> bool:
        """Ingest and process a research paper."""
        try:
            if not paper_id:
                paper_id = f"paper_{datetime.now().timestamp()}"
            
            logger.info(f"Processing paper: {pdf_path}")
            
            # Extract content from PDF
            extracted_data = self.pdf_processor.extract_text_from_pdf(pdf_path)
            
            # Create paper object
            paper = ResearchPaper(
                paper_id=paper_id,
                title=extracted_data['title'],
                authors=extracted_data['authors'],
                abstract=extracted_data['abstract'],
                content=extracted_data['content'],
                citations=[Citation(**cite.__dict__) if hasattr(cite, '__dict__') 
                          else cite for cite in extracted_data['citations']],
                metadata=extracted_data['metadata'],
                file_path=pdf_path
            )
            
            # Store paper
            self.papers[paper_id] = paper
            
            # Create chunks
            chunks = self.chunker.chunk_paper(paper)
            
            # Add to vector store
            self.vector_store.add_paper_chunks(chunks)
            
            # Add citations to analyzer
            self.citation_analyzer.add_paper_citations(paper)
            
            logger.info(f"Successfully processed paper: {paper_id}")
            return True
            
        except Exception as e:
            logger.error(f"Error ingesting paper: {e}")
            return False
    
    def query_papers(self, question: str, synthesis: bool = False) -> Dict[str, Any]:
        """Query across all ingested papers."""
        return self.qa_system.answer_question(question, synthesis_mode=synthesis)
    
    def get_research_insights(self) -> Dict[str, Any]:
        """Get comprehensive insights across all papers."""
        try:
            # Vector store statistics
            vector_stats = self.vector_store.get_paper_statistics()
            
            # Citation analysis
            influential_papers = self.citation_analyzer.find_influential_papers()
            citation_trends = self.citation_analyzer.analyze_temporal_trends()
            
            # Paper metadata analysis
            paper_years = []
            author_counts = []
            
            for paper in self.papers.values():
                if 'year' in paper.metadata:
                    paper_years.append(paper.metadata['year'])
                author_counts.append(len(paper.authors))
            
            insights = {
                'collection_stats': vector_stats,
                'influential_papers': influential_papers,
                'citation_trends': citation_trends,
                'paper_analysis': {
                    'total_papers': len(self.papers),
                    'year_range': (min(paper_years), max(paper_years)) if paper_years else (0, 0),
                    'avg_authors': np.mean(author_counts) if author_counts else 0,
                    'total_citations': sum(len(p.citations) for p in self.papers.values())
                }
            }
            
            return insights
            
        except Exception as e:
            logger.error(f"Insights generation error: {e}")
            return {'error': str(e)}

def create_sample_papers() -> List[Dict[str, str]]:
    """Create sample paper data for demonstration."""
    return [
        {
            'title': 'Attention Is All You Need',
            'abstract': 'We propose a new network architecture, the Transformer, based solely on attention mechanisms...',
            'content': 'Sample content about transformers and attention mechanisms...',
            'year': '2017'
        },
        {
            'title': 'BERT: Pre-training Bidirectional Representations',
            'abstract': 'We introduce BERT, a new language representation model that is designed to pre-train deep bidirectional representations...',
            'content': 'Sample content about BERT and bidirectional training...',
            'year': '2018'
        }
    ]

def main():
    """Main Streamlit application."""
    st.set_page_config(
        page_title="Research Paper Insights Agent",
        page_icon="📚",
        layout="wide"
    )
    
    st.title("📚 LLM-Powered Research Paper Insights Agent")
    st.markdown("Intelligent analysis and question-answering for academic papers")
    
    # Sidebar
    with st.sidebar:
        st.header("⚙️ Configuration")
        openai_api_key = st.text_input("OpenAI API Key", type="password")
        
        st.header("📄 Document Management")
        uploaded_files = st.file_uploader(
            "Upload PDF Papers",
            type=['pdf'],
            accept_multiple_files=True
        )
        
        if st.button("📊 Load Sample Data"):
            st.session_state['sample_papers'] = create_sample_papers()
            st.success("Sample papers loaded!")
        
        st.header("🔍 Search Filters")
        paper_filter = st.selectbox(
            "Filter by Paper",
            ["All Papers"] + list(st.session_state.get('agent_papers', {}).keys())
        )
        
        synthesis_mode = st.checkbox("Synthesis Mode", 
                                   help="Combine insights from multiple papers")
    
    if not openai_api_key:
        st.warning("Please enter your OpenAI API key in the sidebar to continue.")
        return
    
    # Initialize agent
    if 'agent' not in st.session_state:
        try:
            st.session_state['agent'] = ResearchPaperAgent(openai_api_key)
            st.session_state['agent_papers'] = {}
        except Exception as e:
            st.error(f"Error initializing agent: {e}")
            return
    
    agent = st.session_state['agent']
    
    # Process uploaded files
    if uploaded_files:
        with st.spinner("Processing uploaded papers..."):
            for uploaded_file in uploaded_files:
                try:
                    # Save uploaded file temporarily
                    temp_path = f"temp_{uploaded_file.name}"
                    with open(temp_path, "wb") as f:
                        f.write(uploaded_file.getbuffer())
                    
                    # Process paper
                    paper_id = f"uploaded_{uploaded_file.name.split('.')[0]}"
                    
                    if agent.ingest_paper(temp_path, paper_id):
                        st.session_state['agent_papers'][paper_id] = uploaded_file.name
                        st.success(f"Processed: {uploaded_file.name}")
                    else:
                        st.error(f"Failed to process: {uploaded_file.name}")
                    
                    # Clean up
                    os.remove(temp_path)
                    
                except Exception as e:
                    st.error(f"Error processing {uploaded_file.name}: {e}")
    
    # Main tabs
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "🔍 Paper Q&A",
        "📊 Research Insights",
        "🕸️ Citation Network",
        "📈 Analytics Dashboard",
        "📝 Paper Management"
    ])
    
    with tab1:
        st.header("🔍 Research Paper Question & Answer")
        
        # Question input
        st.subheader("Ask Questions About Your Papers")
        
        col1, col2 = st.columns([4, 1])
        
        with col1:
            question = st.text_input(
                "Question",
                placeholder="What are the main findings about transformer architectures?",
                key="qa_question"
            )
        
        with col2:
            ask_button = st.button("🚀 Ask", type="primary")
        
        # Sample questions
        st.subheader("💡 Sample Questions")
        sample_questions = [
            "What are the key innovations in attention mechanisms?",
            "How do different papers approach the transformer architecture?",
            "What are the main limitations mentioned across papers?",
            "What future research directions are suggested?",
            "How do the experimental results compare across studies?"
        ]
        
        cols = st.columns(len(sample_questions))
        for i, sample_q in enumerate(sample_questions):
            with cols[i]:
                if st.button(f"📌 {sample_q[:30]}...", key=f"sample_q_{i}"):
                    question = sample_q
                    st.session_state['qa_question'] = question
                    st.rerun()
        
        # Process question
        if ask_button and question:
            if not st.session_state.get('agent_papers'):
                st.warning("Please upload some PDF papers first.")
            else:
                with st.spinner("Searching papers and generating answer..."):
                    try:
                        # Apply paper filter
                        paper_filter_dict = None
                        if paper_filter != "All Papers":
                            paper_filter_dict = {'paper_id': paper_filter}
                        
                        result = agent.query_papers(question, synthesis=synthesis_mode)
                        
                        # Display answer
                        st.subheader("📖 Answer")
                        st.markdown(result['answer'])
                        
                        # Display confidence and metadata
                        col1, col2, col3 = st.columns(3)
                        
                        with col1:
                            confidence_color = "green" if result['confidence'] > 0.7 else "orange" if result['confidence'] > 0.4 else "red"
                            st.markdown(f"**Confidence:** :{confidence_color}[{result['confidence']:.1%}]")
                        
                        with col2:
                            st.metric("Sources Used", result['context_used'])
                        
                        with col3:
                            st.metric("Total Sources", len(result['sources']))
                        
                        # Display sources
                        if result['sources']:
                            st.subheader("📚 Sources")
                            
                            for i, source in enumerate(result['sources'][:5]):  # Show top 5
                                with st.expander(f"Source {i+1}: {source['paper_id']} - {source['section']}"):
                                    col1, col2 = st.columns(2)
                                    
                                    with col1:
                                        st.write(f"**Paper ID:** {source['paper_id']}")
                                        st.write(f"**Section:** {source['section']}")
                                    
                                    with col2:
                                        st.write(f"**Page:** {source.get('page', 'N/A')}")
                                        st.write(f"**Similarity:** {source['similarity']:.3f}")
                        
                    except Exception as e:
                        st.error(f"Error processing question: {e}")
        
        # Recent questions
        if 'recent_questions' not in st.session_state:
            st.session_state['recent_questions'] = []
        
        if st.session_state['recent_questions']:
            st.subheader("📝 Recent Questions")
            for i, recent_q in enumerate(st.session_state['recent_questions'][-5:]):
                if st.button(f"🔄 {recent_q}", key=f"recent_{i}"):
                    st.session_state['qa_question'] = recent_q
                    st.rerun()
    
    with tab2:
        st.header("📊 Research Insights & Analysis")
        
        if not st.session_state.get('agent_papers'):
            st.info("Upload some PDF papers to see research insights.")
        else:
            try:
                insights = agent.get_research_insights()
                
                # Collection overview
                st.subheader("📚 Collection Overview")
                
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric("Total Papers", insights['paper_analysis']['total_papers'])
                
                with col2:
                    st.metric("Total Chunks", insights['collection_stats'].get('total_chunks', 0))
                
                with col3:
                    year_range = insights['paper_analysis']['year_range']
                    if year_range[1] > 0:
                        st.metric("Year Range", f"{year_range[0]}-{year_range[1]}")
                    else:
                        st.metric("Year Range", "N/A")
                
                with col4:
                    avg_authors = insights['paper_analysis']['avg_authors']
                    st.metric("Avg Authors", f"{avg_authors:.1f}")
                
                # Influential papers
                if insights['influential_papers']:
                    st.subheader("⭐ Most Influential Papers")
                    
                    influence_df = pd.DataFrame(insights['influential_papers'])
                    
                    for _, paper in influence_df.iterrows():
                        with st.expander(f"{paper['title']} ({paper['citation_count']} citations)"):
                            st.write(f"**Authors:** {', '.join(paper['authors'])}")
                            st.write(f"**Year:** {paper['year']}")
                            st.write(f"**Citations:** {paper['citation_count']}")
                
                # Citation trends
                if insights['citation_trends'] and insights['citation_trends'].get('years'):
                    st.subheader("📈 Publication Trends")
                    
                    trend_data = insights['citation_trends']
                    
                    fig = px.line(
                        x=trend_data['years'],
                        y=trend_data['citation_counts'],
                        title="Papers Over Time",
                        labels={'x': 'Year', 'y': 'Number of Papers'}
                    )
                    st.plotly_chart(fig, use_container_width=True)
                
                # Generate research summary
                st.subheader("📋 Research Summary")
                
                if st.button("🎯 Generate Comprehensive Summary"):
                    with st.spinner("Generating research summary..."):
                        try:
                            paper_ids = list(st.session_state['agent_papers'].keys())
                            summary = agent.qa_system.generate_research_summary(paper_ids)
                            
                            st.markdown("### 📄 Research Collection Summary")
                            st.markdown(summary)
                            
                        except Exception as e:
                            st.error(f"Error generating summary: {e}")
                
            except Exception as e:
                st.error(f"Error loading insights: {e}")
    
    with tab3:
        st.header("🕸️ Citation Network Analysis")
        
        if not st.session_state.get('agent_papers'):
            st.info("Upload some PDF papers to analyze citation networks.")
        else:
            try:
                # Citation network visualization
                st.subheader("🔗 Citation Relationships")
                
                # Get citation graph data
                graph = agent.citation_analyzer.citation_graph
                
                if graph.number_of_nodes() > 0:
                    # Create network visualization
                    pos = nx.spring_layout(graph, k=1, iterations=50)
                    
                    # Prepare data for plotting
                    edge_x = []
                    edge_y = []
                    
                    for edge in graph.edges():
                        x0, y0 = pos[edge[0]]
                        x1, y1 = pos[edge[1]]
                        edge_x.extend([x0, x1, None])
                        edge_y.extend([y0, y1, None])
                    
                    # Create plot
                    fig = go.Figure()
                    
                    # Add edges
                    fig.add_trace(go.Scatter(
                        x=edge_x, y=edge_y,
                        line=dict(width=1, color='lightgray'),
                        hoverinfo='none',
                        mode='lines'
                    ))
                    
                    # Add nodes
                    node_x = []
                    node_y = []
                    node_text = []
                    node_size = []
                    
                    for node in graph.nodes():
                        x, y = pos[node]
                        node_x.append(x)
                        node_y.append(y)
                        
                        # Node info
                        node_data = graph.nodes[node]
                        title = node_data.get('title', node)[:50]
                        year = node_data.get('year', 'Unknown')
                        
                        node_text.append(f"{title}<br>Year: {year}")
                        
                        # Size based on citation count
                        citation_count = graph.in_degree(node)
                        node_size.append(max(10, citation_count * 5))
                    
                    fig.add_trace(go.Scatter(
                        x=node_x, y=node_y,
                        mode='markers+text',
                        hoverinfo='text',
                        text=node_text,
                        marker=dict(
                            size=node_size,
                            color='lightblue',
                            line=dict(width=2, color='darkblue')
                        )
                    ))
                    
                    fig.update_layout(
                        title="Citation Network",
                        showlegend=False,
                        hovermode='closest',
                        margin=dict(b=20,l=5,r=5,t=40),
                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # Network statistics
                    st.subheader("📊 Network Statistics")
                    
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.metric("Total Nodes", graph.number_of_nodes())
                    
                    with col2:
                        st.metric("Total Edges", graph.number_of_edges())
                    
                    with col3:
                        density = nx.density(graph)
                        st.metric("Network Density", f"{density:.3f}")
                
                else:
                    st.info("No citation relationships found in the uploaded papers.")
                
                # Citation analysis
                st.subheader("🎯 Citation Analysis")
                
                influential_papers = agent.citation_analyzer.find_influential_papers(top_k=5)
                
                if influential_papers:
                    st.write("**Most Cited Papers:**")
                    
                    for paper in influential_papers:
                        st.write(f"• **{paper['title']}** ({paper['citation_count']} citations)")
                        st.write(f"  Authors: {', '.join(paper['authors'])}")
                        st.write(f"  Year: {paper['year']}")
                        st.write("")
                
            except Exception as e:
                st.error(f"Error in citation analysis: {e}")
    
    with tab4:
        st.header("📈 Analytics Dashboard")
        
        if not st.session_state.get('agent_papers'):
            st.info("Upload some PDF papers to see analytics.")
        else:
            try:
                # Vector store statistics
                vector_stats = agent.vector_store.get_paper_statistics()
                
                # Display metrics
                st.subheader("📊 Document Processing Metrics")
                
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric("Papers Processed", vector_stats.get('unique_papers', 0))
                
                with col2:
                    st.metric("Text Chunks", vector_stats.get('total_chunks', 0))
                
                with col3:
                    avg_chunks = vector_stats.get('average_chunks_per_paper', 0)
                    st.metric("Avg Chunks/Paper", f"{avg_chunks:.1f}")
                
                with col4:
                    # Calculate total characters (estimate)
                    total_chars = vector_stats.get('total_chunks', 0) * 1000  # Rough estimate
                    st.metric("Est. Total Chars", f"{total_chars:,}")
                
                # Section distribution
                if 'sections_distribution' in vector_stats:
                    st.subheader("📄 Content Distribution")
                    
                    sections = vector_stats['sections_distribution']
                    
                    if sections:
                        fig = px.pie(
                            values=list(sections.values()),
                            names=list(sections.keys()),
                            title="Content by Section Type"
                        )
                        st.plotly_chart(fig, use_container_width=True)
                
                # Processing timeline
                st.subheader("⏱️ Processing Timeline")
                
                papers_data = []
                for paper_id, paper in agent.papers.items():
                    papers_data.append({
                        'Paper ID': paper_id,
                        'Title': paper.title[:50] + "...",
                        'Authors': len(paper.authors),
                        'Citations': len(paper.citations),
                        'Processed': paper.processed_at.strftime('%Y-%m-%d %H:%M')
                    })
                
                if papers_data:
                    df = pd.DataFrame(papers_data)
                    st.dataframe(df, use_container_width=True)
                
                # Search performance
                st.subheader("🔍 Search Performance")
                
                if st.button("🧪 Test Search Performance"):
                    test_queries = [
                        "transformer architecture",
                        "attention mechanism",
                        "neural networks",
                        "machine learning"
                    ]
                    
                    performance_data = []
                    
                    for query in test_queries:
                        start_time = datetime.now()
                        results = agent.vector_store.similarity_search(query, k=5)
                        end_time = datetime.now()
                        
                        duration = (end_time - start_time).total_seconds()
                        
                        performance_data.append({
                            'Query': query,
                            'Results': len(results),
                            'Duration (s)': f"{duration:.3f}",
                            'Avg Similarity': f"{np.mean([r['similarity_score'] for r in results]):.3f}" if results else "0.000"
                        })
                    
                    perf_df = pd.DataFrame(performance_data)
                    st.dataframe(perf_df, use_container_width=True)
                
            except Exception as e:
                st.error(f"Error loading analytics: {e}")
    
    with tab5:
        st.header("📝 Paper Management")
        
        # Uploaded papers list
        st.subheader("📚 Uploaded Papers")
        
        if st.session_state.get('agent_papers'):
            for paper_id, filename in st.session_state['agent_papers'].items():
                with st.expander(f"📄 {filename}"):
                    if paper_id in agent.papers:
                        paper = agent.papers[paper_id]
                        
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            st.write(f"**Title:** {paper.title or 'Not extracted'}")
                            st.write(f"**Authors:** {len(paper.authors)} author(s)")
                        
                        with col2:
                            st.write(f"**Citations:** {len(paper.citations)}")
                            st.write(f"**Processed:** {paper.processed_at.strftime('%Y-%m-%d %H:%M')}")
                        
                        if paper.abstract:
                            st.write(f"**Abstract:** {paper.abstract[:200]}...")
                        
                        # Action buttons
                        col1, col2, col3 = st.columns(3)
                        
                        with col1:
                            if st.button(f"🔍 Analyze", key=f"analyze_{paper_id}"):
                                st.info("Analysis feature would be implemented here")
                        
                        with col2:
                            if st.button(f"📊 Stats", key=f"stats_{paper_id}"):
                                # Show paper statistics
                                chunks = agent.chunker.chunk_paper(paper)
                                st.write(f"Text chunks: {len(chunks)}")
                                st.write(f"Content length: {len(paper.content):,} chars")
                        
                        with col3:
                            if st.button(f"🗑️ Remove", key=f"remove_{paper_id}"):
                                # Note: Full removal would require vector store cleanup
                                st.warning("Paper removal not implemented in demo")
        else:
            st.info("No papers uploaded yet. Use the sidebar to upload PDF files.")
        
        # Bulk operations
        st.subheader("🔧 Bulk Operations")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("📊 Recompute All Stats"):
                if st.session_state.get('agent_papers'):
                    with st.spinner("Recomputing statistics..."):
                        try:
                            insights = agent.get_research_insights()
                            st.success("Statistics recomputed successfully!")
                        except Exception as e:
                            st.error(f"Error: {e}")
                else:
                    st.warning("No papers to analyze")
        
        with col2:
            if st.button("🔄 Rebuild Index"):
                st.info("Index rebuilding not implemented in demo")
        
        with col3:
            if st.button("📤 Export Data"):
                st.info("Data export not implemented in demo")

if __name__ == "__main__":
    main()
````

### Environment Configuration

````python
OPENAI_API_KEY=your_openai_api_key_here
````

## Project Summary

The LLM-Powered Research Paper Insights Agent represents a comprehensive academic research acceleration platform that transforms how researchers interact with scientific literature. By combining advanced PDF processing, semantic search, and intelligent question-answering capabilities, it provides unprecedented access to research insights and cross-document analysis.

### Key Value Propositions:
- **Intelligent Document Processing**: Advanced PDF extraction handling complex academic layouts, equations, and citation structures
- **Semantic Literature Search**: Vector-based similarity search enabling discovery of conceptually related content across papers
- **Interactive Research Assistant**: Natural language querying with context-aware responses and proper source attribution
- **Citation Network Analysis**: Automated extraction and visualization of research relationships and influential works
- **Multi-Paper Synthesis**: Cross-document analysis for identifying trends, contradictions, and research opportunities

### Technical Highlights:
- Multi-modal PDF processing using PyMuPDF and pdfplumber for robust text and metadata extraction
- ChromaDB and FAISS integration for scalable vector storage and fast similarity search
- Advanced chunking strategies respecting academic document structure and semantic boundaries
- LangChain-powered RAG architecture ensuring grounded responses with source verification
- NetworkX-based citation analysis for understanding research impact and relationships
- Comprehensive analytics dashboard tracking processing performance and content distribution

This system demonstrates how AI can dramatically accelerate academic research workflows, enabling researchers to quickly extract insights from large literature collections, identify knowledge gaps, and make data-driven research decisions while maintaining rigorous citation integrity and source attribution.