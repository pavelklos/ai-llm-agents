<small>Claude Sonnet 4 **(Personal Knowledge Manager - AI-Enhanced MCP Integration)**</small>
# Personal Knowledge Manager

## Key Concepts Explanation

### Model Context Protocol (MCP)
Standardized communication framework enabling seamless integration between AI models and knowledge management systems, facilitating intelligent context-aware interactions with personal information repositories.

### Vector Databases
High-dimensional storage systems that represent documents and concepts as mathematical vectors, enabling semantic similarity search and advanced AI-powered knowledge retrieval based on meaning rather than keyword matching.

### Retrieval-Augmented Generation (RAG)
AI architecture combining vector-based document retrieval with large language models to generate contextually accurate responses by grounding AI outputs in relevant personal knowledge and documents.

### Document Indexing
Automated process of analyzing, categorizing, and structuring documents using AI to extract metadata, relationships, and semantic content for efficient storage and retrieval.

### Semantic Search
Advanced search capability that understands context, intent, and meaning to find relevant information even when exact keywords don't match, using natural language understanding.

### Note-Taking Integration
Bidirectional synchronization with popular knowledge management platforms like Obsidian and Notion, enabling seamless workflow integration while maintaining AI-enhanced capabilities.

## Comprehensive Project Explanation

The Personal Knowledge Manager represents a revolutionary approach to information management, combining the power of artificial intelligence with modern knowledge management workflows. This system transforms traditional note-taking and document storage into an intelligent, context-aware knowledge ecosystem that learns from user behavior and provides proactive insights.

### Objectives
- **Intelligent Knowledge Organization**: Automatically categorize and link information based on semantic relationships
- **Context-Aware Retrieval**: Find relevant information based on current tasks and interests
- **Proactive Insights**: Surface relevant knowledge before users explicitly search for it
- **Seamless Integration**: Work naturally within existing note-taking and productivity workflows
- **Personal AI Assistant**: Provide personalized assistance based on accumulated knowledge

### Challenges
- **Privacy and Security**: Ensuring personal information remains protected while enabling AI processing
- **Real-Time Performance**: Providing instant search and insights without noticeable delays
- **Knowledge Graph Complexity**: Managing intricate relationships between diverse information types
- **Platform Integration**: Maintaining synchronization across multiple knowledge management tools
- **Adaptive Learning**: Personalizing AI behavior based on individual usage patterns

### Potential Impact
This system could transform personal productivity by creating a "second brain" that not only stores information but actively helps users discover connections, generate insights, and maintain comprehensive knowledge maps of their personal and professional domains.

## Comprehensive Project Example with Python Implementation

````python
import asyncio
import json
import hashlib
import uuid
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from pathlib import Path
import logging
import aiofiles
import aiohttp
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker, declarative_base
from sqlalchemy import Column, String, DateTime, Text, Float, JSON, Integer
import numpy as np
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings
import pinecone
from langchain.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings
from langchain.vectorstores import Chroma, Pinecone as LangchainPinecone
from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownTextSplitter
from langchain.document_loaders import TextLoader, PDFLoader, MarkdownLoader
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA, ConversationalRetrievalChain
from langchain.memory import ConversationBufferWindowMemory
from llama_index import VectorStoreIndex, ServiceContext, Document
from llama_index.embeddings import HuggingFaceEmbedding
import networkx as nx
from sklearn.cluster import KMeans
import spacy
from transformers import pipeline
import markdown
from bs4 import BeautifulSoup
import requests
from fastapi import FastAPI, HTTPException, WebSocket
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Database Models
Base = declarative_base()

class KnowledgeItem(Base):
    __tablename__ = "knowledge_items"
    
    id = Column(String, primary_key=True)
    title = Column(String, nullable=False)
    content = Column(Text, nullable=False)
    content_type = Column(String, nullable=False)  # note, document, web_page, etc.
    source = Column(String)  # obsidian, notion, manual, etc.
    tags = Column(JSON)
    metadata = Column(JSON)
    embedding_id = Column(String)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    access_count = Column(Integer, default=0)
    importance_score = Column(Float, default=0.0)

class KnowledgeRelation(Base):
    __tablename__ = "knowledge_relations"
    
    id = Column(String, primary_key=True)
    source_id = Column(String, nullable=False)
    target_id = Column(String, nullable=False)
    relation_type = Column(String, nullable=False)  # semantic, temporal, hierarchical
    strength = Column(Float, default=1.0)
    metadata = Column(JSON)
    created_at = Column(DateTime, default=datetime.utcnow)

@dataclass
class ProcessedDocument:
    id: str
    title: str
    content: str
    content_type: str
    source: str
    chunks: List[str]
    embeddings: np.ndarray
    metadata: Dict[str, Any]
    tags: List[str] = field(default_factory=list)
    entities: List[Dict[str, Any]] = field(default_factory=list)
    summary: str = ""

class AdvancedTextProcessor:
    """Advanced text processing with NLP and semantic analysis"""
    
    def __init__(self):
        self.nlp = spacy.load("en_core_web_sm")
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", ". ", "! ", "? ", " ", ""]
        )
        self.markdown_splitter = MarkdownTextSplitter(chunk_size=1000, chunk_overlap=200)
    
    async def process_document(self, content: str, title: str, content_type: str, 
                             source: str) -> ProcessedDocument:
        """Comprehensively process a document"""
        try:
            doc_id = str(uuid.uuid4())
            
            # Clean and preprocess content
            cleaned_content = await self._clean_content(content, content_type)
            
            # Extract entities and metadata
            entities = await self._extract_entities(cleaned_content)
            
            # Generate summary
            summary = await self._generate_summary(cleaned_content)
            
            # Split into chunks
            chunks = await self._split_content(cleaned_content, content_type)
            
            # Generate embeddings
            embeddings = await self._generate_embeddings(chunks)
            
            # Extract tags
            tags = await self._extract_tags(cleaned_content, entities)
            
            # Generate metadata
            metadata = await self._generate_metadata(cleaned_content, entities, summary)
            
            return ProcessedDocument(
                id=doc_id,
                title=title,
                content=cleaned_content,
                content_type=content_type,
                source=source,
                chunks=chunks,
                embeddings=embeddings,
                metadata=metadata,
                tags=tags,
                entities=entities,
                summary=summary
            )
            
        except Exception as e:
            logger.error(f"Error processing document: {e}")
            raise
    
    async def _clean_content(self, content: str, content_type: str) -> str:
        """Clean and normalize content based on type"""
        if content_type == "markdown":
            # Convert markdown to text while preserving structure
            html = markdown.markdown(content)
            soup = BeautifulSoup(html, 'html.parser')
            return soup.get_text()
        elif content_type == "html":
            soup = BeautifulSoup(content, 'html.parser')
            return soup.get_text()
        else:
            return content.strip()
    
    async def _extract_entities(self, content: str) -> List[Dict[str, Any]]:
        """Extract named entities and important concepts"""
        doc = self.nlp(content)
        entities = []
        
        for ent in doc.ents:
            entities.append({
                'text': ent.text,
                'label': ent.label_,
                'start': ent.start_char,
                'end': ent.end_char,
                'confidence': 1.0  # spaCy doesn't provide confidence scores by default
            })
        
        return entities
    
    async def _generate_summary(self, content: str) -> str:
        """Generate an extractive summary"""
        if len(content) < 100:
            return content
        
        try:
            # Truncate content if too long for summarizer
            max_length = 1024
            truncated_content = content[:max_length] if len(content) > max_length else content
            
            summary = self.summarizer(truncated_content, max_length=150, min_length=30, do_sample=False)
            return summary[0]['summary_text']
        except Exception as e:
            logger.warning(f"Summarization failed: {e}")
            return content[:200] + "..." if len(content) > 200 else content
    
    async def _split_content(self, content: str, content_type: str) -> List[str]:
        """Split content into semantic chunks"""
        if content_type == "markdown":
            return self.markdown_splitter.split_text(content)
        else:
            return self.text_splitter.split_text(content)
    
    async def _generate_embeddings(self, chunks: List[str]) -> np.ndarray:
        """Generate embeddings for content chunks"""
        if not chunks:
            return np.array([])
        
        embeddings = self.embedding_model.encode(chunks)
        return np.array(embeddings)
    
    async def _extract_tags(self, content: str, entities: List[Dict[str, Any]]) -> List[str]:
        """Extract relevant tags from content and entities"""
        tags = set()
        
        # Add entity labels as tags
        for entity in entities:
            if entity['label'] in ['PERSON', 'ORG', 'GPE', 'EVENT']:
                tags.add(entity['text'].lower())
        
        # Extract key phrases using simple keyword extraction
        doc = self.nlp(content)
        for token in doc:
            if (token.pos_ in ['NOUN', 'PROPN'] and 
                len(token.text) > 3 and 
                not token.is_stop and 
                token.is_alpha):
                tags.add(token.lemma_.lower())
        
        return list(tags)[:20]  # Limit to top 20 tags
    
    async def _generate_metadata(self, content: str, entities: List[Dict[str, Any]], 
                                summary: str) -> Dict[str, Any]:
        """Generate comprehensive metadata"""
        return {
            'word_count': len(content.split()),
            'char_count': len(content),
            'entity_count': len(entities),
            'entity_types': list(set(e['label'] for e in entities)),
            'summary_length': len(summary.split()),
            'language': 'en',  # Could be detected
            'readability_score': self._calculate_readability(content),
            'processed_at': datetime.utcnow().isoformat()
        }
    
    def _calculate_readability(self, content: str) -> float:
        """Simple readability score (Flesch Reading Ease approximation)"""
        sentences = len([s for s in content.split('.') if s.strip()])
        words = len(content.split())
        
        if sentences == 0 or words == 0:
            return 0.0
        
        avg_sentence_length = words / sentences
        # Simplified readability score
        score = 206.835 - (1.015 * avg_sentence_length)
        return max(0, min(100, score))

class IntelligentVectorStore:
    """Advanced vector store with multiple backend support"""
    
    def __init__(self, store_type: str = "chroma", config: Dict[str, Any] = None):
        self.store_type = store_type
        self.config = config or {}
        self.embedding_model = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
        self.vector_store = None
        self.index_metadata = {}
        self._initialize_store()
    
    def _initialize_store(self):
        """Initialize the chosen vector store backend"""
        if self.store_type == "chroma":
            self._initialize_chroma()
        elif self.store_type == "pinecone":
            self._initialize_pinecone()
        elif self.store_type == "faiss":
            self._initialize_faiss()
    
    def _initialize_chroma(self):
        """Initialize ChromaDB"""
        try:
            chroma_client = chromadb.Client(Settings(
                chroma_db_impl="duckdb+parquet",
                persist_directory=self.config.get('persist_directory', './chroma_db')
            ))
            
            self.vector_store = Chroma(
                client=chroma_client,
                collection_name="knowledge_base",
                embedding_function=self.embedding_model
            )
            logger.info("ChromaDB initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize ChromaDB: {e}")
            raise
    
    def _initialize_pinecone(self):
        """Initialize Pinecone"""
        try:
            api_key = self.config.get('pinecone_api_key')
            if not api_key:
                raise ValueError("Pinecone API key required")
            
            pinecone.init(api_key=api_key)
            
            index_name = self.config.get('index_name', 'knowledge-base')
            if index_name not in pinecone.list_indexes():
                pinecone.create_index(
                    index_name,
                    dimension=384,  # MiniLM embedding dimension
                    metric='cosine'
                )
            
            self.vector_store = LangchainPinecone.from_existing_index(
                index_name, self.embedding_model
            )
            logger.info("Pinecone initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize Pinecone: {e}")
            raise
    
    async def add_document(self, document: ProcessedDocument) -> bool:
        """Add a processed document to the vector store"""
        try:
            # Create text chunks with metadata
            texts = []
            metadatas = []
            
            for i, chunk in enumerate(document.chunks):
                texts.append(chunk)
                metadatas.append({
                    'document_id': document.id,
                    'title': document.title,
                    'chunk_index': i,
                    'content_type': document.content_type,
                    'source': document.source,
                    'tags': ','.join(document.tags),
                    'summary': document.summary,
                    'created_at': datetime.utcnow().isoformat()
                })
            
            # Add to vector store
            self.vector_store.add_texts(texts=texts, metadatas=metadatas)
            
            # Store document metadata
            self.index_metadata[document.id] = {
                'title': document.title,
                'content_type': document.content_type,
                'source': document.source,
                'tags': document.tags,
                'chunk_count': len(document.chunks),
                'indexed_at': datetime.utcnow().isoformat()
            }
            
            logger.info(f"Document {document.id} added to vector store")
            return True
            
        except Exception as e:
            logger.error(f"Failed to add document to vector store: {e}")
            return False
    
    async def semantic_search(self, query: str, k: int = 10, 
                            filters: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """Perform semantic search with optional filtering"""
        try:
            # Perform similarity search
            results = self.vector_store.similarity_search_with_score(
                query, k=k * 2  # Get more results for post-filtering
            )
            
            # Apply filters and format results
            filtered_results = []
            for doc, score in results:
                metadata = doc.metadata
                
                # Apply filters
                if self._passes_filters(metadata, filters):
                    filtered_results.append({
                        'content': doc.page_content,
                        'metadata': metadata,
                        'similarity_score': float(score),
                        'document_id': metadata.get('document_id'),
                        'title': metadata.get('title'),
                        'tags': metadata.get('tags', '').split(',') if metadata.get('tags') else []
                    })
                
                if len(filtered_results) >= k:
                    break
            
            return filtered_results
            
        except Exception as e:
            logger.error(f"Semantic search failed: {e}")
            return []
    
    def _passes_filters(self, metadata: Dict[str, Any], filters: Dict[str, Any]) -> bool:
        """Check if a document passes the given filters"""
        if not filters:
            return True
        
        for key, value in filters.items():
            if key == 'content_type' and metadata.get('content_type') != value:
                return False
            elif key == 'source' and metadata.get('source') != value:
                return False
            elif key == 'tags' and value not in metadata.get('tags', ''):
                return False
            elif key == 'date_range':
                # Implement date range filtering
                pass
        
        return True

class KnowledgeGraph:
    """Knowledge graph for managing relationships between concepts"""
    
    def __init__(self):
        self.graph = nx.DiGraph()
        self.concept_embeddings = {}
        self.relation_types = {
            'semantic': 1.0,
            'temporal': 0.8,
            'hierarchical': 0.9,
            'reference': 0.7,
            'similarity': 0.6
        }
    
    async def add_concept(self, concept_id: str, concept_data: Dict[str, Any]):
        """Add a concept to the knowledge graph"""
        self.graph.add_node(concept_id, **concept_data)
        
        # Store embedding if available
        if 'embedding' in concept_data:
            self.concept_embeddings[concept_id] = concept_data['embedding']
    
    async def add_relation(self, source_id: str, target_id: str, 
                          relation_type: str, strength: float = None):
        """Add a relationship between concepts"""
        if strength is None:
            strength = self.relation_types.get(relation_type, 0.5)
        
        self.graph.add_edge(source_id, target_id, 
                           relation_type=relation_type, 
                           strength=strength)
    
    async def find_related_concepts(self, concept_id: str, 
                                  max_depth: int = 2, 
                                  min_strength: float = 0.3) -> List[Dict[str, Any]]:
        """Find concepts related to a given concept"""
        if concept_id not in self.graph:
            return []
        
        related = []
        visited = set()
        
        def explore(current_id: str, depth: int, path_strength: float):
            if depth > max_depth or current_id in visited:
                return
            
            visited.add(current_id)
            
            for neighbor in self.graph.neighbors(current_id):
                edge_data = self.graph[current_id][neighbor]
                edge_strength = edge_data.get('strength', 0.5)
                combined_strength = path_strength * edge_strength
                
                if combined_strength >= min_strength:
                    related.append({
                        'concept_id': neighbor,
                        'relation_type': edge_data.get('relation_type'),
                        'strength': combined_strength,
                        'depth': depth + 1,
                        'path_strength': combined_strength
                    })
                    
                    explore(neighbor, depth + 1, combined_strength)
        
        explore(concept_id, 0, 1.0)
        
        # Sort by strength and return top results
        related.sort(key=lambda x: x['strength'], reverse=True)
        return related[:20]
    
    async def detect_communities(self) -> Dict[str, List[str]]:
        """Detect communities/clusters in the knowledge graph"""
        try:
            communities = nx.community.greedy_modularity_communities(
                self.graph.to_undirected()
            )
            
            community_dict = {}
            for i, community in enumerate(communities):
                community_dict[f"community_{i}"] = list(community)
            
            return community_dict
        except Exception as e:
            logger.error(f"Community detection failed: {e}")
            return {}

class RAGSystem:
    """Retrieval-Augmented Generation system for knowledge queries"""
    
    def __init__(self, vector_store: IntelligentVectorStore, 
                 knowledge_graph: KnowledgeGraph):
        self.vector_store = vector_store
        self.knowledge_graph = knowledge_graph
        self.llm = OpenAI(temperature=0.7, max_tokens=500)
        self.memory = ConversationBufferWindowMemory(
            memory_key="chat_history",
            return_messages=True,
            k=5
        )
        self.qa_chain = None
        self._initialize_chain()
    
    def _initialize_chain(self):
        """Initialize the RAG chain"""
        self.qa_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=self.vector_store.vector_store.as_retriever(
                search_kwargs={"k": 5}
            ),
            memory=self.memory,
            return_source_documents=True
        )
    
    async def query(self, question: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """Answer a question using RAG"""
        try:
            # Enhance query with context
            enhanced_query = await self._enhance_query(question, context)
            
            # Get relevant documents
            relevant_docs = await self.vector_store.semantic_search(
                enhanced_query, k=5
            )
            
            # Use RAG chain for generation
            result = self.qa_chain({
                "question": enhanced_query,
                "chat_history": self.memory.chat_memory.messages
            })
            
            # Find related concepts from knowledge graph
            related_concepts = []
            for doc in relevant_docs:
                doc_id = doc.get('document_id')
                if doc_id:
                    concepts = await self.knowledge_graph.find_related_concepts(doc_id)
                    related_concepts.extend(concepts)
            
            return {
                'answer': result['answer'],
                'source_documents': [
                    {
                        'content': doc['content'],
                        'title': doc['metadata'].get('title', ''),
                        'source': doc['metadata'].get('source', ''),
                        'similarity_score': doc['similarity_score']
                    }
                    for doc in relevant_docs
                ],
                'related_concepts': related_concepts[:5],
                'confidence_score': self._calculate_confidence(result, relevant_docs),
                'query_enhancement': enhanced_query != question
            }
            
        except Exception as e:
            logger.error(f"RAG query failed: {e}")
            return {
                'answer': "I'm sorry, I couldn't process your question at the moment.",
                'error': str(e)
            }
    
    async def _enhance_query(self, query: str, context: Dict[str, Any] = None) -> str:
        """Enhance query with additional context"""
        if not context:
            return query
        
        # Add context information to query
        enhanced_parts = [query]
        
        if context.get('current_document'):
            enhanced_parts.append(f"In the context of: {context['current_document']}")
        
        if context.get('tags'):
            enhanced_parts.append(f"Related to: {', '.join(context['tags'])}")
        
        return " ".join(enhanced_parts)
    
    def _calculate_confidence(self, result: Dict[str, Any], 
                            relevant_docs: List[Dict[str, Any]]) -> float:
        """Calculate confidence score for the answer"""
        if not relevant_docs:
            return 0.1
        
        # Simple confidence calculation based on similarity scores
        avg_similarity = np.mean([doc['similarity_score'] for doc in relevant_docs])
        doc_count_factor = min(1.0, len(relevant_docs) / 3)
        
        return min(0.95, avg_similarity * doc_count_factor)

class PersonalKnowledgeManager:
    """Main Personal Knowledge Manager class"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or self._default_config()
        
        # Initialize components
        self.text_processor = AdvancedTextProcessor()
        self.vector_store = IntelligentVectorStore(
            store_type=self.config['vector_store_type'],
            config=self.config['vector_store_config']
        )
        self.knowledge_graph = KnowledgeGraph()
        self.rag_system = RAGSystem(self.vector_store, self.knowledge_graph)
        
        # Initialize database
        self.engine = None
        self.session_factory = None
        
        # Performance metrics
        self.metrics = {
            'documents_processed': 0,
            'queries_answered': 0,
            'average_query_time': 0.0,
            'total_documents': 0
        }
    
    def _default_config(self) -> Dict[str, Any]:
        """Default configuration"""
        return {
            'vector_store_type': 'chroma',
            'vector_store_config': {
                'persist_directory': './knowledge_db'
            },
            'database_url': 'sqlite+aiosqlite:///./knowledge.db',
            'max_document_size': 50 * 1024 * 1024,  # 50MB
            'supported_formats': ['txt', 'md', 'pdf', 'html'],
            'auto_sync_enabled': True,
            'obsidian_vault_path': None,
            'notion_token': None
        }
    
    async def initialize(self):
        """Initialize the knowledge manager"""
        try:
            # Initialize database
            self.engine = create_async_engine(self.config['database_url'])
            self.session_factory = sessionmaker(
                self.engine, class_=AsyncSession, expire_on_commit=False
            )
            
            # Create tables
            async with self.engine.begin() as conn:
                await conn.run_sync(Base.metadata.create_all)
            
            logger.info("Personal Knowledge Manager initialized successfully")
            
        except Exception as e:
            logger.error(f"Initialization failed: {e}")
            raise
    
    async def add_document(self, content: str, title: str, 
                          content_type: str = "text", 
                          source: str = "manual") -> str:
        """Add a document to the knowledge base"""
        try:
            start_time = datetime.utcnow()
            
            # Process the document
            processed_doc = await self.text_processor.process_document(
                content, title, content_type, source
            )
            
            # Add to vector store
            success = await self.vector_store.add_document(processed_doc)
            if not success:
                raise Exception("Failed to add document to vector store")
            
            # Add to knowledge graph
            await self.knowledge_graph.add_concept(
                processed_doc.id,
                {
                    'title': processed_doc.title,
                    'content_type': processed_doc.content_type,
                    'tags': processed_doc.tags,
                    'embedding': processed_doc.embeddings.mean(axis=0) if len(processed_doc.embeddings) > 0 else None
                }
            )
            
            # Store in database
            async with self.session_factory() as session:
                db_item = KnowledgeItem(
                    id=processed_doc.id,
                    title=processed_doc.title,
                    content=processed_doc.content,
                    content_type=processed_doc.content_type,
                    source=processed_doc.source,
                    tags=processed_doc.tags,
                    metadata=processed_doc.metadata,
                    embedding_id=processed_doc.id
                )
                session.add(db_item)
                await session.commit()
            
            # Update metrics
            self.metrics['documents_processed'] += 1
            self.metrics['total_documents'] += 1
            
            processing_time = (datetime.utcnow() - start_time).total_seconds()
            logger.info(f"Document processed in {processing_time:.2f}s: {title}")
            
            return processed_doc.id
            
        except Exception as e:
            logger.error(f"Failed to add document: {e}")
            raise
    
    async def search_knowledge(self, query: str, 
                              filters: Dict[str, Any] = None,
                              include_related: bool = True) -> Dict[str, Any]:
        """Search the knowledge base"""
        try:
            start_time = datetime.utcnow()
            
            # Perform semantic search
            search_results = await self.vector_store.semantic_search(
                query, k=10, filters=filters
            )
            
            # Get RAG-based answer
            rag_result = await self.rag_system.query(query)
            
            # Find related concepts if requested
            related_concepts = []
            if include_related and search_results:
                for result in search_results[:3]:
                    doc_id = result.get('document_id')
                    if doc_id:
                        concepts = await self.knowledge_graph.find_related_concepts(doc_id)
                        related_concepts.extend(concepts)
            
            query_time = (datetime.utcnow() - start_time).total_seconds()
            
            # Update metrics
            self.metrics['queries_answered'] += 1
            self.metrics['average_query_time'] = (
                (self.metrics['average_query_time'] * (self.metrics['queries_answered'] - 1) + query_time) /
                self.metrics['queries_answered']
            )
            
            return {
                'query': query,
                'rag_answer': rag_result,
                'search_results': search_results,
                'related_concepts': related_concepts[:10],
                'query_time': query_time,
                'total_results': len(search_results)
            }
            
        except Exception as e:
            logger.error(f"Knowledge search failed: {e}")
            return {'error': str(e)}
    
    async def sync_with_obsidian(self, vault_path: str) -> Dict[str, Any]:
        """Sync with Obsidian vault"""
        if not Path(vault_path).exists():
            return {'error': 'Vault path does not exist'}
        
        synced_files = []
        errors = []
        
        try:
            for md_file in Path(vault_path).rglob("*.md"):
                try:
                    async with aiofiles.open(md_file, 'r', encoding='utf-8') as f:
                        content = await f.read()
                    
                    title = md_file.stem
                    doc_id = await self.add_document(
                        content, title, "markdown", "obsidian"
                    )
                    synced_files.append({
                        'file': str(md_file),
                        'doc_id': doc_id,
                        'title': title
                    })
                    
                except Exception as e:
                    errors.append({'file': str(md_file), 'error': str(e)})
            
            return {
                'synced_files': synced_files,
                'errors': errors,
                'total_synced': len(synced_files),
                'total_errors': len(errors)
            }
            
        except Exception as e:
            logger.error(f"Obsidian sync failed: {e}")
            return {'error': str(e)}

# FastAPI application for MCP server
class MCPServer:
    """MCP server for Personal Knowledge Manager"""
    
    def __init__(self, pkm: PersonalKnowledgeManager):
        self.app = FastAPI(title="Personal Knowledge Manager MCP Server")
        self.pkm = pkm
        self.setup_routes()
        self.setup_middleware()
    
    def setup_middleware(self):
        """Setup CORS middleware"""
        self.app.add_middleware(
            CORSMiddleware,
            allow_origins=["*"],
            allow_credentials=True,
            allow_methods=["*"],
            allow_headers=["*"],
        )
    
    def setup_routes(self):
        """Setup API routes"""
        
        @self.app.post("/documents/add")
        async def add_document(request: Dict[str, Any]):
            try:
                doc_id = await self.pkm.add_document(
                    content=request['content'],
                    title=request['title'],
                    content_type=request.get('content_type', 'text'),
                    source=request.get('source', 'api')
                )
                return {'doc_id': doc_id, 'status': 'success'}
            except Exception as e:
                raise HTTPException(status_code=400, detail=str(e))
        
        @self.app.post("/search")
        async def search_knowledge(request: Dict[str, Any]):
            try:
                results = await self.pkm.search_knowledge(
                    query=request['query'],
                    filters=request.get('filters'),
                    include_related=request.get('include_related', True)
                )
                return results
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.post("/sync/obsidian")
        async def sync_obsidian(request: Dict[str, Any]):
            try:
                vault_path = request['vault_path']
                results = await self.pkm.sync_with_obsidian(vault_path)
                return results
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/metrics")
        async def get_metrics():
            return self.pkm.metrics
        
        @self.app.websocket("/ws")
        async def websocket_endpoint(websocket: WebSocket):
            await websocket.accept()
            try:
                while True:
                    data = await websocket.receive_json()
                    
                    if data['type'] == 'search':
                        results = await self.pkm.search_knowledge(data['query'])
                        await websocket.send_json({
                            'type': 'search_results',
                            'data': results
                        })
                    elif data['type'] == 'add_document':
                        doc_id = await self.pkm.add_document(
                            data['content'], data['title']
                        )
                        await websocket.send_json({
                            'type': 'document_added',
                            'data': {'doc_id': doc_id}
                        })
            except Exception as e:
                logger.error(f"WebSocket error: {e}")

# Demo and testing
async def demo():
    """Demonstration of the Personal Knowledge Manager"""
    
    print("🧠 Personal Knowledge Manager Demo\n")
    
    # Sample documents
    sample_docs = [
        {
            'title': 'Machine Learning Fundamentals',
            'content': '''
# Machine Learning Fundamentals

Machine learning is a subset of artificial intelligence that focuses on developing algorithms that can learn and improve from experience without being explicitly programmed.

## Key Concepts

### Supervised Learning
In supervised learning, algorithms learn from labeled training data to make predictions on new, unseen data. Common examples include:
- Linear Regression
- Decision Trees
- Support Vector Machines
- Neural Networks

### Unsupervised Learning
Unsupervised learning finds hidden patterns in data without labeled examples:
- Clustering (K-means, DBSCAN)
- Dimensionality Reduction (PCA, t-SNE)
- Association Rules

### Reinforcement Learning
Agents learn through interaction with an environment, receiving rewards or penalties for their actions.

## Applications
- Image Recognition
- Natural Language Processing
- Recommendation Systems
- Autonomous Vehicles
            ''',
            'content_type': 'markdown',
            'source': 'manual'
        },
        {
            'title': 'Python Data Science Libraries',
            'content': '''
# Essential Python Libraries for Data Science

## NumPy
NumPy is the fundamental package for scientific computing in Python. It provides:
- Powerful N-dimensional array object
- Sophisticated broadcasting functions
- Linear algebra, Fourier transform, and random number capabilities

## Pandas
Pandas offers data structures and operations for manipulating numerical tables and time series:
- DataFrame and Series objects
- Data cleaning and preparation tools
- Powerful groupby functionality

## Matplotlib and Seaborn
Visualization libraries for creating static, animated, and interactive plots:
- Matplotlib: Low-level plotting interface
- Seaborn: High-level statistical visualization

## Scikit-learn
Machine learning library featuring:
- Classification, regression, and clustering algorithms
- Model selection and evaluation tools
- Preprocessing utilities

## Example Usage
```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# Load data
data = pd.read_csv('dataset.csv')
X = data.drop('target', axis=1)
y = data['target']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Train model
model = RandomForestClassifier()
model.fit(X_train, y_train)
```
            ''',
            'content_type': 'markdown',
            'source': 'manual'
        },
        {
            'title': 'Deep Learning Architectures',
            'content': '''
Deep learning uses artificial neural networks with multiple layers to model and understand complex patterns in data.

## Popular Architectures

### Convolutional Neural Networks (CNNs)
Primarily used for image processing and computer vision tasks. Key components:
- Convolutional layers
- Pooling layers
- Fully connected layers

### Recurrent Neural Networks (RNNs)
Designed for sequential data and time series:
- LSTM (Long Short-Term Memory)
- GRU (Gated Recurrent Unit)
- Bidirectional RNNs

### Transformers
Revolutionary architecture for natural language processing:
- Self-attention mechanism
- Encoder-decoder structure
- BERT, GPT models

### Generative Adversarial Networks (GANs)
Two neural networks competing against each other:
- Generator creates fake data
- Discriminator identifies real vs fake

## Applications
- Computer Vision: Object detection, image classification
- NLP: Language translation, text generation
- Speech Recognition: Voice assistants
- Game Playing: AlphaGo, OpenAI Five
            ''',
            'content_type': 'text',
            'source': 'manual'
        }
    ]
    
    try:
        # Initialize PKM
        config = {
            'vector_store_type': 'chroma',
            'vector_store_config': {'persist_directory': './demo_knowledge_db'},
            'database_url': 'sqlite+aiosqlite:///./demo_knowledge.db'
        }
        
        pkm = PersonalKnowledgeManager(config)
        await pkm.initialize()
        
        print("✅ Personal Knowledge Manager initialized\n")
        
        # Add sample documents
        print("📄 Adding sample documents...")
        doc_ids = []
        for doc in sample_docs:
            doc_id = await pkm.add_document(**doc)
            doc_ids.append(doc_id)
            print(f"  ✓ Added: {doc['title']} (ID: {doc_id[:8]}...)")
        
        print(f"\n📊 Total documents added: {len(doc_ids)}")
        
        # Perform sample searches
        sample_queries = [
            "What is supervised learning?",
            "Python libraries for data visualization",
            "Explain neural network architectures",
            "How do transformers work?",
            "What are GANs used for?"
        ]
        
        print(f"\n🔍 Testing search functionality...")
        for query in sample_queries:
            print(f"\nQuery: {query}")
            results = await pkm.search_knowledge(query, include_related=True)
            
            if 'error' not in results:
                rag_answer = results['rag_answer']['answer']
                print(f"Answer: {rag_answer[:200]}...")
                print(f"Found {results['total_results']} relevant documents")
                print(f"Query time: {results['query_time']:.3f}s")
                
                if results['related_concepts']:
                    print(f"Related concepts: {len(results['related_concepts'])}")
            else:
                print(f"Error: {results['error']}")
        
        # Display metrics
        print(f"\n📈 Performance Metrics:")
        metrics = pkm.metrics
        for key, value in metrics.items():
            print(f"  {key}: {value}")
        
        # Test MCP server setup
        print(f"\n🌐 Setting up MCP Server...")
        mcp_server = MCPServer(pkm)
        print(f"✅ MCP Server configured (routes: {len(mcp_server.app.routes)})")
        
        print(f"\n✅ Demo completed successfully!")
        print(f"🚀 To start the MCP server, run: uvicorn main:mcp_server.app --host 0.0.0.0 --port 8000")
        
    except Exception as e:
        print(f"❌ Demo failed: {e}")
        logger.error(f"Demo error: {e}")

if __name__ == "__main__":
    # Set up environment variables for API keys if needed
    import os
    os.environ.setdefault('OPENAI_API_KEY', 'your-openai-api-key-here')
    
    asyncio.run(demo())
````

### Setup and Dependencies

````bash
# Install required dependencies
pip install fastapi uvicorn aiofiles aiohttp
pip install sqlalchemy aiosqlite
pip install sentence-transformers transformers torch
pip install chromadb pinecone-client
pip install langchain openai
pip install llama-index
pip install networkx scikit-learn
pip install spacy beautifulsoup4 markdown
pip install numpy pandas

# Download spaCy model
python -m spacy download en_core_web_sm

# For Pinecone (optional)
pip install pinecone-client

# Set environment variables
export OPENAI_API_KEY="your-openai-api-key"
export PINECONE_API_KEY="your-pinecone-api-key"  # Optional
````

## Project Summary

The Personal Knowledge Manager represents a paradigm shift in how individuals interact with their accumulated knowledge. By combining advanced AI technologies with intuitive knowledge management workflows, this system creates a true "second brain" that not only stores information but actively helps users discover insights and connections.

### Key Value Propositions

1. **Intelligent Organization**: Automatically categorizes and links information using semantic understanding, eliminating manual tagging and folder management.

2. **Contextual Retrieval**: Advanced RAG system provides precise answers grounded in personal knowledge, with full source attribution and confidence scoring.

3. **Relationship Discovery**: Knowledge graph reveals hidden connections between concepts, enabling serendipitous discovery of relevant information.

4. **Seamless Integration**: Native support for popular tools like Obsidian and Notion ensures the system enhances existing workflows rather than replacing them.

### Key Takeaways

- **Privacy-First Design**: All processing can be done locally, ensuring sensitive personal information never leaves the user's control
- **Scalable Architecture**: Vector databases and graph structures efficiently handle large knowledge collections without performance degradation
- **Adaptive Intelligence**: The system learns from user behavior to improve relevance and discover personalized insights
- **Future-Ready Integration**: MCP protocol ensures compatibility with emerging AI tools and productivity applications

This Personal Knowledge Manager transforms information consumption into knowledge creation, helping users build comprehensive understanding across domains while maintaining the flexibility and control that knowledge workers demand.