<small>Claude Sonnet 4 **(Content Creation & SEO Optimizer s MCP (Model Context Protocol))**</small>
# Content Creation & SEO Optimizer

## 1. N√°zev Projektu

**Inteligentn√≠ Content Creation & SEO Optimizer** - Automatizovan√Ω syst√©m pro tvorbu optimalizovan√©ho obsahu s vyu≈æit√≠m Model Context Protocol pro koordinaci AI agent≈Ø, web scraping, anal√Ωzu kl√≠ƒçov√Ωch slov a integraci se soci√°ln√≠mi s√≠tƒõmi a CMS platformami.

## 2. Vysvƒõtlen√≠ Kl√≠ƒçov√Ωch Koncept≈Ø

### Model Context Protocol (MCP)
Protokol umo≈æ≈àuj√≠c√≠ AI model≈Øm komunikovat s extern√≠mi syst√©my a n√°stroji strukturovan√Ωm zp≈Øsobem. Poskytuje standardizovan√© rozhran√≠ pro p≈ôipojen√≠ n√°stroj≈Ø, datab√°z√≠ a slu≈æeb.

### Web Scraping
Automatizovan√© z√≠sk√°v√°n√≠ dat z webov√Ωch str√°nek pro anal√Ωzu trend≈Ø, konkurence a obsahu v dan√© oblasti.

### Keyword Analysis
Anal√Ωza kl√≠ƒçov√Ωch slov pro optimalizaci SEO zahrnuj√≠c√≠ vyhled√°v√°n√≠ relevantn√≠ch term√≠n≈Ø, anal√Ωzu obt√≠≈ænosti a konkurence.

### Content Planning
Strategick√© pl√°nov√°n√≠ obsahu na z√°kladƒõ SEO anal√Ωzy, trend≈Ø a c√≠lov√© skupiny.

### Social Media APIs
Rozhran√≠ pro automatickou publikaci a spr√°vu obsahu na soci√°ln√≠ch s√≠t√≠ch.

### WordPress/Shopify Integration
Integrace s popul√°rn√≠mi CMS platformami pro automatickou publikaci optimalizovan√©ho obsahu.

## 3. Podrobn√© Vysvƒõtlen√≠ Projektu

### C√≠le Projektu
Projekt si klade za c√≠l vytvo≈ôit komplexn√≠ syst√©m, kter√Ω automatizuje cel√Ω proces tvorby a optimalizace obsahu - od anal√Ωzy kl√≠ƒçov√Ωch slov p≈ôes generov√°n√≠ obsahu a≈æ po publikaci na r≈Øzn√Ωch platform√°ch.

### Hlavn√≠ V√Ωzvy
- **Koordinace AI agent≈Ø**: Synchronizace pr√°ce mezi r≈Øzn√Ωmi specializovan√Ωmi agenty
- **SEO optimalizace**: Zaji≈°tƒõn√≠ vysok√© kvality a relevantnosti obsahu
- **Multi-platform publikace**: Spr√°va r≈Øzn√Ωch form√°t≈Ø pro r≈Øzn√© platformy
- **Kvalita obsahu**: Udr≈æen√≠ vysok√© kvality p≈ôi automatizaci

### Potenci√°ln√≠ Dopad
Syst√©m umo≈æn√≠ mal√Ωm i velk√Ωm spoleƒçnostem dramaticky zv√Ω≈°it efektivitu content marketingu, sn√≠≈æit n√°klady na tvorbu obsahu a zlep≈°it SEO v√Ωsledky.

## 4. Komplexn√≠ Implementace v Pythonu

### Instalace Z√°vislost√≠

````python
fastapi==0.104.1
uvicorn==0.24.0
langchain==0.1.0
openai==1.3.0
requests==2.31.0
beautifulsoup4==4.12.2
selenium==4.15.0
pandas==2.1.3
python-wordpress-xmlrpc==2.3
tweepy==4.14.0
facebook-sdk==3.1.0
shopify-python-api==8.4.1
python-dotenv==1.0.0
chromadb==0.4.18
serpapi==0.1.5
textstat==0.7.3
nltk==3.8.1
````

### Hlavn√≠ MCP Server

````python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import asyncio
import logging
from datetime import datetime
import json

app = FastAPI(title="Content Creation & SEO Optimizer MCP", version="1.0.0")

class MCPRequest(BaseModel):
    method: str
    params: Dict[str, Any]
    id: Optional[str] = None

class MCPResponse(BaseModel):
    result: Optional[Dict[str, Any]] = None
    error: Optional[Dict[str, Any]] = None
    id: Optional[str] = None

class MCPServer:
    def __init__(self):
        self.tools = {}
        self.resources = {}
        self.setup_logging()
        
    def setup_logging(self):
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
    def register_tool(self, name: str, handler):
        """Registrace n√°stroje v MCP serveru"""
        self.tools[name] = handler
        self.logger.info(f"N√°stroj '{name}' byl zaregistrov√°n")
        
    def register_resource(self, name: str, resource):
        """Registrace zdroje v MCP serveru"""
        self.resources[name] = resource
        self.logger.info(f"Zdroj '{name}' byl zaregistrov√°n")

mcp_server = MCPServer()

@app.post("/mcp/tools/call")
async def call_tool(request: MCPRequest) -> MCPResponse:
    """Vol√°n√≠ n√°stroje p≈ôes MCP protokol"""
    try:
        tool_name = request.params.get("name")
        arguments = request.params.get("arguments", {})
        
        if tool_name not in mcp_server.tools:
            raise HTTPException(404, f"N√°stroj '{tool_name}' nenalezen")
            
        result = await mcp_server.tools[tool_name](arguments)
        return MCPResponse(result={"content": result}, id=request.id)
        
    except Exception as e:
        return MCPResponse(
            error={"code": -1, "message": str(e)}, 
            id=request.id
        )

@app.get("/mcp/tools/list")
async def list_tools():
    """Seznam dostupn√Ωch n√°stroj≈Ø"""
    tools = []
    for name in mcp_server.tools.keys():
        tools.append({
            "name": name,
            "description": f"N√°stroj pro {name}",
            "inputSchema": {"type": "object", "properties": {}}
        })
    return {"tools": tools}
````

### SEO Analyzer Agent

````python
import requests
from bs4 import BeautifulSoup
import re
from typing import List, Dict
import textstat
import nltk
from serpapi import GoogleSearch
import os
from dotenv import load_dotenv

load_dotenv()

class SEOAnalyzer:
    def __init__(self):
        self.serpapi_key = os.getenv("SERPAPI_KEY")
        
    async def analyze_keywords(self, args: Dict) -> Dict:
        """Anal√Ωza kl√≠ƒçov√Ωch slov"""
        try:
            keyword = args.get("keyword", "")
            country = args.get("country", "cz")
            
            # SERP anal√Ωza
            search_params = {
                "q": keyword,
                "location": f"{country}",
                "hl": "cs",
                "gl": country,
                "api_key": self.serpapi_key
            }
            
            search = GoogleSearch(search_params)
            results = search.get_dict()
            
            # Anal√Ωza konkurence
            competitors = []
            organic_results = results.get("organic_results", [])[:10]
            
            for result in organic_results:
                competitors.append({
                    "title": result.get("title", ""),
                    "url": result.get("link", ""),
                    "snippet": result.get("snippet", ""),
                    "position": result.get("position", 0)
                })
            
            # Keyword obt√≠≈ænost (simulovan√°)
            difficulty = min(100, len(organic_results) * 10)
            
            return {
                "keyword": keyword,
                "difficulty": difficulty,
                "search_volume": "N/A (Demo)",
                "competitors": competitors,
                "suggestions": self._generate_keyword_suggestions(keyword)
            }
            
        except Exception as e:
            return {"error": f"Chyba p≈ôi anal√Ωze kl√≠ƒçov√Ωch slov: {str(e)}"}
    
    def _generate_keyword_suggestions(self, keyword: str) -> List[str]:
        """Generov√°n√≠ n√°vrh≈Ø kl√≠ƒçov√Ωch slov"""
        base_suggestions = [
            f"{keyword} n√°vod",
            f"{keyword} tipy",
            f"jak {keyword}",
            f"nejlep≈°√≠ {keyword}",
            f"{keyword} 2024"
        ]
        return base_suggestions[:5]
    
    async def analyze_content_seo(self, args: Dict) -> Dict:
        """Anal√Ωza SEO obsahu"""
        try:
            content = args.get("content", "")
            target_keyword = args.get("target_keyword", "")
            
            # Z√°kladn√≠ SEO metriky
            word_count = len(content.split())
            readability = textstat.flesch_reading_ease(content)
            
            # Keyword density
            keyword_count = content.lower().count(target_keyword.lower())
            keyword_density = (keyword_count / word_count * 100) if word_count > 0 else 0
            
            # SEO doporuƒçen√≠
            recommendations = []
            
            if word_count < 300:
                recommendations.append("Zvƒõt≈°it d√©lku obsahu na minim√°lnƒõ 300 slov")
            
            if keyword_density < 1:
                recommendations.append(f"Zv√Ω≈°it hustotu kl√≠ƒçov√©ho slova '{target_keyword}'")
            elif keyword_density > 3:
                recommendations.append(f"Sn√≠≈æit hustotu kl√≠ƒçov√©ho slova '{target_keyword}'")
                
            if readability < 60:
                recommendations.append("Zlep≈°it ƒçitelnost textu")
            
            return {
                "word_count": word_count,
                "readability_score": readability,
                "keyword_density": round(keyword_density, 2),
                "recommendations": recommendations,
                "seo_score": self._calculate_seo_score(word_count, readability, keyword_density)
            }
            
        except Exception as e:
            return {"error": f"Chyba p≈ôi SEO anal√Ωze: {str(e)}"}
    
    def _calculate_seo_score(self, word_count: int, readability: float, keyword_density: float) -> int:
        """V√Ωpoƒçet celkov√©ho SEO sk√≥re"""
        score = 0
        
        # D√©lka obsahu (max 30 bod≈Ø)
        if word_count >= 1000:
            score += 30
        elif word_count >= 500:
            score += 20
        elif word_count >= 300:
            score += 10
            
        # ƒåitelnost (max 40 bod≈Ø)
        if readability >= 80:
            score += 40
        elif readability >= 60:
            score += 30
        elif readability >= 40:
            score += 20
            
        # Keyword density (max 30 bod≈Ø)
        if 1 <= keyword_density <= 2.5:
            score += 30
        elif 0.5 <= keyword_density < 1 or 2.5 < keyword_density <= 3.5:
            score += 20
        
        return min(100, score)

# Registrace v MCP serveru
seo_analyzer = SEOAnalyzer()
mcp_server.register_tool("analyze_keywords", seo_analyzer.analyze_keywords)
mcp_server.register_tool("analyze_content_seo", seo_analyzer.analyze_content_seo)
````

### Content Generator Agent

````python
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
import openai
import os
from typing import Dict, List
from dotenv import load_dotenv

load_dotenv()

class ContentGenerator:
    def __init__(self):
        openai.api_key = os.getenv("OPENAI_API_KEY")
        self.llm = OpenAI(temperature=0.7, openai_api_key=openai.api_key)
        
    async def generate_blog_post(self, args: Dict) -> Dict:
        """Generov√°n√≠ blog postu"""
        try:
            topic = args.get("topic", "")
            keywords = args.get("keywords", [])
            word_count = args.get("word_count", 800)
            tone = args.get("tone", "profesion√°ln√≠")
            
            prompt_template = PromptTemplate(
                input_variables=["topic", "keywords", "word_count", "tone"],
                template="""
Napi≈° blog post v ƒçe≈°tinƒõ na t√©ma: {topic}

Po≈æadavky:
- D√©lka: p≈ôibli≈ænƒõ {word_count} slov
- T√≥n: {tone}
- Zahr≈à kl√≠ƒçov√° slova: {keywords}
- Struktura: nadpis, √∫vod, 3-4 hlavn√≠ sekce, z√°vƒõr
- SEO optimalizovan√Ω obsah
- Pou≈æij markdown form√°tov√°n√≠

Blog post:
"""
            )
            
            chain = LLMChain(llm=self.llm, prompt=prompt_template)
            
            result = chain.run(
                topic=topic,
                keywords=", ".join(keywords) if keywords else "",
                word_count=word_count,
                tone=tone
            )
            
            return {
                "content": result,
                "word_count": len(result.split()),
                "type": "blog_post",
                "metadata": {
                    "topic": topic,
                    "keywords": keywords,
                    "tone": tone
                }
            }
            
        except Exception as e:
            return {"error": f"Chyba p≈ôi generov√°n√≠ blog postu: {str(e)}"}
    
    async def generate_social_media_post(self, args: Dict) -> Dict:
        """Generov√°n√≠ p≈ô√≠spƒõvku na soci√°ln√≠ s√≠tƒõ"""
        try:
            content = args.get("content", "")
            platform = args.get("platform", "facebook")
            hashtags = args.get("hashtags", [])
            
            platform_specs = {
                "facebook": {"max_length": 2000, "style": "p≈ô√°telsk√Ω a engaging"},
                "twitter": {"max_length": 280, "style": "struƒçn√Ω a v√Ωsti≈æn√Ω"},
                "instagram": {"max_length": 2200, "style": "vizu√°ln√≠ a inspirativn√≠"},
                "linkedin": {"max_length": 3000, "style": "profesion√°ln√≠ a informativn√≠"}
            }
            
            spec = platform_specs.get(platform, platform_specs["facebook"])
            
            prompt_template = PromptTemplate(
                input_variables=["content", "platform", "max_length", "style", "hashtags"],
                template="""
Vytvo≈ô p≈ô√≠spƒõvek pro {platform} na z√°kladƒõ tohoto obsahu:
{content}

Po≈æadavky:
- Maxim√°ln√≠ d√©lka: {max_length} znak≈Ø
- Styl: {style}
- Zahr≈à hashtags: {hashtags}
- Optimalizuj pro engagement na {platform}
- Pou≈æij ƒçesk√Ω jazyk

P≈ô√≠spƒõvek:
"""
            )
            
            chain = LLMChain(llm=self.llm, prompt=prompt_template)
            
            result = chain.run(
                content=content[:500],  # Omezit vstupn√≠ obsah
                platform=platform,
                max_length=spec["max_length"],
                style=spec["style"],
                hashtags=" ".join(hashtags) if hashtags else ""
            )
            
            return {
                "content": result,
                "platform": platform,
                "character_count": len(result),
                "hashtags": hashtags
            }
            
        except Exception as e:
            return {"error": f"Chyba p≈ôi generov√°n√≠ social media postu: {str(e)}"}
    
    async def optimize_content_for_seo(self, args: Dict) -> Dict:
        """Optimalizace obsahu pro SEO"""
        try:
            content = args.get("content", "")
            target_keywords = args.get("target_keywords", [])
            
            prompt_template = PromptTemplate(
                input_variables=["content", "keywords"],
                template="""
Optimalizuj n√°sleduj√≠c√≠ obsah pro SEO s c√≠lov√Ωmi kl√≠ƒçov√Ωmi slovy: {keywords}

P≈Øvodn√≠ obsah:
{content}

Pokyny pro optimalizaci:
1. P≈ôirozenƒõ zahr≈à c√≠lov√° kl√≠ƒçov√° slova
2. Zlep≈°i strukturu pomoc√≠ nadpis≈Ø (H2, H3)
3. P≈ôidej meta description
4. Optimalizuj pro ƒçitelnost
5. Zachovej ƒçesk√Ω jazyk a p≈ôirozen√Ω tok textu

Optimalizovan√Ω obsah:
"""
            )
            
            chain = LLMChain(llm=self.llm, prompt=prompt_template)
            
            result = chain.run(
                content=content,
                keywords=", ".join(target_keywords)
            )
            
            return {
                "optimized_content": result,
                "target_keywords": target_keywords,
                "optimization_applied": True
            }
            
        except Exception as e:
            return {"error": f"Chyba p≈ôi SEO optimalizaci: {str(e)}"}

# Registrace v MCP serveru
content_generator = ContentGenerator()
mcp_server.register_tool("generate_blog_post", content_generator.generate_blog_post)
mcp_server.register_tool("generate_social_media_post", content_generator.generate_social_media_post)
mcp_server.register_tool("optimize_content_for_seo", content_generator.optimize_content_for_seo)
````

### Web Scraper Agent

````python
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import re
from typing import Dict, List
from urllib.parse import urljoin, urlparse

class WebScraper:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
    async def scrape_competitor_content(self, args: Dict) -> Dict:
        """Scraping obsahu konkurence"""
        try:
            urls = args.get("urls", [])
            keyword = args.get("keyword", "")
            
            competitor_data = []
            
            for url in urls[:5]:  # Omezit na 5 URL
                try:
                    response = requests.get(url, headers=self.headers, timeout=10)
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # Extrakce z√°kladn√≠ch informac√≠
                    title = soup.find('title')
                    title_text = title.text.strip() if title else ""
                    
                    # Meta description
                    meta_desc = soup.find('meta', attrs={'name': 'description'})
                    meta_desc_text = meta_desc.get('content', '') if meta_desc else ""
                    
                    # Hlavn√≠ obsah
                    content_tags = soup.find_all(['p', 'h1', 'h2', 'h3', 'article'])
                    content = ' '.join([tag.get_text().strip() for tag in content_tags])
                    
                    # Poƒçet slov
                    word_count = len(content.split())
                    
                    # Anal√Ωza kl√≠ƒçov√Ωch slov
                    keyword_mentions = content.lower().count(keyword.lower())
                    
                    competitor_data.append({
                        "url": url,
                        "title": title_text,
                        "meta_description": meta_desc_text,
                        "word_count": word_count,
                        "keyword_mentions": keyword_mentions,
                        "content_preview": content[:200] + "..." if len(content) > 200 else content
                    })
                    
                except Exception as e:
                    competitor_data.append({
                        "url": url,
                        "error": f"Chyba p≈ôi scrapingu: {str(e)}"
                    })
                    
                time.sleep(1)  # Rate limiting
            
            return {
                "competitor_analysis": competitor_data,
                "total_scraped": len(competitor_data),
                "keyword": keyword
            }
            
        except Exception as e:
            return {"error": f"Chyba p≈ôi scraping konkurence: {str(e)}"}
    
    async def scrape_trending_topics(self, args: Dict) -> Dict:
        """Scraping trendov√Ωch t√©mat"""
        try:
            industry = args.get("industry", "technology")
            
            # Simulace scraping trendov√Ωch t√©mat
            trending_sources = [
                "https://trends.google.com/trends/explore",
                "https://www.reddit.com/r/technology/hot",
                "https://news.ycombinator.com/"
            ]
            
            trending_topics = []
            
            # Pro demo √∫ƒçely vr√°t√≠me simulovan√° data
            demo_topics = [
                {"topic": "Umƒõl√° inteligence v marketingu", "trend_score": 85, "volume": "Vysok√Ω"},
                {"topic": "Automatizace obsahu", "trend_score": 78, "volume": "St≈ôedn√≠"},
                {"topic": "SEO trendy 2024", "trend_score": 72, "volume": "Vysok√Ω"},
                {"topic": "Content marketing n√°stroje", "trend_score": 69, "volume": "St≈ôedn√≠"},
                {"topic": "Soci√°ln√≠ s√≠tƒõ algoritmy", "trend_score": 65, "volume": "N√≠zk√Ω"}
            ]
            
            return {
                "trending_topics": demo_topics,
                "industry": industry,
                "scraped_sources": len(trending_sources),
                "last_updated": time.strftime("%Y-%m-%d %H:%M:%S")
            }
            
        except Exception as e:
            return {"error": f"Chyba p≈ôi scraping trend≈Ø: {str(e)}"}
    
    async def extract_content_ideas(self, args: Dict) -> Dict:
        """Extrakce n√°pad≈Ø na obsah z web≈Ø"""
        try:
            urls = args.get("urls", [])
            topic = args.get("topic", "")
            
            content_ideas = []
            
            for url in urls[:3]:  # Omezit na 3 URL
                try:
                    response = requests.get(url, headers=self.headers, timeout=10)
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # Extrakce nadpis≈Ø
                    headings = soup.find_all(['h1', 'h2', 'h3'])
                    
                    for heading in headings[:10]:  # Max 10 nadpis≈Ø per URL
                        heading_text = heading.get_text().strip()
                        if len(heading_text) > 10 and topic.lower() in heading_text.lower():
                            content_ideas.append({
                                "idea": heading_text,
                                "source_url": url,
                                "heading_level": heading.name,
                                "relevance_score": self._calculate_relevance(heading_text, topic)
                            })
                    
                except Exception as e:
                    continue
                    
                time.sleep(1)
            
            # Se≈ôadit podle relevance
            content_ideas.sort(key=lambda x: x["relevance_score"], reverse=True)
            
            return {
                "content_ideas": content_ideas[:20],  # Top 20 n√°pad≈Ø
                "topic": topic,
                "sources_analyzed": len(urls)
            }
            
        except Exception as e:
            return {"error": f"Chyba p≈ôi extrakci n√°pad≈Ø: {str(e)}"}
    
    def _calculate_relevance(self, text: str, topic: str) -> float:
        """V√Ωpoƒçet relevance textu k t√©matu"""
        text_lower = text.lower()
        topic_lower = topic.lower()
        
        # Z√°kladn√≠ sk√≥re za obsahov√°n√≠ t√©matu
        score = 0.0
        if topic_lower in text_lower:
            score += 1.0
            
        # Bonus za kl√≠ƒçov√° slova
        keywords = topic_lower.split()
        for keyword in keywords:
            if keyword in text_lower:
                score += 0.5
                
        # Penalizace za p≈ô√≠li≈° kr√°tk√Ω nebo dlouh√Ω text
        if len(text) < 20:
            score *= 0.5
        elif len(text) > 100:
            score *= 0.8
            
        return score

# Registrace v MCP serveru
web_scraper = WebScraper()
mcp_server.register_tool("scrape_competitor_content", web_scraper.scrape_competitor_content)
mcp_server.register_tool("scrape_trending_topics", web_scraper.scrape_trending_topics)
mcp_server.register_tool("extract_content_ideas", web_scraper.extract_content_ideas)
````

### Publisher Agent

````python
import requests
from wordpress_xmlrpc import Client, WordPressPost
from wordpress_xmlrpc.methods.posts import NewPost, GetPosts
import tweepy
import facebook
from typing import Dict, List
import os
from dotenv import load_dotenv

load_dotenv()

class Publisher:
    def __init__(self):
        self.setup_apis()
        
    def setup_apis(self):
        """Nastaven√≠ API p≈ôipojen√≠"""
        # WordPress
        self.wp_url = os.getenv("WORDPRESS_URL", "")
        self.wp_username = os.getenv("WORDPRESS_USERNAME", "")
        self.wp_password = os.getenv("WORDPRESS_PASSWORD", "")
        
        # Twitter API
        self.twitter_api_key = os.getenv("TWITTER_API_KEY", "")
        self.twitter_api_secret = os.getenv("TWITTER_API_SECRET", "")
        self.twitter_access_token = os.getenv("TWITTER_ACCESS_TOKEN", "")
        self.twitter_access_token_secret = os.getenv("TWITTER_ACCESS_TOKEN_SECRET", "")
        
        # Facebook API
        self.facebook_access_token = os.getenv("FACEBOOK_ACCESS_TOKEN", "")
        
    async def publish_to_wordpress(self, args: Dict) -> Dict:
        """Publikace na WordPress"""
        try:
            title = args.get("title", "")
            content = args.get("content", "")
            tags = args.get("tags", [])
            category = args.get("category", "")
            status = args.get("status", "draft")  # draft nebo publish
            
            if not all([self.wp_url, self.wp_username, self.wp_password]):
                return {"error": "WordPress p≈ôihla≈°ovac√≠ √∫daje nejsou nastaveny"}
            
            # P≈ôipojen√≠ k WordPress
            client = Client(f"{self.wp_url}/xmlrpc.php", self.wp_username, self.wp_password)
            
            # Vytvo≈ôen√≠ p≈ô√≠spƒõvku
            post = WordPressPost()
            post.title = title
            post.content = content
            post.terms_names = {
                'post_tag': tags,
                'category': [category] if category else []
            }
            post.post_status = status
            
            # Publikace
            post_id = client.call(NewPost(post))
            
            return {
                "success": True,
                "post_id": post_id,
                "platform": "WordPress",
                "url": f"{self.wp_url}/?p={post_id}",
                "status": status
            }
            
        except Exception as e:
            return {"error": f"Chyba p≈ôi publikaci na WordPress: {str(e)}"}
    
    async def publish_to_twitter(self, args: Dict) -> Dict:
        """Publikace na Twitter"""
        try:
            content = args.get("content", "")
            
            if not all([self.twitter_api_key, self.twitter_api_secret, 
                       self.twitter_access_token, self.twitter_access_token_secret]):
                return {"error": "Twitter API kl√≠ƒçe nejsou nastaveny"}
            
            # Nastaven√≠ Twitter API
            auth = tweepy.OAuthHandler(self.twitter_api_key, self.twitter_api_secret)
            auth.set_access_token(self.twitter_access_token, self.twitter_access_token_secret)
            api = tweepy.API(auth)
            
            # Zkr√°cen√≠ obsahu pokud je p≈ô√≠li≈° dlouh√Ω
            if len(content) > 280:
                content = content[:277] + "..."
            
            # Tweet
            tweet = api.update_status(content)
            
            return {
                "success": True,
                "tweet_id": tweet.id,
                "platform": "Twitter",
                "url": f"https://twitter.com/user/status/{tweet.id}",
                "content": content
            }
            
        except Exception as e:
            return {"error": f"Chyba p≈ôi publikaci na Twitter: {str(e)}"}
    
    async def publish_to_facebook(self, args: Dict) -> Dict:
        """Publikace na Facebook"""
        try:
            content = args.get("content", "")
            page_id = args.get("page_id", "")
            
            if not self.facebook_access_token:
                return {"error": "Facebook access token nen√≠ nastaven"}
            
            graph = facebook.GraphAPI(access_token=self.facebook_access_token)
            
            # Publikace p≈ô√≠spƒõvku
            if page_id:
                # Publikace na str√°nku
                result = graph.put_object(
                    parent_object=page_id,
                    connection_name='feed',
                    message=content
                )
            else:
                # Publikace na osobn√≠ profil
                result = graph.put_object(
                    parent_object='me',
                    connection_name='feed',
                    message=content
                )
            
            return {
                "success": True,
                "post_id": result['id'],
                "platform": "Facebook",
                "content": content
            }
            
        except Exception as e:
            return {"error": f"Chyba p≈ôi publikaci na Facebook: {str(e)}"}
    
    async def schedule_multi_platform_post(self, args: Dict) -> Dict:
        """Napl√°nov√°n√≠ p≈ô√≠spƒõvku na v√≠ce platform√°ch"""
        try:
            content_data = args.get("content_data", {})
            platforms = args.get("platforms", [])
            schedule_time = args.get("schedule_time", None)
            
            results = []
            
            for platform in platforms:
                platform_content = content_data.get(platform, {})
                
                if platform == "wordpress":
                    result = await self.publish_to_wordpress(platform_content)
                elif platform == "twitter":
                    result = await self.publish_to_twitter(platform_content)
                elif platform == "facebook":
                    result = await self.publish_to_facebook(platform_content)
                else:
                    result = {"error": f"Nepodporovan√° platforma: {platform}"}
                
                results.append({
                    "platform": platform,
                    "result": result
                })
            
            return {
                "multi_platform_results": results,
                "total_platforms": len(platforms),
                "successful_posts": sum(1 for r in results if r["result"].get("success")),
                "scheduled_time": schedule_time
            }
            
        except Exception as e:
            return {"error": f"Chyba p≈ôi multi-platform publikaci: {str(e)}"}

# Registrace v MCP serveru
publisher = Publisher()
mcp_server.register_tool("publish_to_wordpress", publisher.publish_to_wordpress)
mcp_server.register_tool("publish_to_twitter", publisher.publish_to_twitter)
mcp_server.register_tool("publish_to_facebook", publisher.publish_to_facebook)
mcp_server.register_tool("schedule_multi_platform_post", publisher.schedule_multi_platform_post)
````

### Hlavn√≠ Orchestrator

````python
from fastapi import FastAPI
from agents.seo_analyzer import seo_analyzer
from agents.content_generator import content_generator
from agents.web_scraper import web_scraper
from agents.publisher import publisher
import asyncio
from typing import Dict, List
import uvicorn

class ContentCreationOrchestrator:
    def __init__(self):
        self.agents = {
            "seo_analyzer": seo_analyzer,
            "content_generator": content_generator,
            "web_scraper": web_scraper,
            "publisher": publisher
        }
    
    async def create_complete_content_workflow(self, args: Dict) -> Dict:
        """Kompletn√≠ workflow tvorby obsahu"""
        try:
            topic = args.get("topic", "")
            target_keywords = args.get("target_keywords", [])
            platforms = args.get("platforms", ["wordpress"])
            
            workflow_results = {}
            
            # 1. SEO anal√Ωza kl√≠ƒçov√Ωch slov
            keyword_analysis = await seo_analyzer.analyze_keywords({
                "keyword": target_keywords[0] if target_keywords else topic,
                "country": "cz"
            })
            workflow_results["keyword_analysis"] = keyword_analysis
            
            # 2. Scraping konkurence
            if keyword_analysis.get("competitors"):
                competitor_urls = [comp["url"] for comp in keyword_analysis["competitors"][:3]]
                competitor_analysis = await web_scraper.scrape_competitor_content({
                    "urls": competitor_urls,
                    "keyword": target_keywords[0] if target_keywords else topic
                })
                workflow_results["competitor_analysis"] = competitor_analysis
            
            # 3. Generov√°n√≠ hlavn√≠ho obsahu
            blog_post = await content_generator.generate_blog_post({
                "topic": topic,
                "keywords": target_keywords,
                "word_count": 800,
                "tone": "profesion√°ln√≠"
            })
            workflow_results["blog_post"] = blog_post
            
            # 4. SEO optimalizace obsahu
            if blog_post.get("content"):
                seo_optimized = await content_generator.optimize_content_for_seo({
                    "content": blog_post["content"],
                    "target_keywords": target_keywords
                })
                workflow_results["seo_optimized_content"] = seo_optimized
                
                # 5. SEO anal√Ωza fin√°ln√≠ho obsahu
                content_seo_analysis = await seo_analyzer.analyze_content_seo({
                    "content": seo_optimized.get("optimized_content", blog_post["content"]),
                    "target_keyword": target_keywords[0] if target_keywords else topic
                })
                workflow_results["content_seo_analysis"] = content_seo_analysis
            
            # 6. Generov√°n√≠ obsahu pro soci√°ln√≠ s√≠tƒõ
            social_content = {}
            final_content = workflow_results.get("seo_optimized_content", {}).get("optimized_content", blog_post.get("content", ""))
            
            for platform in ["facebook", "twitter", "linkedin"]:
                social_post = await content_generator.generate_social_media_post({
                    "content": final_content,
                    "platform": platform,
                    "hashtags": [f"#{kw.replace(' ', '')}" for kw in target_keywords[:3]]
                })
                social_content[platform] = social_post
            
            workflow_results["social_content"] = social_content
            
            # 7. Publikace (pouze pokud je po≈æadov√°na)
            if args.get("auto_publish", False):
                publication_results = []
                
                if "wordpress" in platforms and final_content:
                    wp_result = await publisher.publish_to_wordpress({
                        "title": topic,
                        "content": final_content,
                        "tags": target_keywords,
                        "status": "draft"
                    })
                    publication_results.append({"platform": "wordpress", "result": wp_result})
                
                workflow_results["publication_results"] = publication_results
            
            return {
                "workflow_completed": True,
                "topic": topic,
                "target_keywords": target_keywords,
                "results": workflow_results,
                "summary": self._generate_workflow_summary(workflow_results)
            }
            
        except Exception as e:
            return {"error": f"Chyba ve workflow: {str(e)}"}
    
    def _generate_workflow_summary(self, results: Dict) -> Dict:
        """Generov√°n√≠ souhrnu workflow"""
        summary = {
            "steps_completed": [],
            "content_metrics": {},
            "recommendations": []
        }
        
        if "keyword_analysis" in results:
            summary["steps_completed"].append("SEO anal√Ωza kl√≠ƒçov√Ωch slov")
            
        if "blog_post" in results:
            summary["steps_completed"].append("Generov√°n√≠ obsahu")
            blog_post = results["blog_post"]
            summary["content_metrics"]["word_count"] = blog_post.get("word_count", 0)
            
        if "content_seo_analysis" in results:
            summary["steps_completed"].append("SEO anal√Ωza obsahu")
            seo_analysis = results["content_seo_analysis"]
            summary["content_metrics"]["seo_score"] = seo_analysis.get("seo_score", 0)
            summary["recommendations"] = seo_analysis.get("recommendations", [])
            
        if "social_content" in results:
            summary["steps_completed"].append("Generov√°n√≠ obsahu pro soci√°ln√≠ s√≠tƒõ")
            summary["content_metrics"]["social_platforms"] = len(results["social_content"])
            
        return summary

# Registrace orchestratoru
orchestrator = ContentCreationOrchestrator()
mcp_server.register_tool("create_complete_content_workflow", orchestrator.create_complete_content_workflow)

@app.get("/")
async def root():
    return {
        "message": "Content Creation & SEO Optimizer MCP Server",
        "version": "1.0.0",
        "available_tools": list(mcp_server.tools.keys())
    }

@app.post("/workflow/complete")
async def run_complete_workflow(request: dict):
    """Spu≈°tƒõn√≠ kompletn√≠ho workflow"""
    return await orchestrator.create_complete_content_workflow(request)

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
````

### Environment Configuration

````bash
# OpenAI API
OPENAI_API_KEY=your_openai_api_key_here

# SERP API pro keyword research
SERPAPI_KEY=your_serpapi_key_here

# WordPress
WORDPRESS_URL=https://your-wordpress-site.com
WORDPRESS_USERNAME=your_wp_username
WORDPRESS_PASSWORD=your_wp_password

# Twitter API
TWITTER_API_KEY=your_twitter_api_key
TWITTER_API_SECRET=your_twitter_api_secret
TWITTER_ACCESS_TOKEN=your_twitter_access_token
TWITTER_ACCESS_TOKEN_SECRET=your_twitter_access_token_secret

# Facebook API
FACEBOOK_ACCESS_TOKEN=your_facebook_access_token

# Shopify (voliteln√©)
SHOPIFY_API_KEY=your_shopify_api_key
SHOPIFY_PASSWORD=your_shopify_password
SHOPIFY_STORE_URL=your-store.myshopify.com
````

### Spu≈°tƒõn√≠ Aplikace

````bash
import uvicorn
from main import app

if __name__ == "__main__":
    print("üöÄ Spou≈°tƒõn√≠ Content Creation & SEO Optimizer MCP Serveru...")
    print("üìù Dostupn√© n√°stroje:")
    print("  - SEO anal√Ωza kl√≠ƒçov√Ωch slov")
    print("  - Generov√°n√≠ blog post≈Ø")
    print("  - Web scraping konkurence")
    print("  - Publikace na WordPress a soci√°ln√≠ s√≠tƒõ")
    print("  - Kompletn√≠ content workflow")
    print("\nüåê Server bude dostupn√Ω na: http://localhost:8000")
    print("üìö API dokumentace: http://localhost:8000/docs")
    
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)
````

## 5. Shrnut√≠ Projektu

### Kl√≠ƒçov√© V√Ωhody
- **Automatizace cel√©ho procesu**: Od v√Ωzkumu kl√≠ƒçov√Ωch slov po publikaci
- **SEO optimalizace**: Integrovan√° anal√Ωza a optimalizace pro vyhled√°vaƒçe
- **Multi-platform publikace**: Souƒçasn√° publikace na v√≠ce platform√°ch
- **Scalabilita**: Modul√°rn√≠ architektura umo≈æ≈àuj√≠c√≠ snadn√© roz≈°√≠≈ôen√≠

### Technick√© P≈ôednosti
- **Model Context Protocol**: Standardizovan√© rozhran√≠ pro AI agenty
- **Mikroservisov√° architektura**: Ka≈æd√Ω agent m√° specifickou zodpovƒõdnost
- **Modern√≠ technologie**: Vyu≈æit√≠ nejnovƒõj≈°√≠ch AI framework≈Ø
- **Robustn√≠ error handling**: Comprehensive zpracov√°n√≠ chyb

### Business Hodnota
Syst√©m umo≈æ≈àuje dramatick√© sn√≠≈æen√≠ ƒçasu pot≈ôebn√©ho na tvorbu kvalitn√≠ho, SEO optimalizovan√©ho obsahu z hodin na minuty, p≈ôi zachov√°n√≠ vysok√© kvality a relevantnosti. Ide√°ln√≠ pro mal√© firmy i enterprise klienty hledaj√≠c√≠ efektivn√≠ content marketing ≈ôe≈°en√≠.

### Budouc√≠ Roz≈°√≠≈ôen√≠
- Integrace s dal≈°√≠mi CMS platformami
- Pokroƒçil√° AI anal√Ωza sentiment
- Automatick√© A/B testov√°n√≠ obsahu
- Inteligentn√≠ pl√°nov√°n√≠ publikace na z√°kladƒõ audience analytics