<small>Claude Sonnet 4 **(AutonomnÃ­ Framework pro TestovÃ¡nÃ­ Software a QA (Multi-Agent SystÃ©my))**</small>
# Autonomous Software Testing and QA Framework

## 1. KlÃ­ÄovÃ© Koncepty

### Multi-Agent SystÃ©my
SystÃ©m vÃ­ce nezÃ¡vislÃ½ch softwarovÃ½ch agentÅ¯, kteÅ™Ã­ spolupracujÃ­ na dosaÅ¾enÃ­ spoleÄnÃ©ho cÃ­le. KaÅ¾dÃ½ agent mÃ¡ specifickÃ© role a schopnosti, komunikuje s ostatnÃ­mi a pÅ™ispÃ­vÃ¡ k celkovÃ©mu Å™eÅ¡enÃ­.

### GenerovÃ¡nÃ­ TestovacÃ­ch PÅ™Ã­padÅ¯
AutomatickÃ© vytvÃ¡Å™enÃ­ testovacÃ­ch scÃ©nÃ¡Å™Å¯ na zÃ¡kladÄ› analÃ½zy kÃ³du, specifikacÃ­ a poÅ¾adavkÅ¯. Zahrnuje pozitivnÃ­ i negativnÃ­ testovacÃ­ pÅ™Ã­pady pokrÃ½vajÃ­cÃ­ rÅ¯znÃ© execution paths.

### Detekce Chyb
AutomatickÃ© vyhledÃ¡vÃ¡nÃ­ a identifikace defektÅ¯ v software pomocÃ­ statickÃ© a dynamickÃ© analÃ½zy kÃ³du, pattern matching a heuristickÃ½ch metod.

### Performance Testing
MÄ›Å™enÃ­ a analÃ½za vÃ½konu aplikace pod rÅ¯znÃ½mi zÃ¡tÄ›Å¾emi, vÄetnÄ› load testingu, stress testingu a endurance testingu.

### SkenovÃ¡nÃ­ BezpeÄnostnÃ­ch ZranitelnostÃ­
AutomatickÃ© vyhledÃ¡vÃ¡nÃ­ potenciÃ¡lnÃ­ch bezpeÄnostnÃ­ch rizik jako SQL injection, XSS Ãºtoky, neautorizovanÃ½ pÅ™Ã­stup k datÅ¯m.

### RegresnÃ­ TestovÃ¡nÃ­
OvÄ›Å™ovÃ¡nÃ­, Å¾e novÃ© zmÄ›ny v kÃ³du neporuÅ¡ily existujÃ­cÃ­ funkcionalitu pomocÃ­ opÄ›tovnÃ©ho spuÅ¡tÄ›nÃ­ dÅ™Ã­ve ÃºspÄ›Å¡nÃ½ch testÅ¯.

## 2. KomplexnÃ­ VysvÄ›tlenÃ­ Projektu

### CÃ­le Projektu
Tento framework pÅ™edstavuje revoluÄnÃ­ pÅ™Ã­stup k automatizaci testovÃ¡nÃ­ software pomocÃ­ koordinovanÃ©ho tÃ½mu AI agentÅ¯. KaÅ¾dÃ½ agent se specializuje na specifickou oblast testovÃ¡nÃ­ a spoleÄnÄ› vytvÃ¡Å™ejÃ­ komplexnÃ­ QA ekosystÃ©m.

### HlavnÃ­ VÃ½zvy
- **Koordinace AgentÅ¯**: Synchronizace prÃ¡ce mezi rÅ¯znÃ½mi specializovanÃ½mi agenty
- **InteligentnÃ­ AnalÃ½za**: Schopnost porozumÄ›t sloÅ¾itÃ½m codebases a generovat relevantnÃ­ testy
- **FaleÅ¡nÃ© Pozitivy**: Minimalizace false positive vÃ½sledkÅ¯ pÅ™i detekci chyb
- **Å kÃ¡lovÃ¡nÃ­**: EfektivnÃ­ prÃ¡ce s velkÃ½mi projekty a komplexnÃ­mi aplikacemi

### PotenciÃ¡lnÃ­ Dopad
Framework mÅ¯Å¾e dramaticky snÃ­Å¾it Äas a nÃ¡klady na QA, zvÃ½Å¡it pokrytÃ­ testÅ¯ a odhalit skrytÃ© problÃ©my dÅ™Ã­ve, neÅ¾ se dostanou do produkce.

## 3. KomplexnÃ­ Implementace s Pythonem

````python
# requirements.txt
"""
crewai>=0.28.0
langchain>=0.1.0
openai>=1.0.0
ast-tools>=0.1.0
pytest>=7.0.0
coverage>=7.0.0
bandit>=1.7.0
locust>=2.0.0
selenium>=4.0.0
requests>=2.28.0
python-dotenv>=1.0.0
"""
````

````python
import ast
import os
import subprocess
import time
import json
import logging
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path

from crewai import Agent, Task, Crew
from langchain.llms import OpenAI
from langchain.tools import Tool
from langchain.schema import BaseRetriever
import pytest
import requests
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

# Konfigurace loggingu
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class TestResult:
    """Struktura pro vÃ½sledky testovÃ¡nÃ­"""
    test_type: str
    status: str
    details: Dict[str, Any]
    timestamp: float
    duration: float

@dataclass
class VulnerabilityReport:
    """Struktura pro bezpeÄnostnÃ­ zranitelnosti"""
    severity: str
    category: str
    description: str
    file_path: str
    line_number: int
    recommendation: str

class CodeAnalyzer:
    """AnalyzÃ¡tor kÃ³du pro extrakci funkcionalit"""
    
    def __init__(self, project_path: str):
        self.project_path = Path(project_path)
    
    def extract_functions(self, file_path: str) -> List[Dict[str, Any]]:
        """Extrakce funkcÃ­ z Python souboru"""
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                tree = ast.parse(file.read())
            
            functions = []
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    functions.append({
                        'name': node.name,
                        'args': [arg.arg for arg in node.args.args],
                        'line_number': node.lineno,
                        'docstring': ast.get_docstring(node)
                    })
            return functions
        except Exception as e:
            logger.error(f"Chyba pÅ™i analÃ½ze souboru {file_path}: {e}")
            return []
    
    def get_project_structure(self) -> Dict[str, List[str]]:
        """ZÃ­skÃ¡nÃ­ struktury projektu"""
        structure = {}
        for py_file in self.project_path.rglob("*.py"):
            if '__pycache__' not in str(py_file):
                functions = self.extract_functions(str(py_file))
                structure[str(py_file)] = functions
        return structure

class TestCaseGenerator:
    """Agent pro generovÃ¡nÃ­ testovacÃ­ch pÅ™Ã­padÅ¯"""
    
    def __init__(self, llm):
        self.llm = llm
    
    def generate_unit_tests(self, function_info: Dict[str, Any]) -> str:
        """GenerovÃ¡nÃ­ unit testÅ¯ pro funkci"""
        prompt = f"""
        VytvoÅ™ komplexnÃ­ unit testy pro funkci:
        NÃ¡zev: {function_info['name']}
        Argumenty: {function_info['args']}
        Dokumentace: {function_info.get('docstring', 'NenÃ­ k dispozici')}
        
        ZahrÅˆ:
        - PozitivnÃ­ testovacÃ­ pÅ™Ã­pady
        - NegativnÃ­ testovacÃ­ pÅ™Ã­pady
        - Edge cases
        - ParametrizovanÃ© testy
        
        FormÃ¡t odpovÄ›di: ÄŒistÃ½ Python kÃ³d s pytest.
        """
        
        try:
            response = self.llm.predict(prompt)
            return response
        except Exception as e:
            logger.error(f"Chyba pÅ™i generovÃ¡nÃ­ testÅ¯: {e}")
            return f"# Chyba pÅ™i generovÃ¡nÃ­ testÅ¯ pro {function_info['name']}"
    
    def generate_integration_tests(self, module_info: Dict[str, Any]) -> str:
        """GenerovÃ¡nÃ­ integraÄnÃ­ch testÅ¯"""
        prompt = f"""
        VytvoÅ™ integraÄnÃ­ testy pro modul s funkcemi:
        {[func['name'] for func in module_info]}
        
        ZahrÅˆ:
        - TestovÃ¡nÃ­ interakcÃ­ mezi funkcemi
        - End-to-end scÃ©nÃ¡Å™e
        - Data flow testovÃ¡nÃ­
        
        FormÃ¡t: Python kÃ³d s pytest a fixtures.
        """
        
        try:
            response = self.llm.predict(prompt)
            return response
        except Exception as e:
            logger.error(f"Chyba pÅ™i generovÃ¡nÃ­ integraÄnÃ­ch testÅ¯: {e}")
            return "# Chyba pÅ™i generovÃ¡nÃ­ integraÄnÃ­ch testÅ¯"

class BugDetector:
    """Agent pro detekci chyb"""
    
    def __init__(self, llm):
        self.llm = llm
    
    def analyze_code_patterns(self, file_path: str) -> List[Dict[str, Any]]:
        """AnalÃ½za kÃ³du pro detekci moÅ¾nÃ½ch chyb"""
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                code = file.read()
            
            prompt = f"""
            Analyzuj nÃ¡sledujÃ­cÃ­ Python kÃ³d a identifikuj moÅ¾nÃ© problÃ©my:
            
            {code}
            
            Hledej:
            - PotenciÃ¡lnÃ­ runtime chyby
            - NeoÅ¡etÅ™enÃ© vÃ½jimky
            - Memory leaks
            - Performance problÃ©my
            - LogickÃ© chyby
            
            FormÃ¡t odpovÄ›di: JSON seznam s detaily problÃ©mÅ¯.
            """
            
            response = self.llm.predict(prompt)
            # ParsovÃ¡nÃ­ odpovÄ›di (zjednoduÅ¡enÃ©)
            return [{"type": "analysis", "details": response}]
            
        except Exception as e:
            logger.error(f"Chyba pÅ™i analÃ½ze souboru {file_path}: {e}")
            return []
    
    def run_static_analysis(self, project_path: str) -> List[Dict[str, Any]]:
        """SpuÅ¡tÄ›nÃ­ statickÃ© analÃ½zy pomocÃ­ externÃ­ch nÃ¡strojÅ¯"""
        results = []
        
        try:
            # Flake8 analÃ½za
            result = subprocess.run(
                ['flake8', project_path, '--format=json'],
                capture_output=True,
                text=True,
                timeout=60
            )
            
            if result.stdout:
                try:
                    flake8_issues = json.loads(result.stdout)
                    results.extend(flake8_issues)
                except json.JSONDecodeError:
                    logger.warning("Nelze parsovat flake8 vÃ½sledky")
            
        except (subprocess.TimeoutExpired, FileNotFoundError) as e:
            logger.warning(f"Flake8 analÃ½za selhala: {e}")
        
        return results

class SecurityScanner:
    """Agent pro bezpeÄnostnÃ­ skenovÃ¡nÃ­"""
    
    def __init__(self, llm):
        self.llm = llm
    
    def scan_vulnerabilities(self, project_path: str) -> List[VulnerabilityReport]:
        """SkenovÃ¡nÃ­ bezpeÄnostnÃ­ch zranitelnostÃ­"""
        vulnerabilities = []
        
        try:
            # Bandit skenovÃ¡nÃ­
            result = subprocess.run(
                ['bandit', '-r', project_path, '-f', 'json'],
                capture_output=True,
                text=True,
                timeout=120
            )
            
            if result.stdout:
                try:
                    bandit_data = json.loads(result.stdout)
                    for issue in bandit_data.get('results', []):
                        vulnerability = VulnerabilityReport(
                            severity=issue.get('issue_severity', 'UNKNOWN'),
                            category=issue.get('test_name', 'General'),
                            description=issue.get('issue_text', ''),
                            file_path=issue.get('filename', ''),
                            line_number=issue.get('line_number', 0),
                            recommendation=issue.get('issue_confidence', '')
                        )
                        vulnerabilities.append(vulnerability)
                        
                except json.JSONDecodeError:
                    logger.warning("Nelze parsovat bandit vÃ½sledky")
                    
        except (subprocess.TimeoutExpired, FileNotFoundError) as e:
            logger.warning(f"Bandit skenovÃ¡nÃ­ selhalo: {e}")
        
        return vulnerabilities
    
    def analyze_dependencies(self, requirements_file: str) -> List[Dict[str, Any]]:
        """AnalÃ½za bezpeÄnosti zÃ¡vislostÃ­"""
        try:
            # Safety check (pokud je nainstalovanÃ½)
            result = subprocess.run(
                ['safety', 'check', '-r', requirements_file, '--json'],
                capture_output=True,
                text=True,
                timeout=60
            )
            
            if result.stdout:
                return json.loads(result.stdout)
                
        except (subprocess.TimeoutExpired, FileNotFoundError, json.JSONDecodeError) as e:
            logger.warning(f"Safety analÃ½za selhala: {e}")
        
        return []

class PerformanceTester:
    """Agent pro testovÃ¡nÃ­ vÃ½konu"""
    
    def __init__(self, llm):
        self.llm = llm
    
    def run_load_tests(self, target_url: str, concurrent_users: int = 10) -> Dict[str, Any]:
        """SpuÅ¡tÄ›nÃ­ zÃ¡tÄ›Å¾ovÃ½ch testÅ¯"""
        try:
            # Simulace load testu pomocÃ­ requests
            results = {
                'total_requests': 0,
                'successful_requests': 0,
                'failed_requests': 0,
                'average_response_time': 0,
                'max_response_time': 0,
                'min_response_time': float('inf')
            }
            
            response_times = []
            
            def make_request():
                try:
                    start_time = time.time()
                    response = requests.get(target_url, timeout=10)
                    end_time = time.time()
                    
                    response_time = end_time - start_time
                    response_times.append(response_time)
                    
                    if response.status_code == 200:
                        return {'success': True, 'response_time': response_time}
                    else:
                        return {'success': False, 'response_time': response_time}
                        
                except Exception as e:
                    return {'success': False, 'error': str(e)}
            
            # ParalelnÃ­ poÅ¾adavky
            with ThreadPoolExecutor(max_workers=concurrent_users) as executor:
                futures = [executor.submit(make_request) for _ in range(100)]
                
                for future in futures:
                    result = future.result()
                    results['total_requests'] += 1
                    
                    if result.get('success'):
                        results['successful_requests'] += 1
                    else:
                        results['failed_requests'] += 1
            
            if response_times:
                results['average_response_time'] = sum(response_times) / len(response_times)
                results['max_response_time'] = max(response_times)
                results['min_response_time'] = min(response_times)
            
            return results
            
        except Exception as e:
            logger.error(f"Chyba pÅ™i load testovÃ¡nÃ­: {e}")
            return {'error': str(e)}
    
    def profile_function_performance(self, function_code: str) -> Dict[str, Any]:
        """ProfilovÃ¡nÃ­ vÃ½konu funkcÃ­"""
        prompt = f"""
        Analyzuj vÃ½kon nÃ¡sledujÃ­cÃ­ funkce a navrhni optimalizace:
        
        {function_code}
        
        ZamÄ›Å™ se na:
        - ÄŒasovou sloÅ¾itost
        - PamÄ›Å¥ovou sloÅ¾itost
        - MoÅ¾nÃ© optimalizace
        - Bottlenecks
        
        FormÃ¡t: JSON s analÃ½zou a doporuÄenÃ­mi.
        """
        
        try:
            response = self.llm.predict(prompt)
            return {'analysis': response}
        except Exception as e:
            logger.error(f"Chyba pÅ™i profilovÃ¡nÃ­: {e}")
            return {'error': str(e)}

class RegressionTester:
    """Agent pro regresnÃ­ testovÃ¡nÃ­"""
    
    def __init__(self, llm):
        self.llm = llm
        self.baseline_results = {}
    
    def run_regression_tests(self, test_directory: str) -> Dict[str, Any]:
        """SpuÅ¡tÄ›nÃ­ regresnÃ­ch testÅ¯"""
        try:
            # SpuÅ¡tÄ›nÃ­ pytest s coverage
            result = subprocess.run(
                ['pytest', test_directory, '--cov=.', '--cov-report=json', '-v'],
                capture_output=True,
                text=True,
                timeout=300
            )
            
            return {
                'exit_code': result.returncode,
                'stdout': result.stdout,
                'stderr': result.stderr,
                'passed': result.returncode == 0
            }
            
        except subprocess.TimeoutExpired:
            return {'error': 'Testy pÅ™ekroÄily ÄasovÃ½ limit'}
        except Exception as e:
            return {'error': str(e)}
    
    def compare_with_baseline(self, current_results: Dict[str, Any]) -> Dict[str, Any]:
        """PorovnÃ¡nÃ­ s baseline vÃ½sledky"""
        if not self.baseline_results:
            self.baseline_results = current_results
            return {'status': 'baseline_set', 'message': 'Baseline vÃ½sledky nastaveny'}
        
        comparison = {
            'performance_regression': False,
            'test_failures': [],
            'coverage_change': 0
        }
        
        # PorovnÃ¡nÃ­ vÃ½konnostnÃ­ch metrik
        baseline_time = self.baseline_results.get('execution_time', 0)
        current_time = current_results.get('execution_time', 0)
        
        if current_time > baseline_time * 1.2:  # 20% zhorÅ¡enÃ­
            comparison['performance_regression'] = True
        
        return comparison

class QAOrchestrator:
    """HlavnÃ­ orchestrÃ¡tor celÃ©ho QA procesu"""
    
    def __init__(self, openai_api_key: str, project_path: str):
        self.project_path = project_path
        self.llm = OpenAI(openai_api_key=openai_api_key, temperature=0.1)
        
        # Inicializace agentÅ¯
        self.code_analyzer = CodeAnalyzer(project_path)
        self.test_generator = TestCaseGenerator(self.llm)
        self.bug_detector = BugDetector(self.llm)
        self.security_scanner = SecurityScanner(self.llm)
        self.performance_tester = PerformanceTester(self.llm)
        self.regression_tester = RegressionTester(self.llm)
        
        self.results = []
    
    def create_crew_agents(self):
        """VytvoÅ™enÃ­ CrewAI agentÅ¯"""
        
        test_generation_agent = Agent(
            role='Test Generator Specialist',
            goal='Generovat komplexnÃ­ a efektivnÃ­ testovacÃ­ pÅ™Ã­pady',
            backstory='ExpertnÃ­ agent specializujÃ­cÃ­ se na vytvÃ¡Å™enÃ­ kvalitnÃ­ch testÅ¯ pro vÅ¡echny typy aplikacÃ­.',
            verbose=True,
            allow_delegation=False,
            llm=self.llm
        )
        
        security_agent = Agent(
            role='Security Specialist',
            goal='Identifikovat a analyzovat bezpeÄnostnÃ­ zranitelnosti',
            backstory='BezpeÄnostnÃ­ expert s hlubokÃ½mi znalostmi cyber security a best practices.',
            verbose=True,
            allow_delegation=False,
            llm=self.llm
        )
        
        performance_agent = Agent(
            role='Performance Engineer',
            goal='Optimalizovat vÃ½kon aplikacÃ­ a identifikovat bottlenecks',
            backstory='VÃ½konnostnÃ­ inÅ¾enÃ½r s expertÃ­zou v profilovÃ¡nÃ­ a optimalizaci.',
            verbose=True,
            allow_delegation=False,
            llm=self.llm
        )
        
        return [test_generation_agent, security_agent, performance_agent]
    
    def run_comprehensive_analysis(self) -> Dict[str, Any]:
        """SpuÅ¡tÄ›nÃ­ kompletnÃ­ analÃ½zy projektu"""
        logger.info("Zahajuji komplexnÃ­ QA analÃ½zu...")
        
        start_time = time.time()
        
        # 1. AnalÃ½za struktury projektu
        logger.info("Analyzuji strukturu projektu...")
        project_structure = self.code_analyzer.get_project_structure()
        
        # 2. GenerovÃ¡nÃ­ testÅ¯
        logger.info("Generuji testovacÃ­ pÅ™Ã­pady...")
        generated_tests = {}
        for file_path, functions in project_structure.items():
            for function in functions:
                test_code = self.test_generator.generate_unit_tests(function)
                generated_tests[f"{function['name']}_test"] = test_code
        
        # 3. Detekce chyb
        logger.info("Detekuji moÅ¾nÃ© chyby...")
        bug_reports = []
        for file_path in project_structure.keys():
            bugs = self.bug_detector.analyze_code_patterns(file_path)
            bug_reports.extend(bugs)
        
        static_analysis = self.bug_detector.run_static_analysis(self.project_path)
        bug_reports.extend(static_analysis)
        
        # 4. BezpeÄnostnÃ­ skenovÃ¡nÃ­
        logger.info("ProvÃ¡dÃ­m bezpeÄnostnÃ­ skenovÃ¡nÃ­...")
        vulnerabilities = self.security_scanner.scan_vulnerabilities(self.project_path)
        
        # 5. TestovÃ¡nÃ­ vÃ½konu (pokud je k dispozici webovÃ¡ aplikace)
        performance_results = {}
        
        # 6. RegresnÃ­ testovÃ¡nÃ­
        logger.info("SpouÅ¡tÃ­m regresnÃ­ testy...")
        test_dir = os.path.join(self.project_path, 'tests')
        if os.path.exists(test_dir):
            regression_results = self.regression_tester.run_regression_tests(test_dir)
        else:
            regression_results = {'message': 'TestovacÃ­ adresÃ¡Å™ nenalezen'}
        
        end_time = time.time()
        
        # SestavenÃ­ finÃ¡lnÃ­ho reportu
        report = {
            'execution_time': end_time - start_time,
            'project_structure': project_structure,
            'generated_tests': generated_tests,
            'bug_reports': bug_reports,
            'security_vulnerabilities': [
                {
                    'severity': vuln.severity,
                    'category': vuln.category,
                    'description': vuln.description,
                    'file': vuln.file_path,
                    'line': vuln.line_number
                } for vuln in vulnerabilities
            ],
            'performance_results': performance_results,
            'regression_results': regression_results,
            'timestamp': time.time()
        }
        
        self.results.append(report)
        logger.info(f"QA analÃ½za dokonÄena za {end_time - start_time:.2f} sekund")
        
        return report
    
    def generate_detailed_report(self) -> str:
        """GenerovÃ¡nÃ­ detailnÃ­ho reportu"""
        if not self.results:
            return "Å½Ã¡dnÃ© vÃ½sledky k zobrazenÃ­"
        
        latest_result = self.results[-1]
        
        report = f"""
# QA AnalÃ½za Reportu

## PÅ™ehled
- **Doba provedenÃ­**: {latest_result['execution_time']:.2f} sekund
- **AnalyzovanÃ© soubory**: {len(latest_result['project_structure'])}
- **GenerovanÃ© testy**: {len(latest_result['generated_tests'])}
- **DetekovanÃ© problÃ©my**: {len(latest_result['bug_reports'])}
- **BezpeÄnostnÃ­ zranitelnosti**: {len(latest_result['security_vulnerabilities'])}

## BezpeÄnostnÃ­ Zranitelnosti
"""
        
        for vuln in latest_result['security_vulnerabilities']:
            report += f"""
### {vuln['severity']} - {vuln['category']}
- **Soubor**: {vuln['file']}
- **Å˜Ã¡dek**: {vuln['line']}
- **Popis**: {vuln['description']}
"""
        
        report += f"""
## RegresnÃ­ TestovÃ¡nÃ­
- **Status**: {'âœ… ÃšspÄ›Å¡nÃ©' if latest_result['regression_results'].get('passed') else 'âŒ NeÃºspÄ›Å¡nÃ©'}
- **Detaily**: {latest_result['regression_results'].get('stdout', 'N/A')}

## DoporuÄenÃ­
1. Implementujte generovanÃ© unit testy
2. Opravte identifikovanÃ© bezpeÄnostnÃ­ zranitelnosti
3. ProveÄte refactoring problematickÃ½ch ÄÃ¡stÃ­ kÃ³du
4. Nastavte CI/CD pipeline s automatickÃ½m QA
"""
        
        return report

# DemonstraÄnÃ­ pouÅ¾itÃ­
def create_sample_project():
    """VytvoÅ™enÃ­ ukÃ¡zkovÃ©ho projektu pro testovÃ¡nÃ­"""
    sample_code = """
# sample_app.py
def calculate_factorial(n):
    \"\"\"VÃ½poÄet faktoriÃ¡lu ÄÃ­sla\"\"\"
    if n < 0:
        raise ValueError("FaktoriÃ¡l nelze vypoÄÃ­tat pro zÃ¡pornÃ¡ ÄÃ­sla")
    if n == 0 or n == 1:
        return 1
    result = 1
    for i in range(2, n + 1):
        result *= i
    return result

def divide_numbers(a, b):
    \"\"\"DÄ›lenÃ­ dvou ÄÃ­sel\"\"\"
    # PotenciÃ¡lnÃ­ bug - chybÃ­ kontrola dÄ›lenÃ­ nulou
    return a / b

def process_user_input(user_data):
    \"\"\"ZpracovÃ¡nÃ­ uÅ¾ivatelskÃ½ch dat\"\"\"
    # BezpeÄnostnÃ­ riziko - nevalidovanÃ½ input
    eval(user_data)  # NebezpeÄnÃ© pouÅ¾itÃ­ eval
    return user_data

class UserManager:
    def __init__(self):
        self.users = {}
    
    def add_user(self, username, password):
        # BezpeÄnostnÃ­ riziko - uklÃ¡dÃ¡nÃ­ hesla v plain text
        self.users[username] = password
    
    def authenticate(self, username, password):
        return self.users.get(username) == password
"""
    
    # VytvoÅ™enÃ­ ukÃ¡zkovÃ©ho souboru
    os.makedirs('sample_project', exist_ok=True)
    with open('sample_project/sample_app.py', 'w', encoding='utf-8') as f:
        f.write(sample_code)
    
    # VytvoÅ™enÃ­ requirements.txt
    with open('sample_project/requirements.txt', 'w') as f:
        f.write("requests==2.25.1\nflask==1.1.2\n")

def main():
    """HlavnÃ­ funkce pro demonstraci"""
    # NastavenÃ­ API klÃ­Äe (v produkci pouÅ¾ijte environment variables)
    api_key = os.getenv('OPENAI_API_KEY', 'your-openai-api-key-here')
    
    if api_key == 'your-openai-api-key-here':
        print("âš ï¸  Nastavte OPENAI_API_KEY environment variable")
        return
    
    # VytvoÅ™enÃ­ ukÃ¡zkovÃ©ho projektu
    create_sample_project()
    
    # Inicializace QA orchestrÃ¡toru
    qa_framework = QAOrchestrator(
        openai_api_key=api_key,
        project_path='sample_project'
    )
    
    # SpuÅ¡tÄ›nÃ­ analÃ½zy
    try:
        results = qa_framework.run_comprehensive_analysis()
        
        # GenerovÃ¡nÃ­ reportu
        report = qa_framework.generate_detailed_report()
        
        print("="*80)
        print("ğŸ¤– AUTONOMNÃ QA FRAMEWORK - VÃSLEDKY")
        print("="*80)
        print(report)
        
        # UloÅ¾enÃ­ detailnÃ­ch vÃ½sledkÅ¯
        with open('qa_analysis_results.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“Š DetailnÃ­ vÃ½sledky uloÅ¾eny do 'qa_analysis_results.json'")
        
    except Exception as e:
        logger.error(f"Chyba pÅ™i spuÅ¡tÄ›nÃ­ QA analÃ½zy: {e}")
        print(f"âŒ Chyba: {e}")

if __name__ == "__main__":
    main()
````

## 4. ShrnutÃ­ Projektu

### Hodnota Projektu
AutonomnÃ­ QA Framework pÅ™edstavuje prÅ¯lomovÃ© Å™eÅ¡enÃ­ pro automatizaci testovÃ¡nÃ­ software. Kombinuje sÃ­lu multi-agent systÃ©mÅ¯ s pokroÄilÃ½mi AI technikami pro vytvoÅ™enÃ­ komplexnÃ­ho testovacÃ­ho ekosystÃ©mu.

### KlÃ­ÄovÃ© VÃ½hody
- **Automatizace**: Minimalizace manuÃ¡lnÃ­ prÃ¡ce v QA procesech
- **Komplexnost**: PokrytÃ­ vÅ¡ech aspektÅ¯ testovÃ¡nÃ­ od unit testÅ¯ po bezpeÄnost
- **Å kÃ¡lovatelnost**: EfektivnÃ­ prÃ¡ce s projekty vÅ¡ech velikostÃ­
- **Inteligence**: AI-powered analÃ½za a generovÃ¡nÃ­ testÅ¯
- **Rychlost**: ParalelnÃ­ zpracovÃ¡nÃ­ pomocÃ­ specializovanÃ½ch agentÅ¯

### TechnologickÃ© Inovace
Framework vyuÅ¾Ã­vÃ¡ nejmodernÄ›jÅ¡Ã­ technologie vÄetnÄ› CrewAI pro orchestraci agentÅ¯, LangChain pro AI workflows a integraci s etablovanÃ½mi testovacÃ­mi nÃ¡stroji.

### BudoucÃ­ RozÅ¡Ã­Å™enÃ­
- Integrace s CI/CD pipelines
- Podpora vÃ­ce programovacÃ­ch jazykÅ¯
- Advanced ML modely pro predikci chyb
- GrafickÃ© uÅ¾ivatelskÃ© rozhranÃ­
- Cloud-native deployment

Tento framework pÅ™edstavuje budoucnost automatizovanÃ©ho testovÃ¡nÃ­ software a mÅ¯Å¾e vÃ½raznÄ› zvÃ½Å¡it kvalitu a rychlost vÃ½voje aplikacÃ­.