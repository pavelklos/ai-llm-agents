<small>Claude Sonnet 4 **(DecentralizovanÃ¡ sÃ­Å¥ pro ovÄ›Å™ovÃ¡nÃ­ zprÃ¡v)**</small>
# Decentralized News Verification Network

## KlÃ­ÄovÃ© koncepty

### Multi-Agent Systems (Multi-agentnÃ­ systÃ©my)
SystÃ©m sloÅ¾enÃ½ z vÃ­ce autonomnÃ­ch softwarovÃ½ch agentÅ¯, kteÅ™Ã­ spolupracujÃ­ pÅ™i Å™eÅ¡enÃ­ komplexnÃ­ch Ãºloh. KaÅ¾dÃ½ agent mÃ¡ specifickou roli a schopnosti, komunikuje s ostatnÃ­mi agenty a pÅ™ispÃ­vÃ¡ k celkovÃ©mu cÃ­li systÃ©mu.

### Fact-checking (OvÄ›Å™ovÃ¡nÃ­ faktÅ¯)
Proces systematickÃ©ho ovÄ›Å™ovÃ¡nÃ­ pravdivosti informacÃ­ a tvrzenÃ­ prostÅ™ednictvÃ­m kontroly primÃ¡rnÃ­ch zdrojÅ¯, odbornÃ½ch databÃ¡zÃ­ a vÄ›rohodnÃ½ch referenÄnÃ­ch materiÃ¡lÅ¯.

### Source Verification (OvÄ›Å™ovÃ¡nÃ­ zdrojÅ¯)
AnalÃ½za a hodnocenÃ­ vÄ›rohodnosti informaÄnÃ­ch zdrojÅ¯ na zÃ¡kladÄ› jejich historie, transparentnosti, odbornosti a nezÃ¡vislosti.

### Bias Detection (Detekce zaujatosti)
Identifikace a analÃ½za politickÃ½ch, ideologickÃ½ch nebo komerÄnÃ­ch pÅ™edsudkÅ¯ v mediÃ¡lnÃ­m obsahu pomocÃ­ textovÃ© analÃ½zy a machine learning algoritmÅ¯.

### Information Credibility Scoring (HodnocenÃ­ vÄ›rohodnosti informacÃ­)
KvantitativnÃ­ systÃ©m pro ohodnocenÃ­ dÅ¯vÄ›ryhodnosti informacÃ­ na zÃ¡kladÄ› mÃºltiple faktorÅ¯ jako je zdroj, konzistence, ovÄ›Å™itelnost a historickÃ¡ pÅ™esnost.

### Misinformation Flagging (OznaÄovÃ¡nÃ­ dezinformacÃ­)
AutomatickÃ© nebo semi-automatickÃ© oznaÄovÃ¡nÃ­ potenciÃ¡lnÄ› nepravdivÃ½ch nebo zavÃ¡dÄ›jÃ­cÃ­ch informacÃ­ pro varovÃ¡nÃ­ uÅ¾ivatelÅ¯.

## KomplexnÃ­ vysvÄ›tlenÃ­ projektu

DecentralizovanÃ¡ sÃ­Å¥ pro ovÄ›Å™ovÃ¡nÃ­ zprÃ¡v pÅ™edstavuje inovativnÃ­ pÅ™Ã­stup k boji proti dezinformacÃ­m v digitÃ¡lnÃ­m vÄ›ku. Projekt vyuÅ¾Ã­vÃ¡ multi-agentnÃ­ architekturu, kde kaÅ¾dÃ½ agent mÃ¡ specializovanou roli v procesu ovÄ›Å™ovÃ¡nÃ­ informacÃ­.

### CÃ­le projektu
- **Automatizace fact-checkingu**: RychlÃ© ovÄ›Å™ovÃ¡nÃ­ faktickÃ½ch tvrzenÃ­ v reÃ¡lnÃ©m Äase
- **DecentralizovanÃ© ovÄ›Å™ovÃ¡nÃ­**: Eliminace single point of failure a zvÃ½Å¡enÃ­ odolnosti systÃ©mu
- **TransparentnÃ­ hodnocenÃ­**: PoskytovÃ¡nÃ­ jasnÃ½ch dÅ¯vodÅ¯ pro hodnocenÃ­ vÄ›rohodnosti
- **Å kÃ¡lovatelnost**: Schopnost zpracovat velkÃ© mnoÅ¾stvÃ­ informacÃ­ souÄasnÄ›

### HlavnÃ­ vÃ½zvy
- **Komplexnost jazyka**: PochopenÃ­ kontextu, sarkasmu a nuancÃ­ v textu
- **Rychlost vs. pÅ™esnost**: VyvÃ¡Å¾enost mezi rychlÃ½m zpracovÃ¡nÃ­m a dÅ¯kladnou analÃ½zou
- **Evoluce dezinformacÃ­**: Adaptace na novÃ© formy a techniky Å¡Ã­Å™enÃ­ nepravdivÃ½ch informacÃ­
- **KulturnÃ­ kontext**: RespektovÃ¡nÃ­ rÅ¯znÃ½ch kulturnÃ­ch a jazykovÃ½ch specifik

### PotenciÃ¡lnÃ­ dopad
SystÃ©m mÅ¯Å¾e vÃ½znamnÄ› snÃ­Å¾it Å¡Ã­Å™enÃ­ dezinformacÃ­, zvÃ½Å¡it mediÃ¡lnÃ­ gramotnost a posÃ­lit dÅ¯vÄ›ru veÅ™ejnosti v kvalitnÃ­ Å¾urnalistiku.

## KomplexnÃ­ pÅ™Ã­klad s implementacÃ­ v Pythonu

````python
import asyncio
import json
import logging
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import hashlib

from langchain.llms import OpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.schema import Document
import requests
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
from textblob import TextBlob
import re

# Konfigurace logovÃ¡nÃ­
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class NewsArticle:
    """Reprezentace zpravodajskÃ©ho ÄlÃ¡nku"""
    id: str
    title: str
    content: str
    source: str
    url: str
    timestamp: datetime
    credibility_score: Optional[float] = None
    bias_score: Optional[float] = None
    fact_check_results: Optional[Dict] = None

@dataclass
class VerificationResult:
    """VÃ½sledek ovÄ›Å™enÃ­ ÄlÃ¡nku"""
    article_id: str
    overall_score: float
    credibility_score: float
    bias_score: float
    fact_accuracy: float
    source_reliability: float
    flags: List[str]
    evidence: List[str]
    reasoning: str

class BaseAgent:
    """ZÃ¡kladnÃ­ tÅ™Ã­da pro vÅ¡echny agenty"""
    
    def __init__(self, name: str, llm_model: str = "gpt-3.5-turbo"):
        self.name = name
        self.llm = OpenAI(model_name=llm_model, temperature=0.1)
        self.embeddings = OpenAIEmbeddings()
        
    async def process(self, article: NewsArticle) -> Dict:
        """ZpracovÃ¡nÃ­ ÄlÃ¡nku agentem"""
        raise NotImplementedError

class SourceVerificationAgent(BaseAgent):
    """Agent pro ovÄ›Å™ovÃ¡nÃ­ zdrojÅ¯"""
    
    def __init__(self):
        super().__init__("SourceVerifier")
        # DatabÃ¡ze znÃ¡mÃ½ch zdrojÅ¯ s jejich hodnocenÃ­m
        self.source_database = {
            "cnn.com": {"reliability": 0.8, "bias": 0.3},
            "bbc.com": {"reliability": 0.9, "bias": 0.1},
            "rt.com": {"reliability": 0.4, "bias": 0.8},
            "wikipedia.org": {"reliability": 0.7, "bias": 0.2},
            # PÅ™idejte vÃ­ce zdrojÅ¯ podle potÅ™eby
        }
    
    async def process(self, article: NewsArticle) -> Dict:
        """OvÄ›Å™enÃ­ vÄ›rohodnosti zdroje"""
        try:
            domain = self._extract_domain(article.url)
            
            # Kontrola v databÃ¡zi znÃ¡mÃ½ch zdrojÅ¯
            if domain in self.source_database:
                reliability = self.source_database[domain]["reliability"]
                bias = self.source_database[domain]["bias"]
            else:
                # AnalÃ½za neznÃ¡mÃ©ho zdroje
                reliability, bias = await self._analyze_unknown_source(domain)
            
            # Kontrola SSL certifikÃ¡tu a technickÃ½ch aspektÅ¯
            technical_score = await self._check_technical_aspects(article.url)
            
            final_score = (reliability + technical_score) / 2
            
            return {
                "source_reliability": final_score,
                "bias_level": bias,
                "domain": domain,
                "flags": self._generate_flags(final_score, bias),
                "evidence": [f"DomÃ©na {domain} mÃ¡ historickou spolehlivost {reliability:.2f}"]
            }
            
        except Exception as e:
            logger.error(f"Chyba pÅ™i ovÄ›Å™ovÃ¡nÃ­ zdroje: {e}")
            return {"source_reliability": 0.5, "bias_level": 0.5, "flags": ["CHYBA_ANALÃZY"]}
    
    def _extract_domain(self, url: str) -> str:
        """Extrakce domÃ©ny z URL"""
        from urllib.parse import urlparse
        return urlparse(url).netloc.lower()
    
    async def _analyze_unknown_source(self, domain: str) -> Tuple[float, float]:
        """AnalÃ½za neznÃ¡mÃ©ho zdroje"""
        # Simulace analÃ½zy neznÃ¡mÃ©ho zdroje
        # V produkÄnÃ­ verzi by zde byla komplexnÃ­ analÃ½za
        return 0.5, 0.5
    
    async def _check_technical_aspects(self, url: str) -> float:
        """Kontrola technickÃ½ch aspektÅ¯ webu"""
        try:
            response = requests.head(url, timeout=5)
            score = 0.5
            
            # HTTPS bonus
            if url.startswith("https://"):
                score += 0.2
                
            # Status kÃ³d kontrola
            if response.status_code == 200:
                score += 0.2
                
            return min(score, 1.0)
            
        except:
            return 0.3
    
    def _generate_flags(self, reliability: float, bias: float) -> List[str]:
        """GenerovÃ¡nÃ­ varovnÃ½ch znaÄek"""
        flags = []
        
        if reliability < 0.3:
            flags.append("NÃZKÃ_SPOLEHLIVOST")
        if bias > 0.7:
            flags.append("VYSOKÃ_ZAUJATOST")
        if reliability < 0.5 and bias > 0.5:
            flags.append("PROBLEMATICKÃ_ZDROJ")
            
        return flags

class FactCheckingAgent(BaseAgent):
    """Agent pro ovÄ›Å™ovÃ¡nÃ­ faktÅ¯"""
    
    def __init__(self):
        super().__init__("FactChecker")
        self.knowledge_base = self._load_knowledge_base()
    
    def _load_knowledge_base(self):
        """NaÄtenÃ­ znalostnÃ­ databÃ¡ze"""
        # VytvoÅ™enÃ­ ukÃ¡zkovÃ½ch faktÅ¯ pro demonstraci
        facts = [
            "ÄŒeskÃ¡ republika mÃ¡ rozlohu 78 867 kmÂ²",
            "Praha je hlavnÃ­ mÄ›sto ÄŒeskÃ© republiky",
            "COVID-19 byl poprvÃ© identifikovÃ¡n v roce 2019",
            "Voda vÅ™e pÅ™i 100Â°C za normÃ¡lnÃ­ho tlaku",
            "Albert Einstein formuloval teorii relativity"
        ]
        
        documents = [Document(page_content=fact) for fact in facts]
        
        # VytvoÅ™enÃ­ vektorovÃ© databÃ¡ze
        vectorstore = Chroma.from_documents(
            documents=documents,
            embedding=self.embeddings,
            persist_directory="./knowledge_base"
        )
        
        return RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=vectorstore.as_retriever()
        )
    
    async def process(self, article: NewsArticle) -> Dict:
        """OvÄ›Å™enÃ­ faktickÃ½ch tvrzenÃ­ v ÄlÃ¡nku"""
        try:
            # Extrakce faktickÃ½ch tvrzenÃ­
            claims = self._extract_claims(article.content)
            
            verified_claims = []
            accuracy_scores = []
            
            for claim in claims:
                # OvÄ›Å™enÃ­ kaÅ¾dÃ©ho tvrzenÃ­
                verification = await self._verify_claim(claim)
                verified_claims.append(verification)
                accuracy_scores.append(verification["accuracy"])
            
            overall_accuracy = np.mean(accuracy_scores) if accuracy_scores else 0.5
            
            return {
                "fact_accuracy": overall_accuracy,
                "verified_claims": verified_claims,
                "flags": self._generate_fact_flags(overall_accuracy),
                "evidence": [f"OvÄ›Å™eno {len(claims)} faktickÃ½ch tvrzenÃ­"]
            }
            
        except Exception as e:
            logger.error(f"Chyba pÅ™i fact-checkingu: {e}")
            return {"fact_accuracy": 0.5, "flags": ["CHYBA_FAKTÅ®"]}
    
    def _extract_claims(self, content: str) -> List[str]:
        """Extrakce faktickÃ½ch tvrzenÃ­ z textu"""
        # JednoduchÃ© rozdÄ›lenÃ­ na vÄ›ty
        sentences = re.split(r'[.!?]+', content)
        
        # FiltrovÃ¡nÃ­ vÄ›t obsahujÃ­cÃ­ch potenciÃ¡lnÃ­ fakta
        claims = []
        fact_indicators = ["podle", "Ãºdaje", "statistiky", "vÃ½zkum", "studie", "ÄÃ­sla"]
        
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 20:  # MinimÃ¡lnÃ­ dÃ©lka
                for indicator in fact_indicators:
                    if indicator in sentence.lower():
                        claims.append(sentence)
                        break
        
        return claims[:5]  # Max 5 tvrzenÃ­ pro demo
    
    async def _verify_claim(self, claim: str) -> Dict:
        """OvÄ›Å™enÃ­ jednotlivÃ©ho tvrzenÃ­"""
        try:
            # Dotaz do znalostnÃ­ databÃ¡ze
            result = self.knowledge_base.run(claim)
            
            # JednoduchÃ© hodnocenÃ­ na zÃ¡kladÄ› podobnosti
            similarity = self._calculate_similarity(claim, result)
            
            return {
                "claim": claim,
                "accuracy": similarity,
                "evidence": result,
                "status": "OVÄšÅ˜ENO" if similarity > 0.7 else "SPORNÃ‰"
            }
            
        except Exception as e:
            return {
                "claim": claim,
                "accuracy": 0.3,
                "evidence": "Chyba pÅ™i ovÄ›Å™ovÃ¡nÃ­",
                "status": "CHYBA"
            }
    
    def _calculate_similarity(self, claim: str, evidence: str) -> float:
        """VÃ½poÄet podobnosti mezi tvrzenÃ­m a dÅ¯kazem"""
        # ZjednoduÅ¡enÃ¡ podobnost zaloÅ¾enÃ¡ na pÅ™ekryvu slov
        claim_words = set(claim.lower().split())
        evidence_words = set(evidence.lower().split())
        
        if not claim_words or not evidence_words:
            return 0.3
        
        intersection = len(claim_words.intersection(evidence_words))
        union = len(claim_words.union(evidence_words))
        
        return intersection / union if union > 0 else 0.3
    
    def _generate_fact_flags(self, accuracy: float) -> List[str]:
        """GenerovÃ¡nÃ­ znaÄek pro faktickou pÅ™esnost"""
        flags = []
        
        if accuracy < 0.3:
            flags.append("FAKTICKÃ‰_CHYBY")
        elif accuracy < 0.5:
            flags.append("SPORNÃ‰_FAKTY")
        elif accuracy > 0.8:
            flags.append("OVÄšÅ˜ENÃ‰_FAKTY")
            
        return flags

class BiasDetectionAgent(BaseAgent):
    """Agent pro detekci zaujatosti"""
    
    def __init__(self):
        super().__init__("BiasDetector")
        self.bias_keywords = {
            "political_left": ["pokrokovÃ½", "sociÃ¡lnÃ­ spravedlnost", "rovnost"],
            "political_right": ["tradice", "svoboda", "konzervativnÃ­"],
            "emotional": ["Å¡okujÃ­cÃ­", "neuvÄ›Å™itelnÃ©", "skandÃ¡lnÃ­", "katastrofa"],
            "sensational": ["exkluzivnÃ­", "tajnÃ©", "odhalenÃ­", "bombastickÃ©"]
        }
    
    async def process(self, article: NewsArticle) -> Dict:
        """Detekce zaujatosti v ÄlÃ¡nku"""
        try:
            # AnalÃ½za sentimentu
            sentiment_score = self._analyze_sentiment(article.content)
            
            # Detekce zaujatÃ½ch slov
            bias_indicators = self._detect_bias_keywords(article.content)
            
            # AnalÃ½za stylu psanÃ­
            writing_style_score = self._analyze_writing_style(article.content)
            
            # Kombinace vÃ½sledkÅ¯
            overall_bias = (abs(sentiment_score) + bias_indicators + writing_style_score) / 3
            
            return {
                "bias_score": overall_bias,
                "sentiment": sentiment_score,
                "bias_indicators": bias_indicators,
                "writing_style": writing_style_score,
                "flags": self._generate_bias_flags(overall_bias),
                "evidence": [f"Sentiment: {sentiment_score:.2f}", f"ZaujatÃ¡ slova: {bias_indicators:.2f}"]
            }
            
        except Exception as e:
            logger.error(f"Chyba pÅ™i detekci zaujatosti: {e}")
            return {"bias_score": 0.5, "flags": ["CHYBA_BIAS"]}
    
    def _analyze_sentiment(self, content: str) -> float:
        """AnalÃ½za sentimentu textu"""
        try:
            blob = TextBlob(content)
            # Sentiment polarity je mezi -1 a 1
            return blob.sentiment.polarity
        except:
            return 0.0
    
    def _detect_bias_keywords(self, content: str) -> float:
        """Detekce zaujatÃ½ch klÃ­ÄovÃ½ch slov"""
        content_lower = content.lower()
        total_bias_words = 0
        total_words = len(content.split())
        
        for category, keywords in self.bias_keywords.items():
            for keyword in keywords:
                total_bias_words += content_lower.count(keyword)
        
        return min(total_bias_words / max(total_words, 1), 1.0)
    
    def _analyze_writing_style(self, content: str) -> float:
        """AnalÃ½za stylu psanÃ­"""
        # PoÄet vykÅ™iÄnÃ­kÅ¯ a otaznÃ­kÅ¯
        exclamations = content.count('!')
        questions = content.count('?')
        
        # DÃ©lka vÄ›t
        sentences = re.split(r'[.!?]+', content)
        avg_sentence_length = np.mean([len(s.split()) for s in sentences if s.strip()])
        
        # HodnocenÃ­ stylu (vÃ­ce vykÅ™iÄnÃ­kÅ¯ = vÃ­ce zaujatosti)
        style_score = min((exclamations + questions) / max(len(sentences), 1), 1.0)
        
        return style_score
    
    def _generate_bias_flags(self, bias_score: float) -> List[str]:
        """GenerovÃ¡nÃ­ znaÄek pro zaujatost"""
        flags = []
        
        if bias_score > 0.7:
            flags.append("VYSOKÃ_ZAUJATOST")
        elif bias_score > 0.5:
            flags.append("MÃRNÃ_ZAUJATOST")
        elif bias_score < 0.2:
            flags.append("NEUTRÃLNÃ_OBSAH")
            
        return flags

class CoordinatorAgent(BaseAgent):
    """KoordinÃ¡tor pro Å™Ã­zenÃ­ celÃ©ho procesu ovÄ›Å™ovÃ¡nÃ­"""
    
    def __init__(self):
        super().__init__("Coordinator")
        self.source_agent = SourceVerificationAgent()
        self.fact_agent = FactCheckingAgent()
        self.bias_agent = BiasDetectionAgent()
    
    async def verify_article(self, article: NewsArticle) -> VerificationResult:
        """KompletnÃ­ ovÄ›Å™enÃ­ ÄlÃ¡nku vÅ¡emi agenty"""
        try:
            logger.info(f"ZahÃ¡jeno ovÄ›Å™ovÃ¡nÃ­ ÄlÃ¡nku: {article.title}")
            
            # ParalelnÃ­ zpracovÃ¡nÃ­ vÅ¡emi agenty
            source_result, fact_result, bias_result = await asyncio.gather(
                self.source_agent.process(article),
                self.fact_agent.process(article),
                self.bias_agent.process(article)
            )
            
            # Kombinace vÃ½sledkÅ¯
            overall_score = self._calculate_overall_score(
                source_result, fact_result, bias_result
            )
            
            # GenerovÃ¡nÃ­ zÃ¡vÄ›reÄnÃ½ch znaÄek
            final_flags = self._combine_flags(
                source_result.get("flags", []),
                fact_result.get("flags", []),
                bias_result.get("flags", [])
            )
            
            # GenerovÃ¡nÃ­ zdÅ¯vodnÄ›nÃ­
            reasoning = self._generate_reasoning(
                source_result, fact_result, bias_result, overall_score
            )
            
            result = VerificationResult(
                article_id=article.id,
                overall_score=overall_score,
                credibility_score=source_result.get("source_reliability", 0.5),
                bias_score=bias_result.get("bias_score", 0.5),
                fact_accuracy=fact_result.get("fact_accuracy", 0.5),
                source_reliability=source_result.get("source_reliability", 0.5),
                flags=final_flags,
                evidence=self._combine_evidence(source_result, fact_result, bias_result),
                reasoning=reasoning
            )
            
            logger.info(f"OvÄ›Å™ovÃ¡nÃ­ dokonÄeno. CelkovÃ© skÃ³re: {overall_score:.2f}")
            return result
            
        except Exception as e:
            logger.error(f"Chyba pÅ™i ovÄ›Å™ovÃ¡nÃ­ ÄlÃ¡nku: {e}")
            return VerificationResult(
                article_id=article.id,
                overall_score=0.3,
                credibility_score=0.3,
                bias_score=0.5,
                fact_accuracy=0.3,
                source_reliability=0.3,
                flags=["SYSTÃ‰MOVÃ_CHYBA"],
                evidence=["Nastala chyba pÅ™i zpracovÃ¡nÃ­"],
                reasoning="ZpracovÃ¡nÃ­ ÄlÃ¡nku selhalo kvÅ¯li systÃ©movÃ© chybÄ›."
            )
    
    def _calculate_overall_score(self, source_result: Dict, fact_result: Dict, bias_result: Dict) -> float:
        """VÃ½poÄet celkovÃ©ho skÃ³re"""
        weights = {
            "source": 0.3,
            "facts": 0.4,
            "bias": 0.3
        }
        
        source_score = source_result.get("source_reliability", 0.5)
        fact_score = fact_result.get("fact_accuracy", 0.5)
        bias_score = 1.0 - bias_result.get("bias_score", 0.5)  # Inverze bias skÃ³re
        
        overall = (
            source_score * weights["source"] +
            fact_score * weights["facts"] +
            bias_score * weights["bias"]
        )
        
        return min(max(overall, 0.0), 1.0)
    
    def _combine_flags(self, *flag_lists) -> List[str]:
        """Kombinace znaÄek ze vÅ¡ech agentÅ¯"""
        all_flags = []
        for flags in flag_lists:
            all_flags.extend(flags)
        return list(set(all_flags))  # OdstranÄ›nÃ­ duplikÃ¡tÅ¯
    
    def _combine_evidence(self, *results) -> List[str]:
        """Kombinace dÅ¯kazÅ¯ ze vÅ¡ech agentÅ¯"""
        all_evidence = []
        for result in results:
            all_evidence.extend(result.get("evidence", []))
        return all_evidence
    
    def _generate_reasoning(self, source_result: Dict, fact_result: Dict, 
                          bias_result: Dict, overall_score: float) -> str:
        """GenerovÃ¡nÃ­ lidsky ÄitelnÃ©ho zdÅ¯vodnÄ›nÃ­"""
        source_score = source_result.get("source_reliability", 0.5)
        fact_score = fact_result.get("fact_accuracy", 0.5)
        bias_score = bias_result.get("bias_score", 0.5)
        
        reasoning_parts = []
        
        # HodnocenÃ­ zdroje
        if source_score > 0.7:
            reasoning_parts.append("Zdroj je povaÅ¾ovÃ¡n za spolehlivÃ½.")
        elif source_score < 0.4:
            reasoning_parts.append("Zdroj mÃ¡ nÃ­zkou spolehlivost.")
        else:
            reasoning_parts.append("Zdroj mÃ¡ stÅ™ednÃ­ spolehlivost.")
        
        # HodnocenÃ­ faktÅ¯
        if fact_score > 0.7:
            reasoning_parts.append("FaktickÃ¡ tvrzenÃ­ jsou pÅ™evÃ¡Å¾nÄ› ovÄ›Å™enÃ¡.")
        elif fact_score < 0.4:
            reasoning_parts.append("Nalezeny byly faktickÃ© nepÅ™esnosti.")
        else:
            reasoning_parts.append("FaktickÃ¡ pÅ™esnost je stÅ™ednÃ­.")
        
        # HodnocenÃ­ zaujatosti
        if bias_score > 0.6:
            reasoning_parts.append("Obsah vykazuje znaÄnou zaujatost.")
        elif bias_score < 0.3:
            reasoning_parts.append("Obsah je relativnÄ› neutrÃ¡lnÃ­.")
        else:
            reasoning_parts.append("Obsah mÃ¡ mÃ­rnou zaujatost.")
        
        # CelkovÃ© hodnocenÃ­
        if overall_score > 0.7:
            reasoning_parts.append("CelkovÄ› je ÄlÃ¡nek hodnocen jako vÄ›rohodnÃ½.")
        elif overall_score < 0.4:
            reasoning_parts.append("ÄŒlÃ¡nek vykazuje vÃ½znamnÃ© problÃ©my s vÄ›rohodnostÃ­.")
        else:
            reasoning_parts.append("ÄŒlÃ¡nek mÃ¡ stÅ™ednÃ­ vÄ›rohodnost - doporuÄuje se opatrnost.")
        
        return " ".join(reasoning_parts)

class NewsVerificationSystem:
    """HlavnÃ­ tÅ™Ã­da systÃ©mu pro ovÄ›Å™ovÃ¡nÃ­ zprÃ¡v"""
    
    def __init__(self):
        self.coordinator = CoordinatorAgent()
        self.verified_articles = {}
        
    async def verify_article_from_url(self, url: str) -> VerificationResult:
        """OvÄ›Å™enÃ­ ÄlÃ¡nku z URL"""
        try:
            # StaÅ¾enÃ­ obsahu ÄlÃ¡nku
            article = await self._scrape_article(url)
            
            # OvÄ›Å™enÃ­ ÄlÃ¡nku
            result = await self.coordinator.verify_article(article)
            
            # UloÅ¾enÃ­ vÃ½sledku
            self.verified_articles[article.id] = result
            
            return result
            
        except Exception as e:
            logger.error(f"Chyba pÅ™i ovÄ›Å™ovÃ¡nÃ­ ÄlÃ¡nku z URL {url}: {e}")
            raise
    
    async def verify_article_text(self, title: str, content: str, source: str = "unknown") -> VerificationResult:
        """OvÄ›Å™enÃ­ ÄlÃ¡nku z textu"""
        try:
            # VytvoÅ™enÃ­ ÄlÃ¡nku
            article = NewsArticle(
                id=hashlib.md5(content.encode()).hexdigest()[:8],
                title=title,
                content=content,
                source=source,
                url="",
                timestamp=datetime.now()
            )
            
            # OvÄ›Å™enÃ­ ÄlÃ¡nku
            result = await self.coordinator.verify_article(article)
            
            # UloÅ¾enÃ­ vÃ½sledku
            self.verified_articles[article.id] = result
            
            return result
            
        except Exception as e:
            logger.error(f"Chyba pÅ™i ovÄ›Å™ovÃ¡nÃ­ textu ÄlÃ¡nku: {e}")
            raise
    
    async def _scrape_article(self, url: str) -> NewsArticle:
        """StaÅ¾enÃ­ a parsovÃ¡nÃ­ ÄlÃ¡nku z URL"""
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extrakce titulku
            title_tag = soup.find('title') or soup.find('h1')
            title = title_tag.get_text().strip() if title_tag else "Bez titulku"
            
            # Extrakce obsahu (zjednoduÅ¡enÃ©)
            content_tags = soup.find_all(['p', 'div'], class_=lambda x: x and any(
                keyword in x.lower() for keyword in ['content', 'article', 'text', 'body']
            ))
            
            if not content_tags:
                content_tags = soup.find_all('p')
            
            content = ' '.join([tag.get_text().strip() for tag in content_tags[:10]])
            
            if not content:
                raise ValueError("NepodaÅ™ilo se extrahovat obsah ÄlÃ¡nku")
            
            return NewsArticle(
                id=hashlib.md5(url.encode()).hexdigest()[:8],
                title=title,
                content=content,
                source=url,
                url=url,
                timestamp=datetime.now()
            )
            
        except Exception as e:
            logger.error(f"Chyba pÅ™i stahovÃ¡nÃ­ ÄlÃ¡nku: {e}")
            raise
    
    def get_verification_history(self) -> List[VerificationResult]:
        """ZÃ­skÃ¡nÃ­ historie ovÄ›Å™ovÃ¡nÃ­"""
        return list(self.verified_articles.values())
    
    def get_statistics(self) -> Dict:
        """ZÃ­skÃ¡nÃ­ statistik systÃ©mu"""
        if not self.verified_articles:
            return {"total_articles": 0}
        
        scores = [result.overall_score for result in self.verified_articles.values()]
        flags = []
        for result in self.verified_articles.values():
            flags.extend(result.flags)
        
        from collections import Counter
        flag_counts = Counter(flags)
        
        return {
            "total_articles": len(self.verified_articles),
            "average_score": np.mean(scores),
            "high_quality_articles": len([s for s in scores if s > 0.7]),
            "low_quality_articles": len([s for s in scores if s < 0.4]),
            "most_common_flags": flag_counts.most_common(5)
        }

# DemonstraÄnÃ­ funkce
async def demo():
    """Demonstrace systÃ©mu"""
    system = NewsVerificationSystem()
    
    # TestovacÃ­ ÄlÃ¡nek
    test_article = {
        "title": "PrÅ¯lomovÃ½ objev: VÄ›dci vyvinuli novou technologii",
        "content": """
        Podle nejnovÄ›jÅ¡Ã­ch vÃ½zkumÅ¯ vÄ›dci z Univerzity Karlovy vyvinuli 
        revoluÄnÃ­ technologii, kterÃ¡ mÅ¯Å¾e zmÄ›nit svÄ›t! Tato neuvÄ›Å™itelnÃ¡ 
        technologie dokÃ¡Å¾e zpracovat obrovskÃ© mnoÅ¾stvÃ­ dat rychlostÃ­ blesku.
        OdbornÃ­ci tvrdÃ­, Å¾e se jednÃ¡ o nejvÄ›tÅ¡Ã­ prÅ¯lom za poslednÃ­ch 50 let.
        Statistiky ukazujÃ­ 300% nÃ¡rÅ¯st efektivity. Praha je hlavnÃ­ mÄ›sto 
        ÄŒeskÃ© republiky, kterÃ¡ mÃ¡ rozlohu 78 867 kmÂ².
        """,
        "source": "example.com"
    }
    
    print("ğŸ” SpuÅ¡tÄ›nÃ­ demonstrace systÃ©mu ovÄ›Å™ovÃ¡nÃ­ zprÃ¡v")
    print("=" * 60)
    
    # OvÄ›Å™enÃ­ ÄlÃ¡nku
    result = await system.verify_article_text(
        test_article["title"],
        test_article["content"],
        test_article["source"]
    )
    
    # ZobrazenÃ­ vÃ½sledkÅ¯
    print(f"ğŸ“° ÄŒlÃ¡nek: {test_article['title']}")
    print(f"ğŸ† CelkovÃ© skÃ³re: {result.overall_score:.2f}")
    print(f"ğŸ“Š VÄ›rohodnost zdroje: {result.source_reliability:.2f}")
    print(f"âœ… FaktickÃ¡ pÅ™esnost: {result.fact_accuracy:.2f}")
    print(f"âš–ï¸  Zaujatost: {result.bias_score:.2f}")
    print(f"ğŸš© ZnaÄky: {', '.join(result.flags) if result.flags else 'Å½Ã¡dnÃ©'}")
    print(f"ğŸ’­ ZdÅ¯vodnÄ›nÃ­: {result.reasoning}")
    
    print("\nğŸ“ˆ Statistiky systÃ©mu:")
    stats = system.get_statistics()
    for key, value in stats.items():
        print(f"  {key}: {value}")

if __name__ == "__main__":
    # Instalace zÃ¡vislostÃ­
    print("ğŸ“¦ Instalace zÃ¡vislostÃ­...")
    import subprocess
    import sys
    
    dependencies = [
        "langchain",
        "openai",
        "chromadb", 
        "beautifulsoup4",
        "textblob",
        "pandas",
        "numpy",
        "requests"
    ]
    
    for dep in dependencies:
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", dep])
        except:
            print(f"âš ï¸  NepodaÅ™ilo se nainstalovat {dep}")
    
    # SpuÅ¡tÄ›nÃ­ demo
    asyncio.run(demo())
````

## Souhrn projektu

DecentralizovanÃ¡ sÃ­Å¥ pro ovÄ›Å™ovÃ¡nÃ­ zprÃ¡v pÅ™edstavuje pokroÄilÃ½ multi-agentnÃ­ systÃ©m, kterÃ½ automatizuje proces fact-checkingu a hodnocenÃ­ vÄ›rohodnosti mediÃ¡lnÃ­ho obsahu. Projekt kombinuje modernÃ­ technologie umÄ›lÃ© inteligence s robustnÃ­ architekturou pro vytvoÅ™enÃ­ Å¡kÃ¡lovatelnÃ©ho Å™eÅ¡enÃ­ problÃ©mu dezinformacÃ­.

### KlÃ­ÄovÃ© hodnoty projektu:
- **Automatizace**: SnÃ­Å¾enÃ­ manuÃ¡lnÃ­ prÃ¡ce pÅ™i ovÄ›Å™ovÃ¡nÃ­ faktÅ¯
- **Transparentnost**: JasnÃ© zdÅ¯vodnÄ›nÃ­ hodnocenÃ­ pro uÅ¾ivatele  
- **Å kÃ¡lovatelnost**: Schopnost zpracovat tisÃ­ce ÄlÃ¡nkÅ¯ dennÄ›
- **PÅ™esnost**: Kombinace vÃ­ce specializovanÃ½ch agentÅ¯ pro vyÅ¡Å¡Ã­ spolehlivost
- **Flexibilita**: MoÅ¾nost pÅ™izpÅ¯sobenÃ­ rÅ¯znÃ½m typÅ¯m obsahu a zdrojÅ¯

### TechnologickÃ© vÃ½hody:
- VyuÅ¾itÃ­ pokroÄilÃ½ch LLM modelÅ¯ pro porozumÄ›nÃ­ kontextu
- VektorovÃ© databÃ¡ze pro efektivnÃ­ vyhledÃ¡vÃ¡nÃ­ faktÅ¯
- AsynchronnÃ­ zpracovÃ¡nÃ­ pro vysokÃ½ vÃ½kon
- ModulÃ¡rnÃ­ architektura umoÅ¾ÅˆujÃ­cÃ­ snadnÃ© rozÅ¡Ã­Å™enÃ­

SystÃ©m mÅ¯Å¾e bÃ½t nasazen jako sluÅ¾ba pro mediÃ¡lnÃ­ organizace, sociÃ¡lnÃ­ sÃ­tÄ› nebo jako samostatnÃ¡ aplikace pro Å¡irokou veÅ™ejnost, pÅ™ispÃ­vajÃ­cÃ­ tak k boji proti dezinformacÃ­m a zvyÅ¡ovÃ¡nÃ­ mediÃ¡lnÃ­ gramotnosti.