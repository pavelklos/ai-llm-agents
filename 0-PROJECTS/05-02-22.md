<small>Claude Sonnet 4 **(Image Captioning & Analysis Agent)**</small>
# Image Captioning & Analysis Agent

## Key Concepts Explanation

### Multimodal AI Processing
Advanced artificial intelligence system that simultaneously processes and understands multiple data modalities including visual (images, videos) and textual information, enabling comprehensive analysis that combines computer vision capabilities with natural language understanding for richer, contextually-aware interpretations.

### Vision-Language Integration
Sophisticated fusion of computer vision models and large language models that enables AI systems to bridge the semantic gap between visual content and natural language descriptions, allowing for accurate image captioning, visual question answering, and detailed scene understanding.

### GPT-4o Vision Capabilities
OpenAI's latest multimodal model that combines advanced language understanding with powerful vision processing, enabling tasks like image analysis, visual reasoning, chart interpretation, document analysis, and generating detailed descriptions of complex visual scenes with high accuracy.

### Contextual Image Analysis
Intelligent visual content analysis that goes beyond object detection to understand relationships, context, emotions, activities, and narrative elements within images, providing comprehensive insights about composition, style, cultural context, and semantic meaning.

### Adaptive Caption Generation
Dynamic text generation system that produces image descriptions tailored to specific use cases, audiences, and requirements, ranging from technical annotations and accessibility descriptions to creative storytelling and educational content based on visual analysis.

## Comprehensive Project Explanation

### Objectives
The Image Captioning & Analysis Agent provides comprehensive visual content understanding through advanced multimodal AI, enabling automated generation of accurate captions, detailed image analysis, visual question answering, and contextual interpretation for various applications including accessibility, content management, and visual storytelling.

### Key Features
- **Intelligent Image Captioning**: Automated generation of accurate, context-aware descriptions
- **Detailed Visual Analysis**: Comprehensive scene understanding and object relationships
- **Visual Question Answering**: Interactive querying about image content
- **Multi-Style Caption Generation**: Adaptable descriptions for different purposes
- **Batch Processing**: Efficient handling of large image collections

### Challenges
- **Multimodal Alignment**: Ensuring accurate correspondence between visual and textual representations
- **Context Understanding**: Capturing subtle visual cues and cultural nuances
- **Scalability**: Processing large volumes of visual content efficiently
- **Quality Consistency**: Maintaining caption accuracy across diverse image types

### Potential Impact
This system democratizes visual content accessibility, enhances digital asset management, improves content discovery through better metadata, supports educational applications with automated visual descriptions, and enables new forms of human-computer interaction through visual understanding.

## Comprehensive Project Example with Python Implementation

### Dependencies and Setup

````python
# requirements.txt
streamlit==1.29.0
openai==1.5.0
pillow==10.1.0
requests==2.31.0
numpy==1.24.3
pandas==2.1.4
plotly==5.17.0
opencv-python==4.8.1.78
matplotlib==3.7.1
seaborn==0.12.2
base64
io
json
logging
datetime
uuid
typing
dataclasses
enum
pathlib
asyncio
threading
sqlite3
````

### Core Implementation

````python
import base64
import io
import json
import logging
import sqlite3
import uuid
import asyncio
import threading
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import numpy as np
import cv2
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image, ImageDraw, ImageFont
import openai
import requests

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CaptionStyle(Enum):
    DESCRIPTIVE = "descriptive"
    TECHNICAL = "technical"
    CREATIVE = "creative"
    ACCESSIBILITY = "accessibility"
    EDUCATIONAL = "educational"
    SOCIAL_MEDIA = "social_media"

class AnalysisType(Enum):
    OBJECTS = "objects"
    SCENE = "scene"
    PEOPLE = "people"
    TEXT = "text"
    EMOTIONS = "emotions"
    ACTIVITIES = "activities"
    COMPOSITION = "composition"

class ImageCategory(Enum):
    NATURE = "nature"
    PEOPLE = "people"
    ARCHITECTURE = "architecture"
    TECHNOLOGY = "technology"
    ANIMALS = "animals"
    FOOD = "food"
    TRANSPORTATION = "transportation"
    ART = "art"
    SPORTS = "sports"
    OTHER = "other"

@dataclass
class ImageMetadata:
    filename: str
    width: int
    height: int
    format: str
    size_bytes: int
    upload_time: datetime = field(default_factory=datetime.now)

@dataclass
class CaptionResult:
    caption_id: str
    style: CaptionStyle
    text: str
    confidence: float
    generation_time: float
    word_count: int

@dataclass
class AnalysisResult:
    analysis_id: str
    analysis_type: AnalysisType
    findings: Dict[str, Any]
    confidence: float
    processing_time: float

@dataclass
class ProcessedImage:
    image_id: str
    metadata: ImageMetadata
    category: ImageCategory
    captions: List[CaptionResult] = field(default_factory=list)
    analyses: List[AnalysisResult] = field(default_factory=list)
    questions_answers: List[Dict[str, str]] = field(default_factory=list)
    tags: List[str] = field(default_factory=list)
    processing_complete: bool = False

class ImageProcessor:
    """Handles image preprocessing and basic analysis."""
    
    def __init__(self):
        self.supported_formats = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}
    
    def load_image(self, image_source: Union[str, bytes, Image.Image]) -> Optional[Image.Image]:
        """Load image from various sources."""
        try:
            if isinstance(image_source, Image.Image):
                return image_source
            elif isinstance(image_source, str):
                # Assume it's a file path or URL
                if image_source.startswith(('http://', 'https://')):
                    response = requests.get(image_source, timeout=10)
                    return Image.open(io.BytesIO(response.content))
                else:
                    return Image.open(image_source)
            elif isinstance(image_source, bytes):
                return Image.open(io.BytesIO(image_source))
            else:
                logger.error(f"Unsupported image source type: {type(image_source)}")
                return None
        
        except Exception as e:
            logger.error(f"Error loading image: {e}")
            return None
    
    def get_image_metadata(self, image: Image.Image, filename: str = "unknown") -> ImageMetadata:
        """Extract image metadata."""
        try:
            # Convert to bytes to get size
            img_byte_arr = io.BytesIO()
            image.save(img_byte_arr, format=image.format or 'PNG')
            size_bytes = img_byte_arr.tell()
            
            return ImageMetadata(
                filename=filename,
                width=image.width,
                height=image.height,
                format=image.format or 'PNG',
                size_bytes=size_bytes
            )
        
        except Exception as e:
            logger.error(f"Error extracting metadata: {e}")
            return ImageMetadata(
                filename=filename,
                width=0,
                height=0,
                format='Unknown',
                size_bytes=0
            )
    
    def resize_image(self, image: Image.Image, max_size: Tuple[int, int] = (1024, 1024)) -> Image.Image:
        """Resize image while maintaining aspect ratio."""
        try:
            image.thumbnail(max_size, Image.Resampling.LANCZOS)
            return image
        except Exception as e:
            logger.error(f"Error resizing image: {e}")
            return image
    
    def enhance_image(self, image: Image.Image) -> Image.Image:
        """Apply basic image enhancement."""
        try:
            from PIL import ImageEnhance
            
            # Convert to RGB if necessary
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            # Enhance contrast slightly
            enhancer = ImageEnhance.Contrast(image)
            image = enhancer.enhance(1.1)
            
            # Enhance sharpness slightly
            enhancer = ImageEnhance.Sharpness(image)
            image = enhancer.enhance(1.1)
            
            return image
        
        except Exception as e:
            logger.error(f"Error enhancing image: {e}")
            return image
    
    def analyze_basic_properties(self, image: Image.Image) -> Dict[str, Any]:
        """Analyze basic image properties."""
        try:
            # Convert to numpy array for analysis
            img_array = np.array(image)
            
            properties = {
                "aspect_ratio": image.width / image.height,
                "total_pixels": image.width * image.height,
                "color_mode": image.mode,
                "has_transparency": image.mode in ('RGBA', 'LA'),
            }
            
            if len(img_array.shape) == 3:  # Color image
                properties.update({
                    "mean_brightness": np.mean(img_array),
                    "std_brightness": np.std(img_array),
                    "dominant_colors": self._get_dominant_colors(img_array),
                    "color_distribution": self._analyze_color_distribution(img_array)
                })
            
            return properties
        
        except Exception as e:
            logger.error(f"Error analyzing image properties: {e}")
            return {}
    
    def _get_dominant_colors(self, img_array: np.ndarray, n_colors: int = 5) -> List[Tuple[int, int, int]]:
        """Extract dominant colors from image."""
        try:
            # Reshape image to list of pixels
            pixels = img_array.reshape(-1, img_array.shape[-1])
            
            # Sample pixels to reduce computation
            if len(pixels) > 10000:
                indices = np.random.choice(len(pixels), 10000, replace=False)
                pixels = pixels[indices]
            
            # Use k-means clustering to find dominant colors
            from sklearn.cluster import KMeans
            
            kmeans = KMeans(n_clusters=n_colors, random_state=42, n_init=10)
            kmeans.fit(pixels)
            
            # Convert to RGB tuples
            colors = [(int(c[0]), int(c[1]), int(c[2])) for c in kmeans.cluster_centers_]
            
            return colors
        
        except Exception as e:
            logger.error(f"Error extracting dominant colors: {e}")
            return [(128, 128, 128)]  # Default gray
    
    def _analyze_color_distribution(self, img_array: np.ndarray) -> Dict[str, float]:
        """Analyze color distribution in image."""
        try:
            if len(img_array.shape) != 3:
                return {}
            
            # Calculate color channel statistics
            red_mean = np.mean(img_array[:, :, 0])
            green_mean = np.mean(img_array[:, :, 1])
            blue_mean = np.mean(img_array[:, :, 2])
            
            # Color temperature estimation
            color_temp = "warm" if red_mean > blue_mean else "cool"
            
            # Saturation estimation
            hsv = cv2.cvtColor(img_array, cv2.COLOR_RGB2HSV)
            saturation = np.mean(hsv[:, :, 1])
            
            return {
                "red_dominance": red_mean / 255.0,
                "green_dominance": green_mean / 255.0,
                "blue_dominance": blue_mean / 255.0,
                "color_temperature": color_temp,
                "average_saturation": saturation / 255.0
            }
        
        except Exception as e:
            logger.error(f"Error analyzing color distribution: {e}")
            return {}

class GPT4VisionAnalyzer:
    """GPT-4 Vision-based image analysis and captioning."""
    
    def __init__(self, api_key: str):
        self.client = openai.OpenAI(api_key=api_key)
        self.model = "gpt-4o"  # Use GPT-4o for vision capabilities
    
    def encode_image(self, image: Image.Image) -> str:
        """Encode image to base64 string."""
        try:
            buffer = io.BytesIO()
            # Convert to RGB if necessary
            if image.mode != 'RGB':
                image = image.convert('RGB')
            image.save(buffer, format='JPEG', quality=85)
            return base64.b64encode(buffer.getvalue()).decode('utf-8')
        
        except Exception as e:
            logger.error(f"Error encoding image: {e}")
            raise
    
    def generate_caption(self, image: Image.Image, style: CaptionStyle = CaptionStyle.DESCRIPTIVE,
                        max_words: int = 50) -> CaptionResult:
        """Generate image caption using GPT-4 Vision."""
        try:
            start_time = datetime.now()
            
            # Encode image
            base64_image = self.encode_image(image)
            
            # Create style-specific prompt
            prompt = self._create_caption_prompt(style, max_words)
            
            # Call GPT-4 Vision API
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{base64_image}",
                                    "detail": "high"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=300,
                temperature=0.3
            )
            
            caption_text = response.choices[0].message.content.strip()
            processing_time = (datetime.now() - start_time).total_seconds()
            
            return CaptionResult(
                caption_id=str(uuid.uuid4()),
                style=style,
                text=caption_text,
                confidence=0.9,  # GPT-4 generally has high confidence
                generation_time=processing_time,
                word_count=len(caption_text.split())
            )
        
        except Exception as e:
            logger.error(f"Error generating caption: {e}")
            raise
    
    def _create_caption_prompt(self, style: CaptionStyle, max_words: int) -> str:
        """Create style-specific prompts for caption generation."""
        base_prompt = f"Generate a {max_words}-word caption for this image. "
        
        style_prompts = {
            CaptionStyle.DESCRIPTIVE: base_prompt + "Provide a clear, objective description of what you see, including objects, people, setting, and activities.",
            
            CaptionStyle.TECHNICAL: base_prompt + "Focus on technical details like composition, lighting, camera angles, equipment visible, and technical aspects.",
            
            CaptionStyle.CREATIVE: base_prompt + "Write a creative, engaging caption that tells a story or evokes emotions while describing the image.",
            
            CaptionStyle.ACCESSIBILITY: base_prompt + "Create an accessibility-focused description for visually impaired users, including all important visual elements, text, and spatial relationships.",
            
            CaptionStyle.EDUCATIONAL: base_prompt + "Provide an educational description suitable for learning, explaining concepts, processes, or historical/scientific context shown.",
            
            CaptionStyle.SOCIAL_MEDIA: base_prompt + "Write a catchy, engaging social media caption that would work well for posts, including relevant context and appeal."
        }
        
        return style_prompts.get(style, style_prompts[CaptionStyle.DESCRIPTIVE])
    
    def analyze_image(self, image: Image.Image, analysis_type: AnalysisType) -> AnalysisResult:
        """Perform specific type of image analysis."""
        try:
            start_time = datetime.now()
            
            # Encode image
            base64_image = self.encode_image(image)
            
            # Create analysis-specific prompt
            prompt = self._create_analysis_prompt(analysis_type)
            
            # Call GPT-4 Vision API
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{base64_image}",
                                    "detail": "high"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=500,
                temperature=0.2
            )
            
            # Parse response as JSON if possible
            response_text = response.choices[0].message.content.strip()
            try:
                findings = json.loads(response_text)
            except json.JSONDecodeError:
                # If not JSON, structure the text response
                findings = {"analysis": response_text}
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            return AnalysisResult(
                analysis_id=str(uuid.uuid4()),
                analysis_type=analysis_type,
                findings=findings,
                confidence=0.85,
                processing_time=processing_time
            )
        
        except Exception as e:
            logger.error(f"Error analyzing image: {e}")
            raise
    
    def _create_analysis_prompt(self, analysis_type: AnalysisType) -> str:
        """Create analysis-specific prompts."""
        prompts = {
            AnalysisType.OBJECTS: """
            Analyze this image and identify all objects present. Return a JSON response with:
            {
                "objects": [{"name": "object_name", "confidence": 0.95, "location": "description", "count": 1}],
                "main_subject": "primary object or focus",
                "background_elements": ["list", "of", "background", "items"]
            }
            """,
            
            AnalysisType.SCENE: """
            Analyze the overall scene in this image. Return a JSON response with:
            {
                "scene_type": "indoor/outdoor/studio/etc",
                "setting": "specific location description",
                "time_of_day": "morning/afternoon/evening/night/unknown",
                "weather": "clear/cloudy/rainy/snowy/unknown",
                "atmosphere": "mood or feeling of the scene",
                "lighting": "natural/artificial/mixed/description"
            }
            """,
            
            AnalysisType.PEOPLE: """
            Analyze any people in this image. Return a JSON response with:
            {
                "people_count": 0,
                "demographics": [{"age_range": "child/adult/elderly", "gender": "apparent", "ethnicity": "if_clearly_apparent"}],
                "activities": ["what", "people", "are", "doing"],
                "expressions": ["happy", "sad", "neutral", "etc"],
                "clothing": ["casual", "formal", "work", "etc"],
                "interactions": "description of social interactions"
            }
            """,
            
            AnalysisType.TEXT: """
            Identify and transcribe any text visible in this image. Return a JSON response with:
            {
                "text_detected": true/false,
                "text_content": ["list", "of", "all", "readable", "text"],
                "text_types": ["signs", "labels", "documents", "etc"],
                "languages": ["detected", "languages"],
                "readability": "clear/partially_obscured/unclear"
            }
            """,
            
            AnalysisType.EMOTIONS: """
            Analyze the emotional content and mood of this image. Return a JSON response with:
            {
                "overall_mood": "happy/sad/neutral/energetic/calm/etc",
                "emotional_indicators": ["visual", "cues", "that", "suggest", "emotions"],
                "facial_expressions": ["if", "people", "visible"],
                "body_language": "description if people present",
                "color_psychology": "how colors contribute to mood",
                "composition_impact": "how visual composition affects feeling"
            }
            """,
            
            AnalysisType.ACTIVITIES: """
            Identify and analyze activities or actions taking place in this image. Return a JSON response with:
            {
                "main_activity": "primary action or event",
                "sub_activities": ["secondary", "actions"],
                "activity_type": "sport/work/leisure/education/etc",
                "participants": "who is involved",
                "setting_relevance": "how setting relates to activity",
                "equipment_tools": ["items", "being", "used"]
            }
            """,
            
            AnalysisType.COMPOSITION: """
            Analyze the artistic and technical composition of this image. Return a JSON response with:
            {
                "composition_rules": ["rule_of_thirds", "leading_lines", "symmetry", "etc"],
                "focal_point": "what draws the eye",
                "depth_of_field": "shallow/deep/description",
                "perspective": "high/low/eye_level/bird's_eye/worm's_eye",
                "color_scheme": "monochromatic/complementary/analogous/etc",
                "balance": "symmetrical/asymmetrical/description",
                "framing": "how the image is framed",
                "technical_quality": "sharpness/exposure/noise/etc"
            }
            """
        }
        
        return prompts.get(analysis_type, prompts[AnalysisType.OBJECTS])
    
    def answer_question(self, image: Image.Image, question: str) -> str:
        """Answer a specific question about the image."""
        try:
            # Encode image
            base64_image = self.encode_image(image)
            
            prompt = f"Look at this image and answer the following question in detail: {question}"
            
            # Call GPT-4 Vision API
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{base64_image}",
                                    "detail": "high"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=300,
                temperature=0.3
            )
            
            return response.choices[0].message.content.strip()
        
        except Exception as e:
            logger.error(f"Error answering question: {e}")
            return f"Error processing question: {str(e)}"
    
    def categorize_image(self, image: Image.Image) -> ImageCategory:
        """Categorize the image into a predefined category."""
        try:
            # Encode image
            base64_image = self.encode_image(image)
            
            categories = [cat.value for cat in ImageCategory]
            prompt = f"Categorize this image into one of these categories: {', '.join(categories)}. Respond with only the category name."
            
            # Call GPT-4 Vision API
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{base64_image}",
                                    "detail": "high"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=20,
                temperature=0.1
            )
            
            category_name = response.choices[0].message.content.strip().lower()
            
            # Find matching category
            for category in ImageCategory:
                if category.value in category_name:
                    return category
            
            return ImageCategory.OTHER
        
        except Exception as e:
            logger.error(f"Error categorizing image: {e}")
            return ImageCategory.OTHER

class ImageDatabase:
    """Manages storage and retrieval of processed images."""
    
    def __init__(self, db_path: str = "image_analysis.db"):
        self.db_path = db_path
        self.connection = self._initialize_database()
    
    def _initialize_database(self) -> sqlite3.Connection:
        """Initialize the database with required tables."""
        conn = sqlite3.connect(self.db_path, check_same_thread=False)
        cursor = conn.cursor()
        
        # Images table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS images (
                image_id TEXT PRIMARY KEY,
                filename TEXT,
                category TEXT,
                width INTEGER,
                height INTEGER,
                format TEXT,
                size_bytes INTEGER,
                upload_time TEXT,
                processing_complete BOOLEAN,
                tags TEXT
            )
        """)
        
        # Captions table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS captions (
                caption_id TEXT PRIMARY KEY,
                image_id TEXT,
                style TEXT,
                text TEXT,
                confidence REAL,
                generation_time REAL,
                word_count INTEGER,
                FOREIGN KEY (image_id) REFERENCES images (image_id)
            )
        """)
        
        # Analyses table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS analyses (
                analysis_id TEXT PRIMARY KEY,
                image_id TEXT,
                analysis_type TEXT,
                findings TEXT,
                confidence REAL,
                processing_time REAL,
                FOREIGN KEY (image_id) REFERENCES images (image_id)
            )
        """)
        
        # Questions and Answers table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS qa_pairs (
                qa_id TEXT PRIMARY KEY,
                image_id TEXT,
                question TEXT,
                answer TEXT,
                timestamp TEXT,
                FOREIGN KEY (image_id) REFERENCES images (image_id)
            )
        """)
        
        conn.commit()
        return conn
    
    def save_processed_image(self, processed_image: ProcessedImage):
        """Save a processed image and all its analysis results."""
        try:
            cursor = self.connection.cursor()
            
            # Save image metadata
            cursor.execute("""
                INSERT OR REPLACE INTO images
                (image_id, filename, category, width, height, format, size_bytes, 
                 upload_time, processing_complete, tags)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                processed_image.image_id,
                processed_image.metadata.filename,
                processed_image.category.value,
                processed_image.metadata.width,
                processed_image.metadata.height,
                processed_image.metadata.format,
                processed_image.metadata.size_bytes,
                processed_image.metadata.upload_time.isoformat(),
                processed_image.processing_complete,
                json.dumps(processed_image.tags)
            ))
            
            # Save captions
            for caption in processed_image.captions:
                cursor.execute("""
                    INSERT OR REPLACE INTO captions
                    (caption_id, image_id, style, text, confidence, generation_time, word_count)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (
                    caption.caption_id,
                    processed_image.image_id,
                    caption.style.value,
                    caption.text,
                    caption.confidence,
                    caption.generation_time,
                    caption.word_count
                ))
            
            # Save analyses
            for analysis in processed_image.analyses:
                cursor.execute("""
                    INSERT OR REPLACE INTO analyses
                    (analysis_id, image_id, analysis_type, findings, confidence, processing_time)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (
                    analysis.analysis_id,
                    processed_image.image_id,
                    analysis.analysis_type.value,
                    json.dumps(analysis.findings),
                    analysis.confidence,
                    analysis.processing_time
                ))
            
            # Save Q&A pairs
            for qa in processed_image.questions_answers:
                cursor.execute("""
                    INSERT OR REPLACE INTO qa_pairs
                    (qa_id, image_id, question, answer, timestamp)
                    VALUES (?, ?, ?, ?, ?)
                """, (
                    str(uuid.uuid4()),
                    processed_image.image_id,
                    qa['question'],
                    qa['answer'],
                    datetime.now().isoformat()
                ))
            
            self.connection.commit()
            logger.info(f"Saved processed image: {processed_image.image_id}")
        
        except Exception as e:
            logger.error(f"Error saving processed image: {e}")
            raise
    
    def get_processed_images(self, limit: int = 50) -> List[Dict[str, Any]]:
        """Retrieve processed images from database."""
        try:
            cursor = self.connection.cursor()
            cursor.execute("""
                SELECT * FROM images 
                ORDER BY upload_time DESC 
                LIMIT ?
            """, (limit,))
            
            columns = [description[0] for description in cursor.description]
            results = []
            
            for row in cursor.fetchall():
                image_dict = dict(zip(columns, row))
                image_dict['tags'] = json.loads(image_dict['tags']) if image_dict['tags'] else []
                results.append(image_dict)
            
            return results
        
        except Exception as e:
            logger.error(f"Error retrieving images: {e}")
            return []
    
    def get_image_captions(self, image_id: str) -> List[Dict[str, Any]]:
        """Get all captions for a specific image."""
        try:
            cursor = self.connection.cursor()
            cursor.execute("""
                SELECT * FROM captions 
                WHERE image_id = ?
                ORDER BY generation_time DESC
            """, (image_id,))
            
            columns = [description[0] for description in cursor.description]
            return [dict(zip(columns, row)) for row in cursor.fetchall()]
        
        except Exception as e:
            logger.error(f"Error retrieving captions: {e}")
            return []
    
    def get_image_analyses(self, image_id: str) -> List[Dict[str, Any]]:
        """Get all analyses for a specific image."""
        try:
            cursor = self.connection.cursor()
            cursor.execute("""
                SELECT * FROM analyses 
                WHERE image_id = ?
                ORDER BY processing_time DESC
            """, (image_id,))
            
            columns = [description[0] for description in cursor.description]
            results = []
            
            for row in cursor.fetchall():
                analysis_dict = dict(zip(columns, row))
                analysis_dict['findings'] = json.loads(analysis_dict['findings']) if analysis_dict['findings'] else {}
                results.append(analysis_dict)
            
            return results
        
        except Exception as e:
            logger.error(f"Error retrieving analyses: {e}")
            return []

class ImageCaptioningAgent:
    """Main agent coordinating image processing, analysis, and captioning."""
    
    def __init__(self, openai_api_key: str):
        self.image_processor = ImageProcessor()
        self.vision_analyzer = GPT4VisionAnalyzer(openai_api_key)
        self.database = ImageDatabase()
        self.processing_queue = asyncio.Queue()
        self.is_processing = False
    
    async def process_image(self, image: Image.Image, filename: str = "uploaded_image",
                          styles: List[CaptionStyle] = None,
                          analyses: List[AnalysisType] = None) -> ProcessedImage:
        """Process an image with captioning and analysis."""
        try:
            logger.info(f"Starting image processing: {filename}")
            
            # Create processed image object
            image_id = str(uuid.uuid4())
            metadata = self.image_processor.get_image_metadata(image, filename)
            
            # Categorize image
            category = self.vision_analyzer.categorize_image(image)
            
            processed_image = ProcessedImage(
                image_id=image_id,
                metadata=metadata,
                category=category
            )
            
            # Default styles and analyses if not specified
            if styles is None:
                styles = [CaptionStyle.DESCRIPTIVE, CaptionStyle.ACCESSIBILITY]
            
            if analyses is None:
                analyses = [AnalysisType.OBJECTS, AnalysisType.SCENE]
            
            # Process image (resize and enhance)
            processed_img = self.image_processor.resize_image(image)
            processed_img = self.image_processor.enhance_image(processed_img)
            
            # Generate captions
            for style in styles:
                try:
                    caption = self.vision_analyzer.generate_caption(processed_img, style)
                    processed_image.captions.append(caption)
                except Exception as e:
                    logger.error(f"Error generating {style.value} caption: {e}")
            
            # Perform analyses
            for analysis_type in analyses:
                try:
                    analysis = self.vision_analyzer.analyze_image(processed_img, analysis_type)
                    processed_image.analyses.append(analysis)
                except Exception as e:
                    logger.error(f"Error performing {analysis_type.value} analysis: {e}")
            
            # Generate tags from analysis results
            processed_image.tags = self._generate_tags(processed_image)
            
            # Mark as complete
            processed_image.processing_complete = True
            
            # Save to database
            self.database.save_processed_image(processed_image)
            
            logger.info(f"Completed image processing: {filename}")
            return processed_image
        
        except Exception as e:
            logger.error(f"Error processing image: {e}")
            raise
    
    def _generate_tags(self, processed_image: ProcessedImage) -> List[str]:
        """Generate tags from analysis results."""
        tags = []
        
        # Add category as tag
        tags.append(processed_image.category.value)
        
        # Extract tags from object analysis
        for analysis in processed_image.analyses:
            if analysis.analysis_type == AnalysisType.OBJECTS:
                findings = analysis.findings
                if 'objects' in findings:
                    for obj in findings['objects']:
                        if 'name' in obj:
                            tags.append(obj['name'])
                
                if 'main_subject' in findings:
                    tags.append(findings['main_subject'])
            
            elif analysis.analysis_type == AnalysisType.SCENE:
                findings = analysis.findings
                if 'scene_type' in findings:
                    tags.append(findings['scene_type'])
                if 'setting' in findings:
                    tags.append(findings['setting'])
                if 'time_of_day' in findings:
                    tags.append(findings['time_of_day'])
        
        # Remove duplicates and empty tags
        tags = list(set([tag.lower().strip() for tag in tags if tag and tag.strip()]))
        
        return tags[:20]  # Limit to 20 tags
    
    async def answer_image_question(self, image_id: str, image: Image.Image, question: str) -> str:
        """Answer a question about a specific image."""
        try:
            answer = self.vision_analyzer.answer_question(image, question)
            
            # Save Q&A to database
            cursor = self.database.connection.cursor()
            cursor.execute("""
                INSERT INTO qa_pairs (qa_id, image_id, question, answer, timestamp)
                VALUES (?, ?, ?, ?, ?)
            """, (
                str(uuid.uuid4()),
                image_id,
                question,
                answer,
                datetime.now().isoformat()
            ))
            self.database.connection.commit()
            
            return answer
        
        except Exception as e:
            logger.error(f"Error answering question: {e}")
            return f"Error processing question: {str(e)}"
    
    def get_processing_statistics(self) -> Dict[str, Any]:
        """Get statistics about processed images."""
        try:
            cursor = self.database.connection.cursor()
            
            # Total images
            cursor.execute("SELECT COUNT(*) FROM images")
            total_images = cursor.fetchone()[0]
            
            # Images by category
            cursor.execute("""
                SELECT category, COUNT(*) 
                FROM images 
                GROUP BY category
            """)
            category_stats = dict(cursor.fetchall())
            
            # Total captions
            cursor.execute("SELECT COUNT(*) FROM captions")
            total_captions = cursor.fetchone()[0]
            
            # Captions by style
            cursor.execute("""
                SELECT style, COUNT(*) 
                FROM captions 
                GROUP BY style
            """)
            style_stats = dict(cursor.fetchall())
            
            # Total analyses
            cursor.execute("SELECT COUNT(*) FROM analyses")
            total_analyses = cursor.fetchone()[0]
            
            # Recent activity
            cursor.execute("""
                SELECT COUNT(*) 
                FROM images 
                WHERE upload_time > datetime('now', '-24 hours')
            """)
            recent_images = cursor.fetchone()[0]
            
            return {
                "total_images": total_images,
                "category_distribution": category_stats,
                "total_captions": total_captions,
                "style_distribution": style_stats,
                "total_analyses": total_analyses,
                "recent_activity": recent_images
            }
        
        except Exception as e:
            logger.error(f"Error getting statistics: {e}")
            return {}

def create_sample_images():
    """Create sample images for demonstration."""
    sample_images = []
    
    # Create a simple sample image with text
    img = Image.new('RGB', (400, 300), color='lightblue')
    draw = ImageDraw.Draw(img)
    
    # Try to load a font, fall back to default if not available
    try:
        font = ImageFont.truetype("arial.ttf", 24)
    except:
        font = ImageFont.load_default()
    
    # Draw some content
    draw.rectangle([50, 50, 350, 100], fill='white', outline='black')
    draw.text((60, 60), "Sample Image", fill='black', font=font)
    draw.ellipse([150, 150, 250, 250], fill='yellow', outline='orange')
    draw.text((180, 190), "‚ò∫", fill='black', font=font)
    
    sample_images.append(("sample_image.png", img))
    
    return sample_images

def main():
    """Main Streamlit application."""
    st.set_page_config(
        page_title="Image Captioning & Analysis Agent",
        page_icon="üñºÔ∏è",
        layout="wide"
    )
    
    st.title("üñºÔ∏è Image Captioning & Analysis Agent")
    st.markdown("**AI-powered image understanding with GPT-4 Vision capabilities**")
    
    # Initialize session state
    if 'agent' not in st.session_state:
        st.session_state['agent'] = None
    if 'current_image' not in st.session_state:
        st.session_state['current_image'] = None
    if 'processed_result' not in st.session_state:
        st.session_state['processed_result'] = None
    
    # Sidebar configuration
    with st.sidebar:
        st.header("‚öôÔ∏è Configuration")
        
        openai_key = st.text_input("OpenAI API Key", type="password",
                                  help="Required for GPT-4 Vision analysis")
        
        if st.button("Initialize Agent"):
            if openai_key:
                try:
                    st.session_state['agent'] = ImageCaptioningAgent(openai_key)
                    st.success("Agent initialized successfully!")
                except Exception as e:
                    st.error(f"Initialization failed: {e}")
            else:
                st.error("Please provide OpenAI API key")
        
        st.header("üìä Quick Stats")
        if st.session_state['agent']:
            try:
                stats = st.session_state['agent'].get_processing_statistics()
                st.metric("Total Images", stats.get('total_images', 0))
                st.metric("Total Captions", stats.get('total_captions', 0))
                st.metric("Total Analyses", stats.get('total_analyses', 0))
                st.metric("Recent (24h)", stats.get('recent_activity', 0))
            except Exception as e:
                st.error(f"Error loading stats: {e}")
    
    if not st.session_state['agent']:
        st.info("üëà Please initialize the agent with your OpenAI API key")
        
        # Show sample images
        st.subheader("Sample Images for Testing")
        sample_images = create_sample_images()
        
        for name, img in sample_images:
            col1, col2 = st.columns([1, 2])
            with col1:
                st.image(img, caption=name, width=200)
            with col2:
                st.write(f"**{name}**")
                st.write("A sample image with text and simple graphics for testing the captioning and analysis capabilities.")
        
        return
    
    agent = st.session_state['agent']
    
    # Main interface tabs
    tab1, tab2, tab3, tab4, tab5 = st.tabs(["üñºÔ∏è Upload & Process", "üìù Captions", "üîç Analysis", "üí¨ Q&A", "üìö Gallery"])
    
    with tab1:
        st.header("üñºÔ∏è Image Upload & Processing")
        
        # Image upload
        uploaded_file = st.file_uploader(
            "Choose an image file",
            type=['png', 'jpg', 'jpeg', 'bmp', 'tiff', 'webp'],
            help="Upload an image for captioning and analysis"
        )
        
        # Sample image option
        if st.button("Use Sample Image"):
            sample_images = create_sample_images()
            if sample_images:
                st.session_state['current_image'] = sample_images[0][1]
                st.session_state['image_filename'] = sample_images[0][0]
                st.success("Sample image loaded!")
        
        if uploaded_file is not None:
            try:
                image = Image.open(uploaded_file)
                st.session_state['current_image'] = image
                st.session_state['image_filename'] = uploaded_file.name
                st.success("Image uploaded successfully!")
            except Exception as e:
                st.error(f"Error loading image: {e}")
        
        # Display current image
        if st.session_state['current_image']:
            col1, col2 = st.columns([2, 1])
            
            with col1:
                st.image(st.session_state['current_image'], 
                        caption=st.session_state.get('image_filename', 'Current Image'),
                        use_column_width=True)
            
            with col2:
                st.subheader("Processing Options")
                
                # Caption style selection
                selected_styles = st.multiselect(
                    "Caption Styles",
                    [style.value for style in CaptionStyle],
                    default=['descriptive', 'accessibility'],
                    format_func=lambda x: x.replace('_', ' ').title()
                )
                
                # Analysis type selection
                selected_analyses = st.multiselect(
                    "Analysis Types",
                    [analysis.value for analysis in AnalysisType],
                    default=['objects', 'scene'],
                    format_func=lambda x: x.replace('_', ' ').title()
                )
                
                # Process button
                if st.button("üöÄ Process Image", type="primary"):
                    if selected_styles and selected_analyses:
                        with st.spinner("Processing image..."):
                            try:
                                # Convert string selections back to enums
                                caption_styles = [CaptionStyle(style) for style in selected_styles]
                                analysis_types = [AnalysisType(analysis) for analysis in selected_analyses]
                                
                                # Process image
                                result = asyncio.run(agent.process_image(
                                    st.session_state['current_image'],
                                    st.session_state.get('image_filename', 'uploaded_image'),
                                    caption_styles,
                                    analysis_types
                                ))
                                
                                st.session_state['processed_result'] = result
                                st.success("Image processed successfully!")
                                st.rerun()
                            
                            except Exception as e:
                                st.error(f"Processing failed: {e}")
                    else:
                        st.error("Please select at least one caption style and analysis type")
    
    with tab2:
        st.header("üìù Generated Captions")
        
        if st.session_state['processed_result']:
            result = st.session_state['processed_result']
            
            if result.captions:
                for caption in result.captions:
                    with st.expander(f"{caption.style.value.replace('_', ' ').title()} Caption"):
                        st.write(f"**Caption:** {caption.text}")
                        
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("Word Count", caption.word_count)
                        with col2:
                            st.metric("Confidence", f"{caption.confidence:.2f}")
                        with col3:
                            st.metric("Generation Time", f"{caption.generation_time:.2f}s")
                        
                        # Copy button (simulated)
                        if st.button(f"üìã Copy Caption", key=f"copy_{caption.caption_id}"):
                            st.success("Caption copied to clipboard! (simulated)")
            else:
                st.info("No captions generated yet. Process an image first.")
        else:
            st.info("No processed image available. Upload and process an image first.")
    
    with tab3:
        st.header("üîç Detailed Analysis")
        
        if st.session_state['processed_result']:
            result = st.session_state['processed_result']
            
            # Show basic image info
            st.subheader("Image Information")
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("Category", result.category.value.title())
            with col2:
                st.metric("Dimensions", f"{result.metadata.width}√ó{result.metadata.height}")
            with col3:
                st.metric("Format", result.metadata.format)
            with col4:
                st.metric("Size", f"{result.metadata.size_bytes/1024:.1f} KB")
            
            # Show tags
            if result.tags:
                st.subheader("Generated Tags")
                tag_cols = st.columns(min(len(result.tags), 5))
                for i, tag in enumerate(result.tags[:10]):  # Show first 10 tags
                    with tag_cols[i % 5]:
                        st.badge(tag)
            
            # Show detailed analyses
            st.subheader("Analysis Results")
            
            if result.analyses:
                for analysis in result.analyses:
                    with st.expander(f"{analysis.analysis_type.value.replace('_', ' ').title()} Analysis"):
                        st.write(f"**Confidence:** {analysis.confidence:.2f}")
                        st.write(f"**Processing Time:** {analysis.processing_time:.2f}s")
                        
                        # Display findings based on analysis type
                        findings = analysis.findings
                        
                        if analysis.analysis_type == AnalysisType.OBJECTS:
                            if 'objects' in findings:
                                st.write("**Objects Detected:**")
                                objects_df = pd.DataFrame(findings['objects'])
                                if not objects_df.empty:
                                    st.dataframe(objects_df, use_container_width=True)
                            
                            if 'main_subject' in findings:
                                st.write(f"**Main Subject:** {findings['main_subject']}")
                        
                        elif analysis.analysis_type == AnalysisType.SCENE:
                            scene_info = []
                            for key, value in findings.items():
                                if value and value != "unknown":
                                    scene_info.append(f"**{key.replace('_', ' ').title()}:** {value}")
                            
                            for info in scene_info:
                                st.write(info)
                        
                        else:
                            # Generic display for other analysis types
                            for key, value in findings.items():
                                if isinstance(value, list):
                                    st.write(f"**{key.replace('_', ' ').title()}:** {', '.join(map(str, value))}")
                                else:
                                    st.write(f"**{key.replace('_', ' ').title()}:** {value}")
            else:
                st.info("No analysis results available. Process an image first.")
        else:
            st.info("No processed image available. Upload and process an image first.")
    
    with tab4:
        st.header("üí¨ Visual Question Answering")
        
        if st.session_state['processed_result'] and st.session_state['current_image']:
            result = st.session_state['processed_result']
            
            st.subheader("Ask Questions About This Image")
            
            # Question input
            question = st.text_input(
                "What would you like to know about this image?",
                placeholder="e.g., What colors are prominent in this image? How many people are there?"
            )
            
            if st.button("ü§î Get Answer") and question:
                with st.spinner("Analyzing image and generating answer..."):
                    try:
                        answer = asyncio.run(agent.answer_image_question(
                            result.image_id,
                            st.session_state['current_image'],
                            question
                        ))
                        
                        st.write(f"**Q:** {question}")
                        st.write(f"**A:** {answer}")
                        
                        # Add to session state for display
                        if 'qa_history' not in st.session_state:
                            st.session_state['qa_history'] = []
                        
                        st.session_state['qa_history'].append({
                            'question': question,
                            'answer': answer,
                            'timestamp': datetime.now()
                        })
                    
                    except Exception as e:
                        st.error(f"Error getting answer: {e}")
            
            # Show Q&A history
            if 'qa_history' in st.session_state and st.session_state['qa_history']:
                st.subheader("Question History")
                
                for i, qa in enumerate(reversed(st.session_state['qa_history'][-5:])):  # Show last 5
                    with st.expander(f"Q{len(st.session_state['qa_history'])-i}: {qa['question'][:50]}..."):
                        st.write(f"**Question:** {qa['question']}")
                        st.write(f"**Answer:** {qa['answer']}")
                        st.write(f"**Time:** {qa['timestamp'].strftime('%H:%M:%S')}")
            
            # Suggested questions
            st.subheader("Suggested Questions")
            
            suggested_questions = [
                "What is the main subject of this image?",
                "What colors are most prominent?",
                "What is the mood or atmosphere?",
                "Are there any people in the image?",
                "What time of day does this appear to be?",
                "What activities are taking place?",
                "What objects can you identify?",
                "Is this indoors or outdoors?"
            ]
            
            cols = st.columns(2)
            for i, suggestion in enumerate(suggested_questions):
                with cols[i % 2]:
                    if st.button(suggestion, key=f"suggest_{i}"):
                        # Auto-fill the question
                        st.session_state['suggested_question'] = suggestion
                        st.rerun()
        
        else:
            st.info("Upload and process an image first to ask questions about it.")
    
    with tab5:
        st.header("üìö Image Gallery")
        
        # Get processed images
        try:
            processed_images = agent.database.get_processed_images(limit=20)
            
            if processed_images:
                st.subheader(f"Recent Images ({len(processed_images)} total)")
                
                # Filter options
                col1, col2 = st.columns(2)
                
                with col1:
                    categories = list(set([img['category'] for img in processed_images]))
                    selected_category = st.selectbox("Filter by Category", ["All"] + categories)
                
                with col2:
                    search_term = st.text_input("Search in filenames and tags", placeholder="Search...")
                
                # Apply filters
                filtered_images = processed_images
                
                if selected_category != "All":
                    filtered_images = [img for img in filtered_images if img['category'] == selected_category]
                
                if search_term:
                    filtered_images = [
                        img for img in filtered_images 
                        if search_term.lower() in img['filename'].lower() 
                        or any(search_term.lower() in tag.lower() for tag in img['tags'])
                    ]
                
                # Display images in grid
                if filtered_images:
                    cols_per_row = 3
                    for i in range(0, len(filtered_images), cols_per_row):
                        cols = st.columns(cols_per_row)
                        
                        for j, img_data in enumerate(filtered_images[i:i+cols_per_row]):
                            with cols[j]:
                                st.write(f"**{img_data['filename']}**")
                                st.write(f"Category: {img_data['category'].title()}")
                                st.write(f"Size: {img_data['width']}√ó{img_data['height']}")
                                
                                # Show tags
                                if img_data['tags']:
                                    st.write("Tags: " + ", ".join(img_data['tags'][:3]))
                                
                                # Show captions and analyses count
                                captions = agent.database.get_image_captions(img_data['image_id'])
                                analyses = agent.database.get_image_analyses(img_data['image_id'])
                                
                                st.write(f"Captions: {len(captions)} | Analyses: {len(analyses)}")
                                
                                if st.button(f"View Details", key=f"view_{img_data['image_id']}"):
                                    st.session_state['selected_image_id'] = img_data['image_id']
                                    st.info("Image details would be displayed here")
                
                else:
                    st.info("No images match the selected filters.")
            
            else:
                st.info("No images processed yet. Upload and process some images to see them here!")
        
        except Exception as e:
            st.error(f"Error loading gallery: {e}")
        
        # Statistics visualization
        if processed_images:
            st.subheader("Gallery Statistics")
            
            # Category distribution
            category_counts = {}
            for img in processed_images:
                cat = img['category']
                category_counts[cat] = category_counts.get(cat, 0) + 1
            
            if category_counts:
                fig_pie = px.pie(
                    values=list(category_counts.values()),
                    names=list(category_counts.keys()),
                    title="Images by Category"
                )
                st.plotly_chart(fig_pie, use_container_width=True)
            
            # Upload timeline
            upload_dates = [datetime.fromisoformat(img['upload_time']).date() 
                          for img in processed_images]
            
            date_counts = {}
            for date in upload_dates:
                date_counts[date] = date_counts.get(date, 0) + 1
            
            if date_counts:
                fig_timeline = px.bar(
                    x=list(date_counts.keys()),
                    y=list(date_counts.values()),
                    title="Upload Activity",
                    labels={'x': 'Date', 'y': 'Images Uploaded'}
                )
                st.plotly_chart(fig_timeline, use_container_width=True)

if __name__ == "__main__":
    main()
````

## Project Summary

The Image Captioning & Analysis Agent leverages GPT-4o's advanced vision capabilities to provide comprehensive visual content understanding, transforming how we interact with and understand images through intelligent captioning, detailed analysis, and interactive visual question answering that makes visual content more accessible and searchable.

### Key Value Propositions:
- **Multimodal Intelligence**: Seamless integration of vision and language understanding
- **Flexible Caption Generation**: Multiple styles for different use cases and audiences
- **Comprehensive Visual Analysis**: Deep understanding of objects, scenes, and context
- **Interactive Q&A**: Natural language querying about visual content

### Technical Architecture:
The system combines OpenAI's GPT-4o for multimodal processing, Pillow and OpenCV for image handling, SQLite for data persistence, and Streamlit for user interaction, creating a scalable solution that processes visual content efficiently while maintaining high accuracy in both captioning and analysis tasks for diverse applications from accessibility to content management.