<small>Claude Sonnet 4 **(Game Strategy Agent)**</small>
# Game Strategy Agent

## Key Concepts Explanation

### Multi-Agent Reinforcement Learning
Advanced machine learning paradigm where multiple AI agents learn optimal strategies through interaction with game environments and each other, using reward mechanisms and experience replay to develop sophisticated decision-making capabilities for complex strategic scenarios.

### Game State Analysis
Comprehensive evaluation of current game conditions including board positions, player resources, available actions, and strategic opportunities using mathematical modeling, pattern recognition, and heuristic evaluation functions to inform optimal decision-making.

### Strategic Planning
Long-term tactical decision-making that considers multiple future game states, opponent behavior prediction, resource optimization, and risk assessment to develop winning strategies across various game types and complexity levels.

### Monte Carlo Tree Search
Probabilistic search algorithm that explores possible future game states by running random simulations, building decision trees based on outcome statistics, and selecting actions with highest expected value for optimal strategic planning.

### Neural Network Policy Learning
Deep learning approach where neural networks learn to evaluate game positions and predict optimal moves through training on large datasets of game scenarios, enabling sophisticated pattern recognition and strategic reasoning.

## Comprehensive Project Explanation

### Objectives
The Game Strategy Agent develops AI systems capable of mastering complex games through multi-agent reinforcement learning, providing intelligent opponents, strategy analysis, and educational tools for game theory and strategic thinking applications.

### Key Features
- **Multi-Agent Training Environment**: Collaborative and competitive learning between multiple AI agents
- **Advanced Game State Evaluation**: Deep analysis of positions, patterns, and strategic opportunities
- **Adaptive Strategy Development**: Dynamic learning and strategy refinement based on opponent behavior
- **Real-time Decision Making**: Fast evaluation and move selection for live gameplay
- **Strategy Visualization**: Clear presentation of decision trees and strategic reasoning

### Challenges
- **Computational Complexity**: Handling large state spaces and deep strategy trees efficiently
- **Multi-Agent Coordination**: Managing learning dynamics between competing agents
- **Strategy Generalization**: Developing approaches that work across different game types
- **Real-time Performance**: Balancing strategy depth with response time requirements

### Potential Impact
This system can advance AI research, create intelligent game companions, enhance educational tools for strategic thinking, and provide insights into complex decision-making processes applicable to business and military strategy.

## Comprehensive Project Example with Python Implementation

### Dependencies and Setup

````python
# requirements.txt
streamlit==1.29.0
torch==2.1.0
stable-baselines3==2.2.1
gymnasium==0.29.1
numpy==1.24.3
pandas==2.1.4
plotly==5.17.0
networkx==3.2.1
pygame==2.5.2
matplotlib==3.8.0
seaborn==0.13.0
scikit-learn==1.3.2
tqdm==4.66.1
````

### Core Implementation

````python
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import random
import logging
from collections import deque, namedtuple
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from enum import Enum
import json
import uuid
from datetime import datetime
import threading
import time

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import networkx as nx
import matplotlib.pyplot as plt
import seaborn as sns

# RL libraries
import gymnasium as gym
from stable_baselines3 import PPO, A2C, DQN
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import BaseCallback

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class GameType(Enum):
    TIC_TAC_TOE = "tic_tac_toe"
    CONNECT_FOUR = "connect_four"
    CHESS = "chess"
    GO = "go"

class PlayerType(Enum):
    HUMAN = "human"
    AI_AGENT = "ai_agent"
    RANDOM = "random"

class ActionType(Enum):
    MOVE = "move"
    PASS = "pass"
    RESIGN = "resign"

@dataclass
class GameMove:
    move_id: str
    player_id: str
    action: Tuple[int, ...]
    timestamp: datetime
    game_state_before: Any
    game_state_after: Any
    evaluation_score: Optional[float] = None

@dataclass
class GameState:
    state_id: str
    board: np.ndarray
    current_player: int
    valid_actions: List[Tuple[int, ...]]
    game_over: bool
    winner: Optional[int] = None
    move_count: int = 0

@dataclass
class AgentPerformance:
    agent_id: str
    games_played: int = 0
    games_won: int = 0
    games_lost: int = 0
    games_drawn: int = 0
    total_reward: float = 0.0
    average_game_length: float = 0.0
    win_rate: float = 0.0

class TicTacToeEnvironment:
    """Tic-tac-toe game environment for RL training."""
    
    def __init__(self):
        self.board_size = 3
        self.reset()
    
    def reset(self) -> GameState:
        """Reset game to initial state."""
        self.board = np.zeros((self.board_size, self.board_size), dtype=int)
        self.current_player = 1
        self.move_count = 0
        self.game_over = False
        self.winner = None
        
        return self._get_game_state()
    
    def _get_game_state(self) -> GameState:
        """Get current game state."""
        valid_actions = self._get_valid_actions()
        
        return GameState(
            state_id=str(uuid.uuid4()),
            board=self.board.copy(),
            current_player=self.current_player,
            valid_actions=valid_actions,
            game_over=self.game_over,
            winner=self.winner,
            move_count=self.move_count
        )
    
    def _get_valid_actions(self) -> List[Tuple[int, int]]:
        """Get list of valid moves."""
        if self.game_over:
            return []
        
        valid_actions = []
        for i in range(self.board_size):
            for j in range(self.board_size):
                if self.board[i, j] == 0:
                    valid_actions.append((i, j))
        
        return valid_actions
    
    def make_move(self, action: Tuple[int, int]) -> Tuple[GameState, float, bool]:
        """Make a move and return new state, reward, and done flag."""
        if self.game_over:
            return self._get_game_state(), 0, True
        
        row, col = action
        
        # Validate move
        if self.board[row, col] != 0:
            return self._get_game_state(), -10, True  # Invalid move penalty
        
        # Make move
        self.board[row, col] = self.current_player
        self.move_count += 1
        
        # Check for win
        winner = self._check_winner()
        reward = 0
        
        if winner != 0:
            self.game_over = True
            self.winner = winner
            reward = 100 if winner == self.current_player else -100
        elif self.move_count >= self.board_size * self.board_size:
            self.game_over = True
            reward = 0  # Draw
        else:
            reward = 1  # Small positive reward for valid moves
        
        # Switch player
        self.current_player = 2 if self.current_player == 1 else 1
        
        return self._get_game_state(), reward, self.game_over
    
    def _check_winner(self) -> int:
        """Check if there's a winner."""
        # Check rows
        for i in range(self.board_size):
            if all(self.board[i, j] == self.board[i, 0] != 0 for j in range(self.board_size)):
                return self.board[i, 0]
        
        # Check columns
        for j in range(self.board_size):
            if all(self.board[i, j] == self.board[0, j] != 0 for i in range(self.board_size)):
                return self.board[0, j]
        
        # Check diagonals
        if all(self.board[i, i] == self.board[0, 0] != 0 for i in range(self.board_size)):
            return self.board[0, 0]
        
        if all(self.board[i, self.board_size-1-i] == self.board[0, self.board_size-1] != 0 
               for i in range(self.board_size)):
            return self.board[0, self.board_size-1]
        
        return 0
    
    def get_state_vector(self) -> np.ndarray:
        """Get flattened state representation for neural networks."""
        # Flatten board and add current player info
        state = self.board.flatten()
        player_info = np.array([self.current_player])
        return np.concatenate([state, player_info])

class DQNAgent:
    """Deep Q-Network agent for game playing."""
    
    def __init__(self, state_size: int, action_size: int, learning_rate: float = 0.001):
        self.state_size = state_size
        self.action_size = action_size
        self.learning_rate = learning_rate
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.memory = deque(maxlen=10000)
        self.batch_size = 32
        
        # Neural network
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.q_network = self._build_network().to(self.device)
        self.target_network = self._build_network().to(self.device)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)
        
        # Update target network
        self.update_target_network()
    
    def _build_network(self) -> nn.Module:
        """Build the neural network."""
        return nn.Sequential(
            nn.Linear(self.state_size, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, self.action_size)
        )
    
    def remember(self, state: np.ndarray, action: int, reward: float, 
                 next_state: np.ndarray, done: bool):
        """Store experience in memory."""
        self.memory.append((state, action, reward, next_state, done))
    
    def act(self, state: np.ndarray, valid_actions: List[int]) -> int:
        """Choose action using epsilon-greedy policy."""
        if np.random.random() <= self.epsilon:
            return random.choice(valid_actions) if valid_actions else 0
        
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        q_values = self.q_network(state_tensor)
        
        # Mask invalid actions
        masked_q_values = q_values.clone()
        for i in range(self.action_size):
            if i not in valid_actions:
                masked_q_values[0, i] = float('-inf')
        
        return masked_q_values.argmax().item()
    
    def replay(self):
        """Train the model on a batch of experiences."""
        if len(self.memory) < self.batch_size:
            return
        
        batch = random.sample(self.memory, self.batch_size)
        states = torch.FloatTensor([e[0] for e in batch]).to(self.device)
        actions = torch.LongTensor([e[1] for e in batch]).to(self.device)
        rewards = torch.FloatTensor([e[2] for e in batch]).to(self.device)
        next_states = torch.FloatTensor([e[3] for e in batch]).to(self.device)
        dones = torch.BoolTensor([e[4] for e in batch]).to(self.device)
        
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
        next_q_values = self.target_network(next_states).max(1)[0].detach()
        target_q_values = rewards + (0.99 * next_q_values * ~dones)
        
        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)
        
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def update_target_network(self):
        """Update target network weights."""
        self.target_network.load_state_dict(self.q_network.state_dict())

class MCTSNode:
    """Monte Carlo Tree Search node."""
    
    def __init__(self, state: GameState, parent=None, action=None):
        self.state = state
        self.parent = parent
        self.action = action
        self.children = {}
        self.visits = 0
        self.wins = 0
        self.untried_actions = state.valid_actions.copy() if state.valid_actions else []
    
    def is_fully_expanded(self) -> bool:
        """Check if all actions have been tried."""
        return len(self.untried_actions) == 0
    
    def best_child(self, c_param: float = 1.4) -> 'MCTSNode':
        """Select best child using UCB1."""
        choices_weights = [
            (child.wins / child.visits) + c_param * np.sqrt(2 * np.log(self.visits) / child.visits)
            for child in self.children.values()
        ]
        return list(self.children.values())[np.argmax(choices_weights)]
    
    def expand(self, env: TicTacToeEnvironment) -> 'MCTSNode':
        """Expand tree by adding a new child."""
        action = self.untried_actions.pop()
        
        # Simulate the action
        env_copy = TicTacToeEnvironment()
        env_copy.board = self.state.board.copy()
        env_copy.current_player = self.state.current_player
        env_copy.move_count = self.state.move_count
        env_copy.game_over = self.state.game_over
        env_copy.winner = self.state.winner
        
        new_state, _, _ = env_copy.make_move(action)
        child_node = MCTSNode(new_state, parent=self, action=action)
        self.children[action] = child_node
        return child_node
    
    def update(self, result: float):
        """Update node statistics."""
        self.visits += 1
        self.wins += result

class MCTSAgent:
    """Monte Carlo Tree Search agent."""
    
    def __init__(self, iterations: int = 1000):
        self.iterations = iterations
    
    def get_action(self, game_state: GameState, env: TicTacToeEnvironment) -> Tuple[int, int]:
        """Get best action using MCTS."""
        if not game_state.valid_actions:
            return (0, 0)
        
        root = MCTSNode(game_state)
        
        for _ in range(self.iterations):
            # Selection
            node = self._select(root)
            
            # Expansion
            if not node.state.game_over and not node.is_fully_expanded():
                node = node.expand(env)
            
            # Simulation
            result = self._simulate(node.state, env)
            
            # Backpropagation
            self._backpropagate(node, result)
        
        # Choose best action
        best_child = root.best_child(c_param=0)
        return best_child.action
    
    def _select(self, node: MCTSNode) -> MCTSNode:
        """Select a node to expand."""
        while not node.state.game_over:
            if not node.is_fully_expanded():
                return node
            else:
                node = node.best_child()
        return node
    
    def _simulate(self, state: GameState, env: TicTacToeEnvironment) -> float:
        """Simulate random game from given state."""
        # Create environment copy
        env_copy = TicTacToeEnvironment()
        env_copy.board = state.board.copy()
        env_copy.current_player = state.current_player
        env_copy.move_count = state.move_count
        env_copy.game_over = state.game_over
        env_copy.winner = state.winner
        
        original_player = state.current_player
        
        # Random simulation
        while not env_copy.game_over:
            valid_actions = env_copy._get_valid_actions()
            if not valid_actions:
                break
            
            action = random.choice(valid_actions)
            _, _, _ = env_copy.make_move(action)
        
        # Return result from perspective of original player
        if env_copy.winner == original_player:
            return 1.0
        elif env_copy.winner == 0:
            return 0.5
        else:
            return 0.0
    
    def _backpropagate(self, node: MCTSNode, result: float):
        """Backpropagate result through tree."""
        while node is not None:
            node.update(result)
            result = 1 - result  # Flip perspective
            node = node.parent

class MultiAgentTrainer:
    """Trains multiple agents against each other."""
    
    def __init__(self):
        self.agents = {}
        self.performance_history = {}
        self.training_history = []
        
    def register_agent(self, agent_id: str, agent: Any):
        """Register an agent for training."""
        self.agents[agent_id] = agent
        self.performance_history[agent_id] = AgentPerformance(agent_id)
    
    def train_agents(self, episodes: int = 1000) -> Dict[str, Any]:
        """Train agents against each other."""
        env = TicTacToeEnvironment()
        agent_ids = list(self.agents.keys())
        
        if len(agent_ids) < 2:
            raise ValueError("Need at least 2 agents for training")
        
        training_stats = {
            'episode_rewards': {agent_id: [] for agent_id in agent_ids},
            'win_rates': {agent_id: [] for agent_id in agent_ids},
            'game_lengths': []
        }
        
        for episode in range(episodes):
            # Randomly select two agents
            player1_id, player2_id = random.sample(agent_ids, 2)
            player1 = self.agents[player1_id]
            player2 = self.agents[player2_id]
            
            # Play game
            game_result = self._play_game(env, player1, player2, player1_id, player2_id)
            
            # Update performance
            self._update_performance(game_result)
            
            # Log progress
            if episode % 100 == 0:
                logger.info(f"Episode {episode}: Training in progress...")
                
                for agent_id in agent_ids:
                    perf = self.performance_history[agent_id]
                    win_rate = perf.games_won / max(1, perf.games_played)
                    training_stats['win_rates'][agent_id].append(win_rate)
        
        return training_stats
    
    def _play_game(self, env: TicTacToeEnvironment, agent1: Any, agent2: Any,
                   agent1_id: str, agent2_id: str) -> Dict[str, Any]:
        """Play a single game between two agents."""
        state = env.reset()
        agents = [agent1, agent2]
        agent_ids = [agent1_id, agent2_id]
        current_agent_idx = 0
        
        game_moves = []
        total_reward = {agent1_id: 0, agent2_id: 0}
        
        while not state.game_over:
            current_agent = agents[current_agent_idx]
            current_agent_id = agent_ids[current_agent_idx]
            
            # Get action from agent
            if isinstance(current_agent, DQNAgent):
                # Convert 2D actions to 1D for DQN
                valid_actions_1d = [r * 3 + c for r, c in state.valid_actions]
                action_1d = current_agent.act(env.get_state_vector(), valid_actions_1d)
                action = (action_1d // 3, action_1d % 3)
            elif isinstance(current_agent, MCTSAgent):
                action = current_agent.get_action(state, env)
            else:
                # Random agent
                action = random.choice(state.valid_actions) if state.valid_actions else (0, 0)
            
            # Store previous state for learning
            prev_state = env.get_state_vector()
            
            # Make move
            new_state, reward, done = env.make_move(action)
            
            # Store move
            move = GameMove(
                move_id=str(uuid.uuid4()),
                player_id=current_agent_id,
                action=action,
                timestamp=datetime.now(),
                game_state_before=state,
                game_state_after=new_state
            )
            game_moves.append(move)
            
            # Update agent if it's a learning agent
            if isinstance(current_agent, DQNAgent):
                action_1d = action[0] * 3 + action[1]
                current_agent.remember(
                    prev_state, action_1d, reward, 
                    env.get_state_vector(), done
                )
                current_agent.replay()
            
            total_reward[current_agent_id] += reward
            state = new_state
            
            # Switch to other agent
            current_agent_idx = 1 - current_agent_idx
        
        return {
            'winner': state.winner,
            'moves': game_moves,
            'rewards': total_reward,
            'game_length': len(game_moves),
            'players': agent_ids
        }
    
    def _update_performance(self, game_result: Dict[str, Any]):
        """Update agent performance statistics."""
        winner = game_result['winner']
        players = game_result['players']
        
        for i, player_id in enumerate(players):
            perf = self.performance_history[player_id]
            perf.games_played += 1
            perf.total_reward += game_result['rewards'][player_id]
            
            if winner == i + 1:
                perf.games_won += 1
            elif winner == 0:
                perf.games_drawn += 1
            else:
                perf.games_lost += 1
            
            # Update win rate
            perf.win_rate = perf.games_won / perf.games_played
            
            # Update average game length
            perf.average_game_length = (
                (perf.average_game_length * (perf.games_played - 1) + game_result['game_length'])
                / perf.games_played
            )
    
    def get_performance_summary(self) -> pd.DataFrame:
        """Get performance summary as DataFrame."""
        data = []
        for agent_id, perf in self.performance_history.items():
            data.append({
                'Agent ID': agent_id,
                'Games Played': perf.games_played,
                'Win Rate': perf.win_rate,
                'Wins': perf.games_won,
                'Losses': perf.games_lost,
                'Draws': perf.games_drawn,
                'Avg Game Length': perf.average_game_length,
                'Total Reward': perf.total_reward
            })
        
        return pd.DataFrame(data)

class GameAnalyzer:
    """Analyzes game patterns and strategies."""
    
    def __init__(self):
        self.game_database = []
    
    def add_game(self, game_result: Dict[str, Any]):
        """Add game to analysis database."""
        self.game_database.append(game_result)
    
    def analyze_opening_moves(self) -> Dict[str, Any]:
        """Analyze common opening moves."""
        opening_stats = {}
        
        for game in self.game_database:
            if game['moves']:
                first_move = game['moves'][0].action
                move_key = f"{first_move[0]},{first_move[1]}"
                
                if move_key not in opening_stats:
                    opening_stats[move_key] = {'count': 0, 'wins': 0, 'losses': 0, 'draws': 0}
                
                opening_stats[move_key]['count'] += 1
                
                winner = game['winner']
                if winner == 1:  # First player won
                    opening_stats[move_key]['wins'] += 1
                elif winner == 0:  # Draw
                    opening_stats[move_key]['draws'] += 1
                else:  # First player lost
                    opening_stats[move_key]['losses'] += 1
        
        # Calculate win rates
        for stats in opening_stats.values():
            total = stats['count']
            stats['win_rate'] = stats['wins'] / total if total > 0 else 0
        
        return opening_stats
    
    def analyze_game_length_patterns(self) -> Dict[str, float]:
        """Analyze game length patterns."""
        if not self.game_database:
            return {}
        
        lengths = [game['game_length'] for game in self.game_database]
        
        return {
            'average_length': np.mean(lengths),
            'median_length': np.median(lengths),
            'min_length': np.min(lengths),
            'max_length': np.max(lengths),
            'std_length': np.std(lengths)
        }
    
    def generate_strategy_report(self) -> Dict[str, Any]:
        """Generate comprehensive strategy analysis report."""
        if not self.game_database:
            return {"message": "No games to analyze"}
        
        return {
            'total_games': len(self.game_database),
            'opening_analysis': self.analyze_opening_moves(),
            'length_analysis': self.analyze_game_length_patterns(),
            'player_performance': self._analyze_player_performance()
        }
    
    def _analyze_player_performance(self) -> Dict[str, Dict[str, Any]]:
        """Analyze individual player performance."""
        player_stats = {}
        
        for game in self.game_database:
            players = game['players']
            winner = game['winner']
            
            for i, player_id in enumerate(players):
                if player_id not in player_stats:
                    player_stats[player_id] = {
                        'games_played': 0,
                        'wins': 0,
                        'losses': 0,
                        'draws': 0,
                        'total_moves': 0
                    }
                
                stats = player_stats[player_id]
                stats['games_played'] += 1
                
                if winner == i + 1:
                    stats['wins'] += 1
                elif winner == 0:
                    stats['draws'] += 1
                else:
                    stats['losses'] += 1
                
                # Count moves by this player
                player_moves = sum(1 for move in game['moves'] if move.player_id == player_id)
                stats['total_moves'] += player_moves
        
        # Calculate derived statistics
        for stats in player_stats.values():
            total_games = stats['games_played']
            stats['win_rate'] = stats['wins'] / total_games if total_games > 0 else 0
            stats['avg_moves_per_game'] = stats['total_moves'] / total_games if total_games > 0 else 0
        
        return player_stats

def main():
    """Main Streamlit application."""
    st.set_page_config(
        page_title="Game Strategy Agent",
        page_icon="üéÆ",
        layout="wide"
    )
    
    st.title("üéÆ Multi-Agent Game Strategy System")
    st.markdown("Advanced AI agents for strategic game playing and analysis")
    
    # Initialize session state
    if 'trainer' not in st.session_state:
        st.session_state['trainer'] = MultiAgentTrainer()
    if 'analyzer' not in st.session_state:
        st.session_state['analyzer'] = GameAnalyzer()
    if 'training_results' not in st.session_state:
        st.session_state['training_results'] = None
    
    trainer = st.session_state['trainer']
    analyzer = st.session_state['analyzer']
    
    # Sidebar for agent configuration
    with st.sidebar:
        st.header("üîß Agent Configuration")
        
        # Agent setup
        st.subheader("Add Agents")
        
        agent_type = st.selectbox("Agent Type", ["DQN", "MCTS", "Random"])
        agent_name = st.text_input("Agent Name", f"{agent_type}_Agent_{len(trainer.agents)+1}")
        
        if st.button("Add Agent") and agent_name:
            if agent_type == "DQN":
                agent = DQNAgent(state_size=10, action_size=9)  # 3x3 board + player = 10, 9 positions
            elif agent_type == "MCTS":
                agent = MCTSAgent(iterations=500)
            else:  # Random
                agent = "random"
            
            trainer.register_agent(agent_name, agent)
            st.success(f"Added {agent_name}")
        
        # Display registered agents
        st.subheader("Registered Agents")
        for agent_id in trainer.agents.keys():
            st.write(f"‚Ä¢ {agent_id}")
        
        # Training configuration
        st.subheader("Training Settings")
        episodes = st.slider("Training Episodes", 100, 5000, 1000)
        
        if st.button("Start Training") and len(trainer.agents) >= 2:
            with st.spinner("Training agents..."):
                training_results = trainer.train_agents(episodes)
                st.session_state['training_results'] = training_results
                st.success("Training completed!")
    
    # Main tabs
    tab1, tab2, tab3, tab4, tab5 = st.tabs(["üèÜ Performance", "üéØ Training", "üéÆ Play Game", "üìä Analysis", "üß† Strategy"])
    
    with tab1:
        st.header("üèÜ Agent Performance")
        
        if trainer.performance_history:
            # Performance summary table
            perf_df = trainer.get_performance_summary()
            st.subheader("Performance Summary")
            st.dataframe(perf_df, use_container_width=True)
            
            # Performance visualizations
            if len(perf_df) > 0:
                col1, col2 = st.columns(2)
                
                with col1:
                    # Win rate comparison
                    fig_winrate = px.bar(
                        perf_df, 
                        x='Agent ID', 
                        y='Win Rate',
                        title="Agent Win Rates",
                        color='Win Rate',
                        color_continuous_scale='viridis'
                    )
                    st.plotly_chart(fig_winrate, use_container_width=True)
                
                with col2:
                    # Games played distribution
                    fig_games = px.pie(
                        perf_df, 
                        values='Games Played', 
                        names='Agent ID',
                        title="Games Played Distribution"
                    )
                    st.plotly_chart(fig_games, use_container_width=True)
                
                # Detailed performance metrics
                st.subheader("Detailed Metrics")
                
                col3, col4 = st.columns(2)
                
                with col3:
                    # Average game length
                    fig_length = px.bar(
                        perf_df, 
                        x='Agent ID', 
                        y='Avg Game Length',
                        title="Average Game Length"
                    )
                    st.plotly_chart(fig_length, use_container_width=True)
                
                with col4:
                    # Total reward
                    fig_reward = px.bar(
                        perf_df, 
                        x='Agent ID', 
                        y='Total Reward',
                        title="Total Reward Accumulated"
                    )
                    st.plotly_chart(fig_reward, use_container_width=True)
        else:
            st.info("No performance data available. Train agents first.")
    
    with tab2:
        st.header("üéØ Training Progress")
        
        training_results = st.session_state['training_results']
        
        if training_results:
            st.subheader("Training Statistics")
            
            # Win rate evolution
            if training_results['win_rates']:
                st.subheader("Win Rate Evolution")
                
                win_rate_data = []
                for agent_id, rates in training_results['win_rates'].items():
                    for i, rate in enumerate(rates):
                        win_rate_data.append({
                            'Episode': (i + 1) * 100,
                            'Win Rate': rate,
                            'Agent': agent_id
                        })
                
                if win_rate_data:
                    win_rate_df = pd.DataFrame(win_rate_data)
                    
                    fig_evolution = px.line(
                        win_rate_df,
                        x='Episode',
                        y='Win Rate',
                        color='Agent',
                        title="Win Rate Evolution During Training"
                    )
                    st.plotly_chart(fig_evolution, use_container_width=True)
            
            # Training summary
            st.subheader("Training Summary")
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("Episodes Completed", episodes)
            with col2:
                st.metric("Agents Trained", len(trainer.agents))
            with col3:
                total_games = sum(perf.games_played for perf in trainer.performance_history.values())
                st.metric("Total Games Played", total_games)
        else:
            st.info("No training data available. Start training to see results.")
    
    with tab3:
        st.header("üéÆ Play Against AI")
        
        if len(trainer.agents) > 0:
            # Game setup
            col1, col2 = st.columns(2)
            
            with col1:
                opponent = st.selectbox("Select Opponent", list(trainer.agents.keys()))
            
            with col2:
                if st.button("Start New Game"):
                    st.session_state['game_env'] = TicTacToeEnvironment()
                    st.session_state['game_state'] = st.session_state['game_env'].reset()
                    st.session_state['human_player'] = 1
                    st.session_state['ai_player'] = 2
                    st.success("New game started!")
            
            # Game display
            if 'game_env' in st.session_state:
                env = st.session_state['game_env']
                state = st.session_state['game_state']
                
                st.subheader("Game Board")
                
                # Display board
                board = state.board
                
                # Create clickable board
                for i in range(3):
                    cols = st.columns(3)
                    for j in range(3):
                        with cols[j]:
                            cell_value = ""
                            if board[i, j] == 1:
                                cell_value = "X"
                            elif board[i, j] == 2:
                                cell_value = "O"
                            
                            # Make move button
                            if st.button(f"{cell_value if cell_value else '‚Ä¢'}", 
                                       key=f"cell_{i}_{j}",
                                       disabled=(cell_value != "" or state.game_over)):
                                
                                if not state.game_over and state.current_player == st.session_state['human_player']:
                                    # Human move
                                    new_state, reward, done = env.make_move((i, j))
                                    st.session_state['game_state'] = new_state
                                    
                                    if not done:
                                        # AI move
                                        ai_agent = trainer.agents[opponent]
                                        
                                        if isinstance(ai_agent, DQNAgent):
                                            valid_actions_1d = [r * 3 + c for r, c in new_state.valid_actions]
                                            if valid_actions_1d:
                                                action_1d = ai_agent.act(env.get_state_vector(), valid_actions_1d)
                                                ai_action = (action_1d // 3, action_1d % 3)
                                            else:
                                                ai_action = (0, 0)
                                        elif isinstance(ai_agent, MCTSAgent):
                                            ai_action = ai_agent.get_action(new_state, env)
                                        else:
                                            ai_action = random.choice(new_state.valid_actions) if new_state.valid_actions else (0, 0)
                                        
                                        final_state, _, _ = env.make_move(ai_action)
                                        st.session_state['game_state'] = final_state
                                    
                                    st.rerun()
                
                # Game status
                if state.game_over:
                    if state.winner == st.session_state['human_player']:
                        st.success("üéâ You won!")
                    elif state.winner == st.session_state['ai_player']:
                        st.error("ü§ñ AI won!")
                    else:
                        st.info("ü§ù It's a draw!")
                else:
                    current_player = "Your turn" if state.current_player == st.session_state['human_player'] else "AI's turn"
                    st.info(f"Current player: {current_player}")
        else:
            st.info("Add agents first to play against them.")
    
    with tab4:
        st.header("üìä Game Analysis")
        
        if analyzer.game_database:
            analysis_report = analyzer.generate_strategy_report()
            
            st.subheader("Analysis Summary")
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("Total Games Analyzed", analysis_report['total_games'])
            with col2:
                length_stats = analysis_report['length_analysis']
                st.metric("Avg Game Length", f"{length_stats.get('average_length', 0):.1f}")
            with col3:
                st.metric("Unique Openings", len(analysis_report['opening_analysis']))
            
            # Opening move analysis
            st.subheader("Opening Move Analysis")
            opening_data = []
            for move, stats in analysis_report['opening_analysis'].items():
                row, col = move.split(',')
                opening_data.append({
                    'Position': f"({row}, {col})",
                    'Games': stats['count'],
                    'Win Rate': stats['win_rate'],
                    'Wins': stats['wins'],
                    'Losses': stats['losses'],
                    'Draws': stats['draws']
                })
            
            if opening_data:
                opening_df = pd.DataFrame(opening_data)
                st.dataframe(opening_df, use_container_width=True)
                
                # Opening move visualization
                fig_opening = px.bar(
                    opening_df,
                    x='Position',
                    y='Win Rate',
                    title="Opening Move Success Rates"
                )
                st.plotly_chart(fig_opening, use_container_width=True)
            
            # Game length distribution
            st.subheader("Game Length Distribution")
            lengths = [game['game_length'] for game in analyzer.game_database]
            
            fig_length_dist = px.histogram(
                x=lengths,
                title="Game Length Distribution",
                labels={'x': 'Game Length (moves)', 'y': 'Frequency'}
            )
            st.plotly_chart(fig_length_dist, use_container_width=True)
        else:
            st.info("No games to analyze. Play some games first.")
    
    with tab5:
        st.header("üß† Strategy Insights")
        
        st.subheader("Tic-Tac-Toe Strategy Guide")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**Optimal Opening Moves:**")
            st.write("‚Ä¢ Corner positions (0,0), (0,2), (2,0), (2,2)")
            st.write("‚Ä¢ Center position (1,1)")
            st.write("‚Ä¢ Avoid edge positions initially")
            
            st.write("**Winning Patterns:**")
            st.write("‚Ä¢ Three in a row horizontally")
            st.write("‚Ä¢ Three in a row vertically") 
            st.write("‚Ä¢ Three in a row diagonally")
        
        with col2:
            st.write("**Strategic Principles:**")
            st.write("‚Ä¢ Always take the center if available")
            st.write("‚Ä¢ Block opponent's winning moves")
            st.write("‚Ä¢ Create multiple winning threats")
            st.write("‚Ä¢ Force opponent into defensive positions")
            
            st.write("**Common Mistakes:**")
            st.write("‚Ä¢ Playing edge positions early")
            st.write("‚Ä¢ Not blocking obvious threats")
            st.write("‚Ä¢ Missing winning opportunities")
        
        # Strategy visualization
        st.subheader("Position Value Heatmap")
        
        # Create a simple position value matrix
        position_values = np.array([
            [3, 2, 3],
            [2, 4, 2], 
            [3, 2, 3]
        ])
        
        fig_heatmap = px.imshow(
            position_values,
            title="Strategic Value of Board Positions",
            color_continuous_scale="viridis",
            aspect="equal"
        )
        fig_heatmap.update_xaxes(title="Column")
        fig_heatmap.update_yaxes(title="Row")
        st.plotly_chart(fig_heatmap, use_container_width=True)
        
        st.write("**Legend:** Higher values indicate more strategically valuable positions")

if __name__ == "__main__":
    main()
````

## Project Summary

The Game Strategy Agent represents a sophisticated multi-agent reinforcement learning system that develops intelligent game-playing capabilities through competitive training, advanced search algorithms, and comprehensive strategy analysis for complex decision-making scenarios.

### Key Value Propositions:
- **Multi-Agent Learning Environment**: Competitive training between DQN, MCTS, and other AI agents for strategy development
- **Advanced Search Algorithms**: Monte Carlo Tree Search implementation for strategic planning and position evaluation
- **Comprehensive Game Analysis**: Pattern recognition, opening analysis, and strategic insight generation
- **Real-time Strategy Assessment**: Live game state evaluation with move prediction and recommendation systems

### Technical Architecture:
The system combines PyTorch for deep learning, custom MCTS implementation for strategic search, multi-agent training environments for competitive learning, and comprehensive analytics for strategy analysis, creating a scalable platform for game AI research and strategic decision-making applications across various domains.