<small>Claude Sonnet 4 **(Medical Research Summarizer - RAG Systém pro Biomedicínský Výzkum)**</small>
# Medical Research Summarizer

## Klíčové Koncepty

### **RAG (Retrieval-Augmented Generation)**
RAG je architektura kombinující vyhledávání relevantních dokumentů s generováním odpovědí pomocí jazykových modelů. Umožňuje využít externí znalostní báze pro přesnější a aktuálnější odpovědi.

### **LlamaIndex**
Framework pro vytváření aplikací s LLM, který se specializuje na indexování a dotazování strukturovaných dat. Poskytuje nástroje pro efektivní správu vektorových databází a kontextového vyhledávání.

### **PubMed API**
Rozhraní pro přístup k databázi biomedicínských publikací MEDLINE/PubMed obsahující miliony vědeckých článků z oblasti medicíny a příbuzných oborů.

### **BioBERT**
Specializovaný BERT model trénovaný na biomedicínských textech, který lépe rozumí medicínské terminologii a kontextu ve srovnání s obecnými jazykovými modely.

### **Vektorové Databáze**
Specializované databáze pro ukládání a vyhledávání vektorových reprezentací textů. Umožňují rychlé sémantické vyhledávání podobných dokumentů.

## Komplexní Vysvětlení Projektu

### **Cíle Projektu**
Medical Research Summarizer je pokročilý RAG systém navržený pro automatické shrnování a analýzu biomedicínského výzkumu. Hlavními cíli jsou:

1. **Automatické Vyhledávání**: Efektivní získávání relevantních vědeckých článků z PubMed databáze
2. **Inteligentní Sumarizace**: Vytváření strukturovaných shrnutí komplexních medicínských studií
3. **Kontextové Odpovědi**: Poskytování přesných odpovědí na specifické medicínské dotazy
4. **Aktuálnost Informací**: Využívání nejnovějších publikovaných výzkumů

### **Technické Výzvy**
- **Komplexita Medicínské Terminologie**: Nutnost správné interpretace odborných termínů
- **Velký Objem Dat**: Efektivní zpracování tisíců dokumentů
- **Přesnost Informací**: Kritická důležitost správnosti v medicínském kontextu
- **Rychlost Odpovědi**: Optimalizace pro praktické použití

### **Dopad a Využití**
Systém může revolutionizovat způsob, jakým zdravotničtí profesionálové, výzkumníci a studenti přistupují k vědecké literatuře, výrazně zkracuje čas potřebný pro research a zvyšuje kvalitu rozhodování.

## Komplexní Implementace v Pythonu

````python
import os
import asyncio
import logging
from typing import List, Dict, Optional
from dataclasses import dataclass
from datetime import datetime

# Core dependencies
import requests
import pandas as pd
from Bio import Entrez
import xml.etree.ElementTree as ET

# LLM and RAG frameworks
from llama_index.core import VectorStoreIndex, Document, Settings
from llama_index.core.node_parser import SentenceSplitter
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.openai import OpenAI
from llama_index.vector_stores.chroma import ChromaVectorStore

# Vector database
import chromadb

# Transformers for BioBERT
from transformers import AutoTokenizer, AutoModel
import torch

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ResearchPaper:
    """Datová třída pro vědecké články"""
    pmid: str
    title: str
    abstract: str
    authors: List[str]
    journal: str
    publication_date: str
    keywords: List[str]

class PubMedRetriever:
    """Třída pro vyhledávání článků v PubMed databázi"""
    
    def __init__(self, email: str):
        Entrez.email = email
        self.max_results = 50
    
    async def search_papers(self, query: str, max_results: int = None) -> List[ResearchPaper]:
        """Vyhledá články podle dotazu"""
        try:
            max_results = max_results or self.max_results
            
            # Vyhledání článků
            handle = Entrez.esearch(
                db="pubmed",
                term=query,
                retmax=max_results,
                sort="relevance"
            )
            search_results = Entrez.read(handle)
            handle.close()
            
            pmids = search_results["IdList"]
            if not pmids:
                logger.warning(f"Nenalezeny žádné články pro dotaz: {query}")
                return []
            
            # Získání detailů článků
            papers = await self._fetch_paper_details(pmids)
            logger.info(f"Získáno {len(papers)} článků pro dotaz: {query}")
            
            return papers
            
        except Exception as e:
            logger.error(f"Chyba při vyhledávání článků: {e}")
            return []
    
    async def _fetch_paper_details(self, pmids: List[str]) -> List[ResearchPaper]:
        """Získá detaily článků podle PMID"""
        papers = []
        
        try:
            # Získání XML dat
            handle = Entrez.efetch(
                db="pubmed",
                id=",".join(pmids),
                rettype="xml"
            )
            xml_data = handle.read()
            handle.close()
            
            # Parsování XML
            root = ET.fromstring(xml_data)
            
            for article in root.findall(".//PubmedArticle"):
                try:
                    paper = self._parse_article_xml(article)
                    if paper:
                        papers.append(paper)
                except Exception as e:
                    logger.warning(f"Chyba při parsování článku: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"Chyba při získávání detailů článků: {e}")
        
        return papers
    
    def _parse_article_xml(self, article) -> Optional[ResearchPaper]:
        """Parsuje XML elementu článku"""
        try:
            # PMID
            pmid_elem = article.find(".//PMID")
            pmid = pmid_elem.text if pmid_elem is not None else ""
            
            # Název
            title_elem = article.find(".//ArticleTitle")
            title = title_elem.text if title_elem is not None else ""
            
            # Abstrakt
            abstract_elem = article.find(".//AbstractText")
            abstract = abstract_elem.text if abstract_elem is not None else ""
            
            # Autoři
            authors = []
            for author in article.findall(".//Author"):
                lastname = author.find("LastName")
                forename = author.find("ForeName")
                if lastname is not None and forename is not None:
                    authors.append(f"{forename.text} {lastname.text}")
            
            # Časopis
            journal_elem = article.find(".//Journal/Title")
            journal = journal_elem.text if journal_elem is not None else ""
            
            # Datum publikace
            pub_date = article.find(".//PubDate/Year")
            publication_date = pub_date.text if pub_date is not None else ""
            
            # Klíčová slova
            keywords = []
            for keyword in article.findall(".//Keyword"):
                if keyword.text:
                    keywords.append(keyword.text)
            
            if title and abstract:
                return ResearchPaper(
                    pmid=pmid,
                    title=title,
                    abstract=abstract,
                    authors=authors,
                    journal=journal,
                    publication_date=publication_date,
                    keywords=keywords
                )
            
        except Exception as e:
            logger.error(f"Chyba při parsování článku: {e}")
        
        return None

class BioBERTEmbedding:
    """Vlastní embedding třída pro BioBERT"""
    
    def __init__(self, model_name: str = "dmis-lab/biobert-v1.1"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
    
    def get_embedding(self, text: str) -> List[float]:
        """Získá vektorovou reprezentaci textu"""
        try:
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                padding=True,
                max_length=512
            )
            
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                # Průměr posledních skrytých stavů
                embeddings = outputs.last_hidden_state.mean(dim=1)
                
            return embeddings.cpu().numpy().flatten().tolist()
            
        except Exception as e:
            logger.error(f"Chyba při vytváření embeddingu: {e}")
            return []

class MedicalRAGSystem:
    """Hlavní RAG systém pro medicínský výzkum"""
    
    def __init__(self, openai_api_key: str, email: str):
        # Konfigurace
        self.openai_api_key = openai_api_key
        os.environ["OPENAI_API_KEY"] = openai_api_key
        
        # Inicializace komponent
        self.pubmed_retriever = PubMedRetriever(email)
        self.biobert_embedding = BioBERTEmbedding()
        
        # Konfigurace LlamaIndex
        Settings.llm = OpenAI(
            model="gpt-4",
            temperature=0.1,
            system_prompt="""Jsi odborný asistent pro medicínský výzkum. 
            Poskytuj přesné, vědecky podložené odpovědi založené pouze na poskytnutých dokumentech.
            Vždy uveď zdroje a buď opatrný s medicínskými doporučeními."""
        )
        
        Settings.embed_model = HuggingFaceEmbedding(
            model_name="dmis-lab/biobert-v1.1"
        )
        
        # Vektorová databáze
        self.chroma_client = chromadb.PersistentClient(path="./medical_rag_db")
        self.collection = self.chroma_client.get_or_create_collection(
            name="medical_papers"
        )
        
        self.vector_store = ChromaVectorStore(chroma_collection=self.collection)
        self.index = None
        
        # Node parser
        self.node_parser = SentenceSplitter(
            chunk_size=512,
            chunk_overlap=50
        )
    
    async def build_knowledge_base(self, research_topics: List[str]):
        """Vytvoří znalostní bázi z PubMed článků"""
        logger.info("Začínám budování znalostní báze...")
        
        all_papers = []
        for topic in research_topics:
            logger.info(f"Vyhledávám články pro téma: {topic}")
            papers = await self.pubmed_retriever.search_papers(topic, max_results=20)
            all_papers.extend(papers)
        
        # Odstranění duplicit
        unique_papers = {paper.pmid: paper for paper in all_papers}.values()
        logger.info(f"Celkem získáno {len(unique_papers)} unikátních článků")
        
        # Vytvoření dokumentů pro LlamaIndex
        documents = []
        for paper in unique_papers:
            content = f"""
            Název: {paper.title}
            
            Autoři: {', '.join(paper.authors)}
            
            Časopis: {paper.journal}
            
            Rok publikace: {paper.publication_date}
            
            Abstrakt: {paper.abstract}
            
            Klíčová slova: {', '.join(paper.keywords)}
            """
            
            doc = Document(
                text=content.strip(),
                metadata={
                    "pmid": paper.pmid,
                    "title": paper.title,
                    "journal": paper.journal,
                    "year": paper.publication_date,
                    "authors": paper.authors
                }
            )
            documents.append(doc)
        
        # Vytvoření indexu
        self.index = VectorStoreIndex.from_documents(
            documents,
            vector_store=self.vector_store,
            node_parser=self.node_parser
        )
        
        logger.info("Znalostní báze úspěšně vytvořena!")
    
    def query_research(self, question: str, max_sources: int = 5) -> Dict:
        """Dotazuje se na znalostní bázi"""
        if not self.index:
            raise ValueError("Znalostní báze není vytvořena. Nejprve spusťte build_knowledge_base()")
        
        try:
            # Vytvoření query engine
            query_engine = self.index.as_query_engine(
                similarity_top_k=max_sources,
                response_mode="tree_summarize"
            )
            
            # Provedení dotazu
            response = query_engine.query(question)
            
            # Extrakce zdrojů
            sources = []
            for node in response.source_nodes:
                sources.append({
                    "title": node.metadata.get("title", ""),
                    "pmid": node.metadata.get("pmid", ""),
                    "journal": node.metadata.get("journal", ""),
                    "year": node.metadata.get("year", ""),
                    "relevance_score": node.score
                })
            
            return {
                "answer": str(response),
                "sources": sources,
                "query": question,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Chyba při dotazování: {e}")
            return {
                "error": str(e),
                "query": question,
                "timestamp": datetime.now().isoformat()
            }
    
    def summarize_topic(self, topic: str) -> Dict:
        """Vytvoří shrnutí na dané téma"""
        summary_query = f"""
        Vytvořte komplexní přehled současného stavu vědeckého poznání na téma '{topic}'.
        Zahrňte:
        1. Klíčové poznatky a trendy
        2. Metodologické přístupy
        3. Hlavní výzvy a omezení
        4. Budoucí směry výzkumu
        5. Klinické implikace (pokud jsou relevantní)
        
        Strukturujte odpověď přehledně a uveďte konkrétní studie jako podporu tvrzení.
        """
        
        return self.query_research(summary_query, max_sources=10)

async def main():
    """Hlavní funkce pro demonstraci systému"""
    
    # Konfigurace (v produkci použijte environment variables)
    OPENAI_API_KEY = "your-openai-api-key"
    EMAIL = "your-email@example.com"
    
    # Vytvoření RAG systému
    rag_system = MedicalRAGSystem(OPENAI_API_KEY, EMAIL)
    
    # Témata pro výzkum
    research_topics = [
        "COVID-19 vaccines effectiveness",
        "machine learning in medical diagnosis",
        "CRISPR gene therapy cancer treatment",
        "artificial intelligence radiology",
        "telemedicine remote patient monitoring"
    ]
    
    print("🔬 Medical Research Summarizer - RAG Systém")
    print("=" * 50)
    
    # Budování znalostní báze
    print("\n📚 Budování znalostní báze...")
    await rag_system.build_knowledge_base(research_topics)
    
    # Příklady dotazů
    queries = [
        "Jaká je účinnost COVID-19 vakcín proti novým variantám?",
        "Jak se používá umělá inteligence v radiologické diagnostice?",
        "Jaké jsou nejnovější pokroky v CRISPR terapii rakoviny?",
        "Výhody a nevýhody telemedicíny pro monitoring pacientů"
    ]
    
    print("\n🔍 Testování dotazů...")
    for i, query in enumerate(queries, 1):
        print(f"\n--- Dotaz {i} ---")
        print(f"❓ {query}")
        
        result = rag_system.query_research(query)
        
        if "error" in result:
            print(f"❌ Chyba: {result['error']}")
        else:
            print(f"💡 Odpověď:")
            print(result["answer"][:500] + "..." if len(result["answer"]) > 500 else result["answer"])
            
            print(f"\n📖 Zdroje ({len(result['sources'])}):")
            for j, source in enumerate(result["sources"][:3], 1):
                print(f"  {j}. {source['title']} ({source['year']})")
                print(f"     PMID: {source['pmid']}, Relevance: {source['relevance_score']:.3f}")
    
    # Shrnutí tématu
    print("\n📋 Vytváření shrnutí tématu...")
    topic_summary = rag_system.summarize_topic("artificial intelligence in healthcare")
    
    print("🎯 Shrnutí tématu 'AI ve zdravotnictví':")
    print(topic_summary["answer"][:800] + "..." if len(topic_summary["answer"]) > 800 else topic_summary["answer"])

if __name__ == "__main__":
    # Instalace závislostí
    print("Instalace závislostí...")
    requirements = """
    llama-index==0.9.0
    llama-index-embeddings-huggingface==0.1.0
    llama-index-llms-openai==0.1.0
    llama-index-vector-stores-chroma==0.1.0
    chromadb==0.4.0
    biopython==1.81
    transformers==4.35.0
    torch==2.1.0
    pandas==2.1.0
    requests==2.31.0
    openai==1.3.0
    """
    
    print("Spusťte následující příkazy pro instalaci:")
    print("pip install " + " ".join(req.strip() for req in requirements.strip().split('\n') if req.strip()))
    print("\nPoté nastavte své API klíče a spusťte program.")
    
    # Spuštění hlavní funkce
    # asyncio.run(main())
````

## Shrnutí Projektu

### **Klíčové Výhody**
- **Specializace na Medicínu**: Využívá BioBERT pro lepší porozumění medicínské terminologii
- **Aktuálnost Dat**: Přímé napojení na PubMed databázi zajišťuje nejnovější vědecké poznatky
- **Skalabilita**: Modulární architektura umožňuje snadné rozšíření o další zdroje dat
- **Transparentnost**: Každá odpověď obsahuje odkazy na zdrojové dokumenty

### **Technické Inovace**
- Kombinace obecného GPT-4 s specializovaným BioBERT embeddings
- Optimalizované chunking pro medicínské texty
- Pokročilé metadatové indexování pro lepší vyhledávání
- Asynchronní zpracování pro vysoký výkon

### **Budoucí Rozšíření**
- Integrace s dalšími medicínskými databázemi (ClinicalTrials.gov, Cochrane)
- Vizualizace trendů ve výzkumu
- Export výsledků do standardních formátů (PDF, citations)
- Vícejazyčná podpora pro globální výzkum

Tento systém představuje významný krok vpřed v automatizaci medicínského výzkumu a může výrazně urychlit proces evidence-based medicíny.