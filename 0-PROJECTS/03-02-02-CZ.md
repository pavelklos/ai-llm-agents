<small>Claude Sonnet 4 **(Medical Research Summarizer - RAG Syst√©m pro Biomedic√≠nsk√Ω V√Ωzkum)**</small>
# Medical Research Summarizer

## Kl√≠ƒçov√© Koncepty

### **RAG (Retrieval-Augmented Generation)**
RAG je architektura kombinuj√≠c√≠ vyhled√°v√°n√≠ relevantn√≠ch dokument≈Ø s generov√°n√≠m odpovƒõd√≠ pomoc√≠ jazykov√Ωch model≈Ø. Umo≈æ≈àuje vyu≈æ√≠t extern√≠ znalostn√≠ b√°ze pro p≈ôesnƒõj≈°√≠ a aktu√°lnƒõj≈°√≠ odpovƒõdi.

### **LlamaIndex**
Framework pro vytv√°≈ôen√≠ aplikac√≠ s LLM, kter√Ω se specializuje na indexov√°n√≠ a dotazov√°n√≠ strukturovan√Ωch dat. Poskytuje n√°stroje pro efektivn√≠ spr√°vu vektorov√Ωch datab√°z√≠ a kontextov√©ho vyhled√°v√°n√≠.

### **PubMed API**
Rozhran√≠ pro p≈ô√≠stup k datab√°zi biomedic√≠nsk√Ωch publikac√≠ MEDLINE/PubMed obsahuj√≠c√≠ miliony vƒõdeck√Ωch ƒçl√°nk≈Ø z oblasti medic√≠ny a p≈ô√≠buzn√Ωch obor≈Ø.

### **BioBERT**
Specializovan√Ω BERT model tr√©novan√Ω na biomedic√≠nsk√Ωch textech, kter√Ω l√©pe rozum√≠ medic√≠nsk√© terminologii a kontextu ve srovn√°n√≠ s obecn√Ωmi jazykov√Ωmi modely.

### **Vektorov√© Datab√°ze**
Specializovan√© datab√°ze pro ukl√°d√°n√≠ a vyhled√°v√°n√≠ vektorov√Ωch reprezentac√≠ text≈Ø. Umo≈æ≈àuj√≠ rychl√© s√©mantick√© vyhled√°v√°n√≠ podobn√Ωch dokument≈Ø.

## Komplexn√≠ Vysvƒõtlen√≠ Projektu

### **C√≠le Projektu**
Medical Research Summarizer je pokroƒçil√Ω RAG syst√©m navr≈æen√Ω pro automatick√© shrnov√°n√≠ a anal√Ωzu biomedic√≠nsk√©ho v√Ωzkumu. Hlavn√≠mi c√≠li jsou:

1. **Automatick√© Vyhled√°v√°n√≠**: Efektivn√≠ z√≠sk√°v√°n√≠ relevantn√≠ch vƒõdeck√Ωch ƒçl√°nk≈Ø z PubMed datab√°ze
2. **Inteligentn√≠ Sumarizace**: Vytv√°≈ôen√≠ strukturovan√Ωch shrnut√≠ komplexn√≠ch medic√≠nsk√Ωch studi√≠
3. **Kontextov√© Odpovƒõdi**: Poskytov√°n√≠ p≈ôesn√Ωch odpovƒõd√≠ na specifick√© medic√≠nsk√© dotazy
4. **Aktu√°lnost Informac√≠**: Vyu≈æ√≠v√°n√≠ nejnovƒõj≈°√≠ch publikovan√Ωch v√Ωzkum≈Ø

### **Technick√© V√Ωzvy**
- **Komplexita Medic√≠nsk√© Terminologie**: Nutnost spr√°vn√© interpretace odborn√Ωch term√≠n≈Ø
- **Velk√Ω Objem Dat**: Efektivn√≠ zpracov√°n√≠ tis√≠c≈Ø dokument≈Ø
- **P≈ôesnost Informac√≠**: Kritick√° d≈Øle≈æitost spr√°vnosti v medic√≠nsk√©m kontextu
- **Rychlost Odpovƒõdi**: Optimalizace pro praktick√© pou≈æit√≠

### **Dopad a Vyu≈æit√≠**
Syst√©m m≈Ø≈æe revolutionizovat zp≈Øsob, jak√Ωm zdravotniƒçt√≠ profesion√°lov√©, v√Ωzkumn√≠ci a studenti p≈ôistupuj√≠ k vƒõdeck√© literatu≈ôe, v√Ωraznƒõ zkracuje ƒças pot≈ôebn√Ω pro research a zvy≈°uje kvalitu rozhodov√°n√≠.

## Komplexn√≠ Implementace v Pythonu

````python
import os
import asyncio
import logging
from typing import List, Dict, Optional
from dataclasses import dataclass
from datetime import datetime

# Core dependencies
import requests
import pandas as pd
from Bio import Entrez
import xml.etree.ElementTree as ET

# LLM and RAG frameworks
from llama_index.core import VectorStoreIndex, Document, Settings
from llama_index.core.node_parser import SentenceSplitter
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.openai import OpenAI
from llama_index.vector_stores.chroma import ChromaVectorStore

# Vector database
import chromadb

# Transformers for BioBERT
from transformers import AutoTokenizer, AutoModel
import torch

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ResearchPaper:
    """Datov√° t≈ô√≠da pro vƒõdeck√© ƒçl√°nky"""
    pmid: str
    title: str
    abstract: str
    authors: List[str]
    journal: str
    publication_date: str
    keywords: List[str]

class PubMedRetriever:
    """T≈ô√≠da pro vyhled√°v√°n√≠ ƒçl√°nk≈Ø v PubMed datab√°zi"""
    
    def __init__(self, email: str):
        Entrez.email = email
        self.max_results = 50
    
    async def search_papers(self, query: str, max_results: int = None) -> List[ResearchPaper]:
        """Vyhled√° ƒçl√°nky podle dotazu"""
        try:
            max_results = max_results or self.max_results
            
            # Vyhled√°n√≠ ƒçl√°nk≈Ø
            handle = Entrez.esearch(
                db="pubmed",
                term=query,
                retmax=max_results,
                sort="relevance"
            )
            search_results = Entrez.read(handle)
            handle.close()
            
            pmids = search_results["IdList"]
            if not pmids:
                logger.warning(f"Nenalezeny ≈æ√°dn√© ƒçl√°nky pro dotaz: {query}")
                return []
            
            # Z√≠sk√°n√≠ detail≈Ø ƒçl√°nk≈Ø
            papers = await self._fetch_paper_details(pmids)
            logger.info(f"Z√≠sk√°no {len(papers)} ƒçl√°nk≈Ø pro dotaz: {query}")
            
            return papers
            
        except Exception as e:
            logger.error(f"Chyba p≈ôi vyhled√°v√°n√≠ ƒçl√°nk≈Ø: {e}")
            return []
    
    async def _fetch_paper_details(self, pmids: List[str]) -> List[ResearchPaper]:
        """Z√≠sk√° detaily ƒçl√°nk≈Ø podle PMID"""
        papers = []
        
        try:
            # Z√≠sk√°n√≠ XML dat
            handle = Entrez.efetch(
                db="pubmed",
                id=",".join(pmids),
                rettype="xml"
            )
            xml_data = handle.read()
            handle.close()
            
            # Parsov√°n√≠ XML
            root = ET.fromstring(xml_data)
            
            for article in root.findall(".//PubmedArticle"):
                try:
                    paper = self._parse_article_xml(article)
                    if paper:
                        papers.append(paper)
                except Exception as e:
                    logger.warning(f"Chyba p≈ôi parsov√°n√≠ ƒçl√°nku: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"Chyba p≈ôi z√≠sk√°v√°n√≠ detail≈Ø ƒçl√°nk≈Ø: {e}")
        
        return papers
    
    def _parse_article_xml(self, article) -> Optional[ResearchPaper]:
        """Parsuje XML elementu ƒçl√°nku"""
        try:
            # PMID
            pmid_elem = article.find(".//PMID")
            pmid = pmid_elem.text if pmid_elem is not None else ""
            
            # N√°zev
            title_elem = article.find(".//ArticleTitle")
            title = title_elem.text if title_elem is not None else ""
            
            # Abstrakt
            abstract_elem = article.find(".//AbstractText")
            abstract = abstract_elem.text if abstract_elem is not None else ""
            
            # Auto≈ôi
            authors = []
            for author in article.findall(".//Author"):
                lastname = author.find("LastName")
                forename = author.find("ForeName")
                if lastname is not None and forename is not None:
                    authors.append(f"{forename.text} {lastname.text}")
            
            # ƒåasopis
            journal_elem = article.find(".//Journal/Title")
            journal = journal_elem.text if journal_elem is not None else ""
            
            # Datum publikace
            pub_date = article.find(".//PubDate/Year")
            publication_date = pub_date.text if pub_date is not None else ""
            
            # Kl√≠ƒçov√° slova
            keywords = []
            for keyword in article.findall(".//Keyword"):
                if keyword.text:
                    keywords.append(keyword.text)
            
            if title and abstract:
                return ResearchPaper(
                    pmid=pmid,
                    title=title,
                    abstract=abstract,
                    authors=authors,
                    journal=journal,
                    publication_date=publication_date,
                    keywords=keywords
                )
            
        except Exception as e:
            logger.error(f"Chyba p≈ôi parsov√°n√≠ ƒçl√°nku: {e}")
        
        return None

class BioBERTEmbedding:
    """Vlastn√≠ embedding t≈ô√≠da pro BioBERT"""
    
    def __init__(self, model_name: str = "dmis-lab/biobert-v1.1"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
    
    def get_embedding(self, text: str) -> List[float]:
        """Z√≠sk√° vektorovou reprezentaci textu"""
        try:
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                padding=True,
                max_length=512
            )
            
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                # Pr≈Ømƒõr posledn√≠ch skryt√Ωch stav≈Ø
                embeddings = outputs.last_hidden_state.mean(dim=1)
                
            return embeddings.cpu().numpy().flatten().tolist()
            
        except Exception as e:
            logger.error(f"Chyba p≈ôi vytv√°≈ôen√≠ embeddingu: {e}")
            return []

class MedicalRAGSystem:
    """Hlavn√≠ RAG syst√©m pro medic√≠nsk√Ω v√Ωzkum"""
    
    def __init__(self, openai_api_key: str, email: str):
        # Konfigurace
        self.openai_api_key = openai_api_key
        os.environ["OPENAI_API_KEY"] = openai_api_key
        
        # Inicializace komponent
        self.pubmed_retriever = PubMedRetriever(email)
        self.biobert_embedding = BioBERTEmbedding()
        
        # Konfigurace LlamaIndex
        Settings.llm = OpenAI(
            model="gpt-4",
            temperature=0.1,
            system_prompt="""Jsi odborn√Ω asistent pro medic√≠nsk√Ω v√Ωzkum. 
            Poskytuj p≈ôesn√©, vƒõdecky podlo≈æen√© odpovƒõdi zalo≈æen√© pouze na poskytnut√Ωch dokumentech.
            V≈ædy uveƒè zdroje a buƒè opatrn√Ω s medic√≠nsk√Ωmi doporuƒçen√≠mi."""
        )
        
        Settings.embed_model = HuggingFaceEmbedding(
            model_name="dmis-lab/biobert-v1.1"
        )
        
        # Vektorov√° datab√°ze
        self.chroma_client = chromadb.PersistentClient(path="./medical_rag_db")
        self.collection = self.chroma_client.get_or_create_collection(
            name="medical_papers"
        )
        
        self.vector_store = ChromaVectorStore(chroma_collection=self.collection)
        self.index = None
        
        # Node parser
        self.node_parser = SentenceSplitter(
            chunk_size=512,
            chunk_overlap=50
        )
    
    async def build_knowledge_base(self, research_topics: List[str]):
        """Vytvo≈ô√≠ znalostn√≠ b√°zi z PubMed ƒçl√°nk≈Ø"""
        logger.info("Zaƒç√≠n√°m budov√°n√≠ znalostn√≠ b√°ze...")
        
        all_papers = []
        for topic in research_topics:
            logger.info(f"Vyhled√°v√°m ƒçl√°nky pro t√©ma: {topic}")
            papers = await self.pubmed_retriever.search_papers(topic, max_results=20)
            all_papers.extend(papers)
        
        # Odstranƒõn√≠ duplicit
        unique_papers = {paper.pmid: paper for paper in all_papers}.values()
        logger.info(f"Celkem z√≠sk√°no {len(unique_papers)} unik√°tn√≠ch ƒçl√°nk≈Ø")
        
        # Vytvo≈ôen√≠ dokument≈Ø pro LlamaIndex
        documents = []
        for paper in unique_papers:
            content = f"""
            N√°zev: {paper.title}
            
            Auto≈ôi: {', '.join(paper.authors)}
            
            ƒåasopis: {paper.journal}
            
            Rok publikace: {paper.publication_date}
            
            Abstrakt: {paper.abstract}
            
            Kl√≠ƒçov√° slova: {', '.join(paper.keywords)}
            """
            
            doc = Document(
                text=content.strip(),
                metadata={
                    "pmid": paper.pmid,
                    "title": paper.title,
                    "journal": paper.journal,
                    "year": paper.publication_date,
                    "authors": paper.authors
                }
            )
            documents.append(doc)
        
        # Vytvo≈ôen√≠ indexu
        self.index = VectorStoreIndex.from_documents(
            documents,
            vector_store=self.vector_store,
            node_parser=self.node_parser
        )
        
        logger.info("Znalostn√≠ b√°ze √∫spƒõ≈°nƒõ vytvo≈ôena!")
    
    def query_research(self, question: str, max_sources: int = 5) -> Dict:
        """Dotazuje se na znalostn√≠ b√°zi"""
        if not self.index:
            raise ValueError("Znalostn√≠ b√°ze nen√≠ vytvo≈ôena. Nejprve spus≈•te build_knowledge_base()")
        
        try:
            # Vytvo≈ôen√≠ query engine
            query_engine = self.index.as_query_engine(
                similarity_top_k=max_sources,
                response_mode="tree_summarize"
            )
            
            # Proveden√≠ dotazu
            response = query_engine.query(question)
            
            # Extrakce zdroj≈Ø
            sources = []
            for node in response.source_nodes:
                sources.append({
                    "title": node.metadata.get("title", ""),
                    "pmid": node.metadata.get("pmid", ""),
                    "journal": node.metadata.get("journal", ""),
                    "year": node.metadata.get("year", ""),
                    "relevance_score": node.score
                })
            
            return {
                "answer": str(response),
                "sources": sources,
                "query": question,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Chyba p≈ôi dotazov√°n√≠: {e}")
            return {
                "error": str(e),
                "query": question,
                "timestamp": datetime.now().isoformat()
            }
    
    def summarize_topic(self, topic: str) -> Dict:
        """Vytvo≈ô√≠ shrnut√≠ na dan√© t√©ma"""
        summary_query = f"""
        Vytvo≈ôte komplexn√≠ p≈ôehled souƒçasn√©ho stavu vƒõdeck√©ho pozn√°n√≠ na t√©ma '{topic}'.
        Zahr≈àte:
        1. Kl√≠ƒçov√© poznatky a trendy
        2. Metodologick√© p≈ô√≠stupy
        3. Hlavn√≠ v√Ωzvy a omezen√≠
        4. Budouc√≠ smƒõry v√Ωzkumu
        5. Klinick√© implikace (pokud jsou relevantn√≠)
        
        Strukturujte odpovƒõƒè p≈ôehlednƒõ a uveƒète konkr√©tn√≠ studie jako podporu tvrzen√≠.
        """
        
        return self.query_research(summary_query, max_sources=10)

async def main():
    """Hlavn√≠ funkce pro demonstraci syst√©mu"""
    
    # Konfigurace (v produkci pou≈æijte environment variables)
    OPENAI_API_KEY = "your-openai-api-key"
    EMAIL = "your-email@example.com"
    
    # Vytvo≈ôen√≠ RAG syst√©mu
    rag_system = MedicalRAGSystem(OPENAI_API_KEY, EMAIL)
    
    # T√©mata pro v√Ωzkum
    research_topics = [
        "COVID-19 vaccines effectiveness",
        "machine learning in medical diagnosis",
        "CRISPR gene therapy cancer treatment",
        "artificial intelligence radiology",
        "telemedicine remote patient monitoring"
    ]
    
    print("üî¨ Medical Research Summarizer - RAG Syst√©m")
    print("=" * 50)
    
    # Budov√°n√≠ znalostn√≠ b√°ze
    print("\nüìö Budov√°n√≠ znalostn√≠ b√°ze...")
    await rag_system.build_knowledge_base(research_topics)
    
    # P≈ô√≠klady dotaz≈Ø
    queries = [
        "Jak√° je √∫ƒçinnost COVID-19 vakc√≠n proti nov√Ωm variant√°m?",
        "Jak se pou≈æ√≠v√° umƒõl√° inteligence v radiologick√© diagnostice?",
        "Jak√© jsou nejnovƒõj≈°√≠ pokroky v CRISPR terapii rakoviny?",
        "V√Ωhody a nev√Ωhody telemedic√≠ny pro monitoring pacient≈Ø"
    ]
    
    print("\nüîç Testov√°n√≠ dotaz≈Ø...")
    for i, query in enumerate(queries, 1):
        print(f"\n--- Dotaz {i} ---")
        print(f"‚ùì {query}")
        
        result = rag_system.query_research(query)
        
        if "error" in result:
            print(f"‚ùå Chyba: {result['error']}")
        else:
            print(f"üí° Odpovƒõƒè:")
            print(result["answer"][:500] + "..." if len(result["answer"]) > 500 else result["answer"])
            
            print(f"\nüìñ Zdroje ({len(result['sources'])}):")
            for j, source in enumerate(result["sources"][:3], 1):
                print(f"  {j}. {source['title']} ({source['year']})")
                print(f"     PMID: {source['pmid']}, Relevance: {source['relevance_score']:.3f}")
    
    # Shrnut√≠ t√©matu
    print("\nüìã Vytv√°≈ôen√≠ shrnut√≠ t√©matu...")
    topic_summary = rag_system.summarize_topic("artificial intelligence in healthcare")
    
    print("üéØ Shrnut√≠ t√©matu 'AI ve zdravotnictv√≠':")
    print(topic_summary["answer"][:800] + "..." if len(topic_summary["answer"]) > 800 else topic_summary["answer"])

if __name__ == "__main__":
    # Instalace z√°vislost√≠
    print("Instalace z√°vislost√≠...")
    requirements = """
    llama-index==0.9.0
    llama-index-embeddings-huggingface==0.1.0
    llama-index-llms-openai==0.1.0
    llama-index-vector-stores-chroma==0.1.0
    chromadb==0.4.0
    biopython==1.81
    transformers==4.35.0
    torch==2.1.0
    pandas==2.1.0
    requests==2.31.0
    openai==1.3.0
    """
    
    print("Spus≈•te n√°sleduj√≠c√≠ p≈ô√≠kazy pro instalaci:")
    print("pip install " + " ".join(req.strip() for req in requirements.strip().split('\n') if req.strip()))
    print("\nPot√© nastavte sv√© API kl√≠ƒçe a spus≈•te program.")
    
    # Spu≈°tƒõn√≠ hlavn√≠ funkce
    # asyncio.run(main())
````

## Shrnut√≠ Projektu

### **Kl√≠ƒçov√© V√Ωhody**
- **Specializace na Medic√≠nu**: Vyu≈æ√≠v√° BioBERT pro lep≈°√≠ porozumƒõn√≠ medic√≠nsk√© terminologii
- **Aktu√°lnost Dat**: P≈ô√≠m√© napojen√≠ na PubMed datab√°zi zaji≈°≈•uje nejnovƒõj≈°√≠ vƒõdeck√© poznatky
- **Skalabilita**: Modul√°rn√≠ architektura umo≈æ≈àuje snadn√© roz≈°√≠≈ôen√≠ o dal≈°√≠ zdroje dat
- **Transparentnost**: Ka≈æd√° odpovƒõƒè obsahuje odkazy na zdrojov√© dokumenty

### **Technick√© Inovace**
- Kombinace obecn√©ho GPT-4 s specializovan√Ωm BioBERT embeddings
- Optimalizovan√© chunking pro medic√≠nsk√© texty
- Pokroƒçil√© metadatov√© indexov√°n√≠ pro lep≈°√≠ vyhled√°v√°n√≠
- Asynchronn√≠ zpracov√°n√≠ pro vysok√Ω v√Ωkon

### **Budouc√≠ Roz≈°√≠≈ôen√≠**
- Integrace s dal≈°√≠mi medic√≠nsk√Ωmi datab√°zemi (ClinicalTrials.gov, Cochrane)
- Vizualizace trend≈Ø ve v√Ωzkumu
- Export v√Ωsledk≈Ø do standardn√≠ch form√°t≈Ø (PDF, citations)
- V√≠cejazyƒçn√° podpora pro glob√°ln√≠ v√Ωzkum

Tento syst√©m p≈ôedstavuje v√Ωznamn√Ω krok vp≈ôed v automatizaci medic√≠nsk√©ho v√Ωzkumu a m≈Ø≈æe v√Ωraznƒõ urychlit proces evidence-based medic√≠ny.