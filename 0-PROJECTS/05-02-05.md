<small>Claude Sonnet 4 **(Code Debugging Agent)**</small>
# Code Debugging Agent

## Key Concepts Explanation

### Multi-Agent Architecture
Collaborative AI system employing specialized agents for different debugging tasks including syntax analysis, logic verification, performance optimization, and test generation, coordinated through agent orchestration frameworks to provide comprehensive code analysis and debugging solutions.

### Static Code Analysis
Automated code examination without execution, analyzing code structure, syntax patterns, potential vulnerabilities, code quality metrics, dependency analysis, and adherence to coding standards using AST parsing and pattern recognition techniques.

### Test Generation
Intelligent creation of unit tests, integration tests, and edge case scenarios based on code analysis, function signatures, and execution paths using AI-powered test case synthesis and coverage optimization strategies.

### Bug Pattern Recognition
Machine learning-based identification of common programming errors, anti-patterns, security vulnerabilities, and performance bottlenecks through trained models on large codebases and known bug databases.

### Code Quality Assessment
Comprehensive evaluation of code maintainability, readability, complexity metrics, documentation coverage, and adherence to best practices using established software engineering principles and automated quality gates.

## Comprehensive Project Explanation

### Objectives
The Code Debugging Agent automates software debugging processes through multi-agent collaboration, static analysis, and intelligent test generation to identify bugs, suggest fixes, optimize performance, and ensure code quality across multiple programming languages.

### Key Features
- **Multi-Agent Debugging Team**: Specialized agents for syntax, logic, security, and performance analysis
- **Intelligent Bug Detection**: AI-powered identification of common and complex programming errors
- **Automated Test Generation**: Comprehensive test suite creation with edge case coverage
- **Code Quality Analysis**: Detailed assessment of maintainability and best practices
- **Fix Suggestions**: AI-generated code corrections and optimization recommendations

### Challenges
- **Language Diversity**: Supporting multiple programming languages and frameworks
- **Context Understanding**: Maintaining code context across large codebases
- **False Positives**: Minimizing incorrect bug reports and suggestions
- **Performance Scaling**: Handling large repositories efficiently

### Potential Impact
This system can significantly reduce debugging time, improve code quality, enhance developer productivity, and serve as an educational tool for programming best practices.

## Comprehensive Project Example with Python Implementation

### Dependencies and Setup

````python
# requirements.txt
autogen==0.2.3
streamlit==1.29.0
langchain==0.1.0
langchain-openai==0.0.5
ast-monitor==0.0.5
pylint==3.0.3
flake8==6.1.0
pytest==7.4.3
coverage==7.3.2
bandit==1.7.5
pandas==2.1.4
plotly==5.17.0
networkx==3.2.1
````

### Core Implementation

````python
import ast
import os
import sys
import subprocess
import json
import logging
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from enum import Enum
import tempfile

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import networkx as nx

# Multi-agent framework
import autogen
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate

# Code analysis tools
import pylint.lint
from flake8.api import legacy as flake8
import bandit.core.manager as bandit_manager

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BugSeverity(Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

class AgentRole(Enum):
    SYNTAX_ANALYZER = "syntax_analyzer"
    LOGIC_REVIEWER = "logic_reviewer"
    SECURITY_AUDITOR = "security_auditor"
    PERFORMANCE_OPTIMIZER = "performance_optimizer"
    TEST_GENERATOR = "test_generator"
    COORDINATOR = "coordinator"

@dataclass
class Bug:
    bug_id: str
    severity: BugSeverity
    description: str
    line_number: int
    column: Optional[int]
    file_path: str
    bug_type: str
    suggested_fix: Optional[str] = None
    agent_source: Optional[AgentRole] = None

@dataclass
class CodeMetrics:
    lines_of_code: int
    cyclomatic_complexity: float
    maintainability_index: float
    test_coverage: float
    documentation_coverage: float
    technical_debt_ratio: float

@dataclass
class TestCase:
    test_name: str
    test_code: str
    test_type: str  # unit, integration, edge_case
    target_function: str
    expected_coverage: float

class StaticAnalyzer:
    """Performs static code analysis."""
    
    def __init__(self):
        self.ast_parser = None
    
    def analyze_syntax(self, code: str, file_path: str = "temp.py") -> List[Bug]:
        """Analyze code syntax using AST."""
        bugs = []
        
        try:
            tree = ast.parse(code)
            self.ast_parser = tree
            
            # Check for common syntax issues
            bugs.extend(self._check_naming_conventions(tree, file_path))
            bugs.extend(self._check_code_complexity(tree, file_path))
            bugs.extend(self._check_unused_imports(tree, code, file_path))
            
        except SyntaxError as e:
            bug = Bug(
                bug_id=f"syntax_{hash(str(e))}",
                severity=BugSeverity.CRITICAL,
                description=f"Syntax Error: {e.msg}",
                line_number=e.lineno or 0,
                column=e.offset,
                file_path=file_path,
                bug_type="syntax_error",
                agent_source=AgentRole.SYNTAX_ANALYZER
            )
            bugs.append(bug)
        
        return bugs
    
    def _check_naming_conventions(self, tree: ast.AST, file_path: str) -> List[Bug]:
        """Check naming conventions."""
        bugs = []
        
        class NamingVisitor(ast.NodeVisitor):
            def visit_FunctionDef(self, node):
                if not node.name.islower() or '__' in node.name[1:-1]:
                    bug = Bug(
                        bug_id=f"naming_{node.lineno}_{node.name}",
                        severity=BugSeverity.LOW,
                        description=f"Function '{node.name}' doesn't follow snake_case convention",
                        line_number=node.lineno,
                        column=node.col_offset,
                        file_path=file_path,
                        bug_type="naming_convention",
                        suggested_fix=f"Rename to '{self._to_snake_case(node.name)}'",
                        agent_source=AgentRole.SYNTAX_ANALYZER
                    )
                    bugs.append(bug)
                self.generic_visit(node)
            
            def visit_ClassDef(self, node):
                if not node.name[0].isupper():
                    bug = Bug(
                        bug_id=f"naming_{node.lineno}_{node.name}",
                        severity=BugSeverity.LOW,
                        description=f"Class '{node.name}' should use PascalCase",
                        line_number=node.lineno,
                        column=node.col_offset,
                        file_path=file_path,
                        bug_type="naming_convention",
                        suggested_fix=f"Rename to '{node.name.title()}'",
                        agent_source=AgentRole.SYNTAX_ANALYZER
                    )
                    bugs.append(bug)
                self.generic_visit(node)
            
            def _to_snake_case(self, name: str) -> str:
                import re
                return re.sub(r'(?<!^)(?=[A-Z])', '_', name).lower()
        
        visitor = NamingVisitor()
        visitor.visit(tree)
        return bugs
    
    def _check_code_complexity(self, tree: ast.AST, file_path: str) -> List[Bug]:
        """Check cyclomatic complexity."""
        bugs = []
        
        class ComplexityVisitor(ast.NodeVisitor):
            def visit_FunctionDef(self, node):
                complexity = self._calculate_complexity(node)
                if complexity > 10:
                    bug = Bug(
                        bug_id=f"complexity_{node.lineno}_{node.name}",
                        severity=BugSeverity.MEDIUM if complexity <= 15 else BugSeverity.HIGH,
                        description=f"Function '{node.name}' has high complexity ({complexity})",
                        line_number=node.lineno,
                        column=node.col_offset,
                        file_path=file_path,
                        bug_type="high_complexity",
                        suggested_fix="Consider breaking this function into smaller functions",
                        agent_source=AgentRole.SYNTAX_ANALYZER
                    )
                    bugs.append(bug)
                self.generic_visit(node)
            
            def _calculate_complexity(self, node: ast.FunctionDef) -> int:
                """Simple complexity calculation."""
                complexity = 1  # Base complexity
                
                for child in ast.walk(node):
                    if isinstance(child, (ast.If, ast.While, ast.For, ast.Try, ast.With)):
                        complexity += 1
                    elif isinstance(child, ast.BoolOp):
                        complexity += len(child.values) - 1
                
                return complexity
        
        visitor = ComplexityVisitor()
        visitor.visit(tree)
        return bugs
    
    def _check_unused_imports(self, tree: ast.AST, code: str, file_path: str) -> List[Bug]:
        """Check for unused imports."""
        bugs = []
        imports = []
        used_names = set()
        
        # Collect imports
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.append((alias.name, alias.asname or alias.name, node.lineno))
            elif isinstance(node, ast.ImportFrom):
                for alias in node.names:
                    imports.append((alias.name, alias.asname or alias.name, node.lineno))
        
        # Collect used names
        for node in ast.walk(tree):
            if isinstance(node, ast.Name):
                used_names.add(node.id)
            elif isinstance(node, ast.Attribute):
                # Handle module.function usage
                if isinstance(node.value, ast.Name):
                    used_names.add(node.value.id)
        
        # Check for unused imports
        for import_name, alias, line_no in imports:
            if alias not in used_names and import_name not in used_names:
                bug = Bug(
                    bug_id=f"unused_import_{line_no}_{alias}",
                    severity=BugSeverity.LOW,
                    description=f"Unused import: {alias}",
                    line_number=line_no,
                    column=0,
                    file_path=file_path,
                    bug_type="unused_import",
                    suggested_fix=f"Remove unused import: {alias}",
                    agent_source=AgentRole.SYNTAX_ANALYZER
                )
                bugs.append(bug)
        
        return bugs

class SecurityAnalyzer:
    """Analyzes code for security vulnerabilities."""
    
    def analyze_security(self, code: str, file_path: str = "temp.py") -> List[Bug]:
        """Analyze code for security issues."""
        bugs = []
        
        # Write code to temporary file for bandit analysis
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            f.write(code)
            temp_path = f.name
        
        try:
            # Use bandit for security analysis
            bugs.extend(self._run_bandit_analysis(temp_path, file_path))
            
            # Custom security checks
            bugs.extend(self._check_hardcoded_secrets(code, file_path))
            bugs.extend(self._check_sql_injection(code, file_path))
            
        finally:
            os.unlink(temp_path)
        
        return bugs
    
    def _run_bandit_analysis(self, temp_path: str, original_path: str) -> List[Bug]:
        """Run bandit security analysis."""
        bugs = []
        
        try:
            from bandit.core.config import BanditConfig
            from bandit.core.manager import BanditManager
            
            config = BanditConfig()
            manager = BanditManager(config, 'file')
            manager.discover_files([temp_path])
            manager.run_tests()
            
            for result in manager.get_issue_list():
                bug = Bug(
                    bug_id=f"security_{result.lineno}_{result.test_id}",
                    severity=self._bandit_severity_to_bug_severity(result.severity),
                    description=f"Security issue: {result.text}",
                    line_number=result.lineno,
                    column=0,
                    file_path=original_path,
                    bug_type="security_vulnerability",
                    suggested_fix=result.text,
                    agent_source=AgentRole.SECURITY_AUDITOR
                )
                bugs.append(bug)
        
        except Exception as e:
            logger.warning(f"Bandit analysis failed: {e}")
        
        return bugs
    
    def _bandit_severity_to_bug_severity(self, bandit_severity: str) -> BugSeverity:
        """Convert bandit severity to bug severity."""
        mapping = {
            'LOW': BugSeverity.LOW,
            'MEDIUM': BugSeverity.MEDIUM,
            'HIGH': BugSeverity.HIGH
        }
        return mapping.get(bandit_severity, BugSeverity.MEDIUM)
    
    def _check_hardcoded_secrets(self, code: str, file_path: str) -> List[Bug]:
        """Check for hardcoded secrets."""
        bugs = []
        
        # Simple patterns for common secrets
        patterns = [
            (r'password\s*=\s*["\'][^"\']+["\']', "Hardcoded password"),
            (r'api_key\s*=\s*["\'][^"\']+["\']', "Hardcoded API key"),
            (r'secret\s*=\s*["\'][^"\']+["\']', "Hardcoded secret"),
            (r'token\s*=\s*["\'][^"\']+["\']', "Hardcoded token")
        ]
        
        lines = code.split('\n')
        for line_no, line in enumerate(lines, 1):
            for pattern, description in patterns:
                if re.search(pattern, line, re.IGNORECASE):
                    bug = Bug(
                        bug_id=f"secret_{line_no}_{hash(line)}",
                        severity=BugSeverity.HIGH,
                        description=description,
                        line_number=line_no,
                        column=0,
                        file_path=file_path,
                        bug_type="hardcoded_secret",
                        suggested_fix="Use environment variables or secure configuration",
                        agent_source=AgentRole.SECURITY_AUDITOR
                    )
                    bugs.append(bug)
        
        return bugs
    
    def _check_sql_injection(self, code: str, file_path: str) -> List[Bug]:
        """Check for SQL injection vulnerabilities."""
        bugs = []
        
        # Simple SQL injection pattern detection
        sql_patterns = [
            r'execute\s*\(\s*["\'].*%.*["\']',
            r'query\s*=.*%.*',
            r'SELECT.*\+.*',
            r'INSERT.*\+.*'
        ]
        
        lines = code.split('\n')
        for line_no, line in enumerate(lines, 1):
            for pattern in sql_patterns:
                if re.search(pattern, line, re.IGNORECASE):
                    bug = Bug(
                        bug_id=f"sql_injection_{line_no}_{hash(line)}",
                        severity=BugSeverity.HIGH,
                        description="Potential SQL injection vulnerability",
                        line_number=line_no,
                        column=0,
                        file_path=file_path,
                        bug_type="sql_injection",
                        suggested_fix="Use parameterized queries or prepared statements",
                        agent_source=AgentRole.SECURITY_AUDITOR
                    )
                    bugs.append(bug)
        
        return bugs

class TestGenerator:
    """Generates test cases for code."""
    
    def __init__(self, llm: Optional[ChatOpenAI] = None):
        self.llm = llm
        self._initialize_prompts()
    
    def _initialize_prompts(self):
        """Initialize test generation prompts."""
        self.test_prompt = ChatPromptTemplate.from_template("""
        Generate comprehensive unit tests for the following Python function:
        
        {function_code}
        
        Generate tests that cover:
        1. Normal cases
        2. Edge cases
        3. Error conditions
        4. Boundary values
        
        Return the test code using pytest format.
        """)
    
    def generate_tests(self, code: str, file_path: str = "temp.py") -> List[TestCase]:
        """Generate test cases for the given code."""
        tests = []
        
        try:
            tree = ast.parse(code)
            functions = [node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
            
            for func in functions:
                if not func.name.startswith('_'):  # Skip private functions
                    test_code = self._generate_function_test(func, code)
                    
                    test_case = TestCase(
                        test_name=f"test_{func.name}",
                        test_code=test_code,
                        test_type="unit",
                        target_function=func.name,
                        expected_coverage=0.8
                    )
                    tests.append(test_case)
        
        except Exception as e:
            logger.error(f"Test generation error: {e}")
        
        return tests
    
    def _generate_function_test(self, func_node: ast.FunctionDef, full_code: str) -> str:
        """Generate test for a specific function."""
        if self.llm:
            try:
                func_code = ast.get_source_segment(full_code, func_node)
                response = self.llm.invoke(
                    self.test_prompt.format(function_code=func_code)
                )
                return response.content
            except Exception as e:
                logger.warning(f"LLM test generation failed: {e}")
        
        # Fallback to template-based generation
        return self._generate_template_test(func_node)
    
    def _generate_template_test(self, func_node: ast.FunctionDef) -> str:
        """Generate basic template test."""
        func_name = func_node.name
        args = [arg.arg for arg in func_node.args.args]
        
        test_template = f"""
def test_{func_name}():
    # Test normal case
    result = {func_name}({', '.join(['1' for _ in args])})
    assert result is not None
    
    # Test edge case
    # Add specific edge case tests here
    
    # Test error conditions
    # Add error condition tests here
"""
        return test_template

class MultiAgentDebugger:
    """Coordinates multiple debugging agents."""
    
    def __init__(self, openai_api_key: Optional[str] = None):
        self.static_analyzer = StaticAnalyzer()
        self.security_analyzer = SecurityAnalyzer()
        
        # Initialize LLM if API key provided
        self.llm = None
        if openai_api_key:
            self.llm = ChatOpenAI(
                temperature=0.2,
                model_name="gpt-4",
                openai_api_key=openai_api_key
            )
        
        self.test_generator = TestGenerator(self.llm)
        
        # Initialize agents using AutoGen
        self._initialize_agents()
    
    def _initialize_agents(self):
        """Initialize specialized debugging agents."""
        self.agents = {}
        
        # Configuration for agents
        config_list = []
        if self.llm:
            config_list = [{
                "model": "gpt-4",
                "api_key": self.llm.openai_api_key
            }]
        
        # Syntax analyzer agent
        self.agents[AgentRole.SYNTAX_ANALYZER] = autogen.AssistantAgent(
            name="syntax_analyzer",
            system_message="You are a code syntax analyzer. Focus on syntax errors, naming conventions, and code structure.",
            llm_config={"config_list": config_list} if config_list else False
        )
        
        # Logic reviewer agent
        self.agents[AgentRole.LOGIC_REVIEWER] = autogen.AssistantAgent(
            name="logic_reviewer",
            system_message="You are a logic reviewer. Identify logical errors, algorithmic issues, and flow problems.",
            llm_config={"config_list": config_list} if config_list else False
        )
        
        # Security auditor agent
        self.agents[AgentRole.SECURITY_AUDITOR] = autogen.AssistantAgent(
            name="security_auditor",
            system_message="You are a security auditor. Find security vulnerabilities and unsafe practices.",
            llm_config={"config_list": config_list} if config_list else False
        )
        
        # Coordinator agent
        self.agents[AgentRole.COORDINATOR] = autogen.AssistantAgent(
            name="coordinator",
            system_message="You coordinate the debugging process and synthesize findings from all agents.",
            llm_config={"config_list": config_list} if config_list else False
        )
    
    def debug_code(self, code: str, file_path: str = "temp.py") -> Dict[str, Any]:
        """Comprehensive code debugging using multiple agents."""
        results = {
            "bugs": [],
            "tests": [],
            "metrics": None,
            "agent_reports": {}
        }
        
        try:
            # Syntax analysis
            syntax_bugs = self.static_analyzer.analyze_syntax(code, file_path)
            results["bugs"].extend(syntax_bugs)
            results["agent_reports"][AgentRole.SYNTAX_ANALYZER] = {
                "bugs_found": len(syntax_bugs),
                "severity_distribution": self._analyze_severity_distribution(syntax_bugs)
            }
            
            # Security analysis
            security_bugs = self.security_analyzer.analyze_security(code, file_path)
            results["bugs"].extend(security_bugs)
            results["agent_reports"][AgentRole.SECURITY_AUDITOR] = {
                "bugs_found": len(security_bugs),
                "severity_distribution": self._analyze_severity_distribution(security_bugs)
            }
            
            # Test generation
            tests = self.test_generator.generate_tests(code, file_path)
            results["tests"] = tests
            results["agent_reports"][AgentRole.TEST_GENERATOR] = {
                "tests_generated": len(tests),
                "coverage_estimate": sum(t.expected_coverage for t in tests) / len(tests) if tests else 0
            }
            
            # Calculate metrics
            results["metrics"] = self._calculate_code_metrics(code)
            
            # Generate summary
            results["summary"] = self._generate_debugging_summary(results)
            
        except Exception as e:
            logger.error(f"Debugging error: {e}")
            results["error"] = str(e)
        
        return results
    
    def _analyze_severity_distribution(self, bugs: List[Bug]) -> Dict[str, int]:
        """Analyze severity distribution of bugs."""
        distribution = {severity.value: 0 for severity in BugSeverity}
        
        for bug in bugs:
            distribution[bug.severity.value] += 1
        
        return distribution
    
    def _calculate_code_metrics(self, code: str) -> CodeMetrics:
        """Calculate code quality metrics."""
        lines = len([line for line in code.split('\n') if line.strip()])
        
        # Simple complexity calculation
        complexity = code.count('if ') + code.count('for ') + code.count('while ') + 1
        
        return CodeMetrics(
            lines_of_code=lines,
            cyclomatic_complexity=complexity / max(1, code.count('def ')),
            maintainability_index=max(0, 100 - complexity * 2),
            test_coverage=0.0,  # Would need actual test execution
            documentation_coverage=code.count('"""') / max(1, code.count('def ')) * 100,
            technical_debt_ratio=min(100, complexity / lines * 100) if lines > 0 else 0
        )
    
    def _generate_debugging_summary(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """Generate debugging session summary."""
        bugs = results["bugs"]
        total_bugs = len(bugs)
        
        if total_bugs == 0:
            return {
                "status": "clean",
                "message": "No issues found in the code",
                "recommendations": ["Code appears to be well-written"]
            }
        
        critical_bugs = len([b for b in bugs if b.severity == BugSeverity.CRITICAL])
        high_bugs = len([b for b in bugs if b.severity == BugSeverity.HIGH])
        
        if critical_bugs > 0:
            status = "critical"
            message = f"Found {critical_bugs} critical issues that need immediate attention"
        elif high_bugs > 0:
            status = "high_priority"
            message = f"Found {high_bugs} high-priority issues to address"
        else:
            status = "minor_issues"
            message = f"Found {total_bugs} minor issues for improvement"
        
        recommendations = self._generate_recommendations(bugs, results["metrics"])
        
        return {
            "status": status,
            "message": message,
            "total_issues": total_bugs,
            "recommendations": recommendations
        }
    
    def _generate_recommendations(self, bugs: List[Bug], metrics: CodeMetrics) -> List[str]:
        """Generate recommendations based on findings."""
        recommendations = []
        
        # Bug-based recommendations
        if any(b.bug_type == "security_vulnerability" for b in bugs):
            recommendations.append("Review and fix security vulnerabilities immediately")
        
        if any(b.bug_type == "high_complexity" for b in bugs):
            recommendations.append("Consider refactoring complex functions for better maintainability")
        
        if any(b.bug_type == "unused_import" for b in bugs):
            recommendations.append("Remove unused imports to clean up the code")
        
        # Metrics-based recommendations
        if metrics.cyclomatic_complexity > 10:
            recommendations.append("High complexity detected - consider breaking down functions")
        
        if metrics.documentation_coverage < 50:
            recommendations.append("Add more documentation to improve code maintainability")
        
        if not recommendations:
            recommendations.append("Code quality looks good - consider adding more tests")
        
        return recommendations

def main():
    """Main Streamlit application."""
    st.set_page_config(
        page_title="Code Debugging Agent",
        page_icon="ðŸ›",
        layout="wide"
    )
    
    st.title("ðŸ› Multi-Agent Code Debugging System")
    st.markdown("AI-powered code analysis with specialized debugging agents")
    
    # Initialize session state
    if 'debugger' not in st.session_state:
        st.session_state['debugger'] = None
    if 'debug_results' not in st.session_state:
        st.session_state['debug_results'] = None
    
    # Sidebar
    with st.sidebar:
        st.header("ðŸ”§ Configuration")
        
        openai_key = st.text_input("OpenAI API Key (Optional)", type="password")
        
        if st.button("Initialize Debugger") or st.session_state['debugger'] is None:
            with st.spinner("Initializing multi-agent debugger..."):
                st.session_state['debugger'] = MultiAgentDebugger(openai_key)
                st.success("Debugger ready!")
        
        st.header("ðŸ“Š Quick Stats")
        if st.session_state['debug_results']:
            results = st.session_state['debug_results']
            st.metric("Total Issues", len(results.get('bugs', [])))
            st.metric("Tests Generated", len(results.get('tests', [])))
            
            metrics = results.get('metrics')
            if metrics:
                st.metric("Complexity", f"{metrics.cyclomatic_complexity:.2f}")
    
    if not st.session_state['debugger']:
        st.info("ðŸ‘ˆ Please initialize the debugger")
        return
    
    debugger = st.session_state['debugger']
    
    # Main tabs
    tab1, tab2, tab3, tab4 = st.tabs(["ðŸ” Code Analysis", "ðŸ§ª Test Generation", "ðŸ“Š Metrics", "ðŸ¥ Agent Reports"])
    
    with tab1:
        st.header("ðŸ” Code Analysis")
        
        # Code input methods
        input_method = st.radio("Code Input Method", ["Upload File", "Paste Code"])
        
        code_content = ""
        file_name = "temp.py"
        
        if input_method == "Upload File":
            uploaded_file = st.file_uploader("Upload Python file", type=['py'])
            if uploaded_file:
                code_content = uploaded_file.getvalue().decode()
                file_name = uploaded_file.name
        else:
            code_content = st.text_area("Paste your Python code:", height=300,
                                      placeholder="def example_function(x, y):\n    return x + y")
        
        if code_content and st.button("ðŸ” Analyze Code"):
            with st.spinner("Multi-agent analysis in progress..."):
                results = debugger.debug_code(code_content, file_name)
                st.session_state['debug_results'] = results
                
                if "error" in results:
                    st.error(f"Analysis failed: {results['error']}")
                else:
                    st.success("Analysis complete!")
        
        # Display results
        if st.session_state['debug_results'] and "error" not in st.session_state['debug_results']:
            results = st.session_state['debug_results']
            
            # Summary
            summary = results.get('summary', {})
            status = summary.get('status', 'unknown')
            
            if status == "critical":
                st.error(f"ðŸš¨ {summary['message']}")
            elif status == "high_priority":
                st.warning(f"âš ï¸ {summary['message']}")
            elif status == "minor_issues":
                st.info(f"â„¹ï¸ {summary['message']}")
            else:
                st.success(f"âœ… {summary['message']}")
            
            # Bugs found
            bugs = results.get('bugs', [])
            if bugs:
                st.subheader("ðŸ› Issues Found")
                
                # Group by severity
                severity_groups = {}
                for bug in bugs:
                    severity = bug.severity.value
                    if severity not in severity_groups:
                        severity_groups[severity] = []
                    severity_groups[severity].append(bug)
                
                # Display by severity
                for severity in ['critical', 'high', 'medium', 'low']:
                    if severity in severity_groups:
                        with st.expander(f"{severity.title()} Issues ({len(severity_groups[severity])})"):
                            for bug in severity_groups[severity]:
                                col1, col2 = st.columns([3, 1])
                                
                                with col1:
                                    st.write(f"**Line {bug.line_number}:** {bug.description}")
                                    if bug.suggested_fix:
                                        st.write(f"ðŸ’¡ **Fix:** {bug.suggested_fix}")
                                    st.write(f"ðŸ·ï¸ Type: {bug.bug_type}")
                                
                                with col2:
                                    st.write(f"Agent: {bug.agent_source.value if bug.agent_source else 'Unknown'}")
                                
                                st.divider()
            
            # Recommendations
            recommendations = summary.get('recommendations', [])
            if recommendations:
                st.subheader("ðŸ’¡ Recommendations")
                for rec in recommendations:
                    st.write(f"â€¢ {rec}")
    
    with tab2:
        st.header("ðŸ§ª Generated Tests")
        
        if st.session_state['debug_results']:
            tests = st.session_state['debug_results'].get('tests', [])
            
            if tests:
                st.write(f"Generated {len(tests)} test cases")
                
                for test in tests:
                    with st.expander(f"Test: {test.test_name}"):
                        st.write(f"**Target Function:** {test.target_function}")
                        st.write(f"**Test Type:** {test.test_type}")
                        st.write(f"**Expected Coverage:** {test.expected_coverage:.1%}")
                        
                        st.code(test.test_code, language="python")
                        
                        if st.button(f"Run {test.test_name}", key=f"run_{test.test_name}"):
                            st.info("Test execution would be implemented here")
            else:
                st.info("No tests generated yet. Analyze code first.")
        else:
            st.info("Run code analysis to generate tests")
    
    with tab3:
        st.header("ðŸ“Š Code Metrics")
        
        if st.session_state['debug_results']:
            metrics = st.session_state['debug_results'].get('metrics')
            
            if metrics:
                # Metrics overview
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric("Lines of Code", metrics.lines_of_code)
                with col2:
                    st.metric("Complexity", f"{metrics.cyclomatic_complexity:.2f}")
                with col3:
                    st.metric("Maintainability", f"{metrics.maintainability_index:.1f}")
                with col4:
                    st.metric("Documentation", f"{metrics.documentation_coverage:.1f}%")
                
                # Metrics visualization
                metrics_data = {
                    "Metric": ["Maintainability", "Documentation", "Technical Debt"],
                    "Score": [
                        metrics.maintainability_index,
                        metrics.documentation_coverage,
                        100 - metrics.technical_debt_ratio
                    ]
                }
                
                fig = px.bar(x=metrics_data["Metric"], y=metrics_data["Score"],
                           title="Code Quality Metrics", range_y=[0, 100])
                st.plotly_chart(fig, use_container_width=True)
                
                # Complexity analysis
                if metrics.cyclomatic_complexity > 10:
                    st.warning("âš ï¸ High complexity detected")
                elif metrics.cyclomatic_complexity > 5:
                    st.info("â„¹ï¸ Moderate complexity")
                else:
                    st.success("âœ… Low complexity - good!")
            else:
                st.info("No metrics available")
        else:
            st.info("Run code analysis to see metrics")
    
    with tab4:
        st.header("ðŸ¥ Agent Reports")
        
        if st.session_state['debug_results']:
            agent_reports = st.session_state['debug_results'].get('agent_reports', {})
            
            if agent_reports:
                for agent_role, report in agent_reports.items():
                    with st.expander(f"{agent_role.value.replace('_', ' ').title()} Report"):
                        
                        if agent_role == AgentRole.SYNTAX_ANALYZER:
                            st.write(f"**Bugs Found:** {report['bugs_found']}")
                            
                            severity_dist = report['severity_distribution']
                            if any(severity_dist.values()):
                                fig = px.pie(
                                    values=list(severity_dist.values()),
                                    names=list(severity_dist.keys()),
                                    title="Severity Distribution"
                                )
                                st.plotly_chart(fig, use_container_width=True)
                        
                        elif agent_role == AgentRole.SECURITY_AUDITOR:
                            st.write(f"**Security Issues:** {report['bugs_found']}")
                            
                            if report['bugs_found'] > 0:
                                st.error("Security vulnerabilities found!")
                            else:
                                st.success("No security issues detected")
                        
                        elif agent_role == AgentRole.TEST_GENERATOR:
                            st.write(f"**Tests Generated:** {report['tests_generated']}")
                            st.write(f"**Estimated Coverage:** {report['coverage_estimate']:.1%}")
                            
                            if report['coverage_estimate'] < 0.7:
                                st.warning("Consider adding more comprehensive tests")
            else:
                st.info("No agent reports available")
        else:
            st.info("Run code analysis to see agent reports")

if __name__ == "__main__":
    import re
    main()
````

## Project Summary

The Code Debugging Agent represents a sophisticated multi-agent system that revolutionizes software debugging through specialized AI agents, comprehensive static analysis, and intelligent test generation, providing developers with automated code quality assessment and debugging assistance.

### Key Value Propositions:
- **Multi-Agent Collaboration**: Specialized agents for syntax, security, logic, and testing working together for comprehensive code analysis
- **Intelligent Bug Detection**: AI-powered identification of syntax errors, security vulnerabilities, logic issues, and code quality problems
- **Automated Test Generation**: AI-generated unit tests with edge cases and error conditions using LLM-powered analysis
- **Real-time Code Quality Assessment**: Continuous metrics tracking including complexity, maintainability, and technical debt analysis

### Technical Architecture:
The system leverages AutoGen for multi-agent orchestration, AST parsing for syntax analysis, Bandit for security scanning, and GPT-4 integration for intelligent test generation, creating a scalable debugging platform that can be extended with additional specialized agents and analysis capabilities.