<small>Claude Sonnet 4 **(AI-Powered Research Paper Summarization (MCP-Driven))**</small>
# AI-Powered Research Paper Summarization (MCP-Driven)

## Kl√≠ƒçov√© koncepty

### Model Context Protocol (MCP)
**Model Context Protocol** je komunikaƒçn√≠ protokol vyvinut√Ω spoleƒçnost√≠ Anthropic, kter√Ω umo≈æ≈àuje jazykov√Ωm model≈Øm (LLM) bezpeƒçnƒõ a strukturovanƒõ p≈ôistupovat k extern√≠m zdroj≈Øm dat a n√°stroj≈Øm. MCP vytv√°≈ô√≠ standardizovan√© rozhran√≠ mezi AI modely a r≈Øzn√Ωmi datov√Ωmi zdroji, co≈æ umo≈æ≈àuje roz≈°√≠≈ôen√≠ schopnost√≠ AI bez p≈ô√≠m√© integrace.

### Academic NLP
**Academic Natural Language Processing** se zamƒõ≈ôuje na zpracov√°n√≠ vƒõdeck√Ωch text≈Ø a dokument≈Ø. Zahrnuje specifick√© techniky pro:
- Anal√Ωzu citac√≠ a referenc√≠
- Extrakci kl√≠ƒçov√Ωch pojm≈Ø a entit
- Identifikaci metodologie a v√Ωsledk≈Ø
- Strukturov√°n√≠ vƒõdeck√©ho obsahu

### Abstractive Summarization
**Abstraktivn√≠ sumarizace** je pokroƒçil√° technika, kter√° vytv√°≈ô√≠ nov√© shrnut√≠ textu pomoc√≠ vlastn√≠ch formulac√≠, na rozd√≠l od extraktivn√≠ sumarizace, kter√° pouze vyb√≠r√° existuj√≠c√≠ vƒõty. Vyu≈æ√≠v√° pokroƒçil√© jazykov√© modely pro porozumƒõn√≠ obsahu a jeho p≈ôeformulov√°n√≠.

### Citation Graphs
**Citaƒçn√≠ grafy** reprezentuj√≠ vztahy mezi vƒõdeck√Ωmi publikacemi prost≈ôednictv√≠m citac√≠. Umo≈æ≈àuj√≠ anal√Ωzu vlivu, trendy v√Ωzkumu a identifikaci kl√≠ƒçov√Ωch prac√≠ v dan√© oblasti.

### BERTopic
**BERTopic** je modern√≠ algoritmus pro topic modeling, kter√Ω vyu≈æ√≠v√° BERT embeddings pro identifikaci a klasifikaci t√©mat v kolekc√≠ch dokument≈Ø s vysokou p≈ôesnost√≠.

## Komplexn√≠ vysvƒõtlen√≠ projektu

### C√≠le projektu
Tento projekt vytv√°≈ô√≠ inteligentn√≠ syst√©m pro automatickou sumarizaci vƒõdeck√Ωch ƒçl√°nk≈Ø vyu≈æ√≠vaj√≠c√≠ Model Context Protocol. Syst√©m dok√°≈æe:

1. **Automaticky stahovat a analyzovat** vƒõdeck√© ƒçl√°nky z r≈Øzn√Ωch zdroj≈Ø
2. **Extrahovat kl√≠ƒçov√© informace** vƒçetnƒõ metodologie, v√Ωsledk≈Ø a z√°vƒõr≈Ø
3. **Generovat kvalitn√≠ abstraktivn√≠ shrnut√≠** p≈ôizp≈Øsoben√° r≈Øzn√Ωm c√≠lov√Ωm skupin√°m
4. **Analyzovat citaƒçn√≠ s√≠tƒõ** pro kontextu√°ln√≠ porozumƒõn√≠
5. **Identifikovat v√Ωzkumn√© trendy** pomoc√≠ topic modelingu

### V√Ωzvy projektu
- **Komplexnost vƒõdeck√©ho jazyka**: Technick√° terminologie a slo≈æit√© struktury
- **R≈Øznorodost form√°t≈Ø**: PDF, HTML, XML dokumenty s r≈Øzn√Ωm layoutem
- **Kvalita sumarizace**: Zachov√°n√≠ d≈Øle≈æit√Ωch informac√≠ p≈ôi zkr√°cen√≠
- **Kontextu√°ln√≠ porozumƒõn√≠**: Spr√°vn√° interpretace vƒõdeck√Ωch koncept≈Ø
- **≈†k√°lovatelnost**: Zpracov√°n√≠ velk√Ωch kolekc√≠ dokument≈Ø

### Potenci√°ln√≠ dopad
Syst√©m m≈Ø≈æe revolucionalizovat zp≈Øsob, jak√Ωm v√Ωzkumn√≠ci, studenti a odborn√≠ci p≈ôistupuj√≠ k vƒõdeck√© literatu≈ôe, v√Ωraznƒõ zkracuje ƒças pot≈ôebn√Ω pro p≈ôehled aktu√°ln√≠ho stavu pozn√°n√≠ v dan√© oblasti.

## Komplexn√≠ p≈ô√≠klad implementace

### Instalace z√°vislost√≠

````python
# Z√°kladn√≠ z√°vislosti pro AI-Powered Research Paper Summarization
langchain==0.1.0
langchain-openai==0.0.5
langchain-anthropic==0.1.0
chromadb==0.4.22
bertopic==0.15.0
sentence-transformers==2.2.2
arxiv==1.4.8
pypdf2==3.0.1
requests==2.31.0
beautifulsoup4==4.12.2
pandas==2.0.3
numpy==1.24.3
matplotlib==3.7.2
networkx==3.2.1
scikit-learn==1.3.0
openai==1.12.0
anthropic==0.15.0
python-dotenv==1.0.0
pydantic==2.6.1
````

### Hlavn√≠ implementace syst√©mu

````python
import os
import json
import logging
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from datetime import datetime
import asyncio

import arxiv
import requests
from bs4 import BeautifulSoup
import PyPDF2
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
import networkx as nx
from sklearn.metrics.pairwise import cosine_similarity

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain.prompts import ChatPromptTemplate
from langchain.schema import HumanMessage, SystemMessage
from langchain.chains import LLMChain
import chromadb
from chromadb.config import Settings

# Konfigurace logov√°n√≠
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ResearchPaper:
    """Datov√° struktura pro vƒõdeck√Ω ƒçl√°nek"""
    title: str
    authors: List[str]
    abstract: str
    content: str
    citations: List[str]
    keywords: List[str]
    publication_date: datetime
    source: str
    doi: Optional[str] = None
    arxiv_id: Optional[str] = None

@dataclass
class Summary:
    """Datov√° struktura pro shrnut√≠"""
    paper_id: str
    title: str
    executive_summary: str
    key_findings: List[str]
    methodology: str
    implications: str
    target_audience: str
    confidence_score: float
    generated_at: datetime

class MCPClient:
    """Model Context Protocol klient pro extern√≠ zdroje"""
    
    def __init__(self):
        self.arxiv_client = arxiv.Client()
        self.session = requests.Session()
        
    async def fetch_arxiv_papers(self, query: str, max_results: int = 10) -> List[ResearchPaper]:
        """Sta≈æen√≠ ƒçl√°nk≈Ø z ArXiv"""
        try:
            search = arxiv.Search(
                query=query,
                max_results=max_results,
                sort_by=arxiv.SortCriterion.SubmittedDate
            )
            
            papers = []
            for result in self.arxiv_client.results(search):
                paper = ResearchPaper(
                    title=result.title,
                    authors=[author.name for author in result.authors],
                    abstract=result.summary,
                    content=await self._download_pdf_content(result.pdf_url),
                    citations=await self._extract_citations(result.summary),
                    keywords=await self._extract_keywords(result.title + " " + result.summary),
                    publication_date=result.published,
                    source="ArXiv",
                    arxiv_id=result.entry_id.split('/')[-1]
                )
                papers.append(paper)
                
            logger.info(f"Sta≈æeno {len(papers)} ƒçl√°nk≈Ø z ArXiv")
            return papers
            
        except Exception as e:
            logger.error(f"Chyba p≈ôi stahov√°n√≠ z ArXiv: {e}")
            return []
    
    async def _download_pdf_content(self, pdf_url: str) -> str:
        """Sta≈æen√≠ a extrakce textu z PDF"""
        try:
            response = self.session.get(pdf_url, timeout=30)
            response.raise_for_status()
            
            # Simulace extrakce textu (v praxi by se pou≈æil pokroƒçilej≈°√≠ parser)
            return f"Obsah PDF dokumentu z {pdf_url} - simulovan√Ω text pro demonstraci."
            
        except Exception as e:
            logger.error(f"Chyba p≈ôi stahov√°n√≠ PDF: {e}")
            return ""
    
    async def _extract_citations(self, text: str) -> List[str]:
        """Extrakce citac√≠ z textu"""
        # Jednoduch√° implementace - v praxi by se pou≈æily pokroƒçilej≈°√≠ NLP techniky
        citations = []
        # Simulace extrakce citac√≠
        if "et al." in text:
            citations.append("Smith et al. (2023)")
        if "journal" in text.lower():
            citations.append("Nature (2023)")
        return citations
    
    async def _extract_keywords(self, text: str) -> List[str]:
        """Extrakce kl√≠ƒçov√Ωch slov"""
        # Jednoduch√° implementace zalo≈æen√° na frekvenci slov
        words = text.lower().split()
        keywords = [word for word in words if len(word) > 5]
        return keywords[:10]

class TopicAnalyzer:
    """Analyz√°tor t√©mat pomoc√≠ BERTopic"""
    
    def __init__(self):
        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.topic_model = None
        
    def analyze_topics(self, papers: List[ResearchPaper]) -> Dict[str, Any]:
        """Anal√Ωza t√©mat v kolekci ƒçl√°nk≈Ø"""
        try:
            # P≈ô√≠prava textov√Ωch dat
            documents = [f"{paper.title} {paper.abstract}" for paper in papers]
            
            # Vytvo≈ôen√≠ topic modelu
            self.topic_model = BERTopic(
                embedding_model=self.sentence_model,
                nr_topics="auto",
                verbose=True
            )
            
            # Tr√©nov√°n√≠ modelu
            topics, probabilities = self.topic_model.fit_transform(documents)
            
            # Anal√Ωza v√Ωsledk≈Ø
            topic_info = self.topic_model.get_topic_info()
            
            return {
                "topics": topics,
                "probabilities": probabilities,
                "topic_info": topic_info.to_dict(),
                "dominant_topics": self._get_dominant_topics(),
                "topic_keywords": self._get_topic_keywords()
            }
            
        except Exception as e:
            logger.error(f"Chyba p≈ôi anal√Ωze t√©mat: {e}")
            return {}
    
    def _get_dominant_topics(self) -> List[Dict]:
        """Z√≠sk√°n√≠ dominantn√≠ch t√©mat"""
        if not self.topic_model:
            return []
            
        topic_info = self.topic_model.get_topic_info()
        return topic_info.head(5).to_dict('records')
    
    def _get_topic_keywords(self) -> Dict[int, List[str]]:
        """Z√≠sk√°n√≠ kl√≠ƒçov√Ωch slov pro ka≈æd√© t√©ma"""
        if not self.topic_model:
            return {}
            
        keywords = {}
        for topic_id in range(len(self.topic_model.get_topics())):
            if topic_id != -1:  # Vylouƒçen√≠ outlier t√©matu
                topic_words = self.topic_model.get_topic(topic_id)
                keywords[topic_id] = [word for word, _ in topic_words[:10]]
        
        return keywords

class CitationAnalyzer:
    """Analyz√°tor citaƒçn√≠ch s√≠t√≠"""
    
    def __init__(self):
        self.citation_graph = nx.DiGraph()
    
    def build_citation_network(self, papers: List[ResearchPaper]) -> Dict[str, Any]:
        """Sestaven√≠ citaƒçn√≠ s√≠tƒõ"""
        try:
            # P≈ôid√°n√≠ uzl≈Ø (ƒçl√°nk≈Ø)
            for paper in papers:
                self.citation_graph.add_node(
                    paper.title,
                    authors=paper.authors,
                    publication_date=paper.publication_date,
                    source=paper.source
                )
            
            # P≈ôid√°n√≠ hran (citac√≠)
            for paper in papers:
                for citation in paper.citations:
                    if citation in [p.title for p in papers]:
                        self.citation_graph.add_edge(paper.title, citation)
            
            # Anal√Ωza s√≠tƒõ
            analysis = {
                "node_count": self.citation_graph.number_of_nodes(),
                "edge_count": self.citation_graph.number_of_edges(),
                "density": nx.density(self.citation_graph),
                "centrality": self._calculate_centrality(),
                "clusters": self._detect_clusters(),
                "influential_papers": self._get_influential_papers()
            }
            
            return analysis
            
        except Exception as e:
            logger.error(f"Chyba p≈ôi anal√Ωze citac√≠: {e}")
            return {}
    
    def _calculate_centrality(self) -> Dict[str, float]:
        """V√Ωpoƒçet centrality uzl≈Ø"""
        try:
            return nx.pagerank(self.citation_graph)
        except:
            return {}
    
    def _detect_clusters(self) -> List[List[str]]:
        """Detekce klastr≈Ø v s√≠ti"""
        try:
            # Konverze na neorientovan√Ω graf pro clustering
            undirected_graph = self.citation_graph.to_undirected()
            clusters = list(nx.connected_components(undirected_graph))
            return [list(cluster) for cluster in clusters]
        except:
            return []
    
    def _get_influential_papers(self, top_k: int = 5) -> List[str]:
        """Identifikace nejvlivnƒõj≈°√≠ch ƒçl√°nk≈Ø"""
        centrality = self._calculate_centrality()
        if not centrality:
            return []
        
        sorted_papers = sorted(centrality.items(), key=lambda x: x[1], reverse=True)
        return [paper[0] for paper in sorted_papers[:top_k]]

class AbstractiveSummarizer:
    """Abstraktivn√≠ sumariz√°tor vyu≈æ√≠vaj√≠c√≠ LLM"""
    
    def __init__(self, model_name: str = "gpt-3.5-turbo"):
        self.llm = ChatOpenAI(
            model_name=model_name,
            temperature=0.3,
            max_tokens=1500
        )
        
        self.summary_prompt = ChatPromptTemplate.from_messages([
            SystemMessage(content="""Jsi expert na sumarizaci vƒõdeck√Ωch ƒçl√°nk≈Ø. 
            Vytvo≈ô kvalitn√≠, strukturovan√© shrnut√≠ n√°sleduj√≠c√≠ho vƒõdeck√©ho ƒçl√°nku v ƒçe≈°tinƒõ.
            
            Struktura shrnut√≠:
            1. Struƒçn√Ω p≈ôehled (2-3 vƒõty)
            2. Kl√≠ƒçov√© zji≈°tƒõn√≠ (3-5 bod≈Ø)
            3. Metodologie (1-2 vƒõty)
            4. Praktick√© d≈Øsledky (2-3 vƒõty)
            
            Pou≈æij jasn√Ω, srozumiteln√Ω jazyk vhodn√Ω pro odbornou ve≈ôejnost."""),
            HumanMessage(content="""
            N√°zev: {title}
            Auto≈ôi: {authors}
            Abstrakt: {abstract}
            Kl√≠ƒçov√° slova: {keywords}
            
            Vytvo≈ô strukturovan√© shrnut√≠ tohoto ƒçl√°nku:
            """)
        ])
        
        self.chain = LLMChain(llm=self.llm, prompt=self.summary_prompt)
    
    async def summarize_paper(self, paper: ResearchPaper, target_audience: str = "general") -> Summary:
        """Vytvo≈ôen√≠ shrnut√≠ ƒçl√°nku"""
        try:
            # P≈ô√≠prava vstupn√≠ch dat
            input_data = {
                "title": paper.title,
                "authors": ", ".join(paper.authors),
                "abstract": paper.abstract,
                "keywords": ", ".join(paper.keywords)
            }
            
            # Generov√°n√≠ shrnut√≠
            result = await self.chain.arun(**input_data)
            
            # Parsov√°n√≠ v√Ωsledku
            summary_parts = self._parse_summary(result)
            
            # V√Ωpoƒçet sk√≥re spolehlivosti
            confidence_score = self._calculate_confidence(paper, summary_parts)
            
            summary = Summary(
                paper_id=paper.arxiv_id or paper.title,
                title=paper.title,
                executive_summary=summary_parts.get("overview", ""),
                key_findings=summary_parts.get("findings", []),
                methodology=summary_parts.get("methodology", ""),
                implications=summary_parts.get("implications", ""),
                target_audience=target_audience,
                confidence_score=confidence_score,
                generated_at=datetime.now()
            )
            
            return summary
            
        except Exception as e:
            logger.error(f"Chyba p≈ôi sumarizaci: {e}")
            return None
    
    def _parse_summary(self, text: str) -> Dict[str, Any]:
        """Parsov√°n√≠ strukturovan√©ho shrnut√≠"""
        parts = {
            "overview": "",
            "findings": [],
            "methodology": "",
            "implications": ""
        }
        
        # Jednoduch√Ω parser - v praxi by se pou≈æily pokroƒçilej≈°√≠ techniky
        lines = text.split('\n')
        current_section = None
        
        for line in lines:
            line = line.strip()
            if "p≈ôehled" in line.lower():
                current_section = "overview"
            elif "zji≈°tƒõn√≠" in line.lower() or "findings" in line.lower():
                current_section = "findings"
            elif "metodologie" in line.lower():
                current_section = "methodology"
            elif "d≈Øsledky" in line.lower() or "implications" in line.lower():
                current_section = "implications"
            elif line and current_section:
                if current_section == "findings":
                    parts[current_section].append(line)
                else:
                    parts[current_section] += line + " "
        
        return parts
    
    def _calculate_confidence(self, paper: ResearchPaper, summary_parts: Dict) -> float:
        """V√Ωpoƒçet sk√≥re spolehlivosti shrnut√≠"""
        # Jednoduch√° metrika zalo≈æen√° na dostupnosti dat
        score = 0.5  # Z√°kladn√≠ sk√≥re
        
        if paper.abstract:
            score += 0.2
        if paper.citations:
            score += 0.1
        if paper.keywords:
            score += 0.1
        if summary_parts.get("findings"):
            score += 0.1
        
        return min(score, 1.0)

class VectorStore:
    """Vektorov√° datab√°ze pro ukl√°d√°n√≠ a vyhled√°v√°n√≠"""
    
    def __init__(self, collection_name: str = "research_papers"):
        self.client = chromadb.Client(Settings(anonymized_telemetry=False))
        self.collection = self.client.create_collection(
            name=collection_name,
            get_or_create=True
        )
        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
    
    def add_papers(self, papers: List[ResearchPaper]):
        """P≈ôid√°n√≠ ƒçl√°nk≈Ø do vektorov√© datab√°ze"""
        try:
            documents = []
            metadatas = []
            ids = []
            
            for i, paper in enumerate(papers):
                # Vytvo≈ôen√≠ dokumentu pro embedding
                doc_text = f"{paper.title} {paper.abstract}"
                documents.append(doc_text)
                
                # Metadata
                metadata = {
                    "title": paper.title,
                    "authors": json.dumps(paper.authors),
                    "source": paper.source,
                    "publication_date": paper.publication_date.isoformat(),
                    "keywords": json.dumps(paper.keywords)
                }
                metadatas.append(metadata)
                
                # ID
                paper_id = paper.arxiv_id or f"paper_{i}"
                ids.append(paper_id)
            
            # P≈ôid√°n√≠ do kolekce
            self.collection.add(
                documents=documents,
                metadatas=metadatas,
                ids=ids
            )
            
            logger.info(f"P≈ôid√°no {len(papers)} ƒçl√°nk≈Ø do vektorov√© datab√°ze")
            
        except Exception as e:
            logger.error(f"Chyba p≈ôi p≈ôid√°v√°n√≠ do datab√°ze: {e}")
    
    def search_similar_papers(self, query: str, n_results: int = 5) -> List[Dict]:
        """Vyhled√°n√≠ podobn√Ωch ƒçl√°nk≈Ø"""
        try:
            results = self.collection.query(
                query_texts=[query],
                n_results=n_results
            )
            
            return results
            
        except Exception as e:
            logger.error(f"Chyba p≈ôi vyhled√°v√°n√≠: {e}")
            return []

class ResearchSummarizationSystem:
    """Hlavn√≠ syst√©m pro sumarizaci v√Ωzkumn√Ωch ƒçl√°nk≈Ø"""
    
    def __init__(self):
        self.mcp_client = MCPClient()
        self.topic_analyzer = TopicAnalyzer()
        self.citation_analyzer = CitationAnalyzer()
        self.summarizer = AbstractiveSummarizer()
        self.vector_store = VectorStore()
        
    async def process_research_query(self, query: str, max_papers: int = 10) -> Dict[str, Any]:
        """Kompletn√≠ zpracov√°n√≠ v√Ωzkumn√©ho dotazu"""
        try:
            logger.info(f"Zpracov√°v√°m dotaz: {query}")
            
            # 1. Sta≈æen√≠ ƒçl√°nk≈Ø pomoc√≠ MCP
            papers = await self.mcp_client.fetch_arxiv_papers(query, max_papers)
            if not papers:
                return {"error": "Nepoda≈ôilo se naj√≠t ≈æ√°dn√© ƒçl√°nky"}
            
            # 2. Ulo≈æen√≠ do vektorov√© datab√°ze
            self.vector_store.add_papers(papers)
            
            # 3. Anal√Ωza t√©mat
            topic_analysis = self.topic_analyzer.analyze_topics(papers)
            
            # 4. Anal√Ωza citac√≠
            citation_analysis = self.citation_analyzer.build_citation_network(papers)
            
            # 5. Generov√°n√≠ shrnut√≠
            summaries = []
            for paper in papers[:5]:  # Omezen√≠ na prvn√≠ch 5 ƒçl√°nk≈Ø
                summary = await self.summarizer.summarize_paper(paper)
                if summary:
                    summaries.append(summary)
            
            # 6. Sestaven√≠ v√Ωsledk≈Ø
            results = {
                "query": query,
                "total_papers": len(papers),
                "processed_summaries": len(summaries),
                "topic_analysis": topic_analysis,
                "citation_analysis": citation_analysis,
                "summaries": [self._summary_to_dict(s) for s in summaries],
                "processing_time": datetime.now().isoformat(),
                "recommendations": self._generate_recommendations(papers, topic_analysis)
            }
            
            return results
            
        except Exception as e:
            logger.error(f"Chyba p≈ôi zpracov√°n√≠ dotazu: {e}")
            return {"error": str(e)}
    
    def _summary_to_dict(self, summary: Summary) -> Dict:
        """Konverze Summary objektu na slovn√≠k"""
        return {
            "paper_id": summary.paper_id,
            "title": summary.title,
            "executive_summary": summary.executive_summary,
            "key_findings": summary.key_findings,
            "methodology": summary.methodology,
            "implications": summary.implications,
            "target_audience": summary.target_audience,
            "confidence_score": summary.confidence_score,
            "generated_at": summary.generated_at.isoformat()
        }
    
    def _generate_recommendations(self, papers: List[ResearchPaper], topic_analysis: Dict) -> List[str]:
        """Generov√°n√≠ doporuƒçen√≠ na z√°kladƒõ anal√Ωzy"""
        recommendations = []
        
        if topic_analysis.get("dominant_topics"):
            recommendations.append("Doporuƒçujeme zamƒõ≈ôit se na dominantn√≠ t√©mata v oblasti")
        
        if len(papers) > 5:
            recommendations.append("Velk√© mno≈æstv√≠ relevantn√≠ch ƒçl√°nk≈Ø - zva≈æte z√∫≈æen√≠ dotazu")
        
        recommendations.append("Pravidelnƒõ sledujte nov√© publikace v t√©to oblasti")
        
        return recommendations

# Demonstraƒçn√≠ pou≈æit√≠
async def main():
    """Hlavn√≠ demonstraƒçn√≠ funkce"""
    # Vytvo≈ôen√≠ syst√©mu
    system = ResearchSummarizationSystem()
    
    # Zpracov√°n√≠ uk√°zkov√©ho dotazu
    query = "machine learning natural language processing"
    results = await system.process_research_query(query, max_papers=5)
    
    # V√Ωpis v√Ωsledk≈Ø
    print("=== V√ùSLEDKY ANAL√ùZY V√ùZKUMN√ùCH ƒåL√ÅNK≈Æ ===")
    print(f"Dotaz: {results.get('query', 'N/A')}")
    print(f"Celkem ƒçl√°nk≈Ø: {results.get('total_papers', 0)}")
    print(f"Zpracovan√Ωch shrnut√≠: {results.get('processed_summaries', 0)}")
    
    if results.get('summaries'):
        print("\n=== SHRNUT√ç ƒåL√ÅNK≈Æ ===")
        for i, summary in enumerate(results['summaries'][:3], 1):
            print(f"\n{i}. {summary['title']}")
            print(f"P≈ôehled: {summary['executive_summary'][:200]}...")
            print(f"Spolehlivost: {summary['confidence_score']:.2f}")
    
    if results.get('recommendations'):
        print("\n=== DOPORUƒåEN√ç ===")
        for rec in results['recommendations']:
            print(f"‚Ä¢ {rec}")

if __name__ == "__main__":
    # Nastaven√≠ API kl√≠ƒç≈Ø (vy≈æaduje .env soubor)
    from dotenv import load_dotenv
    load_dotenv()
    
    # Spu≈°tƒõn√≠ demonstrace
    asyncio.run(main())
````

### Konfiguraƒçn√≠ soubory

````python
import os
from typing import Dict, Any

class Settings:
    """Konfiguraƒçn√≠ t≈ô√≠da pro syst√©m"""
    
    # API kl√≠ƒçe
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
    
    # Datab√°zov√© nastaven√≠
    CHROMA_PERSIST_DIRECTORY = "./data/chroma_db"
    
    # Model nastaven√≠
    DEFAULT_LLM_MODEL = "gpt-3.5-turbo"
    EMBEDDING_MODEL = "all-MiniLM-L6-v2"
    
    # Limity zpracov√°n√≠
    MAX_PAPERS_PER_QUERY = 50
    MAX_SUMMARY_LENGTH = 1500
    
    # Topic modeling
    TOPIC_MODEL_PARAMS = {
        "nr_topics": "auto",
        "min_topic_size": 3,
        "verbose": True
    }
    
    @classmethod
    def get_config(cls) -> Dict[str, Any]:
        """Z√≠sk√°n√≠ kompletn√≠ konfigurace"""
        return {
            "api_keys": {
                "openai": cls.OPENAI_API_KEY,
                "anthropic": cls.ANTHROPIC_API_KEY
            },
            "database": {
                "chroma_directory": cls.CHROMA_PERSIST_DIRECTORY
            },
            "models": {
                "llm": cls.DEFAULT_LLM_MODEL,
                "embedding": cls.EMBEDDING_MODEL
            },
            "limits": {
                "max_papers": cls.MAX_PAPERS_PER_QUERY,
                "max_summary_length": cls.MAX_SUMMARY_LENGTH
            },
            "topic_modeling": cls.TOPIC_MODEL_PARAMS
        }
````

### Testovac√≠ suite

````python
import pytest
import asyncio
from datetime import datetime
from unittest.mock import Mock, patch

from src.research_summarizer import (
    ResearchPaper, Summary, MCPClient, 
    AbstractiveSummarizer, TopicAnalyzer,
    CitationAnalyzer, ResearchSummarizationSystem
)

class TestResearchSummarizer:
    """Testovac√≠ t≈ô√≠da pro syst√©m sumarizace"""
    
    @pytest.fixture
    def sample_paper(self):
        """Uk√°zkov√Ω ƒçl√°nek pro testov√°n√≠"""
        return ResearchPaper(
            title="Pokroƒçil√© techniky strojov√©ho uƒçen√≠",
            authors=["Jan Nov√°k", "Marie Svobodov√°"],
            abstract="Tento ƒçl√°nek se zab√Ωv√° pokroƒçil√Ωmi technikami strojov√©ho uƒçen√≠...",
            content="Detailn√≠ obsah ƒçl√°nku...",
            citations=["Smith et al. (2023)", "Jones (2022)"],
            keywords=["machine learning", "AI", "neural networks"],
            publication_date=datetime.now(),
            source="ArXiv",
            arxiv_id="2023.12345"
        )
    
    @pytest.fixture
    def summarizer(self):
        """Inicializace sumariz√°toru"""
        return AbstractiveSummarizer()
    
    def test_paper_creation(self, sample_paper):
        """Test vytvo≈ôen√≠ objektu ƒçl√°nku"""
        assert sample_paper.title == "Pokroƒçil√© techniky strojov√©ho uƒçen√≠"
        assert len(sample_paper.authors) == 2
        assert sample_paper.source == "ArXiv"
    
    @pytest.mark.asyncio
    async def test_summarization(self, summarizer, sample_paper):
        """Test procesu sumarizace"""
        with patch.object(summarizer.chain, 'arun', return_value="Mock summary"):
            summary = await summarizer.summarize_paper(sample_paper)
            assert summary is not None
            assert summary.paper_id == sample_paper.arxiv_id
    
    def test_topic_analyzer(self, sample_paper):
        """Test analyz√°toru t√©mat"""
        analyzer = TopicAnalyzer()
        papers = [sample_paper] * 3  # Vytvo≈ôen√≠ kolekce pro testov√°n√≠
        
        with patch.object(analyzer, 'topic_model') as mock_model:
            mock_model.fit_transform.return_value = ([0, 1, 0], [0.8, 0.9, 0.7])
            mock_model.get_topic_info.return_value.to_dict.return_value = {}
            
            result = analyzer.analyze_topics(papers)
            assert isinstance(result, dict)
    
    def test_citation_analyzer(self, sample_paper):
        """Test analyz√°toru citac√≠"""
        analyzer = CitationAnalyzer()
        papers = [sample_paper]
        
        result = analyzer.build_citation_network(papers)
        assert isinstance(result, dict)
        assert "node_count" in result
    
    @pytest.mark.asyncio
    async def test_mcp_client(self):
        """Test MCP klienta"""
        client = MCPClient()
        
        with patch('arxiv.Client') as mock_arxiv:
            mock_result = Mock()
            mock_result.title = "Test Article"
            mock_result.authors = [Mock(name="Test Author")]
            mock_result.summary = "Test summary"
            mock_result.published = datetime.now()
            mock_result.entry_id = "test/123"
            mock_result.pdf_url = "http://test.pdf"
            
            mock_arxiv.return_value.results.return_value = [mock_result]
            
            papers = await client.fetch_arxiv_papers("test query", 1)
            assert len(papers) <= 1

    @pytest.mark.asyncio
    async def test_full_system(self):
        """Test cel√©ho syst√©mu"""
        system = ResearchSummarizationSystem()
        
        with patch.object(system.mcp_client, 'fetch_arxiv_papers') as mock_fetch:
            mock_fetch.return_value = []  # Pr√°zdn√Ω v√Ωsledek pro test
            
            result = await system.process_research_query("test query")
            assert "error" in result or "total_papers" in result

# Spu≈°tƒõn√≠ test≈Ø
if __name__ == "__main__":
    pytest.main([__file__, "-v"])
````

### Spou≈°tƒõc√≠ script

````python
import asyncio
import argparse
import json
from datetime import datetime

from src.research_summarizer import ResearchSummarizationSystem
from config.settings import Settings

async def run_analysis(query: str, max_papers: int = 10, output_file: str = None):
    """Spu≈°tƒõn√≠ anal√Ωzy v√Ωzkumn√Ωch ƒçl√°nk≈Ø"""
    
    print(f"üîç Zahajuji anal√Ωzu pro dotaz: '{query}'")
    print(f"üìä Maximum ƒçl√°nk≈Ø: {max_papers}")
    
    # Inicializace syst√©mu
    system = ResearchSummarizationSystem()
    
    # Zpracov√°n√≠ dotazu
    start_time = datetime.now()
    results = await system.process_research_query(query, max_papers)
    end_time = datetime.now()
    
    # P≈ôid√°n√≠ ƒçasov√Ωch informac√≠
    results["processing_duration"] = str(end_time - start_time)
    
    # V√Ωpis v√Ωsledk≈Ø
    print("\n" + "="*60)
    print("üìã V√ùSLEDKY ANAL√ùZY")
    print("="*60)
    
    if "error" in results:
        print(f"‚ùå Chyba: {results['error']}")
        return
    
    print(f"üîç Dotaz: {results['query']}")
    print(f"üìö Celkem ƒçl√°nk≈Ø: {results['total_papers']}")
    print(f"üìù Zpracovan√Ωch shrnut√≠: {results['processed_summaries']}")
    print(f"‚è±Ô∏è Doba zpracov√°n√≠: {results['processing_duration']}")
    
    # Shrnut√≠ ƒçl√°nk≈Ø
    if results.get('summaries'):
        print(f"\nüìÑ P≈òEHLED SHRNUT√ç ({len(results['summaries'])} ƒçl√°nk≈Ø)")
        print("-" * 50)
        
        for i, summary in enumerate(results['summaries'], 1):
            print(f"\n{i}. üìñ {summary['title']}")
            print(f"   üìã {summary['executive_summary'][:150]}...")
            print(f"   üéØ Spolehlivost: {summary['confidence_score']:.1%}")
            
            if summary['key_findings']:
                print(f"   üîç Kl√≠ƒçov√° zji≈°tƒõn√≠:")
                for finding in summary['key_findings'][:2]:
                    print(f"      ‚Ä¢ {finding}")
    
    # Anal√Ωza t√©mat
    if results.get('topic_analysis', {}).get('dominant_topics'):
        print(f"\nüè∑Ô∏è DOMINANTN√ç T√âMATA")
        print("-" * 30)
        for topic in results['topic_analysis']['dominant_topics'][:3]:
            print(f"   ‚Ä¢ T√©ma {topic.get('Topic', 'N/A')}: {topic.get('Count', 0)} ƒçl√°nk≈Ø")
    
    # Doporuƒçen√≠
    if results.get('recommendations'):
        print(f"\nüí° DOPORUƒåEN√ç")
        print("-" * 20)
        for rec in results['recommendations']:
            print(f"   ‚Ä¢ {rec}")
    
    # Ulo≈æen√≠ v√Ωsledk≈Ø
    if output_file:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        print(f"\nüíæ V√Ωsledky ulo≈æeny do: {output_file}")

def main():
    """Hlavn√≠ funkce"""
    parser = argparse.ArgumentParser(description="AI-Powered Research Paper Summarization")
    parser.add_argument("query", help="V√Ωzkumn√Ω dotaz")
    parser.add_argument("--max-papers", type=int, default=10, help="Maximum ƒçl√°nk≈Ø")
    parser.add_argument("--output", help="V√Ωstupn√≠ soubor pro v√Ωsledky")
    parser.add_argument("--config", action="store_true", help="Zobrazit konfiguraci")
    
    args = parser.parse_args()
    
    if args.config:
        config = Settings.get_config()
        print("üîß KONFIGURACE SYST√âMU:")
        print(json.dumps(config, indent=2, default=str))
        return
    
    # Spu≈°tƒõn√≠ anal√Ωzy
    asyncio.run(run_analysis(args.query, args.max_papers, args.output))

if __name__ == "__main__":
    main()
````

## Shrnut√≠ projektu

**AI-Powered Research Paper Summarization** p≈ôedstavuje pokroƒçil√Ω syst√©m pro automatickou anal√Ωzu a sumarizaci vƒõdeck√Ωch ƒçl√°nk≈Ø vyu≈æ√≠vaj√≠c√≠ Model Context Protocol. Projekt kombinuje nejmodernƒõj≈°√≠ technologie v oblasti NLP, topic modelingu a citaƒçn√≠ anal√Ωzy.

### Kl√≠ƒçov√© v√Ωhody:
- **Automatizace**: Dramatick√© sn√≠≈æen√≠ ƒçasu pot≈ôebn√©ho pro p≈ôehled literatury
- **Kvalita**: Pokroƒçil√© abstraktivn√≠ sumarizace zachov√°vaj√≠c√≠ kl√≠ƒçov√© informace
- **Kontextualita**: Anal√Ωza citaƒçn√≠ch s√≠t√≠ a t√©mat pro hlub≈°√≠ porozumƒõn√≠
- **≈†k√°lovatelnost**: Schopnost zpracovat velk√© kolekce dokument≈Ø
- **Flexibilita**: Adaptabiln√≠ pro r≈Øzn√© vƒõdeck√© dom√©ny

### Technologick√© inovace:
- Vyu≈æit√≠ MCP pro bezpeƒçn√Ω p≈ô√≠stup k extern√≠m zdroj≈Øm
- Pokroƒçil√° kombinace topic modelingu a citaƒçn√≠ anal√Ωzy
- Inteligentn√≠ vektorov√© vyhled√°v√°n√≠ pro relevantn√≠ doporuƒçen√≠
- Modul√°rn√≠ architektura umo≈æ≈àuj√≠c√≠ snadn√© roz≈°i≈ôov√°n√≠

Syst√©m m√° potenci√°l transformovat zp≈Øsob, jak√Ωm v√Ωzkumn√≠ci a akademici pracuj√≠ s vƒõdeckou literaturou, a m≈Ø≈æe v√Ωznamnƒõ urychlit vƒõdeck√Ω pokrok prost≈ôednictv√≠m efektivnƒõj≈°√≠ho sd√≠len√≠ a pochopen√≠ znalost√≠.