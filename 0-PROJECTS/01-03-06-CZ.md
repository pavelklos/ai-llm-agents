<small>Claude Sonnet 4 **(AI-Powered Research Paper Summarization (MCP-Driven))**</small>
# AI-Powered Research Paper Summarization (MCP-Driven)

## Klíčové koncepty

### Model Context Protocol (MCP)
**Model Context Protocol** je komunikační protokol vyvinutý společností Anthropic, který umožňuje jazykovým modelům (LLM) bezpečně a strukturovaně přistupovat k externím zdrojům dat a nástrojům. MCP vytváří standardizované rozhraní mezi AI modely a různými datovými zdroji, což umožňuje rozšíření schopností AI bez přímé integrace.

### Academic NLP
**Academic Natural Language Processing** se zaměřuje na zpracování vědeckých textů a dokumentů. Zahrnuje specifické techniky pro:
- Analýzu citací a referencí
- Extrakci klíčových pojmů a entit
- Identifikaci metodologie a výsledků
- Strukturování vědeckého obsahu

### Abstractive Summarization
**Abstraktivní sumarizace** je pokročilá technika, která vytváří nové shrnutí textu pomocí vlastních formulací, na rozdíl od extraktivní sumarizace, která pouze vybírá existující věty. Využívá pokročilé jazykové modely pro porozumění obsahu a jeho přeformulování.

### Citation Graphs
**Citační grafy** reprezentují vztahy mezi vědeckými publikacemi prostřednictvím citací. Umožňují analýzu vlivu, trendy výzkumu a identifikaci klíčových prací v dané oblasti.

### BERTopic
**BERTopic** je moderní algoritmus pro topic modeling, který využívá BERT embeddings pro identifikaci a klasifikaci témat v kolekcích dokumentů s vysokou přesností.

## Komplexní vysvětlení projektu

### Cíle projektu
Tento projekt vytváří inteligentní systém pro automatickou sumarizaci vědeckých článků využívající Model Context Protocol. Systém dokáže:

1. **Automaticky stahovat a analyzovat** vědecké články z různých zdrojů
2. **Extrahovat klíčové informace** včetně metodologie, výsledků a závěrů
3. **Generovat kvalitní abstraktivní shrnutí** přizpůsobená různým cílovým skupinám
4. **Analyzovat citační sítě** pro kontextuální porozumění
5. **Identifikovat výzkumné trendy** pomocí topic modelingu

### Výzvy projektu
- **Komplexnost vědeckého jazyka**: Technická terminologie a složité struktury
- **Různorodost formátů**: PDF, HTML, XML dokumenty s různým layoutem
- **Kvalita sumarizace**: Zachování důležitých informací při zkrácení
- **Kontextuální porozumění**: Správná interpretace vědeckých konceptů
- **Škálovatelnost**: Zpracování velkých kolekcí dokumentů

### Potenciální dopad
Systém může revolucionalizovat způsob, jakým výzkumníci, studenti a odborníci přistupují k vědecké literatuře, výrazně zkracuje čas potřebný pro přehled aktuálního stavu poznání v dané oblasti.

## Komplexní příklad implementace

### Instalace závislostí

````python
# Základní závislosti pro AI-Powered Research Paper Summarization
langchain==0.1.0
langchain-openai==0.0.5
langchain-anthropic==0.1.0
chromadb==0.4.22
bertopic==0.15.0
sentence-transformers==2.2.2
arxiv==1.4.8
pypdf2==3.0.1
requests==2.31.0
beautifulsoup4==4.12.2
pandas==2.0.3
numpy==1.24.3
matplotlib==3.7.2
networkx==3.2.1
scikit-learn==1.3.0
openai==1.12.0
anthropic==0.15.0
python-dotenv==1.0.0
pydantic==2.6.1
````

### Hlavní implementace systému

````python
import os
import json
import logging
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from datetime import datetime
import asyncio

import arxiv
import requests
from bs4 import BeautifulSoup
import PyPDF2
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
import networkx as nx
from sklearn.metrics.pairwise import cosine_similarity

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain.prompts import ChatPromptTemplate
from langchain.schema import HumanMessage, SystemMessage
from langchain.chains import LLMChain
import chromadb
from chromadb.config import Settings

# Konfigurace logování
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ResearchPaper:
    """Datová struktura pro vědecký článek"""
    title: str
    authors: List[str]
    abstract: str
    content: str
    citations: List[str]
    keywords: List[str]
    publication_date: datetime
    source: str
    doi: Optional[str] = None
    arxiv_id: Optional[str] = None

@dataclass
class Summary:
    """Datová struktura pro shrnutí"""
    paper_id: str
    title: str
    executive_summary: str
    key_findings: List[str]
    methodology: str
    implications: str
    target_audience: str
    confidence_score: float
    generated_at: datetime

class MCPClient:
    """Model Context Protocol klient pro externí zdroje"""
    
    def __init__(self):
        self.arxiv_client = arxiv.Client()
        self.session = requests.Session()
        
    async def fetch_arxiv_papers(self, query: str, max_results: int = 10) -> List[ResearchPaper]:
        """Stažení článků z ArXiv"""
        try:
            search = arxiv.Search(
                query=query,
                max_results=max_results,
                sort_by=arxiv.SortCriterion.SubmittedDate
            )
            
            papers = []
            for result in self.arxiv_client.results(search):
                paper = ResearchPaper(
                    title=result.title,
                    authors=[author.name for author in result.authors],
                    abstract=result.summary,
                    content=await self._download_pdf_content(result.pdf_url),
                    citations=await self._extract_citations(result.summary),
                    keywords=await self._extract_keywords(result.title + " " + result.summary),
                    publication_date=result.published,
                    source="ArXiv",
                    arxiv_id=result.entry_id.split('/')[-1]
                )
                papers.append(paper)
                
            logger.info(f"Staženo {len(papers)} článků z ArXiv")
            return papers
            
        except Exception as e:
            logger.error(f"Chyba při stahování z ArXiv: {e}")
            return []
    
    async def _download_pdf_content(self, pdf_url: str) -> str:
        """Stažení a extrakce textu z PDF"""
        try:
            response = self.session.get(pdf_url, timeout=30)
            response.raise_for_status()
            
            # Simulace extrakce textu (v praxi by se použil pokročilejší parser)
            return f"Obsah PDF dokumentu z {pdf_url} - simulovaný text pro demonstraci."
            
        except Exception as e:
            logger.error(f"Chyba při stahování PDF: {e}")
            return ""
    
    async def _extract_citations(self, text: str) -> List[str]:
        """Extrakce citací z textu"""
        # Jednoduchá implementace - v praxi by se použily pokročilejší NLP techniky
        citations = []
        # Simulace extrakce citací
        if "et al." in text:
            citations.append("Smith et al. (2023)")
        if "journal" in text.lower():
            citations.append("Nature (2023)")
        return citations
    
    async def _extract_keywords(self, text: str) -> List[str]:
        """Extrakce klíčových slov"""
        # Jednoduchá implementace založená na frekvenci slov
        words = text.lower().split()
        keywords = [word for word in words if len(word) > 5]
        return keywords[:10]

class TopicAnalyzer:
    """Analyzátor témat pomocí BERTopic"""
    
    def __init__(self):
        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.topic_model = None
        
    def analyze_topics(self, papers: List[ResearchPaper]) -> Dict[str, Any]:
        """Analýza témat v kolekci článků"""
        try:
            # Příprava textových dat
            documents = [f"{paper.title} {paper.abstract}" for paper in papers]
            
            # Vytvoření topic modelu
            self.topic_model = BERTopic(
                embedding_model=self.sentence_model,
                nr_topics="auto",
                verbose=True
            )
            
            # Trénování modelu
            topics, probabilities = self.topic_model.fit_transform(documents)
            
            # Analýza výsledků
            topic_info = self.topic_model.get_topic_info()
            
            return {
                "topics": topics,
                "probabilities": probabilities,
                "topic_info": topic_info.to_dict(),
                "dominant_topics": self._get_dominant_topics(),
                "topic_keywords": self._get_topic_keywords()
            }
            
        except Exception as e:
            logger.error(f"Chyba při analýze témat: {e}")
            return {}
    
    def _get_dominant_topics(self) -> List[Dict]:
        """Získání dominantních témat"""
        if not self.topic_model:
            return []
            
        topic_info = self.topic_model.get_topic_info()
        return topic_info.head(5).to_dict('records')
    
    def _get_topic_keywords(self) -> Dict[int, List[str]]:
        """Získání klíčových slov pro každé téma"""
        if not self.topic_model:
            return {}
            
        keywords = {}
        for topic_id in range(len(self.topic_model.get_topics())):
            if topic_id != -1:  # Vyloučení outlier tématu
                topic_words = self.topic_model.get_topic(topic_id)
                keywords[topic_id] = [word for word, _ in topic_words[:10]]
        
        return keywords

class CitationAnalyzer:
    """Analyzátor citačních sítí"""
    
    def __init__(self):
        self.citation_graph = nx.DiGraph()
    
    def build_citation_network(self, papers: List[ResearchPaper]) -> Dict[str, Any]:
        """Sestavení citační sítě"""
        try:
            # Přidání uzlů (článků)
            for paper in papers:
                self.citation_graph.add_node(
                    paper.title,
                    authors=paper.authors,
                    publication_date=paper.publication_date,
                    source=paper.source
                )
            
            # Přidání hran (citací)
            for paper in papers:
                for citation in paper.citations:
                    if citation in [p.title for p in papers]:
                        self.citation_graph.add_edge(paper.title, citation)
            
            # Analýza sítě
            analysis = {
                "node_count": self.citation_graph.number_of_nodes(),
                "edge_count": self.citation_graph.number_of_edges(),
                "density": nx.density(self.citation_graph),
                "centrality": self._calculate_centrality(),
                "clusters": self._detect_clusters(),
                "influential_papers": self._get_influential_papers()
            }
            
            return analysis
            
        except Exception as e:
            logger.error(f"Chyba při analýze citací: {e}")
            return {}
    
    def _calculate_centrality(self) -> Dict[str, float]:
        """Výpočet centrality uzlů"""
        try:
            return nx.pagerank(self.citation_graph)
        except:
            return {}
    
    def _detect_clusters(self) -> List[List[str]]:
        """Detekce klastrů v síti"""
        try:
            # Konverze na neorientovaný graf pro clustering
            undirected_graph = self.citation_graph.to_undirected()
            clusters = list(nx.connected_components(undirected_graph))
            return [list(cluster) for cluster in clusters]
        except:
            return []
    
    def _get_influential_papers(self, top_k: int = 5) -> List[str]:
        """Identifikace nejvlivnějších článků"""
        centrality = self._calculate_centrality()
        if not centrality:
            return []
        
        sorted_papers = sorted(centrality.items(), key=lambda x: x[1], reverse=True)
        return [paper[0] for paper in sorted_papers[:top_k]]

class AbstractiveSummarizer:
    """Abstraktivní sumarizátor využívající LLM"""
    
    def __init__(self, model_name: str = "gpt-3.5-turbo"):
        self.llm = ChatOpenAI(
            model_name=model_name,
            temperature=0.3,
            max_tokens=1500
        )
        
        self.summary_prompt = ChatPromptTemplate.from_messages([
            SystemMessage(content="""Jsi expert na sumarizaci vědeckých článků. 
            Vytvoř kvalitní, strukturované shrnutí následujícího vědeckého článku v češtině.
            
            Struktura shrnutí:
            1. Stručný přehled (2-3 věty)
            2. Klíčové zjištění (3-5 bodů)
            3. Metodologie (1-2 věty)
            4. Praktické důsledky (2-3 věty)
            
            Použij jasný, srozumitelný jazyk vhodný pro odbornou veřejnost."""),
            HumanMessage(content="""
            Název: {title}
            Autoři: {authors}
            Abstrakt: {abstract}
            Klíčová slova: {keywords}
            
            Vytvoř strukturované shrnutí tohoto článku:
            """)
        ])
        
        self.chain = LLMChain(llm=self.llm, prompt=self.summary_prompt)
    
    async def summarize_paper(self, paper: ResearchPaper, target_audience: str = "general") -> Summary:
        """Vytvoření shrnutí článku"""
        try:
            # Příprava vstupních dat
            input_data = {
                "title": paper.title,
                "authors": ", ".join(paper.authors),
                "abstract": paper.abstract,
                "keywords": ", ".join(paper.keywords)
            }
            
            # Generování shrnutí
            result = await self.chain.arun(**input_data)
            
            # Parsování výsledku
            summary_parts = self._parse_summary(result)
            
            # Výpočet skóre spolehlivosti
            confidence_score = self._calculate_confidence(paper, summary_parts)
            
            summary = Summary(
                paper_id=paper.arxiv_id or paper.title,
                title=paper.title,
                executive_summary=summary_parts.get("overview", ""),
                key_findings=summary_parts.get("findings", []),
                methodology=summary_parts.get("methodology", ""),
                implications=summary_parts.get("implications", ""),
                target_audience=target_audience,
                confidence_score=confidence_score,
                generated_at=datetime.now()
            )
            
            return summary
            
        except Exception as e:
            logger.error(f"Chyba při sumarizaci: {e}")
            return None
    
    def _parse_summary(self, text: str) -> Dict[str, Any]:
        """Parsování strukturovaného shrnutí"""
        parts = {
            "overview": "",
            "findings": [],
            "methodology": "",
            "implications": ""
        }
        
        # Jednoduchý parser - v praxi by se použily pokročilejší techniky
        lines = text.split('\n')
        current_section = None
        
        for line in lines:
            line = line.strip()
            if "přehled" in line.lower():
                current_section = "overview"
            elif "zjištění" in line.lower() or "findings" in line.lower():
                current_section = "findings"
            elif "metodologie" in line.lower():
                current_section = "methodology"
            elif "důsledky" in line.lower() or "implications" in line.lower():
                current_section = "implications"
            elif line and current_section:
                if current_section == "findings":
                    parts[current_section].append(line)
                else:
                    parts[current_section] += line + " "
        
        return parts
    
    def _calculate_confidence(self, paper: ResearchPaper, summary_parts: Dict) -> float:
        """Výpočet skóre spolehlivosti shrnutí"""
        # Jednoduchá metrika založená na dostupnosti dat
        score = 0.5  # Základní skóre
        
        if paper.abstract:
            score += 0.2
        if paper.citations:
            score += 0.1
        if paper.keywords:
            score += 0.1
        if summary_parts.get("findings"):
            score += 0.1
        
        return min(score, 1.0)

class VectorStore:
    """Vektorová databáze pro ukládání a vyhledávání"""
    
    def __init__(self, collection_name: str = "research_papers"):
        self.client = chromadb.Client(Settings(anonymized_telemetry=False))
        self.collection = self.client.create_collection(
            name=collection_name,
            get_or_create=True
        )
        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
    
    def add_papers(self, papers: List[ResearchPaper]):
        """Přidání článků do vektorové databáze"""
        try:
            documents = []
            metadatas = []
            ids = []
            
            for i, paper in enumerate(papers):
                # Vytvoření dokumentu pro embedding
                doc_text = f"{paper.title} {paper.abstract}"
                documents.append(doc_text)
                
                # Metadata
                metadata = {
                    "title": paper.title,
                    "authors": json.dumps(paper.authors),
                    "source": paper.source,
                    "publication_date": paper.publication_date.isoformat(),
                    "keywords": json.dumps(paper.keywords)
                }
                metadatas.append(metadata)
                
                # ID
                paper_id = paper.arxiv_id or f"paper_{i}"
                ids.append(paper_id)
            
            # Přidání do kolekce
            self.collection.add(
                documents=documents,
                metadatas=metadatas,
                ids=ids
            )
            
            logger.info(f"Přidáno {len(papers)} článků do vektorové databáze")
            
        except Exception as e:
            logger.error(f"Chyba při přidávání do databáze: {e}")
    
    def search_similar_papers(self, query: str, n_results: int = 5) -> List[Dict]:
        """Vyhledání podobných článků"""
        try:
            results = self.collection.query(
                query_texts=[query],
                n_results=n_results
            )
            
            return results
            
        except Exception as e:
            logger.error(f"Chyba při vyhledávání: {e}")
            return []

class ResearchSummarizationSystem:
    """Hlavní systém pro sumarizaci výzkumných článků"""
    
    def __init__(self):
        self.mcp_client = MCPClient()
        self.topic_analyzer = TopicAnalyzer()
        self.citation_analyzer = CitationAnalyzer()
        self.summarizer = AbstractiveSummarizer()
        self.vector_store = VectorStore()
        
    async def process_research_query(self, query: str, max_papers: int = 10) -> Dict[str, Any]:
        """Kompletní zpracování výzkumného dotazu"""
        try:
            logger.info(f"Zpracovávám dotaz: {query}")
            
            # 1. Stažení článků pomocí MCP
            papers = await self.mcp_client.fetch_arxiv_papers(query, max_papers)
            if not papers:
                return {"error": "Nepodařilo se najít žádné články"}
            
            # 2. Uložení do vektorové databáze
            self.vector_store.add_papers(papers)
            
            # 3. Analýza témat
            topic_analysis = self.topic_analyzer.analyze_topics(papers)
            
            # 4. Analýza citací
            citation_analysis = self.citation_analyzer.build_citation_network(papers)
            
            # 5. Generování shrnutí
            summaries = []
            for paper in papers[:5]:  # Omezení na prvních 5 článků
                summary = await self.summarizer.summarize_paper(paper)
                if summary:
                    summaries.append(summary)
            
            # 6. Sestavení výsledků
            results = {
                "query": query,
                "total_papers": len(papers),
                "processed_summaries": len(summaries),
                "topic_analysis": topic_analysis,
                "citation_analysis": citation_analysis,
                "summaries": [self._summary_to_dict(s) for s in summaries],
                "processing_time": datetime.now().isoformat(),
                "recommendations": self._generate_recommendations(papers, topic_analysis)
            }
            
            return results
            
        except Exception as e:
            logger.error(f"Chyba při zpracování dotazu: {e}")
            return {"error": str(e)}
    
    def _summary_to_dict(self, summary: Summary) -> Dict:
        """Konverze Summary objektu na slovník"""
        return {
            "paper_id": summary.paper_id,
            "title": summary.title,
            "executive_summary": summary.executive_summary,
            "key_findings": summary.key_findings,
            "methodology": summary.methodology,
            "implications": summary.implications,
            "target_audience": summary.target_audience,
            "confidence_score": summary.confidence_score,
            "generated_at": summary.generated_at.isoformat()
        }
    
    def _generate_recommendations(self, papers: List[ResearchPaper], topic_analysis: Dict) -> List[str]:
        """Generování doporučení na základě analýzy"""
        recommendations = []
        
        if topic_analysis.get("dominant_topics"):
            recommendations.append("Doporučujeme zaměřit se na dominantní témata v oblasti")
        
        if len(papers) > 5:
            recommendations.append("Velké množství relevantních článků - zvažte zúžení dotazu")
        
        recommendations.append("Pravidelně sledujte nové publikace v této oblasti")
        
        return recommendations

# Demonstrační použití
async def main():
    """Hlavní demonstrační funkce"""
    # Vytvoření systému
    system = ResearchSummarizationSystem()
    
    # Zpracování ukázkového dotazu
    query = "machine learning natural language processing"
    results = await system.process_research_query(query, max_papers=5)
    
    # Výpis výsledků
    print("=== VÝSLEDKY ANALÝZY VÝZKUMNÝCH ČLÁNKŮ ===")
    print(f"Dotaz: {results.get('query', 'N/A')}")
    print(f"Celkem článků: {results.get('total_papers', 0)}")
    print(f"Zpracovaných shrnutí: {results.get('processed_summaries', 0)}")
    
    if results.get('summaries'):
        print("\n=== SHRNUTÍ ČLÁNKŮ ===")
        for i, summary in enumerate(results['summaries'][:3], 1):
            print(f"\n{i}. {summary['title']}")
            print(f"Přehled: {summary['executive_summary'][:200]}...")
            print(f"Spolehlivost: {summary['confidence_score']:.2f}")
    
    if results.get('recommendations'):
        print("\n=== DOPORUČENÍ ===")
        for rec in results['recommendations']:
            print(f"• {rec}")

if __name__ == "__main__":
    # Nastavení API klíčů (vyžaduje .env soubor)
    from dotenv import load_dotenv
    load_dotenv()
    
    # Spuštění demonstrace
    asyncio.run(main())
````

### Konfigurační soubory

````python
import os
from typing import Dict, Any

class Settings:
    """Konfigurační třída pro systém"""
    
    # API klíče
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
    
    # Databázové nastavení
    CHROMA_PERSIST_DIRECTORY = "./data/chroma_db"
    
    # Model nastavení
    DEFAULT_LLM_MODEL = "gpt-3.5-turbo"
    EMBEDDING_MODEL = "all-MiniLM-L6-v2"
    
    # Limity zpracování
    MAX_PAPERS_PER_QUERY = 50
    MAX_SUMMARY_LENGTH = 1500
    
    # Topic modeling
    TOPIC_MODEL_PARAMS = {
        "nr_topics": "auto",
        "min_topic_size": 3,
        "verbose": True
    }
    
    @classmethod
    def get_config(cls) -> Dict[str, Any]:
        """Získání kompletní konfigurace"""
        return {
            "api_keys": {
                "openai": cls.OPENAI_API_KEY,
                "anthropic": cls.ANTHROPIC_API_KEY
            },
            "database": {
                "chroma_directory": cls.CHROMA_PERSIST_DIRECTORY
            },
            "models": {
                "llm": cls.DEFAULT_LLM_MODEL,
                "embedding": cls.EMBEDDING_MODEL
            },
            "limits": {
                "max_papers": cls.MAX_PAPERS_PER_QUERY,
                "max_summary_length": cls.MAX_SUMMARY_LENGTH
            },
            "topic_modeling": cls.TOPIC_MODEL_PARAMS
        }
````

### Testovací suite

````python
import pytest
import asyncio
from datetime import datetime
from unittest.mock import Mock, patch

from src.research_summarizer import (
    ResearchPaper, Summary, MCPClient, 
    AbstractiveSummarizer, TopicAnalyzer,
    CitationAnalyzer, ResearchSummarizationSystem
)

class TestResearchSummarizer:
    """Testovací třída pro systém sumarizace"""
    
    @pytest.fixture
    def sample_paper(self):
        """Ukázkový článek pro testování"""
        return ResearchPaper(
            title="Pokročilé techniky strojového učení",
            authors=["Jan Novák", "Marie Svobodová"],
            abstract="Tento článek se zabývá pokročilými technikami strojového učení...",
            content="Detailní obsah článku...",
            citations=["Smith et al. (2023)", "Jones (2022)"],
            keywords=["machine learning", "AI", "neural networks"],
            publication_date=datetime.now(),
            source="ArXiv",
            arxiv_id="2023.12345"
        )
    
    @pytest.fixture
    def summarizer(self):
        """Inicializace sumarizátoru"""
        return AbstractiveSummarizer()
    
    def test_paper_creation(self, sample_paper):
        """Test vytvoření objektu článku"""
        assert sample_paper.title == "Pokročilé techniky strojového učení"
        assert len(sample_paper.authors) == 2
        assert sample_paper.source == "ArXiv"
    
    @pytest.mark.asyncio
    async def test_summarization(self, summarizer, sample_paper):
        """Test procesu sumarizace"""
        with patch.object(summarizer.chain, 'arun', return_value="Mock summary"):
            summary = await summarizer.summarize_paper(sample_paper)
            assert summary is not None
            assert summary.paper_id == sample_paper.arxiv_id
    
    def test_topic_analyzer(self, sample_paper):
        """Test analyzátoru témat"""
        analyzer = TopicAnalyzer()
        papers = [sample_paper] * 3  # Vytvoření kolekce pro testování
        
        with patch.object(analyzer, 'topic_model') as mock_model:
            mock_model.fit_transform.return_value = ([0, 1, 0], [0.8, 0.9, 0.7])
            mock_model.get_topic_info.return_value.to_dict.return_value = {}
            
            result = analyzer.analyze_topics(papers)
            assert isinstance(result, dict)
    
    def test_citation_analyzer(self, sample_paper):
        """Test analyzátoru citací"""
        analyzer = CitationAnalyzer()
        papers = [sample_paper]
        
        result = analyzer.build_citation_network(papers)
        assert isinstance(result, dict)
        assert "node_count" in result
    
    @pytest.mark.asyncio
    async def test_mcp_client(self):
        """Test MCP klienta"""
        client = MCPClient()
        
        with patch('arxiv.Client') as mock_arxiv:
            mock_result = Mock()
            mock_result.title = "Test Article"
            mock_result.authors = [Mock(name="Test Author")]
            mock_result.summary = "Test summary"
            mock_result.published = datetime.now()
            mock_result.entry_id = "test/123"
            mock_result.pdf_url = "http://test.pdf"
            
            mock_arxiv.return_value.results.return_value = [mock_result]
            
            papers = await client.fetch_arxiv_papers("test query", 1)
            assert len(papers) <= 1

    @pytest.mark.asyncio
    async def test_full_system(self):
        """Test celého systému"""
        system = ResearchSummarizationSystem()
        
        with patch.object(system.mcp_client, 'fetch_arxiv_papers') as mock_fetch:
            mock_fetch.return_value = []  # Prázdný výsledek pro test
            
            result = await system.process_research_query("test query")
            assert "error" in result or "total_papers" in result

# Spuštění testů
if __name__ == "__main__":
    pytest.main([__file__, "-v"])
````

### Spouštěcí script

````python
import asyncio
import argparse
import json
from datetime import datetime

from src.research_summarizer import ResearchSummarizationSystem
from config.settings import Settings

async def run_analysis(query: str, max_papers: int = 10, output_file: str = None):
    """Spuštění analýzy výzkumných článků"""
    
    print(f"🔍 Zahajuji analýzu pro dotaz: '{query}'")
    print(f"📊 Maximum článků: {max_papers}")
    
    # Inicializace systému
    system = ResearchSummarizationSystem()
    
    # Zpracování dotazu
    start_time = datetime.now()
    results = await system.process_research_query(query, max_papers)
    end_time = datetime.now()
    
    # Přidání časových informací
    results["processing_duration"] = str(end_time - start_time)
    
    # Výpis výsledků
    print("\n" + "="*60)
    print("📋 VÝSLEDKY ANALÝZY")
    print("="*60)
    
    if "error" in results:
        print(f"❌ Chyba: {results['error']}")
        return
    
    print(f"🔍 Dotaz: {results['query']}")
    print(f"📚 Celkem článků: {results['total_papers']}")
    print(f"📝 Zpracovaných shrnutí: {results['processed_summaries']}")
    print(f"⏱️ Doba zpracování: {results['processing_duration']}")
    
    # Shrnutí článků
    if results.get('summaries'):
        print(f"\n📄 PŘEHLED SHRNUTÍ ({len(results['summaries'])} článků)")
        print("-" * 50)
        
        for i, summary in enumerate(results['summaries'], 1):
            print(f"\n{i}. 📖 {summary['title']}")
            print(f"   📋 {summary['executive_summary'][:150]}...")
            print(f"   🎯 Spolehlivost: {summary['confidence_score']:.1%}")
            
            if summary['key_findings']:
                print(f"   🔍 Klíčová zjištění:")
                for finding in summary['key_findings'][:2]:
                    print(f"      • {finding}")
    
    # Analýza témat
    if results.get('topic_analysis', {}).get('dominant_topics'):
        print(f"\n🏷️ DOMINANTNÍ TÉMATA")
        print("-" * 30)
        for topic in results['topic_analysis']['dominant_topics'][:3]:
            print(f"   • Téma {topic.get('Topic', 'N/A')}: {topic.get('Count', 0)} článků")
    
    # Doporučení
    if results.get('recommendations'):
        print(f"\n💡 DOPORUČENÍ")
        print("-" * 20)
        for rec in results['recommendations']:
            print(f"   • {rec}")
    
    # Uložení výsledků
    if output_file:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        print(f"\n💾 Výsledky uloženy do: {output_file}")

def main():
    """Hlavní funkce"""
    parser = argparse.ArgumentParser(description="AI-Powered Research Paper Summarization")
    parser.add_argument("query", help="Výzkumný dotaz")
    parser.add_argument("--max-papers", type=int, default=10, help="Maximum článků")
    parser.add_argument("--output", help="Výstupní soubor pro výsledky")
    parser.add_argument("--config", action="store_true", help="Zobrazit konfiguraci")
    
    args = parser.parse_args()
    
    if args.config:
        config = Settings.get_config()
        print("🔧 KONFIGURACE SYSTÉMU:")
        print(json.dumps(config, indent=2, default=str))
        return
    
    # Spuštění analýzy
    asyncio.run(run_analysis(args.query, args.max_papers, args.output))

if __name__ == "__main__":
    main()
````

## Shrnutí projektu

**AI-Powered Research Paper Summarization** představuje pokročilý systém pro automatickou analýzu a sumarizaci vědeckých článků využívající Model Context Protocol. Projekt kombinuje nejmodernější technologie v oblasti NLP, topic modelingu a citační analýzy.

### Klíčové výhody:
- **Automatizace**: Dramatické snížení času potřebného pro přehled literatury
- **Kvalita**: Pokročilé abstraktivní sumarizace zachovávající klíčové informace
- **Kontextualita**: Analýza citačních sítí a témat pro hlubší porozumění
- **Škálovatelnost**: Schopnost zpracovat velké kolekce dokumentů
- **Flexibilita**: Adaptabilní pro různé vědecké domény

### Technologické inovace:
- Využití MCP pro bezpečný přístup k externím zdrojům
- Pokročilá kombinace topic modelingu a citační analýzy
- Inteligentní vektorové vyhledávání pro relevantní doporučení
- Modulární architektura umožňující snadné rozšiřování

Systém má potenciál transformovat způsob, jakým výzkumníci a akademici pracují s vědeckou literaturou, a může významně urychlit vědecký pokrok prostřednictvím efektivnějšího sdílení a pochopení znalostí.