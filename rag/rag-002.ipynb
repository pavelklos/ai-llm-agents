{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d3e95cd-c8bf-4164-97f0-c33489862bee",
   "metadata": {},
   "source": [
    "## AI Document Retrieval System using Pinecone and LangChain\n",
    "\n",
    "This code demonstrates how to:\n",
    "1. Download various document types (txt, md, pdf, docx, xlsx, html)\n",
    "2. Process and chunk documents\n",
    "3. Create embeddings using OpenAI\n",
    "4. Store embeddings in Pinecone vector database\n",
    "5. Retrieve relevant documents based on queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc55426-f51a-432b-a559-a4e138dc4d52",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4b9ec7b-ba87-447f-8455-9f6693c1f4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain openai pinecone-client python-dotenv requests pandas beautifulsoup4 unstructured pypdf docx2txt openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00e216a-71f1-4fa6-9ea6-a132335b1d8e",
   "metadata": {},
   "source": [
    "### Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b30e3c0b-811b-4880-a03d-10ce1fde1b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.document_loaders import (\n",
    "    TextLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    PyPDFLoader,\n",
    "    Docx2txtLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    ")\n",
    "from langchain.document_loaders.excel import UnstructuredExcelLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "# from langchain.vectorstores import Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.chains import RetrievalQA\n",
    "# from langchain.llms import OpenAI\n",
    "import pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from dotenv import load_dotenv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15da4ca3-50eb-4c0d-8026-a94fa76ecb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\n",
      "DATA_DIR: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\n",
      "MODEL_GPT: gpt-4o-mini\n",
      "PINECONE_ENVIRONMENT: us-east-1\n",
      "PINECONE_INDEX_NAME: document-retrieval\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get API keys from environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_ENVIRONMENT = os.getenv(\"PINECONE_ENVIRONMENT\", \"gcp-starter\")\n",
    "\n",
    "# Set up directories\n",
    "# BASE_DIR = os.path.dirname(os.path.abspath(__file__))  # Python script\n",
    "BASE_DIR = os.getcwd()  # Jupyter Notebook\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data-002\")\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini' # 'gpt-3.5-turbo'\n",
    "PINECONE_INDEX_NAME = \"document-retrieval\"\n",
    "\n",
    "print(\"BASE_DIR:\", BASE_DIR)\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"MODEL_GPT:\", MODEL_GPT)\n",
    "print(\"PINECONE_ENVIRONMENT:\", PINECONE_ENVIRONMENT)\n",
    "print(\"PINECONE_INDEX_NAME:\", PINECONE_INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed41d29-b3d4-4bb5-81d4-f08e43375958",
   "metadata": {},
   "source": [
    "### Download documents\n",
    "(MD, IPYNB, PDF, DOCX, XLSX, HTML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3916bb96-009e-4490-8ff1-12c7ce899b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents to download (URL, filename, type)\n",
    "DOCUMENTS = [\n",
    "    (\"https://raw.githubusercontent.com/langchain-ai/langchain/master/README.md\", \"langchain_readme.md\", \"markdown\"),\n",
    "    # (\"https://raw.githubusercontent.com/pinecone-io/examples/master/learn/generation/langchain/handbook/05-langchain-retrieval-augmentation.ipynb\", \"pinecone_example.ipynb\", \"text\"),\n",
    "    (\"https://raw.githubusercontent.com/pinecone-io/examples/master/learn/generation/langchain/handbook/05-langchain-retrieval-augmentation.ipynb\", \"pinecone_example.ipynb\", \"notebook\"),\n",
    "    (\"https://arxiv.org/pdf/2005.11401.pdf\", \"neural_networks.pdf\", \"pdf\"),\n",
    "    (\"https://calibre-ebook.com/downloads/demos/demo.docx\", \"calibre_demo.docx\", \"docx\"),\n",
    "    (\"https://filesamples.com/samples/document/xlsx/sample1.xlsx\", \"sample_data.xlsx\", \"excel\"),\n",
    "    (\"https://www.w3.org/WAI/tutorials/page-structure/\", \"web_accessibility.html\", \"html\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fbe104e-1f2e-4e31-aaf6-f327dbd40932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, file_path):\n",
    "    \"\"\"Download a file from a URL and save it locally.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        print(f\"Successfully downloaded {url} to {file_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c423788-4edf-4eb4-80f0-486c55caf9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_document(file_path, doc_type):\n",
    "#     \"\"\"Load a document based on its type.\"\"\"\n",
    "#     try:\n",
    "#         if doc_type == \"text\":\n",
    "#             return TextLoader(file_path).load()\n",
    "#         if doc_type == \"notebook\":\n",
    "#             return TextLoader(file_path).load()\n",
    "#         elif doc_type == \"markdown\":\n",
    "#             return UnstructuredMarkdownLoader(file_path).load()\n",
    "#         elif doc_type == \"pdf\":\n",
    "#             return PyPDFLoader(file_path).load()\n",
    "#         elif doc_type == \"docx\":\n",
    "#             return Docx2txtLoader(file_path).load()\n",
    "#         elif doc_type == \"excel\":\n",
    "#             return UnstructuredExcelLoader(file_path).load()\n",
    "#         elif doc_type == \"html\":\n",
    "#             return UnstructuredHTMLLoader(file_path).load()\n",
    "#         else:\n",
    "#             print(f\"Unsupported document type: {doc_type}\")\n",
    "#             return []\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading {file_path}: {e}\")\n",
    "#         return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0df7676-d17c-450f-955c-0f0bcea20041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from langchain_core.documents import Document\n",
    "# from langchain.document_loaders import (\n",
    "#     TextLoader,\n",
    "#     UnstructuredMarkdownLoader,\n",
    "#     PyPDFLoader,\n",
    "#     Docx2txtLoader,\n",
    "#     UnstructuredHTMLLoader\n",
    "# )\n",
    "\n",
    "# def load_document(file_path, doc_type):\n",
    "#     \"\"\"Load a document based on its type.\"\"\"\n",
    "#     try:\n",
    "#         if doc_type == \"text\":\n",
    "#             return TextLoader(file_path).load()\n",
    "        \n",
    "#         elif doc_type == \"notebook\":\n",
    "#             # Currently loading as plain text; can be replaced with smarter logic later\n",
    "#             return TextLoader(file_path).load()\n",
    "        \n",
    "#         elif doc_type == \"markdown\":\n",
    "#             return UnstructuredMarkdownLoader(file_path).load()\n",
    "        \n",
    "#         elif doc_type == \"pdf\":\n",
    "#             return PyPDFLoader(file_path).load()\n",
    "        \n",
    "#         elif doc_type == \"docx\":\n",
    "#             return Docx2txtLoader(file_path).load()\n",
    "        \n",
    "#         elif doc_type == \"excel\":\n",
    "#             df = pd.read_excel(file_path)\n",
    "#             text_content = f\"Excel file with {len(df)} rows and {len(df.columns)} columns.\\n\\n\"\n",
    "#             text_content += f\"Column names: {', '.join(df.columns.astype(str))}\\n\\n\"\n",
    "#             text_content += \"Sample data (first 5 rows):\\n\"\n",
    "#             text_content += df.head().to_string()\n",
    "            \n",
    "#             return [Document(\n",
    "#                 page_content=text_content,\n",
    "#                 metadata={\"source\": file_path, \"file_type\": \"excel\", \"rows\": len(df), \"columns\": len(df.columns)}\n",
    "#             )]\n",
    "        \n",
    "#         elif doc_type == \"html\":\n",
    "#             return UnstructuredHTMLLoader(file_path).load()\n",
    "        \n",
    "#         else:\n",
    "#             print(f\"Unsupported document type: {doc_type}\")\n",
    "#             return []\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading {file_path}: {e}\")\n",
    "#         return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f710ee37-f66a-4a51-b12e-536b3c3c13f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import json\n",
    "# from langchain_core.documents import Document\n",
    "# from langchain.document_loaders import (\n",
    "#     TextLoader,\n",
    "#     UnstructuredMarkdownLoader,\n",
    "#     PyPDFLoader,\n",
    "#     Docx2txtLoader,\n",
    "#     UnstructuredHTMLLoader\n",
    "# )\n",
    "\n",
    "# def load_document(file_path, doc_type):\n",
    "#     \"\"\"Load a document based on its type.\"\"\"\n",
    "#     try:\n",
    "#         if doc_type == \"text\":\n",
    "#             return TextLoader(file_path).load()\n",
    "        \n",
    "#         elif doc_type == \"markdown\":\n",
    "#             return UnstructuredMarkdownLoader(file_path).load()\n",
    "        \n",
    "#         elif doc_type == \"pdf\":\n",
    "#             return PyPDFLoader(file_path).load()\n",
    "        \n",
    "#         elif doc_type == \"docx\":\n",
    "#             return Docx2txtLoader(file_path).load()\n",
    "        \n",
    "#         elif doc_type == \"excel\":\n",
    "#             df = pd.read_excel(file_path)\n",
    "            \n",
    "#             content = f\"Excel file with {len(df)} rows and {len(df.columns)} columns.\\n\\n\"\n",
    "#             content += f\"Columns: {', '.join(df.columns.astype(str))}\\n\\n\"\n",
    "#             content += f\"Sample (first 5 rows):\\n{df.head().to_string()}\\n\\n\"\n",
    "            \n",
    "#             try:\n",
    "#                 content += f\"Summary stats:\\n{df.describe().to_string()}\\n\\n\"\n",
    "#             except:\n",
    "#                 pass\n",
    "            \n",
    "#             return [Document(\n",
    "#                 page_content=content,\n",
    "#                 metadata={\"source\": file_path, \"file_type\": \"excel\", \"rows\": len(df), \"columns\": len(df.columns)}\n",
    "#             )]\n",
    "        \n",
    "#         elif doc_type == \"html\":\n",
    "#             return UnstructuredHTMLLoader(file_path).load()\n",
    "        \n",
    "#         elif doc_type == \"notebook\" or file_path.endswith(\".ipynb\"):\n",
    "#             with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#                 notebook = json.load(f)\n",
    "            \n",
    "#             content = \"\"\n",
    "#             for cell in notebook.get('cells', []):\n",
    "#                 if cell.get('cell_type') == 'markdown':\n",
    "#                     content += \"\".join(cell.get('source', [])) + \"\\n\\n\"\n",
    "#                 elif cell.get('cell_type') == 'code':\n",
    "#                     content += \"```python\\n\" + \"\".join(cell.get('source', [])) + \"\\n```\\n\\n\"\n",
    "            \n",
    "#             return [Document(\n",
    "#                 page_content=content,\n",
    "#                 metadata={\"source\": file_path, \"file_type\": \"jupyter_notebook\"}\n",
    "#             )]\n",
    "        \n",
    "#         else:\n",
    "#             print(f\"Unsupported document type: {doc_type}\")\n",
    "#             return []\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading {file_path}: {e}\")\n",
    "#         return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aae8ba1-8b04-4c0c-b114-ac9002fa5902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import json\n",
    "# from langchain_core.documents import Document\n",
    "# from langchain.document_loaders import (\n",
    "#     TextLoader,\n",
    "#     UnstructuredMarkdownLoader,\n",
    "#     PyPDFLoader,\n",
    "#     Docx2txtLoader,\n",
    "#     UnstructuredHTMLLoader\n",
    "# )\n",
    "\n",
    "# def load_document(file_path, doc_type):\n",
    "#     \"\"\"Load a document based on its type.\"\"\"\n",
    "#     try:\n",
    "#         if doc_type == \"text\":\n",
    "#             return TextLoader(file_path).load()\n",
    "\n",
    "#         elif doc_type == \"markdown\":\n",
    "#             return UnstructuredMarkdownLoader(file_path).load()\n",
    "\n",
    "#         elif doc_type == \"pdf\":\n",
    "#             return PyPDFLoader(file_path).load()\n",
    "\n",
    "#         elif doc_type == \"docx\":\n",
    "#             return Docx2txtLoader(file_path).load()\n",
    "\n",
    "#         elif doc_type == \"excel\":\n",
    "#             df = pd.read_excel(file_path)\n",
    "\n",
    "#             content = f\"Excel file containing {len(df)} rows and {len(df.columns)} columns.\\n\\n\"\n",
    "#             content += f\"Column names: {', '.join(df.columns.astype(str))}\\n\\n\"\n",
    "#             content += f\"Sample data (first 5 rows):\\n{df.head().to_string()}\\n\\n\"\n",
    "\n",
    "#             try:\n",
    "#                 content += f\"Numeric column statistics:\\n{df.describe().to_string()}\\n\\n\"\n",
    "#             except Exception:\n",
    "#                 pass\n",
    "\n",
    "#             return [Document(\n",
    "#                 page_content=content,\n",
    "#                 metadata={\"source\": file_path, \"file_type\": \"excel\", \"rows\": len(df), \"columns\": len(df.columns)}\n",
    "#             )]\n",
    "\n",
    "#         elif doc_type == \"html\":\n",
    "#             return UnstructuredHTMLLoader(file_path).load()\n",
    "\n",
    "#         elif doc_type == \"notebook\" or (isinstance(file_path, str) and file_path.endswith(\".ipynb\")):\n",
    "#             try:\n",
    "#                 with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#                     notebook = json.load(f)\n",
    "\n",
    "#                 content = f\"Jupyter Notebook with {len(notebook.get('cells', []))} cells\\n\\n\"\n",
    "\n",
    "#                 for i, cell in enumerate(notebook.get('cells', [])):\n",
    "#                     cell_type = cell.get('cell_type')\n",
    "#                     cell_source = \"\".join(cell.get('source', []))\n",
    "\n",
    "#                     if cell_type == 'markdown':\n",
    "#                         content += f\"[MARKDOWN CELL {i+1}]\\n{cell_source}\\n\\n\"\n",
    "#                     elif cell_type == 'code':\n",
    "#                         content += f\"[CODE CELL {i+1}]\\n```python\\n{cell_source}\\n```\\n\\n\"\n",
    "\n",
    "#                         outputs = cell.get('outputs', [])\n",
    "#                         output_text = \"\"\n",
    "#                         for output in outputs:\n",
    "#                             if 'text' in output:\n",
    "#                                 output_text += \"\".join(output['text'])\n",
    "#                             elif 'data' in output and 'text/plain' in output['data']:\n",
    "#                                 data_output = output['data']['text/plain']\n",
    "#                                 if isinstance(data_output, list):\n",
    "#                                     output_text += \"\".join(data_output)\n",
    "#                                 else:\n",
    "#                                     output_text += str(data_output)\n",
    "#                         if output_text:\n",
    "#                             content += f\"[OUTPUT]\\n{output_text}\\n\\n\"\n",
    "\n",
    "#                 return [Document(\n",
    "#                     page_content=content,\n",
    "#                     metadata={\"source\": file_path, \"file_type\": \"jupyter_notebook\"}\n",
    "#                 )]\n",
    "\n",
    "#             except json.JSONDecodeError:\n",
    "#                 print(f\"Warning: Could not parse {file_path} as JSON, falling back to text loader\")\n",
    "#                 return TextLoader(file_path).load()\n",
    "\n",
    "#         else:\n",
    "#             print(f\"Unsupported document type: {doc_type}\")\n",
    "#             return []\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading {file_path}: {e}\")\n",
    "#         return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc9feb72-609b-4f93-bc06-360337cb029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from langchain_core.documents import Document\n",
    "from langchain.document_loaders import (\n",
    "    TextLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    PyPDFLoader,\n",
    "    Docx2txtLoader,\n",
    "    UnstructuredHTMLLoader\n",
    ")\n",
    "\n",
    "def load_document(file_path, doc_type):\n",
    "    \"\"\"Load a document based on its type.\"\"\"\n",
    "    try:\n",
    "        if doc_type == \"text\":\n",
    "            return TextLoader(file_path).load()\n",
    "        \n",
    "        elif doc_type == \"markdown\":\n",
    "            return UnstructuredMarkdownLoader(file_path).load()\n",
    "        \n",
    "        elif doc_type == \"pdf\":\n",
    "            return PyPDFLoader(file_path).load()\n",
    "        \n",
    "        elif doc_type == \"docx\":\n",
    "            return Docx2txtLoader(file_path).load()\n",
    "        \n",
    "        elif doc_type == \"excel\":\n",
    "            df = pd.read_excel(file_path)\n",
    "            \n",
    "            content = f\"Excel file containing {len(df)} rows and {len(df.columns)} columns.\\n\\n\"\n",
    "            content += f\"Column names: {', '.join(df.columns.astype(str))}\\n\\n\"\n",
    "            content += f\"Sample data (first 5 rows):\\n{df.head().to_string()}\\n\\n\"\n",
    "            \n",
    "            try:\n",
    "                content += f\"Numeric column statistics:\\n{df.describe().to_string()}\\n\\n\"\n",
    "            except Exception:\n",
    "                pass\n",
    "            \n",
    "            return [Document(\n",
    "                page_content=content,\n",
    "                metadata={\"source\": file_path, \"file_type\": \"excel\", \"rows\": len(df), \"columns\": len(df.columns)}\n",
    "            )]\n",
    "        \n",
    "        elif doc_type == \"notebook\" or file_path.endswith(\".ipynb\"):\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    notebook = json.load(f)\n",
    "                \n",
    "                content = f\"Jupyter Notebook with {len(notebook.get('cells', []))} cells\\n\\n\"\n",
    "                \n",
    "                for i, cell in enumerate(notebook.get('cells', [])):\n",
    "                    cell_type = cell.get('cell_type')\n",
    "                    cell_source = \"\".join(cell.get('source', []))\n",
    "                    \n",
    "                    if cell_type == 'markdown':\n",
    "                        content += f\"[MARKDOWN CELL {i+1}]\\n{cell_source}\\n\\n\"\n",
    "                    elif cell_type == 'code':\n",
    "                        content += f\"[CODE CELL {i+1}]\\n```python\\n{cell_source}\\n```\\n\\n\"\n",
    "                        \n",
    "                        outputs = cell.get('outputs', [])\n",
    "                        if outputs:\n",
    "                            output_text = \"\"\n",
    "                            for output in outputs:\n",
    "                                if 'text' in output:\n",
    "                                    output_text += \"\".join(output['text'])\n",
    "                                elif 'data' in output and 'text/plain' in output['data']:\n",
    "                                    data_output = output['data']['text/plain']\n",
    "                                    if isinstance(data_output, list):\n",
    "                                        output_text += \"\".join(data_output)\n",
    "                                    else:\n",
    "                                        output_text += str(data_output)\n",
    "                            if output_text:\n",
    "                                content += f\"[OUTPUT]\\n{output_text}\\n\\n\"\n",
    "                \n",
    "                return [Document(\n",
    "                    page_content=content,\n",
    "                    metadata={\"source\": file_path, \"file_type\": \"jupyter_notebook\"}\n",
    "                )]\n",
    "            \n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Warning: Could not parse {file_path} as JSON, falling back to text loader\")\n",
    "                return TextLoader(file_path).load()\n",
    "        \n",
    "        elif doc_type == \"html\":\n",
    "            return UnstructuredHTMLLoader(file_path).load()\n",
    "        \n",
    "        else:\n",
    "            print(f\"Unsupported document type: {doc_type}\")\n",
    "            return []\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05b3995a-a3ff-4d0a-9c1a-f4445eb0e1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading documents...\n",
      "Successfully downloaded https://raw.githubusercontent.com/langchain-ai/langchain/master/README.md to C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\langchain_readme.md\n",
      "Successfully downloaded https://raw.githubusercontent.com/pinecone-io/examples/master/learn/generation/langchain/handbook/05-langchain-retrieval-augmentation.ipynb to C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\pinecone_example.ipynb\n",
      "Successfully downloaded https://arxiv.org/pdf/2005.11401.pdf to C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "Successfully downloaded https://calibre-ebook.com/downloads/demos/demo.docx to C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\calibre_demo.docx\n",
      "Successfully downloaded https://filesamples.com/samples/document/xlsx/sample1.xlsx to C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\sample_data.xlsx\n",
      "Successfully downloaded https://www.w3.org/WAI/tutorials/page-structure/ to C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\web_accessibility.html\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading documents...\")\n",
    "downloaded_files = []\n",
    "\n",
    "for url, filename, doc_type in DOCUMENTS:\n",
    "    file_path = os.path.join(DATA_DIR, filename)\n",
    "    if download_file(url, file_path):\n",
    "        downloaded_files.append((file_path, doc_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3785a0e6-bc5d-4842-ac44-11b8b16b113f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(downloaded_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814f54d0-6454-4a4a-92eb-aa8e0a2f838f",
   "metadata": {},
   "source": [
    "### Processing documents and creating chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f6073b3-3c35-4b7e-bd50-2658502a1c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document(s) from C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\langchain_readme.md\n",
      "Loaded 1 document(s) from C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\pinecone_example.ipynb\n",
      "Loaded 19 document(s) from C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "Loaded 1 document(s) from C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\calibre_demo.docx\n",
      "Loaded 1 document(s) from C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\sample_data.xlsx\n",
      "Loaded 1 document(s) from C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\web_accessibility.html\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "for file_path, doc_type in downloaded_files:\n",
    "    docs = load_document(file_path, doc_type)\n",
    "    documents.extend(docs)\n",
    "    print(f\"Loaded {len(docs)} document(s) from {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32ab3384-ff73-4418-9e0c-473763862ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93832bbe-34b9-4e80-901b-bd09c02fe0f9",
   "metadata": {},
   "source": [
    "### Debug document loading [FIXING]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c69ba90-8325-4345-a558-d6853d7a95bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_content_preview(doc):\n",
    "#     return doc.page_content[:1500] + \"...\" if len(doc.page_content) > 1500 else doc.page_content\n",
    "\n",
    "# def debug_document_loading():\n",
    "#     \"\"\"Debug function to check if Excel and Jupyter Notebook files are properly loaded.\"\"\"\n",
    "#     print(\"\\n🔍 DEBUGGING DOCUMENT LOADING...\")\n",
    "    \n",
    "#     # 1. Check Excel files\n",
    "#     excel_files = [doc for doc in documents if \"sample_data.xlsx\" in doc.metadata.get(\"source\", \"\")]\n",
    "#     print(f\"\\n📊 Excel Files: {len(excel_files)} documents found\")\n",
    "    \n",
    "#     if excel_files:\n",
    "#         for idx, doc in enumerate(excel_files):\n",
    "#             print(f\"  Excel Doc #{idx+1}:\")\n",
    "#             print(f\"  - Source: {doc.metadata.get('source')}\")\n",
    "#             print(f\"  - Content length: {len(doc.page_content)} characters\")\n",
    "#             print(f\"  - Metadata: {doc.metadata}\")\n",
    "#             # print(f\"  - Content preview: {doc.page_content[:200]}...\\n\")\n",
    "#             print(f\"  - Content preview: {get_content_preview(doc)}...\\n\")\n",
    "#     else:\n",
    "#         print(\"  ❌ No Excel documents were loaded!\")\n",
    "        \n",
    "#     # 2. Check Jupyter Notebook files\n",
    "#     notebook_files = [doc for doc in documents if \"pinecone_example.ipynb\" in doc.metadata.get(\"source\", \"\")]\n",
    "#     print(f\"\\n📓 Jupyter Notebooks: {len(notebook_files)} documents found\")\n",
    "    \n",
    "#     if notebook_files:\n",
    "#         for idx, doc in enumerate(notebook_files):\n",
    "#             print(f\"  Notebook Doc #{idx+1}:\")\n",
    "#             print(f\"  - Source: {doc.metadata.get('source')}\")\n",
    "#             print(f\"  - Content length: {len(doc.page_content)} characters\")\n",
    "#             print(f\"  - Metadata: {doc.metadata}\")\n",
    "#             # print(f\"  - Content preview: {doc.page_content[:200]}...\\n\")\n",
    "#             print(f\"  - Content preview: {get_content_preview(doc)}...\\n\")\n",
    "#     else:\n",
    "#         print(\"  ❌ No Jupyter Notebook documents were loaded!\")\n",
    "        \n",
    "#     # 3. Test direct loading of files\n",
    "#     print(\"\\n🧪 TESTING DIRECT FILE LOADING...\")\n",
    "    \n",
    "#     excel_path = os.path.join(DATA_DIR, \"sample_data.xlsx\")\n",
    "#     if os.path.exists(excel_path):\n",
    "#         print(f\"\\nTesting Excel loader on: {excel_path}\")\n",
    "#         try:\n",
    "#             import pandas as pd\n",
    "#             df = pd.read_excel(excel_path)\n",
    "#             print(f\"✅ Successfully loaded Excel with pandas: {len(df)} rows, {len(df.columns)} columns\")\n",
    "#             print(f\"Column names: {list(df.columns)}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ Error loading Excel with pandas: {e}\")\n",
    "    \n",
    "#     notebook_path = os.path.join(DATA_DIR, \"pinecone_example.ipynb\")\n",
    "#     if os.path.exists(notebook_path):\n",
    "#         print(f\"\\nTesting Jupyter Notebook loader on: {notebook_path}\")\n",
    "#         try:\n",
    "#             with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "#                 import json\n",
    "#                 notebook = json.load(f)\n",
    "#                 cell_count = len(notebook.get('cells', []))\n",
    "#                 print(f\"✅ Successfully loaded Notebook: {cell_count} cells found\")\n",
    "                \n",
    "#                 # Count cell types\n",
    "#                 md_cells = sum(1 for cell in notebook.get('cells', []) if cell.get('cell_type') == 'markdown')\n",
    "#                 code_cells = sum(1 for cell in notebook.get('cells', []) if cell.get('cell_type') == 'code')\n",
    "#                 print(f\"  - Markdown cells: {md_cells}\")\n",
    "#                 print(f\"  - Code cells: {code_cells}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ Error loading Jupyter Notebook: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e2dd5a5-4635-470f-b859-ce6ee9989726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_preview(doc):\n",
    "    return doc.page_content[:1500] + \"...\" if len(doc.page_content) > 1500 else doc.page_content\n",
    "\n",
    "def debug_document_loading():\n",
    "    \"\"\"Debug function to check if Excel and Jupyter Notebook files are properly loaded.\"\"\"\n",
    "    print(\"\\n🔍 DEBUGGING DOCUMENT LOADING...\")\n",
    "    \n",
    "    # 1. Check Excel files\n",
    "    excel_files = [doc for doc in documents if \"sample_data.xlsx\" in doc.metadata.get(\"source\", \"\")]\n",
    "    print(f\"\\n📊 Excel Files: {len(excel_files)} documents found\")\n",
    "    \n",
    "    if excel_files:\n",
    "        for idx, doc in enumerate(excel_files):\n",
    "            print(f\"  Excel Doc #{idx+1}:\")\n",
    "            print(f\"  - Source: {doc.metadata.get('source')}\")\n",
    "            print(f\"  - Content length: {len(doc.page_content)} characters\")\n",
    "            # print(f\"  - Content preview: {doc.page_content[:200]}...\\n\")\n",
    "            print(f\"  - Content preview: {get_content_preview(doc)}...\\n\")\n",
    "    else:\n",
    "        print(\"  ❌ No Excel documents were loaded!\")\n",
    "        \n",
    "    # 2. Check Jupyter Notebook files\n",
    "    notebook_files = [doc for doc in documents if \"ipynb\" in doc.metadata.get(\"source\", \"\")]\n",
    "    print(f\"\\n📓 Jupyter Notebooks: {len(notebook_files)} documents found\")\n",
    "    \n",
    "    if notebook_files:\n",
    "        for idx, doc in enumerate(notebook_files):\n",
    "            print(f\"  Notebook Doc #{idx+1}:\")\n",
    "            print(f\"  - Source: {doc.metadata.get('source')}\")\n",
    "            print(f\"  - Content length: {len(doc.page_content)} characters\")\n",
    "            # print(f\"  - Content preview: {doc.page_content[:200]}...\\n\")\n",
    "            print(f\"  - Content preview: {get_content_preview(doc)}...\\n\")\n",
    "    else:\n",
    "        print(\"  ❌ No Jupyter Notebook documents were loaded!\")\n",
    "        \n",
    "    # 3. Test direct loading of files\n",
    "    print(\"\\n🧪 TESTING DIRECT FILE LOADING...\")\n",
    "    \n",
    "    excel_path = os.path.join(DATA_DIR, \"sample_data.xlsx\")\n",
    "    if os.path.exists(excel_path):\n",
    "        print(f\"\\nTesting Excel loader on: {excel_path}\")\n",
    "        try:\n",
    "            import pandas as pd\n",
    "            df = pd.read_excel(excel_path)\n",
    "            print(f\"✅ Successfully loaded Excel with pandas: {len(df)} rows, {len(df.columns)} columns\")\n",
    "            print(f\"Column names: {list(df.columns)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading Excel with pandas: {e}\")\n",
    "    \n",
    "    notebook_path = os.path.join(DATA_DIR, \"pinecone_example.ipynb\")\n",
    "    if os.path.exists(notebook_path):\n",
    "        print(f\"\\nTesting Jupyter Notebook loader on: {notebook_path}\")\n",
    "        try:\n",
    "            with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "                import json\n",
    "                notebook = json.load(f)\n",
    "                cell_count = len(notebook.get('cells', []))\n",
    "                print(f\"✅ Successfully loaded Notebook: {cell_count} cells found\")\n",
    "                \n",
    "                # Count cell types\n",
    "                md_cells = sum(1 for cell in notebook.get('cells', []) if cell.get('cell_type') == 'markdown')\n",
    "                code_cells = sum(1 for cell in notebook.get('cells', []) if cell.get('cell_type') == 'code')\n",
    "                print(f\"  - Markdown cells: {md_cells}\")\n",
    "                print(f\"  - Code cells: {code_cells}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading Jupyter Notebook: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c5913aa-a83e-4cfa-b269-7003f2c1525c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 DEBUGGING DOCUMENT LOADING...\n",
      "\n",
      "📊 Excel Files: 1 documents found\n",
      "  Excel Doc #1:\n",
      "  - Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\sample_data.xlsx\n",
      "  - Content length: 1077 characters\n",
      "  - Content preview: Excel file containing 390 rows and 5 columns.\n",
      "\n",
      "Column names: Postcode, Sales_Rep_ID, Sales_Rep_Name, Year, Value\n",
      "\n",
      "Sample data (first 5 rows):\n",
      "   Postcode  Sales_Rep_ID Sales_Rep_Name  Year         Value\n",
      "0      2121           456           Jane  2011  84219.497311\n",
      "1      2092           789         Ashish  2012  28322.192268\n",
      "2      2128           456           Jane  2013  81878.997241\n",
      "3      2073           123           John  2011  44491.142121\n",
      "4      2134           789         Ashish  2012  71837.720959\n",
      "\n",
      "Numeric column statistics:\n",
      "          Postcode  Sales_Rep_ID         Year         Value\n",
      "count   390.000000    390.000000   390.000000    390.000000\n",
      "mean   2098.430769    456.000000  2012.000000  49229.388305\n",
      "std      58.652206    272.242614     0.817545  28251.271309\n",
      "min    2000.000000    123.000000  2011.000000    106.360599\n",
      "25%    2044.000000    123.000000  2011.000000  26101.507357\n",
      "50%    2097.500000    456.000000  2012.000000  47447.363750\n",
      "75%    2142.000000    789.000000  2013.000000  72277.800608\n",
      "max    2206.000000    789.000000  2013.000000  99878.489209\n",
      "\n",
      "...\n",
      "\n",
      "\n",
      "📓 Jupyter Notebooks: 1 documents found\n",
      "  Notebook Doc #1:\n",
      "  - Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\pinecone_example.ipynb\n",
      "  - Content length: 27923 characters\n",
      "  - Content preview: Jupyter Notebook with 44 cells\n",
      "\n",
      "[MARKDOWN CELL 1]\n",
      "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/05-langchain-retrieval-augmentation.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/05-langchain-retrieval-augmentation.ipynb)\n",
      "\n",
      "[MARKDOWN CELL 2]\n",
      "#### [LangChain Handbook](https://pinecone.io/learn/langchain)\n",
      "\n",
      "# Retrieval Augmentation\n",
      "\n",
      "**L**arge **L**anguage **M**odels (LLMs) have a data freshness problem. The most powerful LLMs in the world, like GPT-4, have no idea about recent world events.\n",
      "\n",
      "The world of LLMs is frozen in time. Their world exists as a static snapshot of the world as it was within their training data.\n",
      "\n",
      "A solution to this problem is *retrieval augmentation*. The idea behind this is that we retrieve relevant information from an external knowledge base and give that information to our LLM. In this notebook we will learn how to do that.\n",
      "\n",
      "[![Open fast notebook](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/fast-link.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/generation/langchain/handbook/05-langchain-retrieval-augmentation-fast.ipynb)\n",
      "\n",
      "To begin, we must install the prerequisite libraries that we will be using......\n",
      "\n",
      "\n",
      "🧪 TESTING DIRECT FILE LOADING...\n",
      "\n",
      "Testing Excel loader on: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\sample_data.xlsx\n",
      "✅ Successfully loaded Excel with pandas: 390 rows, 5 columns\n",
      "Column names: ['Postcode', 'Sales_Rep_ID', 'Sales_Rep_Name', 'Year', 'Value']\n",
      "\n",
      "Testing Jupyter Notebook loader on: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\pinecone_example.ipynb\n",
      "✅ Successfully loaded Notebook: 44 cells found\n",
      "  - Markdown cells: 20\n",
      "  - Code cells: 24\n"
     ]
    }
   ],
   "source": [
    "# Call the debug function\n",
    "debug_document_loading()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8927a855-160d-4d8b-a75a-92e04106ac94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Release Notes\\n\\nCI\\n\\nPyPI - License\\n\\nPyPI - Downloads\\n\\nGitHub star chart\\n\\nOpen Issues\\n\\nOpen in Dev Containers\\n\\n\\n\\nTwitter\\n\\nCodSpeed Badge\\n\\n[!NOTE] Looking for the JS/TS library? Check out LangChain.js.\\n\\nLangChain is a framework for building LLM-powered applications. It helps you chain together interope'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# documents[0].page_content\n",
    "documents[0].page_content[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b98da5a-9ca7-4a44-95a5-4bb811e8baf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\langchain_readme.md\n"
     ]
    }
   ],
   "source": [
    "source = documents[0].metadata.get(\"source\", \"No source metadata found\")\n",
    "print(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "579733e7-69b3-401b-9474-728c15f98589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Document 1 source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\langchain_readme.md\n",
      "📄 Document 2 source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\pinecone_example.ipynb\n",
      "📄 Document 3 source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "📄 Document 4 source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "📄 Document 5 source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "📄 Document 6 source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "📄 Document 7 source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "📄 Document 8 source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "📄 Document 9 source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "📄 Document 10 source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "📄 Document 11 source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "📄 Document 12 source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "📄 Document 13 source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "📄 Document 14 source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "📄 Document 15 source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "📄 Document 16 source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "📄 Document 17 source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "📄 Document 18 source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "📄 Document 19 source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "📄 Document 20 source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "📄 Document 21 source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "📄 Document 22 source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\calibre_demo.docx\n",
      "📄 Document 23 source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\sample_data.xlsx\n",
      "📄 Document 24 source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\web_accessibility.html\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(documents):\n",
    "    source = doc.metadata.get(\"source\", \"No source metadata found\")\n",
    "    print(f\"📄 Document {i + 1} source: {source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6f52cbb-4406-4b0f-bf4f-f8422b29696b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Document 1\n",
      "   ✏️ Total characters: 2782\n",
      "   🔗 Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\langchain_readme.md\n",
      "   🔍 Preview: Release Notes  CI  PyPI - License  PyPI - Downloads  GitHub star chart  Open Issues  Open in Dev Containers    Twitter  CodSpeed Badge  [!NOTE] Looking for the JS/TS library? Check out LangChain.js.  \n",
      "\n",
      "📄 Document 2\n",
      "   ✏️ Total characters: 27923\n",
      "   🔗 Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\pinecone_example.ipynb\n",
      "   🔍 Preview: Jupyter Notebook with 44 cells  [MARKDOWN CELL 1] [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master\n",
      "\n",
      "📄 Document 3\n",
      "   ✏️ Total characters: 2900\n",
      "   🔗 Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   🔍 Preview: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks Patrick Lewis†‡, Ethan Perez⋆, Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†, Mike Lewis†, W\n",
      "\n",
      "📄 Document 4\n",
      "   ✏️ Total characters: 4590\n",
      "   🔗 Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   🔍 Preview: The\tDivine Comedy\t(x) q  Query  Encoder  q(x)  MIPS p θ  Generator pθ (Parametric)  Margin-  alize  This\t14th\tcentury\twork is\tdivided\tinto\t3 sections:\t\"Inferno\", \"Purgatorio\"\t& \"Paradiso\"\t\t\t\t\t\t\t\t\t(y) \n",
      "\n",
      "📄 Document 5\n",
      "   ✏️ Total characters: 3658\n",
      "   🔗 Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   🔍 Preview: by θthat generates a current token based on a context of the previous i−1 tokens y1:i−1, the original input xand a retrieved passage z. To train the retriever and generator end-to-end, we treat the re\n",
      "\n",
      "📄 Document 6\n",
      "   ✏️ Total characters: 4206\n",
      "   🔗 Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   🔍 Preview: minimize the negative marginal log-likelihood of each target, ∑ j−log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it req\n",
      "\n",
      "📄 Document 7\n",
      "   ✏️ Total characters: 4556\n",
      "   🔗 Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   🔍 Preview: MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be answered in a way that matches the reference answer without access to the gold passages, such as “What is the w\n",
      "\n",
      "📄 Document 8\n",
      "   ✏️ Total characters: 4110\n",
      "   🔗 Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   🔍 Preview: Table 1: Open-Domain QA Test Scores. For TQA, left column uses the standard test set for Open- Domain QA, right column uses the TQA-Wiki test set. See Appendix D for further details. Model NQ TQA WQ C\n",
      "\n",
      "📄 Document 9\n",
      "   ✏️ Total characters: 4331\n",
      "   🔗 Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   🔍 Preview: Document 1: his works are considered classics of American literature ... His wartime experiences formed the basis for his novel ”A Farewell to Arms”(1929) ... Document 2: ... artists of the 1920s ”Los\n",
      "\n",
      "📄 Document 10\n",
      "   ✏️ Total characters: 3186\n",
      "   🔗 Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   🔍 Preview: Table 4: Human assessments for the Jeopardy Question Generation Task. Factuality Speciﬁcity BART better 7.1% 16.8% RAG better 42.7% 37.4% Both good 11.7% 11.8% Both poor 17.7% 6.9% No majority 20.8% 2\n",
      "\n",
      "📄 Document 11\n",
      "   ✏️ Total characters: 4124\n",
      "   🔗 Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   🔍 Preview: General-Purpose Architectures for NLP Prior work on general-purpose architectures for NLP tasks has shown great success without the use of retrieval. A single, pre-trained language model has been show\n",
      "\n",
      "📄 Document 12\n",
      "   ✏️ Total characters: 3778\n",
      "   🔗 Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   🔍 Preview: Broader Impact This work offers several positive societal beneﬁts over previous work: the fact that it is more strongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinat\n",
      "\n",
      "📄 Document 13\n",
      "   ✏️ Total characters: 3767\n",
      "   🔗 Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   🔍 Preview: [7] Christopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Compre- hension. arXiv:1710.10723 [cs], October 2017. URL http://arxiv.org/abs/1710.10723. arXiv: 1710.10723. [8] J\n",
      "\n",
      "📄 Document 14\n",
      "   ✏️ Total characters: 3990\n",
      "   🔗 Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   🔍 Preview: [20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: Retrieval-augmented language model pre-training. ArXiv, abs/2002.08909, 2020. URL https: //arxiv.org/abs/2002.08909\n",
      "\n",
      "📄 Document 15\n",
      "   ✏️ Total characters: 4010\n",
      "   🔗 Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   🔍 Preview: for Computational Linguistics, pages 6086–6096, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1612. URL https://www.aclweb.org/ anthology/P19-1612. [32] M\n",
      "\n",
      "📄 Document 16\n",
      "   ✏️ Total characters: 3984\n",
      "   🔗 Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   🔍 Preview: approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings. CEUR-WS.o\n",
      "\n",
      "📄 Document 17\n",
      "   ✏️ Total characters: 4006\n",
      "   🔗 Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   🔍 Preview: [56] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for fact extraction and VERiﬁcation. In Proceedings of the 2018 Conference of the North \n",
      "\n",
      "📄 Document 18\n",
      "   ✏️ Total characters: 1138\n",
      "   🔗 Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   🔍 Preview: [66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Cl\n",
      "\n",
      "📄 Document 19\n",
      "   ✏️ Total characters: 2558\n",
      "   🔗 Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   🔍 Preview: Appendices for Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks A Implementation Details For Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models. Fo\n",
      "\n",
      "📄 Document 20\n",
      "   ✏️ Total characters: 4243\n",
      "   🔗 Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   🔍 Preview: D Further Details on Open-Domain QA For open-domain QA, multiple answer annotations are often available for a given question. These answer annotations are exploited by extractive models during trainin\n",
      "\n",
      "📄 Document 21\n",
      "   ✏️ Total characters: 1962\n",
      "   🔗 Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   🔍 Preview: Table 7: Number of instances in the datasets used. *A hidden subset of this data is used for evaluation Task Train Development Test Natural Questions 79169 8758 3611 TriviaQA 78786 8838 11314 WebQuest\n",
      "\n",
      "📄 Document 22\n",
      "   ✏️ Total characters: 9271\n",
      "   🔗 Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\calibre_demo.docx\n",
      "   🔍 Preview: Demonstration of DOCX support in calibre  This document demonstrates the ability of the calibre DOCX Input plugin to convert the various typographic features in a Microsoft Word (2007 and newer) docum\n",
      "\n",
      "📄 Document 23\n",
      "   ✏️ Total characters: 1077\n",
      "   🔗 Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\sample_data.xlsx\n",
      "   🔍 Preview: Excel file containing 390 rows and 5 columns.  Column names: Postcode, Sales_Rep_ID, Sales_Rep_Name, Year, Value  Sample data (first 5 rows):    Postcode  Sales_Rep_ID Sales_Rep_Name  Year         Val\n",
      "\n",
      "📄 Document 24\n",
      "   ✏️ Total characters: 2818\n",
      "   🔗 Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\web_accessibility.html\n",
      "   🔍 Preview: Page Structure Tutorial  Well-structured content allows more efficient navigation and processing. Use HTML and WAI-ARIA to improve navigation and orientation on web pages and in applications.  Page Re\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for i, doc in enumerate(documents):\n",
    "#     source = doc.metadata.get(\"source\", \"No source metadata found\")\n",
    "#     preview = doc.page_content[:200].replace('\\n', ' ')\n",
    "#     print(f\"📄 Document {i + 1} source: {source}\")\n",
    "#     print(f\"   🔍 Preview: {preview}\\n\")\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    source = doc.metadata.get(\"source\", \"No source metadata found\")\n",
    "    content = doc.page_content\n",
    "    preview = content[:200].replace('\\n', ' ')\n",
    "    total_chars = len(content)\n",
    "    \n",
    "    print(f\"📄 Document {i + 1}\")\n",
    "    print(f\"   ✏️ Total characters: {total_chars}\")\n",
    "    print(f\"   🔗 Source: {source}\")\n",
    "    print(f\"   🔍 Preview: {preview}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5137a85e-7030-4581-81e9-af5aced22b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Total documents: 24\n",
      "🔢 Total tokens: 28862\n",
      "💰 Estimated embedding cost: $0.000577\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def estimate_embedding_cost(texts, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    Estimate total number of tokens and embedding cost for OpenAI embedding models.\n",
    "    \"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    total_tokens = sum(len(encoding.encode(text)) for text in texts)\n",
    "\n",
    "    prices_per_1k = {\n",
    "        \"text-embedding-3-small\": 0.00002,\n",
    "        \"text-embedding-3-large\": 0.00013,\n",
    "    }\n",
    "\n",
    "    price_per_1k = prices_per_1k.get(model, 0.00002)\n",
    "    estimated_cost = (total_tokens / 1000) * price_per_1k\n",
    "\n",
    "    return {\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"estimated_cost_usd\": round(estimated_cost, 6)\n",
    "    }\n",
    "\n",
    "# 🧾 Extract text from your 24 documents and estimate\n",
    "document_texts = [doc.page_content for doc in documents]\n",
    "\n",
    "result = estimate_embedding_cost(document_texts, model=\"text-embedding-3-small\")\n",
    "\n",
    "print(f\"📚 Total documents: {len(documents)}\")\n",
    "print(f\"🔢 Total tokens: {result['total_tokens']}\")\n",
    "print(f\"💰 Estimated embedding cost: ${result['estimated_cost_usd']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "550cf837-6e11-49d1-aae2-b206ff225836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📄 Per-document token & cost breakdown:\n",
      "   Doc  1:   538 tokens | Cost: $0.000011 | Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\langchain_readme.md\n",
      "   Doc  2:  6863 tokens | Cost: $0.000137 | Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\pinecone_example.ipynb\n",
      "   Doc  3:   688 tokens | Cost: $0.000014 | Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   Doc  4:  1148 tokens | Cost: $0.000023 | Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   Doc  5:   938 tokens | Cost: $0.000019 | Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   Doc  6:  1058 tokens | Cost: $0.000021 | Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   Doc  7:  1034 tokens | Cost: $0.000021 | Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   Doc  8:  1203 tokens | Cost: $0.000024 | Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   Doc  9:  1093 tokens | Cost: $0.000022 | Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   Doc 10:  1128 tokens | Cost: $0.000023 | Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   Doc 11:   878 tokens | Cost: $0.000018 | Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   Doc 12:   988 tokens | Cost: $0.000020 | Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   Doc 13:  1153 tokens | Cost: $0.000023 | Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   Doc 14:  1200 tokens | Cost: $0.000024 | Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   Doc 15:  1183 tokens | Cost: $0.000024 | Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   Doc 16:  1207 tokens | Cost: $0.000024 | Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   Doc 17:  1223 tokens | Cost: $0.000024 | Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   Doc 18:   355 tokens | Cost: $0.000007 | Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   Doc 19:   574 tokens | Cost: $0.000011 | Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   Doc 20:   941 tokens | Cost: $0.000019 | Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   Doc 21:   476 tokens | Cost: $0.000010 | Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "   Doc 22:  2028 tokens | Cost: $0.000041 | Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\calibre_demo.docx\n",
      "   Doc 23:   414 tokens | Cost: $0.000008 | Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\sample_data.xlsx\n",
      "   Doc 24:   551 tokens | Cost: $0.000011 | Source: C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\web_accessibility.html\n"
     ]
    }
   ],
   "source": [
    "# print(\"\\n📄 Per-document token breakdown:\")\n",
    "# for i, doc in enumerate(documents):\n",
    "#     text = doc.page_content\n",
    "#     tokens = len(tiktoken.encoding_for_model(\"text-embedding-3-small\").encode(text))\n",
    "#     cost = round((tokens / 1000) * 0.00002, 6)\n",
    "#     # print(f\"   Doc {i+1:>2}: {tokens} tokens | ${cost} | Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "#     print(f\"   Doc {i+1:>2}: {tokens:>5} tokens | Cost: ${cost:.6f} | Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "price_per_1k = 0.00002\n",
    "\n",
    "print(\"\\n📄 Per-document token & cost breakdown:\")\n",
    "for i, doc in enumerate(documents):\n",
    "    text = doc.page_content\n",
    "    tokens = len(encoding.encode(text))\n",
    "    cost = (tokens / 1000) * price_per_1k\n",
    "    source = doc.metadata.get(\"source\", \"N/A\")\n",
    "    print(f\"   Doc {i+1:>2}: {tokens:>5} tokens | Cost: ${cost:.6f} | Source: {source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbef5acf-a323-4d20-9c08-43e27d3b1c89",
   "metadata": {},
   "source": [
    "### Create chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcbfcb08-d7c8-440e-951d-217bca79f5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=1000,\n",
    "#     chunk_overlap=100,\n",
    "#     length_function=len,\n",
    "# )\n",
    "\n",
    "# chunks = text_splitter.split_documents(documents)\n",
    "# print(f\"Created {len(chunks)} chunks from {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dee45e-3bac-4f4c-8ce8-118390cd4126",
   "metadata": {},
   "source": [
    "### Create chunks [FIXING]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c16e4736-1d9a-4118-a67d-1db6fbab6fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Modify Chunking for Excel Data\n",
    "# # - For Excel data, which is more structured than regular text, consider using a different chunking strategy:\n",
    "\n",
    "# # Before creating chunks\n",
    "# regular_docs = [doc for doc in documents if \"sample_data.xlsx\" not in doc.metadata.get(\"source\", \"\")]\n",
    "# excel_docs = [doc for doc in documents if \"sample_data.xlsx\" in doc.metadata.get(\"source\", \"\")]\n",
    "\n",
    "# # Process regular documents with RecursiveCharacterTextSplitter\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=1000,\n",
    "#     chunk_overlap=100,\n",
    "#     length_function=len,\n",
    "# )\n",
    "# regular_chunks = text_splitter.split_documents(regular_docs)\n",
    "\n",
    "# # Process Excel documents with less aggressive chunking to preserve table context\n",
    "# excel_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=3000,  # Larger chunk size for tabular data\n",
    "#     chunk_overlap=200,\n",
    "#     length_function=len,\n",
    "# )\n",
    "# excel_chunks = excel_splitter.split_documents(excel_docs)\n",
    "\n",
    "# # Combine chunks\n",
    "# chunks = regular_chunks + excel_chunks\n",
    "# print(f\"Created {len(chunks)} chunks from {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a057568b-306e-465d-8d37-83e05a9dda49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 149 chunks from 24 documents\n",
      "  - Regular documents: 148 chunks\n",
      "  - Excel documents: 1 chunks\n"
     ]
    }
   ],
   "source": [
    "# Modify Document Processing for Better Retrieval\n",
    "# - When creating chunks, add special handling for Excel data:\n",
    "\n",
    "# Before chunking, separate Excel documents for special handling\n",
    "regular_docs = [doc for doc in documents if \"sample_data.xlsx\" not in doc.metadata.get(\"source\", \"\")]\n",
    "excel_docs = [doc for doc in documents if \"sample_data.xlsx\" in doc.metadata.get(\"source\", \"\")]\n",
    "\n",
    "# Process regular documents with RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    ")\n",
    "regular_chunks = text_splitter.split_documents(regular_docs)\n",
    "\n",
    "# For Excel documents, use larger chunks with less overlap\n",
    "excel_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,  # Larger to keep more Excel context together\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    ")\n",
    "excel_chunks = excel_splitter.split_documents(excel_docs)\n",
    "\n",
    "# Combine all chunks\n",
    "chunks = regular_chunks + excel_chunks\n",
    "print(f\"Created {len(chunks)} chunks from {len(documents)} documents\")\n",
    "print(f\"  - Regular documents: {len(regular_chunks)} chunks\")\n",
    "print(f\"  - Excel documents: {len(excel_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80224cb-d69f-4b74-bbb0-4acbde84cad7",
   "metadata": {},
   "source": [
    "### Add Metadata [FIXING]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dcd24bc1-8472-4fc6-ae3d-ac85d5f37bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\pinecone_example.ipynb', 'file_type': 'jupyter_notebook', 'content_type': 'notebook_content', 'document_type': 'jupyter_notebook'}\n",
      "{'source': 'C:\\\\Users\\\\pavel\\\\projects\\\\ai-llm-agents\\\\rag\\\\data-002\\\\sample_data.xlsx', 'file_type': 'excel', 'rows': 390, 'columns': 5, 'content_type': 'excel_data', 'document_type': 'spreadsheet'}\n"
     ]
    }
   ],
   "source": [
    "# Add Metadata\n",
    "# - Adding specific metadata tags can help improve retrieval\n",
    "\n",
    "for chunk in chunks:\n",
    "    source = chunk.metadata.get(\"source\", \"\")\n",
    "    if \"sample_data.xlsx\" in source:\n",
    "        chunk.metadata[\"content_type\"] = \"excel_data\"\n",
    "        chunk.metadata[\"document_type\"] = \"spreadsheet\"\n",
    "        print(chunk.metadata)\n",
    "    elif source.endswith(\".ipynb\"):\n",
    "        chunk.metadata[\"content_type\"] = \"notebook_content\"\n",
    "        chunk.metadata[\"document_type\"] = \"jupyter_notebook\"\n",
    "        print(chunk.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c45f604-33ca-449e-bdc9-38ed89c0d476",
   "metadata": {},
   "source": [
    "### Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "059f2711-e105-4fa0-b8a8-3abaa4d89205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating embeddings using OpenAI...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCreating embeddings using OpenAI...\")\n",
    "\n",
    "# Initialize OpenAI embeddings\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee94d2a-d242-467e-b8a9-d536db9845b5",
   "metadata": {},
   "source": [
    "### Initialize pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9a56819-c019-485f-888d-e26317bcea17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing Pinecone...\n",
      "Creating new Pinecone index: document-retrieval\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nInitializing Pinecone...\")\n",
    "\n",
    "# Create Pinecone instance\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Create or use existing index\n",
    "index_name = PINECONE_INDEX_NAME\n",
    "dimension = 1536  # OpenAI embedding dimension\n",
    "\n",
    "# Check if index exists\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    print(f\"Creating new Pinecone index: {index_name}\")\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=dimension,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=PINECONE_ENVIRONMENT\n",
    "        )\n",
    "    )\n",
    "    # Wait for index to be ready\n",
    "    time.sleep(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdaea49-8a49-4823-8efe-85e529132a28",
   "metadata": {},
   "source": [
    "### Create vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cbce78b9-ec9f-43fa-83fc-8690c643dfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Storing embeddings in Pinecone...\n",
      "Documents loaded into Pinecone successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStoring embeddings in Pinecone...\")\n",
    "\n",
    "# Connect to the index with Pinecone API\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Create Pinecone vector store with LangChain\n",
    "vectorstore = PineconeVectorStore.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    index_name=index_name\n",
    ")\n",
    "\n",
    "print(\"Documents loaded into Pinecone successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb98f79-57af-4b5a-8909-4c3b0606c8db",
   "metadata": {},
   "source": [
    "### Create retrieval chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd00ca9a-bf11-4baa-88ca-d3484ca86873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting up retrieval system...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSetting up retrieval system...\")\n",
    "\n",
    "# Create a retriever from the vector store\n",
    "# retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4}\n",
    ")\n",
    "\n",
    "# Create a RetrievalQA chain\n",
    "# qa_chain = RetrievalQA.from_chain_type(\n",
    "#     llm=OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY),\n",
    "#     chain_type=\"stuff\",\n",
    "#     retriever=retriever\n",
    "# )\n",
    "llm = ChatOpenAI(temperature=0, model_name=MODEL_GPT)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1f2133-a165-405f-a0fc-4009a1662533",
   "metadata": {},
   "source": [
    "### Test retrieval with sample questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6e83b4e-33e5-489d-b67e-09c403517ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query and display results\n",
    "def ask_question(question):\n",
    "    print(f\"\\n\\n❓ Question: {question}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    result = qa_chain.invoke({\"query\": question})\n",
    "    \n",
    "    print(\"📘 Answer:\")\n",
    "    print(result[\"result\"])\n",
    "\n",
    "    sources = [doc.metadata.get(\"source\", \"N/A\") for doc in result['source_documents']]\n",
    "    print(\"\\n📁 Sources used in the answer:\")\n",
    "    for i, src in enumerate(sources, 1):\n",
    "        print(f\"{i:>2}. {src}\")\n",
    "    \n",
    "    # print(\"\\nSource Documents:\")\n",
    "    # for i, doc in enumerate(result[\"source_documents\"]):\n",
    "    #     print(f\"\\nDocument {i+1}:\")\n",
    "    #     source = doc.metadata.get('source', 'Unknown')\n",
    "    #     if source.endswith('.txt'):\n",
    "    #         doc_type = \"Alice in Wonderland\"\n",
    "    #     elif source.endswith('.md'):\n",
    "    #         doc_type = \"Flask Documentation\"\n",
    "    #     elif source.endswith('.pdf'):\n",
    "    #         doc_type = \"Attention Research Paper\"\n",
    "    #     else:\n",
    "    #         doc_type = \"Unknown\"\n",
    "            \n",
    "    #     print(f\"Source: {doc_type} ({source})\")\n",
    "    #     if 'page' in doc.metadata:\n",
    "    #         print(f\"Page: {doc.metadata['page']}\")\n",
    "    #     print(f\"Content: {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f14e148f-0490-4c8b-97b7-9b0d53aae488",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    # Markdown questions (LangChain README)\n",
    "    \"What is LangChain and what problems does it solve?\",\n",
    "    \"What are the main components or modules of the LangChain framework?\",\n",
    "    \n",
    "    # Text questions (Pinecone RAG Notebook)\n",
    "    \"How does Retrieval Augmented Generation (RAG) work with LangChain and Pinecone?\",\n",
    "    \"What are the key steps to implement a RAG system using Pinecone?\",\n",
    "    \n",
    "    # PDF questions (Neural Networks Paper)\n",
    "    \"What are the key findings or contributions in the neural networks paper?\",\n",
    "    \"What methodologies or algorithms are discussed for training neural networks?\",\n",
    "    \n",
    "    # DOCX questions (Calibre Demo)\n",
    "    \"What features of document formatting are showcased in the Calibre demo document?\",\n",
    "    \"What is the main content or purpose of the Calibre demo document?\",\n",
    "    \n",
    "    # Excel questions (Sample Spreadsheet)\n",
    "    \"What kind of data is in the Excel file?\",\n",
    "    \"How many rows and columns are in the sample Excel data?\",\n",
    "    # \"What types of data are contained in the sample Excel spreadsheet?\",\n",
    "    # \"How is the data structured in the Excel file in terms of organization?\",\n",
    "    \n",
    "    # HTML questions (W3C Accessibility)\n",
    "    \"What are the key principles for creating accessible web page structures?\",\n",
    "    \"What HTML elements or techniques does W3C recommend for web accessibility?\"\n",
    "]\n",
    "\n",
    "# questions = [\n",
    "#     # Markdown questions\n",
    "#     \"What is LangChain and what are its main components?\",\n",
    "#     \"How do LangChain agents work?\",\n",
    "    \n",
    "#     # Text questions\n",
    "#     \"What is RAG and how does it work with Pinecone?\",\n",
    "#     \"What are the key features of Pinecone for vector search?\",\n",
    "    \n",
    "#     # PDF questions\n",
    "#     \"What are the main types of neural networks discussed in the PDF?\",\n",
    "#     \"What applications of neural networks are mentioned in the paper?\",\n",
    "    \n",
    "#     # DOCX questions\n",
    "#     \"What accessibility issues are mentioned in the report?\",\n",
    "#     \"What recommendations are made for improving accessibility?\",\n",
    "    \n",
    "#     # Excel questions\n",
    "#     \"What kind of data is in the Excel file?\",\n",
    "#     \"How many rows and columns are in the sample Excel data?\",\n",
    "    \n",
    "#     # HTML questions\n",
    "#     \"What are the main principles of web accessibility?\",\n",
    "#     \"How should page structure be organized for accessibility?\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c618a0b0-0579-4e23-90e1-39f2d59d3c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "❓ Question: What is LangChain and what problems does it solve?\n",
      "--------------------------------------------------------------------------------\n",
      "📘 Answer:\n",
      "LangChain is a framework for building applications powered by Large Language Models (LLMs). It helps developers chain together interoperable components and third-party integrations to simplify AI application development. \n",
      "\n",
      "The problems LangChain solves include:\n",
      "\n",
      "1. **Real-time Data Augmentation**: It allows easy connection of LLMs to diverse data sources and systems, enabling the use of up-to-date information in applications.\n",
      "\n",
      "2. **Model Interoperability**: Developers can swap models in and out as needed, allowing for experimentation and adaptation to find the best model for specific application needs.\n",
      "\n",
      "3. **Simplified Development**: By providing a standard interface for models, embeddings, vector stores, and more, LangChain streamlines the development process for LLM applications.\n",
      "\n",
      "4. **Future-proofing**: As the underlying technology evolves, LangChain's abstractions help developers adapt quickly without losing momentum.\n",
      "\n",
      "Overall, LangChain enhances the development of LLM applications by providing a comprehensive suite of tools and integrations.\n",
      "\n",
      "📁 Sources used in the answer:\n",
      " 1. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\langchain_readme.md\n",
      " 2. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\langchain_readme.md\n",
      " 3. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\langchain_readme.md\n",
      " 4. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\pinecone_example.ipynb\n",
      "\n",
      "\n",
      "❓ Question: What are the main components or modules of the LangChain framework?\n",
      "--------------------------------------------------------------------------------\n",
      "📘 Answer:\n",
      "The main components or modules of the LangChain framework include:\n",
      "\n",
      "1. **Models**: Standard interface for various language models.\n",
      "2. **Embeddings**: Tools for working with embeddings.\n",
      "3. **Vector Stores**: Storage solutions for vector representations.\n",
      "4. **Retrievers**: Mechanisms for retrieving relevant data.\n",
      "5. **Integrations**: Connections to third-party tools and services.\n",
      "\n",
      "These components work together to simplify the development of LLM-powered applications and allow for model interoperability and real-time data augmentation.\n",
      "\n",
      "📁 Sources used in the answer:\n",
      " 1. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\langchain_readme.md\n",
      " 2. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\langchain_readme.md\n",
      " 3. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\langchain_readme.md\n",
      " 4. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\pinecone_example.ipynb\n",
      "\n",
      "\n",
      "❓ Question: How does Retrieval Augmented Generation (RAG) work with LangChain and Pinecone?\n",
      "--------------------------------------------------------------------------------\n",
      "📘 Answer:\n",
      "I don't know.\n",
      "\n",
      "📁 Sources used in the answer:\n",
      " 1. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      " 2. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      " 3. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      " 4. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "\n",
      "\n",
      "❓ Question: What are the key steps to implement a RAG system using Pinecone?\n",
      "--------------------------------------------------------------------------------\n",
      "📘 Answer:\n",
      "I don't know.\n",
      "\n",
      "📁 Sources used in the answer:\n",
      " 1. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      " 2. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      " 3. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      " 4. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "\n",
      "\n",
      "❓ Question: What are the key findings or contributions in the neural networks paper?\n",
      "--------------------------------------------------------------------------------\n",
      "📘 Answer:\n",
      "I don't know.\n",
      "\n",
      "📁 Sources used in the answer:\n",
      " 1. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      " 2. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      " 3. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      " 4. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "\n",
      "\n",
      "❓ Question: What methodologies or algorithms are discussed for training neural networks?\n",
      "--------------------------------------------------------------------------------\n",
      "📘 Answer:\n",
      "The methodologies and algorithms discussed for training neural networks include:\n",
      "\n",
      "1. **Mixed Precision Training** - This technique is mentioned in the work by Paulius Micikevicius et al. (2018), which focuses on optimizing training by using lower precision arithmetic to speed up computation and reduce memory usage.\n",
      "\n",
      "2. **Stochastic Optimization** - The Adam optimizer, introduced by Diederik P. Kingma and Jimmy Ba, is a popular method for stochastic optimization that combines the advantages of two other extensions of stochastic gradient descent.\n",
      "\n",
      "3. **Retrieval-based Architectures** - There is significant work on learning to retrieve documents using pre-trained neural language models, which can be fine-tuned for various tasks, as discussed in the context of information retrieval.\n",
      "\n",
      "4. **Memory Networks** - The document index is likened to a large external memory for neural networks, allowing them to attend to relevant information, similar to memory networks.\n",
      "\n",
      "5. **Reinforcement Learning** - Some work optimizes retrieval modules using reinforcement learning techniques to improve performance on specific downstream tasks.\n",
      "\n",
      "These methodologies highlight the diverse approaches to training and optimizing neural networks for various applications.\n",
      "\n",
      "📁 Sources used in the answer:\n",
      " 1. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      " 2. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      " 3. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      " 4. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "\n",
      "\n",
      "❓ Question: What features of document formatting are showcased in the Calibre demo document?\n",
      "--------------------------------------------------------------------------------\n",
      "📘 Answer:\n",
      "The Calibre demo document showcases the following features of document formatting:\n",
      "\n",
      "1. Inline formatting (bold, italic, bold-italic, underlined, struck out, superscript, subscript, colored text, highlighted text, text in a box, inverse video).\n",
      "2. Use of embedded fonts (e.g., Ubuntu font family).\n",
      "3. Paragraph level formatting (using document-wide styles).\n",
      "4. Automatic generation of a Table of Contents from headings.\n",
      "5. Support for images.\n",
      "6. Support for tables.\n",
      "7. Support for lists (bulleted, numbered, multi-level, continued lists).\n",
      "8. Support for footnotes and endnotes.\n",
      "9. Support for links (to external websites and internal document locations).\n",
      "10. Use of dropcaps.\n",
      "\n",
      "📁 Sources used in the answer:\n",
      " 1. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\calibre_demo.docx\n",
      " 2. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\calibre_demo.docx\n",
      " 3. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\calibre_demo.docx\n",
      " 4. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\calibre_demo.docx\n",
      "\n",
      "\n",
      "❓ Question: What is the main content or purpose of the Calibre demo document?\n",
      "--------------------------------------------------------------------------------\n",
      "📘 Answer:\n",
      "The main content or purpose of the Calibre demo document is to demonstrate the ability of the Calibre DOCX Input plugin to convert various typographic features from a Microsoft Word document into modern ebook formats, such as AZW3 and EPUB. It showcases the support for images, tables, lists, footnotes, endnotes, links, dropcaps, and different types of text and paragraph level formatting. The document also explains how to use Calibre to convert a DOCX file and highlights features like automatic Table of Contents generation.\n",
      "\n",
      "📁 Sources used in the answer:\n",
      " 1. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\calibre_demo.docx\n",
      " 2. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\calibre_demo.docx\n",
      " 3. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\calibre_demo.docx\n",
      " 4. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\calibre_demo.docx\n",
      "\n",
      "\n",
      "❓ Question: What kind of data is in the Excel file?\n",
      "--------------------------------------------------------------------------------\n",
      "📘 Answer:\n",
      "The Excel file contains sales data with 390 rows and 5 columns. The columns are: Postcode, Sales_Rep_ID, Sales_Rep_Name, Year, and Value. The data includes information about sales representatives, the years of sales, and the corresponding sales values.\n",
      "\n",
      "📁 Sources used in the answer:\n",
      " 1. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\sample_data.xlsx\n",
      " 2. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\calibre_demo.docx\n",
      " 3. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\calibre_demo.docx\n",
      " 4. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\calibre_demo.docx\n",
      "\n",
      "\n",
      "❓ Question: How many rows and columns are in the sample Excel data?\n",
      "--------------------------------------------------------------------------------\n",
      "📘 Answer:\n",
      "The sample Excel data contains 390 rows and 5 columns.\n",
      "\n",
      "📁 Sources used in the answer:\n",
      " 1. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\sample_data.xlsx\n",
      " 2. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\calibre_demo.docx\n",
      " 3. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\calibre_demo.docx\n",
      " 4. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\neural_networks.pdf\n",
      "\n",
      "\n",
      "❓ Question: What are the key principles for creating accessible web page structures?\n",
      "--------------------------------------------------------------------------------\n",
      "📘 Answer:\n",
      "The key principles for creating accessible web page structures include:\n",
      "\n",
      "1. **Use of HTML and WAI-ARIA**: Implement HTML and WAI-ARIA roles to improve navigation and orientation on web pages and applications.\n",
      "\n",
      "2. **Identify and Mark Up Regions**: Clearly identify and mark up different regions of the web page using appropriate roles to help users navigate.\n",
      "\n",
      "3. **Labeling Regions**: Provide clear labels for regions to allow users to distinguish and access them easily.\n",
      "\n",
      "4. **Logical Headings**: Add headings and nest them logically to organize content according to its relationships and importance.\n",
      "\n",
      "5. **Meaningful Content Structure**: Mark up the content using appropriate and meaningful elements to enhance understanding and navigation.\n",
      "\n",
      "6. **Bypass Mechanisms**: Include mechanisms that allow users to bypass repetitive blocks of content.\n",
      "\n",
      "7. **Use of Section Headings**: Organize content with section headings to improve clarity and navigation.\n",
      "\n",
      "These principles help ensure that web pages are more accessible for all users, including those with disabilities.\n",
      "\n",
      "📁 Sources used in the answer:\n",
      " 1. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\web_accessibility.html\n",
      " 2. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\web_accessibility.html\n",
      " 3. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\web_accessibility.html\n",
      " 4. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\calibre_demo.docx\n",
      "\n",
      "\n",
      "❓ Question: What HTML elements or techniques does W3C recommend for web accessibility?\n",
      "--------------------------------------------------------------------------------\n",
      "📘 Answer:\n",
      "W3C recommends the following HTML elements and techniques for web accessibility:\n",
      "\n",
      "1. **HTML5 and WAI-ARIA Roles**: Use these to identify and mark up regions on web pages, which helps users navigate and orient themselves.\n",
      "\n",
      "2. **Labeling Regions**: Clearly label regions of the page to allow users to distinguish and access them easily.\n",
      "\n",
      "3. **Headings**: Add headings and nest them logically to label sections of web pages according to their relationships and importance.\n",
      "\n",
      "4. **Content Structure**: Mark up the content on a page using appropriate and meaningful HTML elements to convey information, structure, and relationships.\n",
      "\n",
      "5. **Bypass Blocks**: Implement mechanisms that allow users to bypass repetitive blocks of content.\n",
      "\n",
      "6. **Use of Headings and Labels**: Ensure that headings and labels describe the topic or purpose of the content.\n",
      "\n",
      "These techniques help improve navigation and processing for all users, especially those with disabilities.\n",
      "\n",
      "📁 Sources used in the answer:\n",
      " 1. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\web_accessibility.html\n",
      " 2. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\web_accessibility.html\n",
      " 3. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\web_accessibility.html\n",
      " 4. C:\\Users\\pavel\\projects\\ai-llm-agents\\rag\\data-002\\calibre_demo.docx\n"
     ]
    }
   ],
   "source": [
    "# for i, question in enumerate(questions):\n",
    "#     print(f\"\\nQuestion {i+1}: {question}\")\n",
    "#     # result = qa_chain.run(question)\n",
    "#     result = qa_chain.invoke({\"query\": question})\n",
    "#     print(f\"Answer: {result}\")\n",
    "\n",
    "# Ask each question\n",
    "for question in questions:\n",
    "    ask_question(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfd0a3c-794d-4c70-8da7-bd268f4d7c09",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e1e0850-925b-42b5-804e-e099543becb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDone!\")\n",
    "\n",
    "# Uncomment below if you want to delete the index after testing\n",
    "# print(\"Cleaning up...\")\n",
    "# pinecone.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36a151d-2df7-4e3e-8038-d6f9a93191f6",
   "metadata": {},
   "source": [
    "## DEBUG: FIXING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fcb3ef6b-a3e1-439c-ba53-e867d35b4a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(excel_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9c5983-a7c0-4d04-a895-51a81b152da0",
   "metadata": {},
   "source": [
    "### DEBUG [1]: Fixing Excel File Retrieval in Your RAG System\n",
    "The issue is that your system isn't properly loading or processing the Excel file data. The UnstructuredExcelLoader might not be extracting the content effectively for vector storage and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "74e40d5b-8adc-4690-a754-2294ada3a344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Replace the Excel Loader\n",
    "\n",
    "# from langchain_community.document_loaders import PandasLoader\n",
    "# import pandas as pd\n",
    "\n",
    "# def load_document(file_path, doc_type):\n",
    "#     \"\"\"Load a document based on its type.\"\"\"\n",
    "#     try:\n",
    "#         if doc_type == \"text\":\n",
    "#             return TextLoader(file_path).load()\n",
    "#         elif doc_type == \"markdown\":\n",
    "#             return UnstructuredMarkdownLoader(file_path).load()\n",
    "#         elif doc_type == \"pdf\":\n",
    "#             return PyPDFLoader(file_path).load()\n",
    "#         elif doc_type == \"docx\":\n",
    "#             return Docx2txtLoader(file_path).load()\n",
    "#         elif doc_type == \"excel\":\n",
    "#             # Replace the UnstructuredExcelLoader with a better approach\n",
    "#             df = pd.read_excel(file_path)\n",
    "#             # Convert DataFrame to a more descriptive text format\n",
    "#             text_content = f\"Excel file with {len(df)} rows and {len(df.columns)} columns.\\n\\n\"\n",
    "#             text_content += f\"Column names: {', '.join(df.columns.astype(str))}\\n\\n\"\n",
    "#             text_content += \"Sample data (first 5 rows):\\n\"\n",
    "#             text_content += df.head().to_string()\n",
    "            \n",
    "#             # Create a document with this content\n",
    "#             from langchain_core.documents import Document\n",
    "#             return [Document(page_content=text_content, metadata={\"source\": file_path})]\n",
    "#         elif doc_type == \"html\":\n",
    "#             return UnstructuredHTMLLoader(file_path).load()\n",
    "#         else:\n",
    "#             print(f\"Unsupported document type: {doc_type}\")\n",
    "#             return []\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading {file_path}: {e}\")\n",
    "#         return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a1f74d2-ab8e-4be5-80d8-a5baacc57445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2. Debug the Excel Loading\n",
    "# # - After modifying the loader, add this code to verify the Excel content was properly extracted:\n",
    "\n",
    "# # After loading documents, check specifically for Excel content:\n",
    "# excel_docs = [doc for doc in documents if \"sample_data.xlsx\" in doc.metadata.get(\"source\", \"\")]\n",
    "# if excel_docs:\n",
    "#     print(\"\\n📊 Excel Document Content:\")\n",
    "#     for doc in excel_docs:\n",
    "#         print(f\"Source: {doc.metadata.get('source')}\")\n",
    "#         print(f\"Content length: {len(doc.page_content)} characters\")\n",
    "#         print(f\"Content preview:\\n{doc.page_content[:500]}...\\n\")\n",
    "# else:\n",
    "#     print(\"\\n❌ No Excel documents were successfully loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f9a9329-c96a-43c3-9e4b-09c1ff11f5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3. Modify Chunking for Excel Data\n",
    "# # - For Excel data, which is more structured than regular text, consider using a different chunking strategy:\n",
    "\n",
    "# # Before creating chunks\n",
    "# regular_docs = [doc for doc in documents if \"sample_data.xlsx\" not in doc.metadata.get(\"source\", \"\")]\n",
    "# excel_docs = [doc for doc in documents if \"sample_data.xlsx\" in doc.metadata.get(\"source\", \"\")]\n",
    "\n",
    "# # Process regular documents with RecursiveCharacterTextSplitter\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=1000,\n",
    "#     chunk_overlap=100,\n",
    "#     length_function=len,\n",
    "# )\n",
    "# regular_chunks = text_splitter.split_documents(regular_docs)\n",
    "\n",
    "# # Process Excel documents with less aggressive chunking to preserve table context\n",
    "# excel_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=3000,  # Larger chunk size for tabular data\n",
    "#     chunk_overlap=200,\n",
    "#     length_function=len,\n",
    "# )\n",
    "# excel_chunks = excel_splitter.split_documents(excel_docs)\n",
    "\n",
    "# # Combine chunks\n",
    "# chunks = regular_chunks + excel_chunks\n",
    "# print(f\"Created {len(chunks)} chunks from {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2ff2a550-f047-4e78-8b2f-309474bc4693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 4. Add Metadata Tags\n",
    "# # - Adding specific metadata tags can help improve retrieval:\n",
    "\n",
    "# # After creating chunks, but before vector store creation\n",
    "# for chunk in chunks:\n",
    "#     if \"sample_data.xlsx\" in chunk.metadata.get(\"source\", \"\"):\n",
    "#         chunk.metadata[\"content_type\"] = \"excel_data\"\n",
    "#         chunk.metadata[\"document_type\"] = \"spreadsheet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012fcbb6-6efc-4ef3-be74-a1873fc4ae1f",
   "metadata": {},
   "source": [
    "```\n",
    "These changes should help your system better process, store, and retrieve Excel data, allowing it to answer questions about the Excel file content.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5706aa-4ab8-4250-a7fc-1ca5a4f0e879",
   "metadata": {},
   "source": [
    "### DEBUG [2]: Fixing Excel and Jupyter Notebook File Processing in RAG System\n",
    "The issue is that both Excel files and Jupyter notebooks aren't being properly processed by the UnstructuredExcelLoader. Let's implement a more robust solution for both file types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "daecadda-281d-4dd6-876d-cedddda8b450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fix 1: Replace the Excel Loader with a Better Implementation\n",
    "\n",
    "# def load_document(file_path, doc_type):\n",
    "#     \"\"\"Load a document based on its type.\"\"\"\n",
    "#     try:\n",
    "#         if doc_type == \"text\":\n",
    "#             return TextLoader(file_path).load()\n",
    "#         elif doc_type == \"markdown\":\n",
    "#             return UnstructuredMarkdownLoader(file_path).load()\n",
    "#         elif doc_type == \"pdf\":\n",
    "#             return PyPDFLoader(file_path).load()\n",
    "#         elif doc_type == \"docx\":\n",
    "#             return Docx2txtLoader(file_path).load()\n",
    "#         elif doc_type == \"excel\":\n",
    "#             # Better Excel processing using pandas\n",
    "#             import pandas as pd\n",
    "#             from langchain_core.documents import Document\n",
    "            \n",
    "#             df = pd.read_excel(file_path)\n",
    "            \n",
    "#             # Create a detailed text description of the Excel content\n",
    "#             content = f\"Excel file containing {len(df)} rows and {len(df.columns)} columns.\\n\\n\"\n",
    "#             content += f\"Column names: {', '.join(df.columns.astype(str))}\\n\\n\"\n",
    "            \n",
    "#             # Add sample data information\n",
    "#             content += f\"Sample data (first 5 rows):\\n{df.head().to_string()}\\n\\n\"\n",
    "            \n",
    "#             # Add summary statistics if applicable\n",
    "#             try:\n",
    "#                 content += f\"Numeric column statistics:\\n{df.describe().to_string()}\\n\\n\"\n",
    "#             except:\n",
    "#                 pass\n",
    "                \n",
    "#             # Create a document with metadata flagging it as Excel\n",
    "#             return [Document(\n",
    "#                 page_content=content,\n",
    "#                 metadata={\"source\": file_path, \"file_type\": \"excel\", \"rows\": len(df), \"columns\": len(df.columns)}\n",
    "#             )]\n",
    "#         elif doc_type == \"html\":\n",
    "#             return UnstructuredHTMLLoader(file_path).load()\n",
    "#         elif doc_type == \"notebook\" or doc_type.endswith(\".ipynb\"):\n",
    "#             # Process Jupyter notebooks as text files\n",
    "#             with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#                 import json\n",
    "#                 notebook = json.load(f)\n",
    "                \n",
    "#                 # Extract text from markdown and code cells\n",
    "#                 content = \"\"\n",
    "#                 for cell in notebook.get('cells', []):\n",
    "#                     if cell.get('cell_type') == 'markdown':\n",
    "#                         content += \"\".join(cell.get('source', [])) + \"\\n\\n\"\n",
    "#                     elif cell.get('cell_type') == 'code':\n",
    "#                         content += \"```python\\n\" + \"\".join(cell.get('source', [])) + \"\\n```\\n\\n\"\n",
    "                \n",
    "#                 return [Document(\n",
    "#                     page_content=content,\n",
    "#                     metadata={\"source\": file_path, \"file_type\": \"jupyter_notebook\"}\n",
    "#                 )]\n",
    "#         else:\n",
    "#             print(f\"Unsupported document type: {doc_type}\")\n",
    "#             return []\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading {file_path}: {e}\")\n",
    "#         return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c4cdef1-4fb9-42e9-8152-26d113d7248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fix 2: Add Debug Code to Check Excel Content\n",
    "# # - Add this code after loading the documents:\n",
    "\n",
    "# # Debug: Check Excel content\n",
    "# excel_docs = [doc for doc in documents if \"sample_data.xlsx\" in doc.metadata.get(\"source\", \"\")]\n",
    "# if excel_docs:\n",
    "#     print(\"\\n📊 Excel file loaded successfully:\")\n",
    "#     for doc in excel_docs:\n",
    "#         content_preview = doc.page_content[:500] + \"...\" if len(doc.page_content) > 500 else doc.page_content\n",
    "#         print(f\"Content length: {len(doc.page_content)} characters\")\n",
    "#         print(f\"Content preview:\\n{content_preview}\\n\")\n",
    "#         print(f\"Metadata: {doc.metadata}\")\n",
    "# else:\n",
    "#     print(\"\\n❌ No Excel documents were successfully loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fd7ba025-ac2e-4cf1-9513-37c059f18fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fix 3: Modify Document Processing for Better Retrieval\n",
    "# # When creating chunks, add special handling for Excel data:\n",
    "\n",
    "# # Before chunking, separate Excel documents for special handling\n",
    "# regular_docs = [doc for doc in documents if \"sample_data.xlsx\" not in doc.metadata.get(\"source\", \"\")]\n",
    "# excel_docs = [doc for doc in documents if \"sample_data.xlsx\" in doc.metadata.get(\"source\", \"\")]\n",
    "\n",
    "# # Process regular documents with RecursiveCharacterTextSplitter\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=1000,\n",
    "#     chunk_overlap=100,\n",
    "#     length_function=len,\n",
    "# )\n",
    "# regular_chunks = text_splitter.split_documents(regular_docs)\n",
    "\n",
    "# # For Excel documents, use larger chunks with less overlap\n",
    "# excel_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=2000,  # Larger to keep more Excel context together\n",
    "#     chunk_overlap=50,\n",
    "#     length_function=len,\n",
    "# )\n",
    "# excel_chunks = excel_splitter.split_documents(excel_docs)\n",
    "\n",
    "# # Combine all chunks\n",
    "# chunks = regular_chunks + excel_chunks\n",
    "# print(f\"Created {len(chunks)} chunks from {len(documents)} documents\")\n",
    "# print(f\"  - Regular documents: {len(regular_chunks)} chunks\")\n",
    "# print(f\"  - Excel documents: {len(excel_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9d62903a-371b-453f-a9ea-53b1f11fe07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fix 4: Enhance Metadata for Better Retrieval\n",
    "# # After creating chunks but before vector store creation:\n",
    "\n",
    "# # Add more specific metadata for better retrieval\n",
    "# for chunk in chunks:\n",
    "#     source = chunk.metadata.get(\"source\", \"\")\n",
    "#     if \"sample_data.xlsx\" in source:\n",
    "#         chunk.metadata[\"content_type\"] = \"excel_data\"\n",
    "#         chunk.metadata[\"document_type\"] = \"spreadsheet\"\n",
    "#     elif source.endswith(\".ipynb\"):\n",
    "#         chunk.metadata[\"content_type\"] = \"notebook_content\"\n",
    "#         chunk.metadata[\"document_type\"] = \"jupyter_notebook\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca9f06f-1650-47f8-9d04-948e8e7a79be",
   "metadata": {},
   "source": [
    "```\n",
    "The primary issue is that UnstructuredExcelLoader doesn't convert Excel files into a format that's good for semantic search. The new implementation:\n",
    "\n",
    "1. Uses pandas to properly extract table data\n",
    "2. Creates a text representation that describes the Excel structure\n",
    "3. Preserves important metadata about the file\n",
    "4. Uses special chunking for tabular data\n",
    "\n",
    "This should significantly improve your ability to retrieve information from Excel files and Jupyter notebooks in your RAG system.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97374518-f5cd-499c-a86a-2063b1365740",
   "metadata": {},
   "source": [
    "### DEBUG [3]: Debugging Jupyter Notebook and Excel Loading in RAG System\n",
    "Here's a dedicated debug function to check both your Excel and Jupyter Notebook processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dbdc32f8-179a-4991-abab-0611af26dc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add this after your document loading code\n",
    "\n",
    "# def debug_document_loading():\n",
    "#     \"\"\"Debug function to check if Excel and Jupyter Notebook files are properly loaded.\"\"\"\n",
    "#     print(\"\\n🔍 DEBUGGING DOCUMENT LOADING...\")\n",
    "    \n",
    "#     # 1. Check Excel files\n",
    "#     excel_files = [doc for doc in documents if \"sample_data.xlsx\" in doc.metadata.get(\"source\", \"\")]\n",
    "#     print(f\"\\n📊 Excel Files: {len(excel_files)} documents found\")\n",
    "    \n",
    "#     if excel_files:\n",
    "#         for idx, doc in enumerate(excel_files):\n",
    "#             print(f\"  Excel Doc #{idx+1}:\")\n",
    "#             print(f\"  - Source: {doc.metadata.get('source')}\")\n",
    "#             print(f\"  - Content length: {len(doc.page_content)} characters\")\n",
    "#             print(f\"  - Content preview: {doc.page_content[:200]}...\\n\")\n",
    "#     else:\n",
    "#         print(\"  ❌ No Excel documents were loaded!\")\n",
    "        \n",
    "#     # 2. Check Jupyter Notebook files\n",
    "#     notebook_files = [doc for doc in documents if \"ipynb\" in doc.metadata.get(\"source\", \"\")]\n",
    "#     print(f\"\\n📓 Jupyter Notebooks: {len(notebook_files)} documents found\")\n",
    "    \n",
    "#     if notebook_files:\n",
    "#         for idx, doc in enumerate(notebook_files):\n",
    "#             print(f\"  Notebook Doc #{idx+1}:\")\n",
    "#             print(f\"  - Source: {doc.metadata.get('source')}\")\n",
    "#             print(f\"  - Content length: {len(doc.page_content)} characters\")\n",
    "#             print(f\"  - Content preview: {doc.page_content[:200]}...\\n\")\n",
    "#     else:\n",
    "#         print(\"  ❌ No Jupyter Notebook documents were loaded!\")\n",
    "        \n",
    "#     # 3. Test direct loading of files\n",
    "#     print(\"\\n🧪 TESTING DIRECT FILE LOADING...\")\n",
    "    \n",
    "#     excel_path = os.path.join(DATA_DIR, \"sample_data.xlsx\")\n",
    "#     if os.path.exists(excel_path):\n",
    "#         print(f\"\\nTesting Excel loader on: {excel_path}\")\n",
    "#         try:\n",
    "#             import pandas as pd\n",
    "#             df = pd.read_excel(excel_path)\n",
    "#             print(f\"✅ Successfully loaded Excel with pandas: {len(df)} rows, {len(df.columns)} columns\")\n",
    "#             print(f\"Column names: {list(df.columns)}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ Error loading Excel with pandas: {e}\")\n",
    "    \n",
    "#     notebook_path = os.path.join(DATA_DIR, \"pinecone_example.ipynb\")\n",
    "#     if os.path.exists(notebook_path):\n",
    "#         print(f\"\\nTesting Jupyter Notebook loader on: {notebook_path}\")\n",
    "#         try:\n",
    "#             with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "#                 import json\n",
    "#                 notebook = json.load(f)\n",
    "#                 cell_count = len(notebook.get('cells', []))\n",
    "#                 print(f\"✅ Successfully loaded Notebook: {cell_count} cells found\")\n",
    "                \n",
    "#                 # Count cell types\n",
    "#                 md_cells = sum(1 for cell in notebook.get('cells', []) if cell.get('cell_type') == 'markdown')\n",
    "#                 code_cells = sum(1 for cell in notebook.get('cells', []) if cell.get('cell_type') == 'code')\n",
    "#                 print(f\"  - Markdown cells: {md_cells}\")\n",
    "#                 print(f\"  - Code cells: {code_cells}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ Error loading Jupyter Notebook: {e}\")\n",
    "\n",
    "# # Call the debug function\n",
    "# debug_document_loading()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4342dcc9-826c-409e-94fc-2bfcd293bf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Improved Jupyter Notebook Loading Function\n",
    "# # Replace your existing notebook loading code with this improved version:\n",
    "\n",
    "# def load_document(file_path, doc_type):\n",
    "#     \"\"\"Load a document based on its type.\"\"\"\n",
    "#     try:\n",
    "#         if doc_type == \"text\":\n",
    "#             return TextLoader(file_path).load()\n",
    "#         elif doc_type == \"markdown\":\n",
    "#             return UnstructuredMarkdownLoader(file_path).load()\n",
    "#         elif doc_type == \"pdf\":\n",
    "#             return PyPDFLoader(file_path).load()\n",
    "#         elif doc_type == \"docx\":\n",
    "#             return Docx2txtLoader(file_path).load()\n",
    "#         elif doc_type == \"excel\":\n",
    "#             # Better Excel processing using pandas\n",
    "#             import pandas as pd\n",
    "#             from langchain_core.documents import Document\n",
    "            \n",
    "#             df = pd.read_excel(file_path)\n",
    "            \n",
    "#             # Create a detailed text description of the Excel content\n",
    "#             content = f\"Excel file containing {len(df)} rows and {len(df.columns)} columns.\\n\\n\"\n",
    "#             content += f\"Column names: {', '.join(df.columns.astype(str))}\\n\\n\"\n",
    "            \n",
    "#             # Add sample data information\n",
    "#             content += f\"Sample data (first 5 rows):\\n{df.head().to_string()}\\n\\n\"\n",
    "            \n",
    "#             # Add summary statistics where possible\n",
    "#             try:\n",
    "#                 content += f\"Numeric column statistics:\\n{df.describe().to_string()}\\n\\n\"\n",
    "#             except:\n",
    "#                 pass\n",
    "                \n",
    "#             # Create a document with metadata flagging it as Excel\n",
    "#             return [Document(\n",
    "#                 page_content=content,\n",
    "#                 metadata={\"source\": file_path, \"file_type\": \"excel\", \"rows\": len(df), \"columns\": len(df.columns)}\n",
    "#             )]\n",
    "#         elif doc_type == \"notebook\" or \"ipynb\" in file_path:\n",
    "#             # Improved Jupyter Notebook processing\n",
    "#             try:\n",
    "#                 with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#                     import json\n",
    "#                     notebook = json.load(f)\n",
    "                    \n",
    "#                     # Extract text from markdown and code cells\n",
    "#                     content = f\"Jupyter Notebook with {len(notebook.get('cells', []))} cells\\n\\n\"\n",
    "                    \n",
    "#                     # Process each cell\n",
    "#                     for i, cell in enumerate(notebook.get('cells', [])):\n",
    "#                         cell_type = cell.get('cell_type')\n",
    "#                         cell_source = \"\".join(cell.get('source', []))\n",
    "                        \n",
    "#                         if cell_type == 'markdown':\n",
    "#                             content += f\"[MARKDOWN CELL {i+1}]\\n{cell_source}\\n\\n\"\n",
    "#                         elif cell_type == 'code':\n",
    "#                             content += f\"[CODE CELL {i+1}]\\n```python\\n{cell_source}\\n```\\n\\n\"\n",
    "                            \n",
    "#                             # If there's output, include it\n",
    "#                             outputs = cell.get('outputs', [])\n",
    "#                             if outputs:\n",
    "#                                 output_text = \"\"\n",
    "#                                 for output in outputs:\n",
    "#                                     if 'text' in output:\n",
    "#                                         output_text += \"\".join(output['text'])\n",
    "#                                     elif 'data' in output and 'text/plain' in output['data']:\n",
    "#                                         output_text += output['data']['text/plain']\n",
    "                                \n",
    "#                                 if output_text:\n",
    "#                                     content += f\"[OUTPUT]\\n{output_text}\\n\\n\"\n",
    "                    \n",
    "#                     return [Document(\n",
    "#                         page_content=content,\n",
    "#                         metadata={\"source\": file_path, \"file_type\": \"jupyter_notebook\"}\n",
    "#                     )]\n",
    "#             except json.JSONDecodeError:\n",
    "#                 # If JSON parsing fails, fall back to text loader\n",
    "#                 print(f\"Warning: Could not parse {file_path} as JSON, falling back to text loader\")\n",
    "#                 return TextLoader(file_path).load()\n",
    "#         elif doc_type == \"html\":\n",
    "#             return UnstructuredHTMLLoader(file_path).load()\n",
    "#         else:\n",
    "#             print(f\"Unsupported document type: {doc_type}\")\n",
    "#             return []\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading {file_path}: {e}\")\n",
    "#         return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5e2ee889-95a1-4cf8-b577-5169638a709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fix Your DOCUMENTS Configuration\n",
    "# # Make sure the file type for the notebook is correctly set:\n",
    "\n",
    "# DOCUMENTS = [\n",
    "#     (\"https://raw.githubusercontent.com/langchain-ai/langchain/master/README.md\", \"langchain_readme.md\", \"markdown\"),\n",
    "#     # Change the file type from \"text\" to \"notebook\"\n",
    "#     (\"https://raw.githubusercontent.com/pinecone-io/examples/master/learn/generation/langchain/handbook/05-langchain-retrieval-augmentation.ipynb\", \"pinecone_example.ipynb\", \"notebook\"),\n",
    "#     (\"https://arxiv.org/pdf/2005.11401.pdf\", \"neural_networks.pdf\", \"pdf\"),\n",
    "#     (\"https://calibre-ebook.com/downloads/demos/demo.docx\", \"calibre_demo.docx\", \"docx\"),\n",
    "#     (\"https://filesamples.com/samples/document/xlsx/sample1.xlsx\", \"sample_data.xlsx\", \"excel\"),\n",
    "#     (\"https://www.w3.org/WAI/tutorials/page-structure/\", \"web_accessibility.html\", \"html\"),\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2672f5e0-58cf-4634-a761-fb18ba571127",
   "metadata": {},
   "source": [
    "```\n",
    "The debug function will help you identify exactly what's happening with both file types, and the improved loading functions should properly handle your Excel and Jupyter Notebook files for better retrieval results.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eb3091-3606-4181-a37d-a675b0069976",
   "metadata": {},
   "source": [
    "```\n",
    "The improved document loaders will help your RAG system properly process both Excel files and Jupyter notebooks by:\n",
    "\n",
    "1. Using pandas to extract structured data from Excel spreadsheets\n",
    "2. Converting tabular data into descriptive text that's better for semantic search\n",
    "3. Adding rich metadata to help with retrieval accuracy\n",
    "4. Using special chunking strategies for different document types\n",
    "\n",
    "When you implement these changes, your system should be able to properly answer questions about all document types, including the Excel spreadsheet data.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
