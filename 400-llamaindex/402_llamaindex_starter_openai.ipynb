{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accbde67-9c7e-41b0-9329-8052e2ef365b",
   "metadata": {},
   "source": [
    "# LlamaIndex: Starter Tutorial (Using OpenAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb42466-306d-49b1-b814-6674b60451e4",
   "metadata": {},
   "source": [
    "- Starter Tutorial (Using OpenAI)<br>\n",
    "  https://docs.llamaindex.ai/en/stable/getting_started/starter_example/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1039ae-8255-4ec0-8c3a-2757a8f0ae95",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ce6eafa-f6ef-4ff1-8d54-17b4fcb71612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables (for API key)\n",
    "load_dotenv()\n",
    "\n",
    "# Set up OpenAI API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"Please set the OPENAI_API_KEY environment variable or add it to a .env file\")\n",
    "\n",
    "# Define the model to use\n",
    "MODEL_GPT = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b504e514-c4a7-48ec-b9fe-e3028ba45fbc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Set OpenAI API key\n",
    "LlamaIndex uses **OpenAI (gpt-3.5-turbo)** by default.<br>\n",
    "```\n",
    "# MacOS/Linux\n",
    "export OPENAI_API_KEY=XXXXX\n",
    "\n",
    "# Windows\n",
    "set OPENAI_API_KEY=XXXXX\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e34161b-43b5-42bf-9f36-44428c23390b",
   "metadata": {},
   "source": [
    "## Basic Agent Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d071a730-6321-4990-8d56-71656c6f8128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Define a simple calculator tool\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Useful for multiplying two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "# Create an agent workflow with our calculator tool\n",
    "agent = FunctionAgent(\n",
    "    name=\"Agent\",\n",
    "    description=\"Useful for multiplying two numbers\",\n",
    "    tools=[multiply],\n",
    "    llm=OpenAI(model=\"gpt-4o-mini\"),\n",
    "    system_prompt=\"You are a helpful assistant that can multiply two numbers.\",\n",
    ")\n",
    "\n",
    "# async def main():\n",
    "#     # Run the agent\n",
    "#     response = await agent.run(\"What is 1234 * 4567?\")\n",
    "#     print(str(response))\n",
    "\n",
    "# # Run the agent\n",
    "# if __name__ == \"__main__\":\n",
    "#     asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fe6247b-4c80-4b49-9386-a3c317dd411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the agent\n",
    "response = await agent.run(\"What is 1234 * 4567?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1ca8ec5-ae9a-4010-8583-2d0f8554e334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of \\( 1234 \\times 4567 \\) is \\( 5,635,678 \\).\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2ea42c-c48f-4d0a-95de-903badfef489",
   "metadata": {},
   "source": [
    "## Adding Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56269f7a-628c-4239-9b83-27e382fe1c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core.workflow import Context\n",
    "\n",
    "# # create context\n",
    "# ctx = Context(agent)\n",
    "\n",
    "# # run agent with context\n",
    "# response = await agent.run(\"My name is Logan\", ctx=ctx)\n",
    "# response = await agent.run(\"What is my name?\", ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c4f6271-6181-44cd-aae9-4ac9672c6f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERROR: AttributeError: 'FunctionAgent' object has no attribute '_get_steps'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97654e02-2698-404b-94c4-8ee2846cc646",
   "metadata": {},
   "source": [
    "## Adding Chat History by (AgentWorkflow Basic Introduction)\n",
    "- https://docs.llamaindex.ai/en/stable/examples/agent/agent_workflow_basic/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbb1eb7f-5acb-41e4-be34-ab76fc974885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index\n",
    "# %pip install tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5725774-3f23-4e2e-b872-d6a34befacb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20e02bdc-fdb0-4f58-8541-aef6d74d9645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a tool\n",
    "import os\n",
    "from tavily import AsyncTavilyClient\n",
    "\n",
    "tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "async def search_web(query: str) -> str:\n",
    "    \"\"\"Useful for using the web to answer questions.\"\"\"\n",
    "    client = AsyncTavilyClient(api_key=tavily_api_key)\n",
    "    return str(await client.search(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74baabf3-d220-4f20-9c02-363bb11a362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an AgentWorkflow that uses the tool\n",
    "from llama_index.core.agent.workflow import AgentWorkflow\n",
    "\n",
    "workflow = AgentWorkflow.from_tools_or_functions(\n",
    "    [search_web],\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are a helpful assistant that can search the web for information.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cd4ef37-b1ba-4509-8037-891f31cb4d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current weather in Prague is as follows:\n",
      "\n",
      "- **Temperature**: 8.0째C (46.4째F)\n",
      "- **Condition**: Light rain\n",
      "- **Humidity**: 81%\n",
      "- **Wind**: 6.0 mph (9.7 kph) from the WNW\n",
      "- **Pressure**: 1018 mb\n",
      "- **Visibility**: 5.0 km\n",
      "\n",
      "It is currently daytime in Prague, and the weather feels like 6.3째C (43.3째F). You can expect light rain, so it might be a good idea to carry an umbrella if you're going out.\n",
      "\n",
      "For more details, you can check the full weather report [here](https://www.weatherapi.com/).\n"
     ]
    }
   ],
   "source": [
    "# Running the Agent\n",
    "# response = await workflow.run(user_msg=\"What is the weather in San Francisco?\")\n",
    "response = await workflow.run(user_msg=\"What is the weather in Prague?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23d36656-9dd6-46b6-94d9-5aedef92c0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you, Logan! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Maintaining State\n",
    "from llama_index.core.workflow import Context\n",
    "\n",
    "ctx = Context(workflow)\n",
    "\n",
    "response = await workflow.run(\n",
    "    user_msg=\"My name is Logan, nice to meet you!\", ctx=ctx\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c350b30d-9a05-4291-870f-93e569d3b4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Logan.\n"
     ]
    }
   ],
   "source": [
    "response = await workflow.run(user_msg=\"What is my name?\", ctx=ctx)\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dab6f33-13a9-4ffb-8bab-c5871134ef52",
   "metadata": {},
   "source": [
    "## Adding RAG Capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d534c3af-830e-4d14-b044-bcdec60008ce",
   "metadata": {},
   "source": [
    "Example data\n",
    "```\n",
    "mkdir data\n",
    "wget https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt -O data/paul_graham_essay.txt\n",
    "```\n",
    "-  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47e8a1df-f8f9-45a2-9cfb-794f7dbf4e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import asyncio\n",
    "import os\n",
    "\n",
    "# Create a RAG tool using LlamaIndex\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Useful for multiplying two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "async def search_documents(query: str) -> str:\n",
    "    \"\"\"Useful for answering natural language questions about an personal essay written by Paul Graham.\"\"\"\n",
    "    response = await query_engine.aquery(query)\n",
    "    return str(response)\n",
    "\n",
    "# Create an enhanced workflow with both tools\n",
    "agent = FunctionAgent(\n",
    "    name=\"Agent\",\n",
    "    description=\"Useful for multiplying two numbers and searching through documents to answer questions.\",\n",
    "    tools=[multiply, search_documents],\n",
    "    llm=OpenAI(model=\"gpt-4o-mini\"),\n",
    "    system_prompt=\"\"\"You are a helpful assistant that can perform calculations\n",
    "    and search through documents to answer questions.\"\"\",\n",
    ")\n",
    "\n",
    "# # Now we can ask questions about the documents or do calculations\n",
    "# async def main():\n",
    "#     response = await agent.run(\n",
    "#         \"What did the author do in college? Also, what's 7 * 8?\"\n",
    "#     )\n",
    "#     print(response)\n",
    "\n",
    "# # Run the agent\n",
    "# if __name__ == \"__main__\":\n",
    "#     asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "657dccc5-e08d-47e7-a760-529b73b97b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In college, the author initially planned to study philosophy but found the courses boring. Consequently, the author switched to studying AI, influenced by a novel called \"The Moon is a Harsh Mistress\" by Heinlein and a PBS documentary featuring Terry Winograd using SHRDLU.\n",
      "\n",
      "As for the multiplication, \\(7 \\times 8 = 56\\).\n"
     ]
    }
   ],
   "source": [
    "response = await agent.run(\"What did the author do in college? Also, what's 7 * 8?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8cb4f1-71e7-4ac4-8e42-8675fe0c34ee",
   "metadata": {},
   "source": [
    "## Storing the RAG Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b8061e8-e078-4177-bae7-97691740979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the index\n",
    "index.storage_context.persist(\"storage\")\n",
    "\n",
    "# Later, load the index\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n",
    "index = load_index_from_storage(storage_context)\n",
    "query_engine = index.as_query_engine()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
