{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accbde67-9c7e-41b0-9329-8052e2ef365b",
   "metadata": {},
   "source": [
    "# LlamaIndex: AgentWorkflow Basic Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb42466-306d-49b1-b814-6674b60451e4",
   "metadata": {},
   "source": [
    "- AgentWorkflow Basic Introduction<br>\n",
    "  https://docs.llamaindex.ai/en/stable/examples/agent/agent_workflow_basic/\n",
    "- **LlamaHub Integrations**<br>\n",
    "  https://llamahub.ai/\n",
    "- Examples - LLMs (**OpenAI**)<br>\n",
    "  https://docs.llamaindex.ai/en/stable/examples/llm/openai/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1039ae-8255-4ec0-8c3a-2757a8f0ae95",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ce6eafa-f6ef-4ff1-8d54-17b4fcb71612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables (for API key)\n",
    "load_dotenv()\n",
    "\n",
    "# Set up OpenAI API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"Please set the OPENAI_API_KEY environment variable or add it to a .env file\")\n",
    "\n",
    "# Define the model to use\n",
    "MODEL_GPT = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fda062c-a2e2-4dcd-aa78-d3fa035c70de",
   "metadata": {},
   "source": [
    "## SETUP (LlamaIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3a365be-bd40-4d72-9592-8637799decae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index\n",
    "# %pip install llama-index-llms-openai\n",
    "# %pip install tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2396cd0-112d-483c-acd9-ed59863e9a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# llm = OpenAI(model=\"gpt-4o-mini\", api_key=\"sk-...\")\n",
    "llm = OpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e529423e-8e05-4529-ba16-f59973472115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a tool\n",
    "import os\n",
    "from tavily import AsyncTavilyClient\n",
    "\n",
    "tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "async def search_web(query: str) -> str:\n",
    "    \"\"\"Useful for using the web to answer questions.\"\"\"\n",
    "    client = AsyncTavilyClient(api_key=tavily_api_key)\n",
    "    return str(await client.search(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3efc17d3-b0bd-46f7-9456-0c50642c30f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import AgentWorkflow\n",
    "\n",
    "workflow = AgentWorkflow.from_tools_or_functions(\n",
    "    [search_web],\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are a helpful assistant that can search the web for information.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260b2561-2787-4e25-8e19-28c096bfcebe",
   "metadata": {},
   "source": [
    "## Running the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c76e73a8-13e0-4d21-b6bd-f673e4fe9083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current weather in San Francisco is as follows:\n",
      "\n",
      "- **Temperature**: 13.9°C (57°F)\n",
      "- **Condition**: Partly cloudy\n",
      "- **Wind**: 10.5 mph (16.9 kph) from the SSW\n",
      "- **Humidity**: 87%\n",
      "- **Visibility**: 16 km (9 miles)\n",
      "- **Feels Like**: 12.6°C (54.6°F)\n",
      "\n",
      "For more details, you can check the full report [here](https://www.weatherapi.com/).\n"
     ]
    }
   ],
   "source": [
    "# response = await workflow.run(user_msg=\"What is the weather in San Francisco?\")\n",
    "response = await workflow.run(user_msg=\"What is the weather in Prague?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a776e2-3dec-44be-ba18-c232fe8d8c81",
   "metadata": {},
   "source": [
    "## Maintaining State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83e3a085-d8c2-4921-b099-b3d44bdf6238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you, Logan! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.workflow import Context\n",
    "\n",
    "ctx = Context(workflow)\n",
    "\n",
    "response = await workflow.run(\n",
    "    user_msg=\"My name is Logan, nice to meet you!\", ctx=ctx\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c3c1e10-830c-463c-87fd-c1e484b2ab5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Logan.\n"
     ]
    }
   ],
   "source": [
    "response = await workflow.run(user_msg=\"What is my name?\", ctx=ctx)\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faba2ceb-17bc-4ba6-84b2-a8747de30eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import JsonPickleSerializer, JsonSerializer\n",
    "\n",
    "ctx_dict = ctx.to_dict(serializer=JsonSerializer())\n",
    "\n",
    "restored_ctx = Context.from_dict(\n",
    "    workflow, ctx_dict, serializer=JsonSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b78fab29-e284-4f25-b108-12da450f6830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['globals', 'streaming_queue', 'queues', 'stepwise', 'events_buffer', 'in_progress', 'accepted_events', 'broker_log', 'waiter_id', 'is_running'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(ctx_dict))\n",
    "print(len(ctx_dict))\n",
    "ctx_dict.keys()\n",
    "# ctx_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9b9c103-00b3-4869-96fa-9e712e0b1bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I remember your name is Logan.\n"
     ]
    }
   ],
   "source": [
    "response = await workflow.run(\n",
    "    user_msg=\"Do you still remember my name?\", ctx=restored_ctx\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffab6c63-7e0d-448d-9025-21b5216dcb2e",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fd2e932-4d5c-41bd-9019-6e0192a58e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current weather in Saskatoon is as follows:\n",
      "\n",
      "- **Temperature**: -1.8°C (28.8°F)\n",
      "- **Condition**: Overcast\n",
      "- **Wind**: 9.8 mph (15.8 kph) from the ESE\n",
      "- **Humidity**: 69%\n",
      "- **Feels Like**: -6.8°C (19.8°F)\n",
      "- **Visibility**: 24 km\n",
      "\n",
      "For more details, you can check the full weather report [here](https://www.weatherapi.com/)."
     ]
    }
   ],
   "source": [
    "from llama_index.core.agent.workflow import (\n",
    "    AgentInput,\n",
    "    AgentOutput,\n",
    "    ToolCall,\n",
    "    ToolCallResult,\n",
    "    AgentStream,\n",
    ")\n",
    "\n",
    "handler = workflow.run(user_msg=\"What is the weather in Saskatoon?\")\n",
    "\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, AgentStream):\n",
    "        print(event.delta, end=\"\", flush=True)\n",
    "        # print(event.response)  # the current full response\n",
    "        # print(event.raw)  # the raw llm api response\n",
    "        # print(event.current_agent_name)  # the current agent name\n",
    "    # elif isinstance(event, AgentInput):\n",
    "    #    print(event.input)  # the current input messages\n",
    "    #    print(event.current_agent_name)  # the current agent name\n",
    "    # elif isinstance(event, AgentOutput):\n",
    "    #    print(event.response)  # the current full response\n",
    "    #    print(event.tool_calls)  # the selected tool calls, if any\n",
    "    #    print(event.raw)  # the raw llm api response\n",
    "    # elif isinstance(event, ToolCallResult):\n",
    "    #    print(event.tool_name)  # the tool name\n",
    "    #    print(event.tool_kwargs)  # the tool kwargs\n",
    "    #    print(event.tool_output)  # the tool output\n",
    "    # elif isinstance(event, ToolCall):\n",
    "    #     print(event.tool_name)  # the tool name\n",
    "    #     print(event.tool_kwargs)  # the tool kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe75dd9-5ee2-45ac-9dee-609fe8a5c1de",
   "metadata": {},
   "source": [
    "## Tools and State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb148fa0-9991-4688-9bf1-460566a547b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Context\n",
    "\n",
    "async def set_name(ctx: Context, name: str) -> str:\n",
    "    state = await ctx.get(\"state\")\n",
    "    state[\"name\"] = name\n",
    "    await ctx.set(\"state\", state)\n",
    "    return f\"Name set to {name}\"\n",
    "\n",
    "workflow = AgentWorkflow.from_tools_or_functions(\n",
    "    [set_name],\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are a helpful assistant that can set a name.\",\n",
    "    initial_state={\"name\": \"unset\"},\n",
    ")\n",
    "\n",
    "ctx = Context(workflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f15d715-d5c0-47b7-be87-bb730b813c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name has been set to Logan.\n",
      "Logan\n"
     ]
    }
   ],
   "source": [
    "response = await workflow.run(user_msg=\"My name is Logan\", ctx=ctx)\n",
    "print(str(response))\n",
    "\n",
    "state = await ctx.get(\"state\")\n",
    "print(state[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a7cab5-8336-4257-b5d6-67bebf78d509",
   "metadata": {},
   "source": [
    "## Human in the Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcbb429f-fe8e-4c0e-a856-a87a75848849",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Context,\n",
    "    InputRequiredEvent,\n",
    "    HumanResponseEvent,\n",
    ")\n",
    "\n",
    "async def dangerous_task(ctx: Context) -> str:\n",
    "    \"\"\"A dangerous task that requires human confirmation.\"\"\"\n",
    "    ctx.write_event_to_stream(\n",
    "        InputRequiredEvent(\n",
    "            prefix=\"Are you sure you want to proceed?\",\n",
    "            user_name=\"Logan\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    response = await ctx.wait_for_event(\n",
    "        HumanResponseEvent, requirements={\"user_name\": \"Logan\"}\n",
    "    )\n",
    "    if response.response == \"yes\":\n",
    "        return \"Dangerous task completed successfully.\"\n",
    "    else:\n",
    "        return \"Dangerous task aborted.\"\n",
    "\n",
    "workflow = AgentWorkflow.from_tools_or_functions(\n",
    "    [dangerous_task],\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are a helpful assistant that can perform dangerous tasks.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "928afd5a-3902-4874-8c13-4fa217f2e63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Are you sure you want to proceed? yes\n"
     ]
    }
   ],
   "source": [
    "handler = workflow.run(user_msg=\"I want to proceed with the dangerous task.\")\n",
    "\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, InputRequiredEvent):\n",
    "        response = input(event.prefix).strip().lower()\n",
    "        handler.ctx.send_event(\n",
    "            HumanResponseEvent(\n",
    "                response=response,\n",
    "                user_name=event.user_name,\n",
    "            )\n",
    "        )\n",
    "\n",
    "# response = await handler\n",
    "# print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "908b598d-bdb2-4be0-a213-7424d872eea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dangerous task has been completed successfully. If you need anything else, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "response = await handler\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "319b9808-7452-4115-ac0b-8b934412d252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Are you sure you want to proceed? yes\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.workflow import JsonSerializer\n",
    "\n",
    "handler = workflow.run(user_msg=\"I want to proceed with the dangerous task.\")\n",
    "\n",
    "input_ev = None\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, InputRequiredEvent):\n",
    "        input_ev = event\n",
    "        break\n",
    "\n",
    "# save the context somewhere for later\n",
    "ctx_dict = handler.ctx.to_dict(serializer=JsonSerializer())\n",
    "\n",
    "# get the response from the user\n",
    "response_str = input(input_ev.prefix).strip().lower()\n",
    "\n",
    "# restore the workflow\n",
    "restored_ctx = Context.from_dict(\n",
    "    workflow, ctx_dict, serializer=JsonSerializer()\n",
    ")\n",
    "\n",
    "handler = workflow.run(ctx=restored_ctx)\n",
    "handler.ctx.send_event(\n",
    "    HumanResponseEvent(\n",
    "        response=response_str,\n",
    "        user_name=input_ev.user_name,\n",
    "    )\n",
    ")\n",
    "\n",
    "# response = await handler\n",
    "# print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "753edaf5-9323-4dbe-b47b-cbe96a0ae443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dangerous task has been completed successfully. If you need anything else, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "response = await handler\n",
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
