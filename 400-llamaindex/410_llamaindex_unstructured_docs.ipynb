{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accbde67-9c7e-41b0-9329-8052e2ef365b",
   "metadata": {},
   "source": [
    "# LlamaIndex: RAG over Unstructured Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb42466-306d-49b1-b814-6674b60451e4",
   "metadata": {},
   "source": [
    "- Question-Answering (RAG)\n",
    "  - https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/\n",
    "- RAG over Unstructured Documents<br>\n",
    "  - **[Semantic search](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/q_and_a/#semantic-search)**\n",
    "  - **[Summarization](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/q_and_a/#summarization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ef772f-e33d-4f9d-abce-467a5bff80bc",
   "metadata": {},
   "source": [
    "LlamaIndex can pull in unstructured text, PDFs, Notion and Slack documents and more and index the data within them.<br>\n",
    "The simplest queries involve either semantic search or summarization.\n",
    "- **Semantic search**: A query about specific information in a document that matches the query terms and/or semantic intent.<br>This is typically executed with simple vector retrieval (top-k).\n",
    "- **Summarization**: condensing a large amount of data into a short summary relevant to your current question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1039ae-8255-4ec0-8c3a-2757a8f0ae95",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ce6eafa-f6ef-4ff1-8d54-17b4fcb71612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables (for API key)\n",
    "load_dotenv()\n",
    "\n",
    "# Set up OpenAI API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"Please set the OPENAI_API_KEY environment variable or add it to a .env file\")\n",
    "\n",
    "# Define the model to use\n",
    "MODEL_GPT = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eea907-5984-4784-abc3-69177eb6c9f5",
   "metadata": {},
   "source": [
    "# Q&A patterns (Semantic Search, Summarization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6202ae24-ef48-4851-a3c7-f4ace1325338",
   "metadata": {},
   "source": [
    "## Semantic Search\n",
    "The most basic example usage of LlamaIndex is through semantic search.<br>\n",
    "We provide a simple in-memory vector store for you to get started, but you can also choose to use any one of our **vector store integrations**.<br>\n",
    "\n",
    "**Using Vector Stores**\n",
    "- https://docs.llamaindex.ai/en/stable/community/integrations/vector_stores/\n",
    "\n",
    "**Vector Stores**\n",
    "- https://docs.llamaindex.ai/en/stable/module_guides/storing/vector_stores/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12a151d4-a8ed-45e7-88a0-062aa036e2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "# documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_files=[\"./data/paul_graham_essay.txt\"]\n",
    ")\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fd351f7-2bdc-49f5-a2e1-b5fb56ef582f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8912c86c-70ff-4949-85f2-34b516389ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2066644-1bf1-4f8f-a16b-ee93b3f37344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author focused on writing and programming before college.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b6180d-6822-415b-9c1b-4b59cd0d9a13",
   "metadata": {},
   "source": [
    "### Setting up LlamaIndex with GPT-4o-mini\n",
    "**OpenAI Pricing**<br>\n",
    "- https://platform.openai.com/docs/pricing\n",
    "\n",
    "**OpenAI models**<br>\n",
    "- https://platform.openai.com/docs/models\n",
    "\n",
    "**OpenAI Embedding models**<br>\n",
    "- https://platform.openai.com/docs/guides/embeddings#embedding-models\n",
    "\n",
    "Model (Pages per dollar) assuming ~800 tokens per page\n",
    "- text-embedding-3-small (62,500)\n",
    "- text-embedding-3-large (9,615)\n",
    "- text-embedding-ada-002 (12,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bfaaedd-f275-48d9-84e9-ca4f00b68880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import Settings, VectorStoreIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# Configure the embedding model\n",
    "embed_model = OpenAIEmbedding(\n",
    "    model=\"text-embedding-3-small\",  # Using text-embedding-3-small for embeddings\n",
    ")\n",
    "\n",
    "# Configure GPT-4o-mini as the LLM\n",
    "llm = OpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# Update the global settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de037220-8665-4acd-aa29-66098a905d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now when you create your VectorStoreIndex, it will use these models\n",
    "# documents = load_your_documents()\n",
    "# index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ffed03c-e9d9-4bf5-96e4-9a5223ae714b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autor vyrůstal v prostředí, kde se věnoval malování a umění. Navštěvoval Accademii, kde se učil malovat, a jeho zájem o malbu ho vedl k tomu, že začal malovat still life v noci ve svém pokoji. Maloval na zbytky plátna, protože si nemohl dovolit nové. Během svého studia se také setkal s modelkou, která žila poblíž a pracovala jako modelka a malířka kopií starých obrazů. Jeho zkušenosti z malování mu umožnily lépe vnímat detaily a krásu každodenního života, což ovlivnilo jeho umělecký styl a přístup k malbě.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What did the author do growing up, answer in Czech language in 100 words?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0e73bb-c1bc-4011-b0d9-ccccee3f04ce",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e5ed43-ebf4-407f-b3ae-178fd512c8e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
