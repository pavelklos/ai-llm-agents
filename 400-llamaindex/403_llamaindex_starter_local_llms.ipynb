{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accbde67-9c7e-41b0-9329-8052e2ef365b",
   "metadata": {},
   "source": [
    "# LlamaIndex: Starter Tutorial (Using Local LLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb42466-306d-49b1-b814-6674b60451e4",
   "metadata": {},
   "source": [
    "- Starter Tutorial (Using Local LLMs)<br>\n",
    "  https://docs.llamaindex.ai/en/stable/getting_started/starter_example_local/\n",
    "- Embedding model **BAAI/bge-base-en-v1.5** and Model **meta-llama/Llama-3.1-8B** on (Hugging Face) \n",
    "  - https://huggingface.co/BAAI/bge-base-en-v1.5\n",
    "  - https://huggingface.co/meta-llama/Llama-3.1-8B\n",
    "- **LlamaHub Integrations**\n",
    "  - https://llamahub.ai/\n",
    "- **Ollama Jupyter Notebook Integration**\n",
    "  - https://www.restack.io/p/ollama-answer-jupyter-notebook-cat-ai\n",
    "- **Ollama Python Library**\n",
    "  - https://github.com/ollama/ollama-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1039ae-8255-4ec0-8c3a-2757a8f0ae95",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ce6eafa-f6ef-4ff1-8d54-17b4fcb71612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables (for API key)\n",
    "load_dotenv()\n",
    "\n",
    "# Set up OpenAI API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"Please set the OPENAI_API_KEY environment variable or add it to a .env file\")\n",
    "\n",
    "# Define the model to use\n",
    "MODEL_GPT = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c78e0f-f0bc-4b58-8d2f-10d81236f089",
   "metadata": {},
   "source": [
    "## Ollama\n",
    "- https://ollama.com/\n",
    "- https://github.com/ollama/ollama\n",
    "\n",
    "**Llama 3.1 8B (4.9 GB)**<br>\n",
    "```python\n",
    "ollama run llama3.1\n",
    "ollama run llama3.1:8b\n",
    "```\n",
    "```python\n",
    "ollama pull llama3.1\n",
    "ollama list\n",
    "ollama ps\n",
    "ollama rm llama3.1\n",
    "```\n",
    "```python\n",
    "ollama show llama3.1:8b\n",
    "```\n",
    "```\n",
    "  Model\n",
    "    architecture        llama\n",
    "    parameters          8.0B\n",
    "    context length      131072\n",
    "    embedding length    4096\n",
    "    quantization        Q4_K_M\n",
    "\n",
    "  Parameters\n",
    "    stop    \"<|start_header_id|>\"\n",
    "    stop    \"<|end_header_id|>\"\n",
    "    stop    \"<|eot_id|>\"\n",
    "\n",
    "  License\n",
    "    LLAMA 3.1 COMMUNITY LICENSE AGREEMENT\n",
    "    Llama 3.1 Version Release Date: July 23, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc007b2-ccd0-4335-a84e-6f48c5bddf07",
   "metadata": {},
   "source": [
    "## SETUP (LlamaIndex, Ollama, HuggingFace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fc3a62b-7631-4218-be2a-450c096ae9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install llama-index-core llama-index-readers-file llama-index-llms-ollama llama-index-embeddings-huggingface\n",
    "# pip install llama-index-llms-ollama llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e4f690c-7295-43f7-8746-22c9b45472ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e34161b-43b5-42bf-9f36-44428c23390b",
   "metadata": {},
   "source": [
    "## Basic Agent Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b57731b5-be6d-4398-80d5-312f7495c1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListResponse(models=[Model(model='llama3.1:latest', modified_at=datetime.datetime(2025, 3, 27, 11, 3, 8, 745338, tzinfo=TzInfo(+01:00)), digest='46e0c10c039e019119339687c3c1757cc81b9da49709a3b3924863ba87ca666e', size=4920753328, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='8.0B', quantization_level='Q4_K_M')), Model(model='llama3.1:8b', modified_at=datetime.datetime(2025, 3, 27, 9, 47, 58, 235191, tzinfo=TzInfo(+01:00)), digest='46e0c10c039e019119339687c3c1757cc81b9da49709a3b3924863ba87ca666e', size=4920753328, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='8.0B', quantization_level='Q4_K_M'))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ollama Jupyter Notebook Integration\n",
    "# - https://www.restack.io/p/ollama-answer-jupyter-notebook-cat-ai\n",
    "# Ollama Python Library\n",
    "# - https://github.com/ollama/ollama-python\n",
    "import ollama\n",
    "\n",
    "ollama.pull('llama3.1')\n",
    "ollama.list()\n",
    "# ollama.delete('llama3.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d071a730-6321-4990-8d56-71656c6f8128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Define a simple calculator tool\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Useful for multiplying two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "# Create an agent workflow with our calculator tool\n",
    "# agent = FunctionAgent(\n",
    "#     tools=[multiply],\n",
    "#     llm=Ollama(model=\"llama3.1\", request_timeout=360.0),\n",
    "#     system_prompt=\"You are a helpful assistant that can multiply two numbers.\",\n",
    "# )\n",
    "agent = FunctionAgent(\n",
    "    name=\"Agent\",\n",
    "    description=\"Useful for multiplying two numbers\",\n",
    "    tools=[multiply],\n",
    "    llm=Ollama(model=\"llama3.1\", request_timeout=360.0),\n",
    "    system_prompt=\"You are a helpful assistant that can multiply two numbers.\",\n",
    ")\n",
    "\n",
    "# async def main():\n",
    "#     # Run the agent\n",
    "#     response = await agent.run(\"What is 1234 * 4567?\")\n",
    "#     print(str(response))\n",
    "\n",
    "# # Run the agent\n",
    "# if __name__ == \"__main__\":\n",
    "#     asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fe6247b-4c80-4b49-9386-a3c317dd411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the agent\n",
    "response = await agent.run(\"What is 1234 * 4567?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1ca8ec5-ae9a-4010-8583-2d0f8554e334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of multiplying 1234 and 4567 is 5,635,678.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2ea42c-c48f-4d0a-95de-903badfef489",
   "metadata": {},
   "source": [
    "## Adding Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d49c55c8-2501-4a5d-9ee0-4b5c58354ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core.workflow import Context\n",
    "\n",
    "# # create context\n",
    "# ctx = Context(agent)\n",
    "\n",
    "# # run agent with context\n",
    "# response = await agent.run(\"My name is Logan\", ctx=ctx)\n",
    "# response = await agent.run(\"What is my name?\", ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c4f6271-6181-44cd-aae9-4ac9672c6f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERROR: AttributeError: 'FunctionAgent' object has no attribute '_get_steps'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97654e02-2698-404b-94c4-8ee2846cc646",
   "metadata": {},
   "source": [
    "## Adding Chat History by (AgentWorkflow Basic Introduction)\n",
    "- https://docs.llamaindex.ai/en/stable/examples/agent/agent_workflow_basic/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cbb1eb7f-5acb-41e4-be34-ab76fc974885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index\n",
    "# %pip install tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5725774-3f23-4e2e-b872-d6a34befacb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.llms.openai import OpenAI\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "llm=Ollama(model=\"llama3.1\", request_timeout=360.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20e02bdc-fdb0-4f58-8541-aef6d74d9645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a tool\n",
    "import os\n",
    "from tavily import AsyncTavilyClient\n",
    "\n",
    "tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "async def search_web(query: str) -> str:\n",
    "    \"\"\"Useful for using the web to answer questions.\"\"\"\n",
    "    client = AsyncTavilyClient(api_key=tavily_api_key)\n",
    "    return str(await client.search(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "74baabf3-d220-4f20-9c02-363bb11a362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an AgentWorkflow that uses the tool\n",
    "from llama_index.core.agent.workflow import AgentWorkflow\n",
    "\n",
    "workflow = AgentWorkflow.from_tools_or_functions(\n",
    "    [search_web],\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are a helpful assistant that can search the web for information.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd4ef37-b1ba-4509-8037-891f31cb4d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the Agent\n",
    "# response = await workflow.run(user_msg=\"What is the weather in San Francisco?\")\n",
    "response = await workflow.run(user_msg=\"What is the weather in Prague?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d36656-9dd6-46b6-94d9-5aedef92c0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maintaining State\n",
    "from llama_index.core.workflow import Context\n",
    "\n",
    "ctx = Context(workflow)\n",
    "\n",
    "response = await workflow.run(\n",
    "    user_msg=\"My name is Logan, nice to meet you!\", ctx=ctx\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c350b30d-9a05-4291-870f-93e569d3b4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await workflow.run(user_msg=\"What is my name?\", ctx=ctx)\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dab6f33-13a9-4ffb-8bab-c5871134ef52",
   "metadata": {},
   "source": [
    "## Adding RAG Capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d534c3af-830e-4d14-b044-bcdec60008ce",
   "metadata": {},
   "source": [
    "Example data\n",
    "```\n",
    "mkdir data\n",
    "wget https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt -O data/paul_graham_essay.txt\n",
    "```\n",
    "-  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "47e8a1df-f8f9-45a2-9cfb-794f7dbf4e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57258deb2c4a47dd9462a491d293b055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9b2a67fa8f424580a1b466f7538c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873ed6506adb45b687ea78fdf4efd426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/94.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3531295da828433997183bfa9e72da3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40fe54cb534a4c429e396a69e8e364d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b32e65d458d4ca5901e0424ab951f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4042751d7a6848db86584b4147e5ccc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e32f53bd6104e4eabb747d1f4278dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c5b73879bc470aa570372e4e80ca85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d648a073d4f14be788445bd14a03dd49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed3e4a100cc64fd3bb1ecea7e6016cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.core.agent.workflow import AgentWorkflow\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import asyncio\n",
    "import os\n",
    "\n",
    "# Settings control global defaults\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "Settings.llm = Ollama(model=\"llama3.1\", request_timeout=360.0)\n",
    "\n",
    "# Create a RAG tool using LlamaIndex\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    # we can optionally override the embed_model here\n",
    "    # embed_model=Settings.embed_model,\n",
    ")\n",
    "query_engine = index.as_query_engine(\n",
    "    # we can optionally override the llm here\n",
    "    # llm=Settings.llm,\n",
    ")\n",
    "\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Useful for multiplying two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "async def search_documents(query: str) -> str:\n",
    "    \"\"\"Useful for answering natural language questions about an personal essay written by Paul Graham.\"\"\"\n",
    "    response = await query_engine.aquery(query)\n",
    "    return str(response)\n",
    "\n",
    "# Create an enhanced workflow with both tools\n",
    "agent = AgentWorkflow.from_tools_or_functions(\n",
    "    [multiply, search_documents],\n",
    "    llm=Settings.llm,\n",
    "    system_prompt=\"\"\"You are a helpful assistant that can perform calculations\n",
    "    and search through documents to answer questions.\"\"\",\n",
    ")\n",
    "\n",
    "# # Now we can ask questions about the documents or do calculations\n",
    "# async def main():\n",
    "#     response = await agent.run(\n",
    "#         \"What did the author do in college? Also, what's 7 * 8?\"\n",
    "#     )\n",
    "#     print(response)\n",
    "\n",
    "# # Run the agent\n",
    "# if __name__ == \"__main__\":\n",
    "#     asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657dccc5-e08d-47e7-a760-529b73b97b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await agent.run(\"What did the author do in college? Also, what's 7 * 8?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8cb4f1-71e7-4ac4-8e42-8675fe0c34ee",
   "metadata": {},
   "source": [
    "## Storing the RAG Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2b8061e8-e078-4177-bae7-97691740979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the index\n",
    "# index.storage_context.persist(\"storage\")\n",
    "\n",
    "# embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "index.storage_context.persist(\"storage_bge\")  # embed_model = BGE\n",
    "\n",
    "# Later, load the index\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "# storage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"storage_bge\")\n",
    "index = load_index_from_storage(\n",
    "    storage_context,\n",
    "    # we can optionally override the embed_model here\n",
    "    # it's important to use the same embed_model as the one used to build the index\n",
    "    # embed_model=Settings.embed_model,\n",
    ")\n",
    "query_engine = index.as_query_engine(\n",
    "    # we can optionally override the llm here\n",
    "    # llm=Settings.llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f9946f0b-4110-463f-b602-618d7bebb9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListResponse(models=[Model(model='llama3.1:latest', modified_at=datetime.datetime(2025, 3, 27, 11, 3, 8, 745338, tzinfo=TzInfo(+01:00)), digest='46e0c10c039e019119339687c3c1757cc81b9da49709a3b3924863ba87ca666e', size=4920753328, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='8.0B', quantization_level='Q4_K_M')), Model(model='llama3.1:8b', modified_at=datetime.datetime(2025, 3, 27, 9, 47, 58, 235191, tzinfo=TzInfo(+01:00)), digest='46e0c10c039e019119339687c3c1757cc81b9da49709a3b3924863ba87ca666e', size=4920753328, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='8.0B', quantization_level='Q4_K_M'))])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1729bad0-27f9-4468-8b0e-96601738651c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StatusResponse(status='success')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.delete('llama3.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1b788848-a202-45ba-a794-b0f90e61d2c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListResponse(models=[Model(model='llama3.1:8b', modified_at=datetime.datetime(2025, 3, 27, 9, 47, 58, 235191, tzinfo=TzInfo(+01:00)), digest='46e0c10c039e019119339687c3c1757cc81b9da49709a3b3924863ba87ca666e', size=4920753328, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='8.0B', quantization_level='Q4_K_M'))])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.list()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
