{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accbde67-9c7e-41b0-9329-8052e2ef365b",
   "metadata": {},
   "source": [
    "# AG2: GroupChat (Resuming a Group Chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb42466-306d-49b1-b814-6674b60451e4",
   "metadata": {},
   "source": [
    "Resuming a Group Chat\n",
    "- https://docs.ag2.ai/docs/user-guide/advanced-concepts/groupchat/resuming-group-chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1039ae-8255-4ec0-8c3a-2757a8f0ae95",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ce6eafa-f6ef-4ff1-8d54-17b4fcb71612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables (for API key)\n",
    "load_dotenv()\n",
    "\n",
    "# Set up OpenAI API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"Please set the OPENAI_API_KEY environment variable or add it to a .env file\")\n",
    "\n",
    "# Define the model to use\n",
    "MODEL_GPT = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87daed2-c11e-4086-b1e8-0f9742e42010",
   "metadata": {},
   "source": [
    "## Resuming a Group Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3843d54-2c90-4c69-9894-4faa257bdf5f",
   "metadata": {},
   "source": [
    "```python\n",
    "# Save chat messages for resuming later on using the chat history\n",
    "messages_json = mygroupchatmanager.messages_to_string(previous_chat_result.chat_history)\n",
    "\n",
    "# Alternatively you can use the GroupChat's messages property\n",
    "messages_json = mygroupchatmanager.messages_to_string(mygroupchatmanager.groupchat.messages)\n",
    "\n",
    "# example of the JSON string\n",
    "[{\"content\": \"Find the latest paper about gpt-4 on arxiv and find its potential applications in software.\", \"role\": \"user\", \"name\": \"Admin\"}, {\"content\": \"Plan:\\n1. **Engineer**: Search for the latest paper on GPT-4 on arXiv.\\n2. **Scientist**: Read the paper and summarize the key findings and potential applications of GPT-4.\\n3. **Engineer**: Identify potential software applications where GPT-4 can be utilized based on the scientist's summary.\\n4. **Scientist**: Provide insights on the feasibility and impact of implementing GPT-4 in the identified software applications.\\n5. **Engineer**: Develop a prototype or proof of concept to demonstrate how GPT-4 can be integrated into the selected software application.\\n6. **Scientist**: Evaluate the prototype, provide feedback, and suggest any improvements or modifications.\\n7. **Engineer**: Make necessary revisions based on the scientist's feedback and finalize the integration of GPT-4 into the software application.\\n8. **Admin**: Review the final software application with GPT-4 integration and approve for further development or implementation.\\n\\nFeedback from admin and critic is needed for further refinement of the plan.\", \"role\": \"user\", \"name\": \"Planner\"}, {\"content\": \"Agree\", \"role\": \"user\", \"name\": \"Admin\"}, {\"content\": \"Great! Let's proceed with the plan outlined earlier. I will start by searching for the latest paper on GPT-4 on arXiv. Once I find the paper, the scientist will summarize the key findings and potential applications of GPT-4. We will then proceed with the rest of the steps as outlined. I will keep you updated on our progress.\", \"role\": \"user\", \"name\": \"Planner\"}]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ba4660-9301-42cf-ba00-4521aee92fcf",
   "metadata": {},
   "source": [
    "## Example of resuming a GroupChat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23eb602d-26c7-421f-a603-0900cfbcf495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import autogen\n",
    "\n",
    "# Put your api key in the environment variable OPENAI_API_KEY\n",
    "config_list = [\n",
    "    {\n",
    "        \"api_type\": \"openai\",\n",
    "        # \"model\": \"gpt-4o\",\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"api_key\": os.environ[\"OPENAI_API_KEY\"],\n",
    "    }\n",
    "]\n",
    "\n",
    "gpt4_config = {\n",
    "    \"cache_seed\": 42,  # change the cache_seed for different trials\n",
    "    \"temperature\": 0,\n",
    "    \"config_list\": config_list,\n",
    "    \"timeout\": 120,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1b5b436-4377-4410-819b-40c358fc746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Agents, GroupChat, and GroupChatManager in line with the original group chat\n",
    "\n",
    "planner = autogen.AssistantAgent(\n",
    "    name=\"Planner\",\n",
    "    system_message=\"\"\"Planner. Suggest a plan. Revise the plan based on feedback from admin and critic, until admin approval.\n",
    "The plan may involve an engineer who can write code and a scientist who doesn't write code.\n",
    "Explain the plan first. Be clear which step is performed by an engineer, and which step is performed by a scientist.\n",
    "\"\"\",\n",
    "    llm_config=gpt4_config,\n",
    ")\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"Admin\",\n",
    "    system_message=\"A human admin. Interact with the planner to discuss the plan. Plan execution needs to be approved by this admin.\",\n",
    "    code_execution_config=False,\n",
    ")\n",
    "\n",
    "engineer = autogen.AssistantAgent(\n",
    "    name=\"Engineer\",\n",
    "    llm_config=gpt4_config,\n",
    "    system_message=\"\"\"Engineer. You follow an approved plan. You write python/shell code to solve tasks. Wrap the code in a code block that specifies the script type. The user can't modify your code. So do not suggest incomplete code which requires others to modify. Don't use a code block if it's not intended to be executed by the executor.\n",
    "Don't include multiple code blocks in one response. Do not ask others to copy and paste the result. Check the execution result returned by the executor.\n",
    "If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.\n",
    "\"\"\",\n",
    ")\n",
    "scientist = autogen.AssistantAgent(\n",
    "    name=\"Scientist\",\n",
    "    llm_config=gpt4_config,\n",
    "    system_message=\"\"\"Scientist. You follow an approved plan. You are able to categorize papers after seeing their abstracts printed. You don't write code.\"\"\",\n",
    ")\n",
    "\n",
    "executor = autogen.UserProxyAgent(\n",
    "    name=\"Executor\",\n",
    "    system_message=\"Executor. Execute the code written by the engineer and report the result.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config={\n",
    "        \"last_n_messages\": 3,\n",
    "        \"work_dir\": \"paper\",\n",
    "        \"use_docker\": False,\n",
    "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
    ")\n",
    "\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, engineer, scientist, planner, executor],\n",
    "    messages=[],\n",
    "    max_round=10,\n",
    ")\n",
    "\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=gpt4_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65c29b0e-7478-4f40-8167-fc4ef3343579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Messages in a JSON string\n",
    "previous_state = r\"\"\"[{\"content\": \"Find the latest paper about gpt-4 on arxiv and find its potential applications in software.\", \"role\": \"user\", \"name\": \"Admin\"}, {\"content\": \"Plan:\\n1. **Engineer**: Search for the latest paper on GPT-4 on arXiv.\\n2. **Scientist**: Read the paper and summarize the key findings and potential applications of GPT-4.\\n3. **Engineer**: Identify potential software applications where GPT-4 can be utilized based on the scientist's summary.\\n4. **Scientist**: Provide insights on the feasibility and impact of implementing GPT-4 in the identified software applications.\\n5. **Engineer**: Develop a prototype or proof of concept to demonstrate how GPT-4 can be integrated into the selected software application.\\n6. **Scientist**: Evaluate the prototype, provide feedback, and suggest any improvements or modifications.\\n7. **Engineer**: Make necessary revisions based on the scientist's feedback and finalize the integration of GPT-4 into the software application.\\n8. **Admin**: Review the final software application with GPT-4 integration and approve for further development or implementation.\\n\\nFeedback from admin and critic is needed for further refinement of the plan.\", \"role\": \"user\", \"name\": \"Planner\"}, {\"content\": \"Agree\", \"role\": \"user\", \"name\": \"Admin\"}, {\"content\": \"Great! Let's proceed with the plan outlined earlier. I will start by searching for the latest paper on GPT-4 on arXiv. Once I find the paper, the scientist will summarize the key findings and potential applications of GPT-4. We will then proceed with the rest of the steps as outlined. I will keep you updated on our progress.\", \"role\": \"user\", \"name\": \"Planner\"}]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccd3715e-df1c-41c7-9d89-48d8dbbbdc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared group chat with 4 messages, the last speaker is \u001b[33mPlanner\u001b[0m\n",
      "\u001b[33mPlanner\u001b[0m (to chat_manager):\n",
      "\n",
      "Great! Let's proceed with the plan outlined earlier. I will start by searching for the latest paper on GPT-4 on arXiv. Once I find the paper, the scientist will summarize the key findings and potential applications of GPT-4. We will then proceed with the rest of the steps as outlined. I will keep you updated on our progress.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Engineer\n",
      "\u001b[0m\n",
      "\u001b[33mEngineer\u001b[0m (to chat_manager):\n",
      "\n",
      "As an engineer, I will now search for the latest paper on GPT-4 on arXiv. I will write a Python script to automate this search process. \n",
      "\n",
      "Here is the code to find the latest paper on GPT-4 from arXiv:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "\n",
      "def fetch_latest_gpt4_paper():\n",
      "    url = \"http://export.arxiv.org/api/query\"\n",
      "    query = \"gpt-4\"\n",
      "    params = {\n",
      "        'search_query': query,\n",
      "        'start': 0,\n",
      "        'max_results': 1,\n",
      "        'sortBy': 'lastUpdatedDate',\n",
      "        'sortOrder': 'descending'\n",
      "    }\n",
      "    \n",
      "    response = requests.get(url, params=params)\n",
      "    if response.status_code == 200:\n",
      "        return response.text\n",
      "    else:\n",
      "        return None\n",
      "\n",
      "latest_paper = fetch_latest_gpt4_paper()\n",
      "print(latest_paper)\n",
      "```\n",
      "\n",
      "This script queries the arXiv API for the latest paper related to GPT-4 and prints the result. Please execute this code to retrieve the latest paper.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Executor\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mExecutor\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<feed xmlns=\"http://www.w3.org/2005/Atom\">\n",
      "  <link href=\"http://arxiv.org/api/query?search_query%3Dgpt-4%26id_list%3D%26start%3D0%26max_results%3D1\" rel=\"self\" type=\"application/atom+xml\"/>\n",
      "  <title type=\"html\">ArXiv Query: search_query=gpt-4&amp;id_list=&amp;start=0&amp;max_results=1</title>\n",
      "  <id>http://arxiv.org/api/4rvPWuk3KvGtWGN6ZdoL1yPNinY</id>\n",
      "  <updated>2025-03-18T00:00:00-04:00</updated>\n",
      "  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">3044</opensearch:totalResults>\n",
      "  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n",
      "  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:itemsPerPage>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2411.14299v4</id>\n",
      "    <updated>2025-03-17T15:22:28Z</updated>\n",
      "    <published>2024-11-21T16:50:11Z</published>\n",
      "    <title>Masala-CHAI: A Large-Scale SPICE Netlist Dataset for Analog Circuits by\n",
      "  Harnessing AI</title>\n",
      "    <summary>  Masala-CHAI is the first fully automated framework leveraging large language\n",
      "models (LLMs) to generate Simulation Programs with Integrated Circuit Emphasis\n",
      "(SPICE) netlists. It addresses a long-standing challenge in automating netlist\n",
      "generation for analog circuits within circuit design automation. Automating\n",
      "this workflow could accelerate the creation of finetuned LLMs for analog\n",
      "circuit design and verification. We identify key challenges in this automation\n",
      "and evaluate the multi-modal capabilities of state-of-the-art LLMs,\n",
      "particularly GPT-4, to address these issues. We propose a three-step workflow\n",
      "to overcome current limitations: labeling analog circuits, prompt tuning, and\n",
      "netlist verification. This approach aims to create an end-to-end SPICE netlist\n",
      "generator from circuit schematic images, tackling the long-standing hurdle of\n",
      "accurate netlist generation. Our framework demonstrates significant performance\n",
      "improvements, tested on approximately 2,100 schematics of varying complexity.\n",
      "We open-source this solution for community-driven development.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Jitendra Bhandari</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vineet Bhat</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yuheng He</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hamed Rahmani</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Siddharth Garg</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ramesh Karri</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2411.14299v4\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2411.14299v4\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "</feed>\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Scientist\n",
      "\u001b[0m\n",
      "\u001b[33mScientist\u001b[0m (to chat_manager):\n",
      "\n",
      "The latest paper related to GPT-4 on arXiv is titled **\"Masala-CHAI: A Large-Scale SPICE Netlist Dataset for Analog Circuits by Harnessing AI.\"** Here are the key details and findings from the paper:\n",
      "\n",
      "### Key Findings:\n",
      "- **Objective**: The paper presents Masala-CHAI, an automated framework that utilizes large language models (LLMs), specifically GPT-4, to generate Simulation Programs with Integrated Circuit Emphasis (SPICE) netlists.\n",
      "- **Challenge Addressed**: It tackles the long-standing issue of automating netlist generation for analog circuits within circuit design automation.\n",
      "- **Workflow**: The proposed solution involves a three-step workflow:\n",
      "  1. **Labeling Analog Circuits**: Identifying and labeling components in circuit schematics.\n",
      "  2. **Prompt Tuning**: Optimizing prompts to improve the performance of the LLM in generating accurate netlists.\n",
      "  3. **Netlist Verification**: Ensuring the generated netlists are accurate and functional.\n",
      "- **Performance**: The framework has shown significant performance improvements when tested on approximately 2,100 schematics of varying complexity.\n",
      "- **Open Source**: The authors have made the solution open-source for community-driven development.\n",
      "\n",
      "### Potential Applications in Software:\n",
      "1. **Circuit Design Automation Tools**: Integrating GPT-4 into software that assists engineers in designing and verifying analog circuits, potentially reducing the time and effort required for netlist generation.\n",
      "2. **Educational Software**: Developing tools that help students and professionals learn about circuit design by providing automated feedback and suggestions based on their schematic designs.\n",
      "3. **Simulation Software**: Enhancing existing simulation tools with automated netlist generation capabilities, allowing for quicker iterations in the design process.\n",
      "4. **AI-Assisted Design Tools**: Creating AI-driven design assistants that can suggest improvements or alternatives based on user-defined parameters and existing designs.\n",
      "\n",
      "Next, we can evaluate the feasibility and impact of implementing GPT-4 in these identified software applications. Would you like to proceed with that?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Scientist\n",
      "\u001b[0m\n",
      "\u001b[33mScientist\u001b[0m (to chat_manager):\n",
      "\n",
      "Yes, let's proceed with evaluating the feasibility and impact of implementing GPT-4 in the identified software applications. \n",
      "\n",
      "### Feasibility Evaluation:\n",
      "1. **Circuit Design Automation Tools**:\n",
      "   - **Feasibility**: High. The integration of GPT-4 can streamline the netlist generation process, making it more efficient. Existing tools can be enhanced with minimal disruption.\n",
      "   - **Impact**: Significant. This could lead to faster design cycles and reduced errors in netlist generation.\n",
      "\n",
      "2. **Educational Software**:\n",
      "   - **Feasibility**: Moderate. While integrating GPT-4 is possible, it requires careful design to ensure that the AI provides accurate and educational feedback.\n",
      "   - **Impact**: High. This could greatly enhance learning experiences for students and professionals, making complex concepts more accessible.\n",
      "\n",
      "3. **Simulation Software**:\n",
      "   - **Feasibility**: High. Many simulation tools already have frameworks for integrating AI, making it easier to add GPT-4 capabilities.\n",
      "   - **Impact**: Significant. Automating netlist generation can lead to quicker simulations and more iterations in the design process.\n",
      "\n",
      "4. **AI-Assisted Design Tools**:\n",
      "   - **Feasibility**: Moderate to High. Developing an AI assistant requires a robust understanding of user needs and design parameters, but GPT-4's capabilities can be leveraged effectively.\n",
      "   - **Impact**: High. This could revolutionize how engineers approach circuit design, providing real-time suggestions and improvements.\n",
      "\n",
      "### Summary of Impact:\n",
      "Implementing GPT-4 in these applications can lead to increased efficiency, reduced errors, and enhanced learning opportunities in the field of circuit design and automation. The potential for open-source collaboration also means that improvements can be rapidly developed and shared within the community.\n",
      "\n",
      "Would you like to move forward with developing a prototype or proof of concept for one of these applications? If so, which application should we focus on?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Engineer\n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "InternalServerError",
     "evalue": "Error code: 500 - {'error': {'message': 'The model produced invalid content. Consider modifying your prompt if you are seeing this error persistently.', 'type': 'model_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInternalServerError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m last_agent, last_message = manager.resume(messages=previous_state)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Resume the chat using the last agent and message\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# result = last_agent.initiate_chat(recipient=manager, message=last_message, clear_history=False)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m chat_result = \u001b[43mlast_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlast_message\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclear_history\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Output the final chat history showing the original 4 messages and resumed messages\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, message \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(groupchat.messages):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\projects\\ai-llm-agents\\.venv\\Lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:1488\u001b[39m, in \u001b[36mConversableAgent.initiate_chat\u001b[39m\u001b[34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[39m\n\u001b[32m   1486\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1487\u001b[39m         msg2send = \u001b[38;5;28mself\u001b[39m.generate_init_message(message, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1488\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg2send\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m=\u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1489\u001b[39m summary = \u001b[38;5;28mself\u001b[39m._summarize_chat(\n\u001b[32m   1490\u001b[39m     summary_method,\n\u001b[32m   1491\u001b[39m     summary_args,\n\u001b[32m   1492\u001b[39m     recipient,\n\u001b[32m   1493\u001b[39m     cache=cache,\n\u001b[32m   1494\u001b[39m )\n\u001b[32m   1495\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mself\u001b[39m, recipient]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\projects\\ai-llm-agents\\.venv\\Lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:1177\u001b[39m, in \u001b[36mConversableAgent.send\u001b[39m\u001b[34m(self, message, recipient, request_reply, silent)\u001b[39m\n\u001b[32m   1175\u001b[39m valid = \u001b[38;5;28mself\u001b[39m._append_oai_message(message, \u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m, recipient, is_sending=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1176\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[32m-> \u001b[39m\u001b[32m1177\u001b[39m     \u001b[43mrecipient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1179\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1180\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMessage can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1181\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\projects\\ai-llm-agents\\.venv\\Lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:1285\u001b[39m, in \u001b[36mConversableAgent.receive\u001b[39m\u001b[34m(self, message, sender, request_reply, silent)\u001b[39m\n\u001b[32m   1283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1284\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m reply = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m=\u001b[49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1287\u001b[39m     \u001b[38;5;28mself\u001b[39m.send(reply, sender, silent=silent)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\projects\\ai-llm-agents\\.venv\\Lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:2431\u001b[39m, in \u001b[36mConversableAgent.generate_reply\u001b[39m\u001b[34m(self, messages, sender, **kwargs)\u001b[39m\n\u001b[32m   2429\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   2430\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._match_trigger(reply_func_tuple[\u001b[33m\"\u001b[39m\u001b[33mtrigger\u001b[39m\u001b[33m\"\u001b[39m], sender):\n\u001b[32m-> \u001b[39m\u001b[32m2431\u001b[39m     final, reply = \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m=\u001b[49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfig\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2432\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[32m   2433\u001b[39m         log_event(\n\u001b[32m   2434\u001b[39m             \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2435\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreply_func_executed\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2439\u001b[39m             reply=reply,\n\u001b[32m   2440\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\projects\\ai-llm-agents\\.venv\\Lib\\site-packages\\autogen\\agentchat\\groupchat.py:1188\u001b[39m, in \u001b[36mGroupChatManager.run_chat\u001b[39m\u001b[34m(self, messages, sender, config)\u001b[39m\n\u001b[32m   1186\u001b[39m         iostream.send(GroupChatRunChatMessage(speaker=speaker, silent=silent))\n\u001b[32m   1187\u001b[39m     \u001b[38;5;66;03m# let the speaker speak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1188\u001b[39m     reply = \u001b[43mspeaker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43msender\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1190\u001b[39m     \u001b[38;5;66;03m# let the admin agent speak if interrupted\u001b[39;00m\n\u001b[32m   1191\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m groupchat.admin_name \u001b[38;5;129;01min\u001b[39;00m groupchat.agent_names:\n\u001b[32m   1192\u001b[39m         \u001b[38;5;66;03m# admin agent is one of the participants\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\projects\\ai-llm-agents\\.venv\\Lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:2431\u001b[39m, in \u001b[36mConversableAgent.generate_reply\u001b[39m\u001b[34m(self, messages, sender, **kwargs)\u001b[39m\n\u001b[32m   2429\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   2430\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._match_trigger(reply_func_tuple[\u001b[33m\"\u001b[39m\u001b[33mtrigger\u001b[39m\u001b[33m\"\u001b[39m], sender):\n\u001b[32m-> \u001b[39m\u001b[32m2431\u001b[39m     final, reply = \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m=\u001b[49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfig\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2432\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[32m   2433\u001b[39m         log_event(\n\u001b[32m   2434\u001b[39m             \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2435\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreply_func_executed\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2439\u001b[39m             reply=reply,\n\u001b[32m   2440\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\projects\\ai-llm-agents\\.venv\\Lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:1810\u001b[39m, in \u001b[36mConversableAgent.generate_oai_reply\u001b[39m\u001b[34m(self, messages, sender, config)\u001b[39m\n\u001b[32m   1808\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m messages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1809\u001b[39m     messages = \u001b[38;5;28mself\u001b[39m._oai_messages[sender]\n\u001b[32m-> \u001b[39m\u001b[32m1810\u001b[39m extracted_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_oai_reply_from_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1811\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_oai_system_message\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient_cache\u001b[49m\n\u001b[32m   1812\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1813\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, extracted_response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\projects\\ai-llm-agents\\.venv\\Lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:1829\u001b[39m, in \u001b[36mConversableAgent._generate_oai_reply_from_client\u001b[39m\u001b[34m(self, llm_client, messages, cache)\u001b[39m\n\u001b[32m   1826\u001b[39m         all_messages.append(message)\n\u001b[32m   1828\u001b[39m \u001b[38;5;66;03m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1829\u001b[39m response = \u001b[43mllm_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1830\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1832\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1833\u001b[39m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1834\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1835\u001b[39m extracted_response = llm_client.extract_text_or_completion_object(response)[\u001b[32m0\u001b[39m]\n\u001b[32m   1837\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\projects\\ai-llm-agents\\.venv\\Lib\\site-packages\\autogen\\oai\\client.py:1093\u001b[39m, in \u001b[36mOpenAIWrapper.create\u001b[39m\u001b[34m(self, **config)\u001b[39m\n\u001b[32m   1091\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1092\u001b[39m     request_ts = get_current_ts()\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1094\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1095\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m openai_result.is_successful:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\projects\\ai-llm-agents\\.venv\\Lib\\site-packages\\autogen\\oai\\client.py:600\u001b[39m, in \u001b[36mOpenAIClient.create\u001b[39m\u001b[34m(self, params)\u001b[39m\n\u001b[32m    598\u001b[39m     \u001b[38;5;28mself\u001b[39m._process_reasoning_model_params(params)\n\u001b[32m    599\u001b[39m params[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m response = \u001b[43mcreate_or_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;66;03m# remove the system_message from the response and add it in the prompt at the start.\u001b[39;00m\n\u001b[32m    602\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_o1:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\projects\\ai-llm-agents\\.venv\\Lib\\site-packages\\autogen\\oai\\client.py:406\u001b[39m, in \u001b[36mOpenAIClient._handle_openai_bad_request_error.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    405\u001b[39m     kwargs = OpenAIClient._patch_messages_for_deepseek_reasoner(**kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m openai.BadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    408\u001b[39m     response_json = e.response.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\projects\\ai-llm-agents\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py:279\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\projects\\ai-llm-agents\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:914\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    871\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    872\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    873\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    911\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    912\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    913\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\projects\\ai-llm-agents\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\projects\\ai-llm-agents\\.venv\\Lib\\site-packages\\openai\\_base_client.py:919\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    917\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\projects\\ai-llm-agents\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1008\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m   1006\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_retry(err.response):\n\u001b[32m   1007\u001b[39m     err.response.close()\n\u001b[32m-> \u001b[39m\u001b[32m1008\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43merr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[32m   1019\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err.response.is_closed:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\projects\\ai-llm-agents\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1057\u001b[39m, in \u001b[36mSyncAPIClient._retry_request\u001b[39m\u001b[34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[39m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[32m   1055\u001b[39m time.sleep(timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1057\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1059\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1060\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1061\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1062\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1063\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\projects\\ai-llm-agents\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1008\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m   1006\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_retry(err.response):\n\u001b[32m   1007\u001b[39m     err.response.close()\n\u001b[32m-> \u001b[39m\u001b[32m1008\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43merr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[32m   1019\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err.response.is_closed:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\projects\\ai-llm-agents\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1057\u001b[39m, in \u001b[36mSyncAPIClient._retry_request\u001b[39m\u001b[34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[39m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[32m   1055\u001b[39m time.sleep(timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1057\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1059\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1060\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1061\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1062\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1063\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\projects\\ai-llm-agents\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1023\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m   1020\u001b[39m         err.response.read()\n\u001b[32m   1022\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_response(\n\u001b[32m   1026\u001b[39m     cast_to=cast_to,\n\u001b[32m   1027\u001b[39m     options=options,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1031\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1032\u001b[39m )\n",
      "\u001b[31mInternalServerError\u001b[39m: Error code: 500 - {'error': {'message': 'The model produced invalid content. Consider modifying your prompt if you are seeing this error persistently.', 'type': 'model_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "# Prepare the group chat for resuming\n",
    "last_agent, last_message = manager.resume(messages=previous_state)\n",
    "\n",
    "# Resume the chat using the last agent and message\n",
    "# result = last_agent.initiate_chat(recipient=manager, message=last_message, clear_history=False)\n",
    "chat_result = last_agent.initiate_chat(recipient=manager, message=last_message, clear_history=False)\n",
    "\n",
    "# Output the final chat history showing the original 4 messages and resumed messages\n",
    "for i, message in enumerate(groupchat.messages):\n",
    "    print(\n",
    "        f\"#{i + 1}, {message['name']}: {message['content'][:80]}\".replace(\"\\n\", \" \"),\n",
    "        f\"{'...' if len(message['content']) > 80 else ''}\".replace(\"\\n\", \" \"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1089417d-d156-4946-b7a5-6bf75a75fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat_result.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b269676-8374-443e-9578-fb14ec4e4333",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(chat_result.chat_history))\n",
    "# print(chat_result.chat_history[0])\n",
    "# print(chat_result.chat_history[-1])\n",
    "print(chat_result.chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f52a6d-8b8a-4298-832b-0671e766d055",
   "metadata": {},
   "source": [
    "## Example of resuming a terminated GroupChat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "367f827e-8eee-4b14-b917-d7fc6dce5bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous state with TERMINATE in the last message\n",
    "previous_state = r\"\"\"[{\"content\": \"Let's get this meeting started. We'll have a set order of speakers. First the Product_Manager will create 3 new product ideas. Then the Chief_Marketing_Officer will speak and talk about the idea they have chosen to move forward with. Then the Digital_Marketer will create 3 marketing strategies for that idea. We MUST stick to the speaking order which is Product_Manager first, Chief_Marketing_Officer second, Digital_Marketer third, and finally the Chief_Marketing_Officer will speak and end the meeting.\", \"role\": \"user\", \"name\": \"Chairperson\"}, {\"content\": \"Sounds like a plan! Let's get started.  As the Product_Manager, I'd like to present three new product ideas for our luxury car models:  1. 'EcoLux': A hybrid electric-gasoline vehicle that combines the best of both worlds, offering exceptional fuel efficiency and reduced carbon footprint. 2. 'AeroSport': A high-performance sports car with advanced aerodynamics, designed to deliver unparalleled speed and agility on the track or on the open road. 3. 'SmartDrive': An intelligent driver-assistance system that uses AI-powered sensors and cameras to anticipate and respond to potential hazards, ensuring a safer driving experience.  Now it's your turn, Chief_Marketing_Officer! Which of these ideas do you think has the most potential?\", \"role\": \"user\", \"name\": \"Chief_Marketing_Officer\"}, {\"content\": \"Thank you for presenting those innovative product ideas, Product_Manager.  After careful consideration, I believe 'EcoLux' has the most potential. With the growing concern about climate change and environmental sustainability, a hybrid electric-gasoline vehicle that offers exceptional fuel efficiency and reduced carbon footprint could be a game-changer in the luxury car market. Additionally, it aligns with our company's commitment to innovation and responsibility.  Now it's your turn, Digital_Marketer! Can you come up with three marketing strategies for 'EcoLux'?\", \"role\": \"user\", \"name\": \"Product_Manager\"}, {\"content\": \"Thank you, Chief_Marketing_Officer!  For 'EcoLux', I propose the following three marketing strategies:  1. 'Green Revolution' Campaign: Highlighting the eco-friendly features of EcoLux through a series of social media ads and influencer partnerships. We can partner with eco-conscious influencers to showcase how EcoLux is not only a luxury car but also an environmentally responsible choice. 2. 'Fuel for Thought' Content Series: Creating a content series that explores the intersection of technology, sustainability, and luxury. This could include blog posts, videos, and podcasts that delve into the innovative features of EcoLux and its impact on the environment. 3. 'EcoLux Experience' Event Marketing: Hosting exclusive events and test drives for potential customers to experience the performance and eco-friendliness of EcoLux firsthand. These events can be held at upscale locations and feature interactive exhibits, product demonstrations, and networking opportunities.  These strategies will help position EcoLux as a leader in the luxury electric-vehicle market while appealing to environmentally conscious consumers who value innovation and sustainability. TERMINATE\", \"role\": \"user\", \"name\": \"Digital_Marketer\"}]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2151b0a9-c534-4cf2-b20c-1f4b574d3e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"Chairperson\",\n",
    "    system_message=\"The chairperson for the meeting.\",\n",
    "    # code_execution_config={},\n",
    "    code_execution_config=False,\n",
    "    human_input_mode=\"TERMINATE\",\n",
    ")\n",
    "\n",
    "cmo = autogen.AssistantAgent(\n",
    "    name=\"Chief_Marketing_Officer\",\n",
    "    # system_message is used in the select speaker message\n",
    "    description=\"The head of the marketing department working with the product manager and digital marketer to execute a strong marketing campaign for your car company.\",\n",
    "    # description is used to prompt the LLM as this agent\n",
    "    system_message=\"You, Jane titled Chief_Marketing_Officer, or CMO, are the head of the marketing department and your objective is to guide your team to producing and marketing unique ideas for your luxury car models. Don't include your name at the start of your response or speak for any other team member, let them come up with their own ideas and strategies, speak just for yourself as the head of marketing. When yourself, the Product_Manager, and the Digital_Marketer have spoken and the meeting is finished, say TERMINATE to conclude the meeting.\",\n",
    "    is_termination_msg=lambda x: \"TERMINATE\" in x.get(\"content\"),\n",
    "    llm_config=gpt4_config,\n",
    ")\n",
    "\n",
    "pm = autogen.AssistantAgent(\n",
    "    name=\"Product_Manager\",\n",
    "    # system_message is used in the select speaker message\n",
    "    description=\"Product head for the luxury model cars product line in the car company. Always coming up with new product enhancements for the cars.\",\n",
    "    # description is used to prompt the LLM as this agent\n",
    "    system_message=\"You, Alice titled Product_Manager, are always coming up with new product enhancements for the luxury car models you look after. Review the meeting so far and respond with the answer to your current task.  Don't include your name at the start of your response and don't speak for anyone else, leave the Chairperson to pick the next person to speak.\",\n",
    "    is_termination_msg=lambda x: \"TERMINATE\" in x.get(\"content\"),\n",
    "    llm_config=gpt4_config,\n",
    ")\n",
    "\n",
    "digital = autogen.AssistantAgent(\n",
    "    name=\"Digital_Marketer\",\n",
    "    # system_message is used in the select speaker message\n",
    "    description=\"A seasoned digital marketer who comes up with online marketing strategies that highlight the key features of the luxury car models.\",\n",
    "    # description is used to prompt the LLM as this agent\n",
    "    system_message=\"You, Elizabeth titled Digital_Marketer, are a senior online marketing specialist who comes up with marketing strategies that highlight the key features of the luxury car models.  Review the meeting so far and respond with the answer to your current task.   Don't include your name at the start of your response and don't speak for anyone else, leave the Chairperson to pick the next person to speak.\",\n",
    "    is_termination_msg=lambda x: \"TERMINATE\" in x.get(\"content\"),\n",
    "    llm_config=gpt4_config,\n",
    ")\n",
    "\n",
    "# Customised message, this is always the first message in the context\n",
    "my_speaker_select_msg = \"\"\"You are a chairperson for a marketing meeting for this car manufacturer where multiple members of the team will speak.\n",
    "The job roles of the team at the meeting, and their responsibilities, are:\n",
    "{roles}\"\"\"\n",
    "\n",
    "# Customised prompt, this is always the last message in the context\n",
    "my_speaker_select_prompt = \"\"\"Read the above conversation.\n",
    "Then select ONLY THE NAME of the next job role from {agentlist} to speak. Do not explain why.\"\"\"\n",
    "\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, cmo, pm, digital],\n",
    "    messages=[],\n",
    "    max_round=10,\n",
    "    select_speaker_message_template=my_speaker_select_msg,\n",
    "    select_speaker_prompt_template=my_speaker_select_prompt,\n",
    "    max_retries_for_selecting_speaker=2,  # New\n",
    "    select_speaker_auto_verbose=False,  # New\n",
    ")\n",
    "\n",
    "manager = autogen.GroupChatManager(\n",
    "    groupchat=groupchat,\n",
    "    llm_config=gpt4_config,\n",
    "    is_termination_msg=lambda x: \"TERMINATE\" in x.get(\"content\", \"\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80b35b7a-2d6a-4706-9ae1-06b89be21bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Last message meets termination criteria and this may terminate the chat.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared group chat with 4 messages, the last speaker is \u001b[33mDigital_Marketer\u001b[0m\n",
      "\u001b[33mDigital_Marketer\u001b[0m (to chat_manager):\n",
      "\n",
      "Thank you, Chief_Marketing_Officer!  For 'EcoLux', I propose the following three marketing strategies:  1. 'Green Revolution' Campaign: Highlighting the eco-friendly features of EcoLux through a series of social media ads and influencer partnerships. We can partner with eco-conscious influencers to showcase how EcoLux is not only a luxury car but also an environmentally responsible choice. 2. 'Fuel for Thought' Content Series: Creating a content series that explores the intersection of technology, sustainability, and luxury. This could include blog posts, videos, and podcasts that delve into the innovative features of EcoLux and its impact on the environment. 3. 'EcoLux Experience' Event Marketing: Hosting exclusive events and test drives for potential customers to experience the performance and eco-friendliness of EcoLux firsthand. These events can be held at upscale locations and feature interactive exhibits, product demonstrations, and networking opportunities.  These strategies will help position EcoLux as a leader in the luxury electric-vehicle market while appealing to environmentally conscious consumers who value innovation and sustainability. TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Prepare the group chat for resuming WITHOUT removing the TERMINATE message\n",
    "last_agent, last_message = manager.resume(messages=previous_state)\n",
    "\n",
    "# Resume and it will terminate immediately\n",
    "result = last_agent.initiate_chat(recipient=manager, message=last_message, clear_history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b173dc41-b678-4e06-8c84-b4ab6fd727a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared group chat with 4 messages, the last speaker is \u001b[33mDigital_Marketer\u001b[0m\n",
      "\u001b[33mDigital_Marketer\u001b[0m (to chat_manager):\n",
      "\n",
      "Thank you, Chief_Marketing_Officer!  For 'EcoLux', I propose the following three marketing strategies:  1. 'Green Revolution' Campaign: Highlighting the eco-friendly features of EcoLux through a series of social media ads and influencer partnerships. We can partner with eco-conscious influencers to showcase how EcoLux is not only a luxury car but also an environmentally responsible choice. 2. 'Fuel for Thought' Content Series: Creating a content series that explores the intersection of technology, sustainability, and luxury. This could include blog posts, videos, and podcasts that delve into the innovative features of EcoLux and its impact on the environment. 3. 'EcoLux Experience' Event Marketing: Hosting exclusive events and test drives for potential customers to experience the performance and eco-friendliness of EcoLux firsthand. These events can be held at upscale locations and feature interactive exhibits, product demonstrations, and networking opportunities.  These strategies will help position EcoLux as a leader in the luxury electric-vehicle market while appealing to environmentally conscious consumers who value innovation and sustainability. \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Chairperson\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mChairperson\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Chief_Marketing_Officer\n",
      "\u001b[0m\n",
      "\u001b[33mChief_Marketing_Officer\u001b[0m (to chat_manager):\n",
      "\n",
      "Thank you, Digital_Marketer, for those insightful strategies. I believe we have a solid plan to move forward with 'EcoLux.' The combination of innovative product features and targeted marketing strategies will position us well in the luxury car market. Let's ensure we execute these ideas effectively to maximize our impact. \n",
      "\n",
      "If there are no further comments or questions, I will conclude the meeting. TERMINATE.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "#1, Chairperson: Let's get this meeting started. We'll have a set order of speakers. First the Pr ...\n",
      "#2, Chief_Marketing_Officer: Sounds like a plan! Let's get started.  As the Product_Manager, I'd like to pres ...\n",
      "#3, Product_Manager: Thank you for presenting those innovative product ideas, Product_Manager.  After ...\n",
      "#4, Digital_Marketer: Thank you, Chief_Marketing_Officer!  For 'EcoLux', I propose the following three ...\n",
      "#5, Chairperson:  \n",
      "#6, Chief_Marketing_Officer: Thank you, Digital_Marketer, for those insightful strategies. I believe we have  ...\n"
     ]
    }
   ],
   "source": [
    "# Prepare the group chat for resuming WITH removal of TERMINATE message\n",
    "last_agent, last_message = manager.resume(messages=previous_state, remove_termination_string=\"TERMINATE\")\n",
    "\n",
    "# Resume the chat using the last agent and message\n",
    "# result = last_agent.initiate_chat(recipient=manager, message=last_message, clear_history=False)\n",
    "chat_result = last_agent.initiate_chat(recipient=manager, message=last_message, clear_history=False)\n",
    "\n",
    "# Output the final chat history showing the original 4 messages and the resumed message\n",
    "for i, message in enumerate(groupchat.messages):\n",
    "    print(\n",
    "        f\"#{i + 1}, {message['name']}: {message['content'][:80]}\".replace(\"\\n\", \" \"),\n",
    "        f\"{'...' if len(message['content']) > 80 else ''}\".replace(\"\\n\", \" \"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f13748f-9b3d-4780-a1d6-3fc7c2a36bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you, Digital_Marketer, for those insightful strategies. I believe we have a solid plan to move forward with 'EcoLux.' The combination of innovative product features and targeted marketing strategies will position us well in the luxury car market. Let's ensure we execute these ideas effectively to maximize our impact. \n",
      "\n",
      "If there are no further comments or questions, I will conclude the meeting. .\n"
     ]
    }
   ],
   "source": [
    "print(chat_result.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6280b2b0-74ea-4f79-a4b9-3d03954cafb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[{'content': \"Let's get this meeting started. We'll have a set order of speakers. First the Product_Manager will create 3 new product ideas. Then the Chief_Marketing_Officer will speak and talk about the idea they have chosen to move forward with. Then the Digital_Marketer will create 3 marketing strategies for that idea. We MUST stick to the speaking order which is Product_Manager first, Chief_Marketing_Officer second, Digital_Marketer third, and finally the Chief_Marketing_Officer will speak and end the meeting.\", 'name': 'Chairperson', 'role': 'user'}, {'content': \"Sounds like a plan! Let's get started.  As the Product_Manager, I'd like to present three new product ideas for our luxury car models:  1. 'EcoLux': A hybrid electric-gasoline vehicle that combines the best of both worlds, offering exceptional fuel efficiency and reduced carbon footprint. 2. 'AeroSport': A high-performance sports car with advanced aerodynamics, designed to deliver unparalleled speed and agility on the track or on the open road. 3. 'SmartDrive': An intelligent driver-assistance system that uses AI-powered sensors and cameras to anticipate and respond to potential hazards, ensuring a safer driving experience.  Now it's your turn, Chief_Marketing_Officer! Which of these ideas do you think has the most potential?\", 'name': 'Chief_Marketing_Officer', 'role': 'user'}, {'content': \"Thank you for presenting those innovative product ideas, Product_Manager.  After careful consideration, I believe 'EcoLux' has the most potential. With the growing concern about climate change and environmental sustainability, a hybrid electric-gasoline vehicle that offers exceptional fuel efficiency and reduced carbon footprint could be a game-changer in the luxury car market. Additionally, it aligns with our company's commitment to innovation and responsibility.  Now it's your turn, Digital_Marketer! Can you come up with three marketing strategies for 'EcoLux'?\", 'name': 'Product_Manager', 'role': 'user'}, {'content': \"Thank you, Chief_Marketing_Officer!  For 'EcoLux', I propose the following three marketing strategies:  1. 'Green Revolution' Campaign: Highlighting the eco-friendly features of EcoLux through a series of social media ads and influencer partnerships. We can partner with eco-conscious influencers to showcase how EcoLux is not only a luxury car but also an environmentally responsible choice. 2. 'Fuel for Thought' Content Series: Creating a content series that explores the intersection of technology, sustainability, and luxury. This could include blog posts, videos, and podcasts that delve into the innovative features of EcoLux and its impact on the environment. 3. 'EcoLux Experience' Event Marketing: Hosting exclusive events and test drives for potential customers to experience the performance and eco-friendliness of EcoLux firsthand. These events can be held at upscale locations and feature interactive exhibits, product demonstrations, and networking opportunities.  These strategies will help position EcoLux as a leader in the luxury electric-vehicle market while appealing to environmentally conscious consumers who value innovation and sustainability. \", 'name': 'Digital_Marketer', 'role': 'assistant'}, {'content': '', 'name': 'Chairperson', 'role': 'user'}, {'content': \"Thank you, Digital_Marketer, for those insightful strategies. I believe we have a solid plan to move forward with 'EcoLux.' The combination of innovative product features and targeted marketing strategies will position us well in the luxury car market. Let's ensure we execute these ideas effectively to maximize our impact. \\n\\nIf there are no further comments or questions, I will conclude the meeting. TERMINATE.\", 'name': 'Chief_Marketing_Officer', 'role': 'user'}]\n"
     ]
    }
   ],
   "source": [
    "print(len(chat_result.chat_history))\n",
    "# print(chat_result.chat_history[0])\n",
    "# print(chat_result.chat_history[-1])\n",
    "print(chat_result.chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902362b1-3782-4950-a4e6-315dcec8a829",
   "metadata": {},
   "source": [
    "## Example of resuming a terminated GroupChat with a new message and agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcae60cc-e376-4f9f-96ba-16754d221972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Last message meets termination criteria and this may terminate the chat.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared group chat with 6 messages, the last speaker is \u001b[33mChief_Marketing_Officer\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Prepare the group chat for resuming using the previous messages. We don't need to remove the TERMINATE string as we aren't using the last message for resuming.\n",
    "last_agent, last_message = manager.resume(messages=groupchat.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "613c24fe-d48f-461f-9c0e-fafbebd3c3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mchat_manager\u001b[0m (to Chief_Marketing_Officer):\n",
      "\n",
      "Team, let's now think of a name for the next vehicle that embodies that idea. Chief_Marketing_Officer and Product_manager can you both suggest one and then we can conclude.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mChief_Marketing_Officer\u001b[0m (to chat_manager):\n",
      "\n",
      "For the next vehicle that embodies the 'EcoLux' idea, I suggest the name \"EcoElite.\" This name conveys a sense of luxury while emphasizing the eco-friendly aspect of the vehicle. It positions the car as a premium choice for environmentally conscious consumers who do not want to compromise on luxury or performance.\n",
      "\n",
      "Now, I would like to hear the Product_Manager's suggestion.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Product_Manager\n",
      "\u001b[0m\n",
      "\u001b[33mProduct_Manager\u001b[0m (to chat_manager):\n",
      "\n",
      "For the 'EcoLux' concept, I propose the name \"EcoElite\" as well. This name effectively communicates the luxury and sophistication associated with our brand while highlighting the vehicle's eco-friendly features. It resonates with our target audience who seek a premium experience without compromising their commitment to sustainability. Additionally, \"EcoElite\" suggests exclusivity and high performance, making it an attractive option for discerning customers. \n",
      "\n",
      "I believe this name will help position our new model as a leader in the luxury hybrid market.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Digital_Marketer\n",
      "\u001b[0m\n",
      "\u001b[33mDigital_Marketer\u001b[0m (to chat_manager):\n",
      "\n",
      "The name \"EcoElite\" is indeed a strong choice that encapsulates both luxury and sustainability. To further enhance our marketing efforts for the EcoElite, I recommend the following strategies:\n",
      "\n",
      "1. **Luxury Lifestyle Partnerships**: Collaborate with high-end brands in fashion, travel, and wellness to create exclusive packages that include the EcoElite. This could involve co-branded experiences that emphasize a luxurious, eco-conscious lifestyle, appealing to our target demographic.\n",
      "\n",
      "2. **Virtual Reality Showroom**: Develop an immersive virtual reality experience that allows potential customers to explore the EcoElite's features and performance in a simulated environment. This innovative approach can attract tech-savvy consumers and provide a unique way to experience the vehicle's luxury and eco-friendly attributes.\n",
      "\n",
      "3. **Sustainability Ambassadors Program**: Launch a program that engages customers who are passionate about sustainability to become ambassadors for the EcoElite. They can share their experiences and promote the vehicle through their networks, creating authentic word-of-mouth marketing that resonates with like-minded individuals.\n",
      "\n",
      "These strategies will not only highlight the EcoElite's luxury and eco-friendly features but also create a community around the brand, fostering loyalty and engagement among our target audience.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Chairperson\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mChairperson\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Chairperson\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mChairperson\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Chief_Marketing_Officer\n",
      "\u001b[0m\n",
      "\u001b[33mChief_Marketing_Officer\u001b[0m (to chat_manager):\n",
      "\n",
      "Thank you all for your valuable contributions today. We have made significant progress in defining our new luxury vehicle, \"EcoElite,\" and the marketing strategies to support it. I appreciate everyone's input and collaboration. \n",
      "\n",
      "TERMINATE.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "#1, Chairperson: Let's get this meeting started. We'll have a set order of speakers. First the Pr ...\n",
      "#2, Chief_Marketing_Officer: Sounds like a plan! Let's get started.  As the Product_Manager, I'd like to pres ...\n",
      "#3, Product_Manager: Thank you for presenting those innovative product ideas, Product_Manager.  After ...\n",
      "#4, Digital_Marketer: Thank you, Chief_Marketing_Officer!  For 'EcoLux', I propose the following three ...\n",
      "#5, Chairperson:  \n",
      "#6, Chief_Marketing_Officer: For the next vehicle that embodies the 'EcoLux' idea, I suggest the name \"EcoEli ...\n",
      "#7, Product_Manager: For the 'EcoLux' concept, I propose the name \"EcoElite\" as well. This name effec ...\n",
      "#8, Digital_Marketer: The name \"EcoElite\" is indeed a strong choice that encapsulates both luxury and  ...\n",
      "#9, Chairperson:  \n",
      "#10, Chairperson:  \n",
      "#11, Chief_Marketing_Officer: Thank you all for your valuable contributions today. We have made significant pr ...\n"
     ]
    }
   ],
   "source": [
    "# Resume the chat using a different agent and message\n",
    "# result = manager.initiate_chat(\n",
    "chat_result = manager.initiate_chat(\n",
    "    recipient=cmo,\n",
    "    message=\"Team, let's now think of a name for the next vehicle that embodies that idea. Chief_Marketing_Officer and Product_manager can you both suggest one and then we can conclude.\",\n",
    "    clear_history=False,\n",
    ")\n",
    "\n",
    "# Output the final chat history showing the original 4 messages and the resumed message\n",
    "for i, message in enumerate(groupchat.messages):\n",
    "    print(\n",
    "        f\"#{i + 1}, {message['name']}: {message['content'][:80]}\".replace(\"\\n\", \" \"),\n",
    "        f\"{'...' if len(message['content']) > 80 else ''}\".replace(\"\\n\", \" \"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ccb3fc6-b538-4680-b8d9-39af62e92edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you all for your valuable contributions today. We have made significant progress in defining our new luxury vehicle, \"EcoElite,\" and the marketing strategies to support it. I appreciate everyone's input and collaboration. \n",
      "\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "print(chat_result.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0061cdf-bede-4178-bed6-9051fe9fa521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "[{'content': \"Let's get this meeting started. We'll have a set order of speakers. First the Product_Manager will create 3 new product ideas. Then the Chief_Marketing_Officer will speak and talk about the idea they have chosen to move forward with. Then the Digital_Marketer will create 3 marketing strategies for that idea. We MUST stick to the speaking order which is Product_Manager first, Chief_Marketing_Officer second, Digital_Marketer third, and finally the Chief_Marketing_Officer will speak and end the meeting.\", 'name': 'Chairperson', 'role': 'assistant'}, {'content': \"Sounds like a plan! Let's get started.  As the Product_Manager, I'd like to present three new product ideas for our luxury car models:  1. 'EcoLux': A hybrid electric-gasoline vehicle that combines the best of both worlds, offering exceptional fuel efficiency and reduced carbon footprint. 2. 'AeroSport': A high-performance sports car with advanced aerodynamics, designed to deliver unparalleled speed and agility on the track or on the open road. 3. 'SmartDrive': An intelligent driver-assistance system that uses AI-powered sensors and cameras to anticipate and respond to potential hazards, ensuring a safer driving experience.  Now it's your turn, Chief_Marketing_Officer! Which of these ideas do you think has the most potential?\", 'name': 'Chief_Marketing_Officer', 'role': 'user'}, {'content': \"Thank you for presenting those innovative product ideas, Product_Manager.  After careful consideration, I believe 'EcoLux' has the most potential. With the growing concern about climate change and environmental sustainability, a hybrid electric-gasoline vehicle that offers exceptional fuel efficiency and reduced carbon footprint could be a game-changer in the luxury car market. Additionally, it aligns with our company's commitment to innovation and responsibility.  Now it's your turn, Digital_Marketer! Can you come up with three marketing strategies for 'EcoLux'?\", 'name': 'Product_Manager', 'role': 'assistant'}, {'content': \"Thank you, Chief_Marketing_Officer!  For 'EcoLux', I propose the following three marketing strategies:  1. 'Green Revolution' Campaign: Highlighting the eco-friendly features of EcoLux through a series of social media ads and influencer partnerships. We can partner with eco-conscious influencers to showcase how EcoLux is not only a luxury car but also an environmentally responsible choice. 2. 'Fuel for Thought' Content Series: Creating a content series that explores the intersection of technology, sustainability, and luxury. This could include blog posts, videos, and podcasts that delve into the innovative features of EcoLux and its impact on the environment. 3. 'EcoLux Experience' Event Marketing: Hosting exclusive events and test drives for potential customers to experience the performance and eco-friendliness of EcoLux firsthand. These events can be held at upscale locations and feature interactive exhibits, product demonstrations, and networking opportunities.  These strategies will help position EcoLux as a leader in the luxury electric-vehicle market while appealing to environmentally conscious consumers who value innovation and sustainability. \", 'name': 'Digital_Marketer', 'role': 'assistant'}, {'content': '', 'name': 'Chairperson', 'role': 'assistant'}, {'content': \"Team, let's now think of a name for the next vehicle that embodies that idea. Chief_Marketing_Officer and Product_manager can you both suggest one and then we can conclude.\", 'role': 'assistant', 'name': 'chat_manager'}, {'content': 'For the next vehicle that embodies the \\'EcoLux\\' idea, I suggest the name \"EcoElite.\" This name conveys a sense of luxury while emphasizing the eco-friendly aspect of the vehicle. It positions the car as a premium choice for environmentally conscious consumers who do not want to compromise on luxury or performance.\\n\\nNow, I would like to hear the Product_Manager\\'s suggestion.', 'role': 'user', 'name': 'Chief_Marketing_Officer'}, {'content': 'For the \\'EcoLux\\' concept, I propose the name \"EcoElite\" as well. This name effectively communicates the luxury and sophistication associated with our brand while highlighting the vehicle\\'s eco-friendly features. It resonates with our target audience who seek a premium experience without compromising their commitment to sustainability. Additionally, \"EcoElite\" suggests exclusivity and high performance, making it an attractive option for discerning customers. \\n\\nI believe this name will help position our new model as a leader in the luxury hybrid market.', 'name': 'Product_Manager', 'role': 'assistant'}, {'content': 'The name \"EcoElite\" is indeed a strong choice that encapsulates both luxury and sustainability. To further enhance our marketing efforts for the EcoElite, I recommend the following strategies:\\n\\n1. **Luxury Lifestyle Partnerships**: Collaborate with high-end brands in fashion, travel, and wellness to create exclusive packages that include the EcoElite. This could involve co-branded experiences that emphasize a luxurious, eco-conscious lifestyle, appealing to our target demographic.\\n\\n2. **Virtual Reality Showroom**: Develop an immersive virtual reality experience that allows potential customers to explore the EcoElite\\'s features and performance in a simulated environment. This innovative approach can attract tech-savvy consumers and provide a unique way to experience the vehicle\\'s luxury and eco-friendly attributes.\\n\\n3. **Sustainability Ambassadors Program**: Launch a program that engages customers who are passionate about sustainability to become ambassadors for the EcoElite. They can share their experiences and promote the vehicle through their networks, creating authentic word-of-mouth marketing that resonates with like-minded individuals.\\n\\nThese strategies will not only highlight the EcoElite\\'s luxury and eco-friendly features but also create a community around the brand, fostering loyalty and engagement among our target audience.', 'name': 'Digital_Marketer', 'role': 'assistant'}, {'content': '', 'name': 'Chairperson', 'role': 'assistant'}, {'content': '', 'name': 'Chairperson', 'role': 'assistant'}, {'content': 'Thank you all for your valuable contributions today. We have made significant progress in defining our new luxury vehicle, \"EcoElite,\" and the marketing strategies to support it. I appreciate everyone\\'s input and collaboration. \\n\\nTERMINATE.', 'role': 'user', 'name': 'Chief_Marketing_Officer'}]\n"
     ]
    }
   ],
   "source": [
    "print(len(chat_result.chat_history))\n",
    "# print(chat_result.chat_history[0])\n",
    "# print(chat_result.chat_history[-1])\n",
    "print(chat_result.chat_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
